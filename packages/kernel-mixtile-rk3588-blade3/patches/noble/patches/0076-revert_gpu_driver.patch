From c690df0f3de29d2117f0450e93777ed3ee189adb Mon Sep 17 00:00:00 2001
From: szszqkjyxgs_17014 <focalcrest.com>
Date: Wed, 18 Jun 2025 11:12:12 +0800
Subject: [PATCH] revert gpu driver

---
 Documentation/ABI/testing/sysfs-device-mali   |    3 +-
 .../sysfs-device-mali-coresight-source        |    2 +-
 .../arm/arm,coresight-mali-source.yaml        |  163 ---
 .../devicetree/bindings/arm/mali-bifrost.txt  |   14 +-
 drivers/base/arm/Kconfig                      |    2 +-
 drivers/base/arm/Makefile                     |    2 -
 .../dma-buf-test-exporter.c                   |   18 +-
 .../memory_group_manager.c                    |   72 +-
 .../protected_memory_allocator.c              |    2 +-
 drivers/gpu/arm/bifrost/Kbuild                |   10 +-
 drivers/gpu/arm/bifrost/Kconfig               |   44 +-
 drivers/gpu/arm/bifrost/Makefile              |   20 +-
 drivers/gpu/arm/bifrost/arbiter/Kbuild        |    3 +-
 .../arm/bifrost/arbiter/mali_kbase_arbif.c    |  162 +--
 .../arm/bifrost/arbiter/mali_kbase_arbif.h    |    3 +-
 .../bifrost/arbiter/mali_kbase_arbiter_pm.c   |  214 ++-
 .../bifrost/arbiter/mali_kbase_arbiter_pm.h   |    4 +-
 drivers/gpu/arm/bifrost/backend/gpu/Kbuild    |    9 +-
 .../gpu/mali_kbase_backend_config.h}          |   22 +-
 .../gpu/mali_kbase_cache_policy_backend.h     |    5 +-
 .../gpu/mali_kbase_clk_rate_trace_mgr.c       |   16 +-
 .../gpu/mali_kbase_debug_job_fault_backend.c  |   46 +-
 .../bifrost/backend/gpu/mali_kbase_devfreq.c  |   24 +-
 .../backend/gpu/mali_kbase_gpuprops_backend.c |   12 +-
 .../backend/gpu/mali_kbase_instr_backend.c    |   50 +-
 .../backend/gpu/mali_kbase_irq_internal.h     |   69 +-
 .../backend/gpu/mali_kbase_irq_linux.c        |  233 +--
 ...mali_kbase_defs.h => mali_kbase_jm_defs.h} |    4 +-
 .../bifrost/backend/gpu/mali_kbase_jm_hw.c    |  227 ++-
 .../bifrost/backend/gpu/mali_kbase_jm_rb.c    |   50 +-
 .../backend/gpu/mali_kbase_js_backend.c       |   18 +-
 .../backend/gpu/mali_kbase_model_dummy.c      |  120 +-
 .../backend/gpu/mali_kbase_model_dummy.h      |    6 +-
 .../gpu/mali_kbase_model_error_generator.c    |  172 +++
 .../backend/gpu/mali_kbase_model_linux.c      |   88 +-
 .../backend/gpu/mali_kbase_model_linux.h      |    7 +-
 .../backend/gpu/mali_kbase_pm_backend.c       |  165 +--
 .../bifrost/backend/gpu/mali_kbase_pm_ca.c    |   30 +-
 .../bifrost/backend/gpu/mali_kbase_pm_ca.h    |   11 -
 .../bifrost/backend/gpu/mali_kbase_pm_defs.h  |   73 +-
 .../backend/gpu/mali_kbase_pm_driver.c        |  319 ++---
 .../backend/gpu/mali_kbase_pm_internal.h      |   42 +-
 .../backend/gpu/mali_kbase_pm_metrics.c       |    2 +-
 .../backend/gpu/mali_kbase_pm_policy.c        |   29 +-
 .../arm/bifrost/backend/gpu/mali_kbase_time.c |  130 +-
 drivers/gpu/arm/bifrost/build.bp              |   32 +-
 .../context/backend/mali_kbase_context_csf.c  |   13 +-
 .../context/backend/mali_kbase_context_jm.c   |   19 +-
 .../arm/bifrost/context/mali_kbase_context.c  |   12 +-
 .../arm/bifrost/context/mali_kbase_context.h  |   13 +-
 drivers/gpu/arm/bifrost/csf/Kbuild            |    4 +-
 .../ipa_control/mali_kbase_csf_ipa_control.c  |   24 +-
 drivers/gpu/arm/bifrost/csf/mali_kbase_csf.c  |  584 +++-----
 drivers/gpu/arm/bifrost/csf/mali_kbase_csf.h  |   34 +-
 .../bifrost/csf/mali_kbase_csf_csg_debugfs.c  |   92 +-
 .../gpu/arm/bifrost/csf/mali_kbase_csf_defs.h |  226 +--
 .../arm/bifrost/csf/mali_kbase_csf_firmware.c |  711 +++------
 .../arm/bifrost/csf/mali_kbase_csf_firmware.h |   58 +-
 .../bifrost/csf/mali_kbase_csf_firmware_cfg.c |   14 +-
 .../bifrost/csf/mali_kbase_csf_firmware_log.c |   13 +-
 .../csf/mali_kbase_csf_firmware_no_mali.c     |  230 ++-
 .../arm/bifrost/csf/mali_kbase_csf_fw_io.c    |  251 ----
 .../arm/bifrost/csf/mali_kbase_csf_fw_io.h    |  362 -----
 .../csf/mali_kbase_csf_fw_io_no_mali.c        |  294 ----
 .../csf/mali_kbase_csf_heap_context_alloc.c   |    7 +-
 .../gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.c |  261 ++--
 .../gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.h |   43 +-
 .../csf/mali_kbase_csf_kcpu_fence_debugfs.c   |    8 +-
 .../csf/mali_kbase_csf_mcu_shared_reg.c       |    4 +-
 .../csf/mali_kbase_csf_protected_memory.c     |    4 +-
 .../bifrost/csf/mali_kbase_csf_registers.h    |  194 +--
 .../bifrost/csf/mali_kbase_csf_reset_gpu.c    |   31 +-
 .../bifrost/csf/mali_kbase_csf_scheduler.c    |  929 +++++-------
 .../bifrost/csf/mali_kbase_csf_scheduler.h    |   51 +-
 .../gpu/arm/bifrost/csf/mali_kbase_csf_sync.c |   94 +-
 .../bifrost/csf/mali_kbase_csf_tiler_heap.c   |   25 +-
 .../csf/mali_kbase_csf_tiler_heap_reclaim.c   |   46 +-
 .../csf/mali_kbase_csf_tiler_heap_reclaim.h   |    4 +-
 .../arm/bifrost/csf/mali_kbase_csf_timeout.c  |    6 +-
 .../bifrost/csf/mali_kbase_csf_tl_reader.c    |   37 +-
 .../bifrost/csf/mali_kbase_csf_trace_buffer.c |   17 +-
 .../bifrost/csf/mali_kbase_csf_trace_buffer.h |    9 -
 .../gpu/arm/bifrost/csf/mali_kbase_csf_util.c |    6 +-
 .../bifrost/csf/mali_kbase_debug_csf_fault.c  |    8 +-
 .../backend/mali_kbase_debug_coresight_csf.c  |    7 +-
 .../mali_kbase_debug_ktrace_codes_csf.h       |    5 +-
 .../backend/mali_kbase_debug_ktrace_csf.c     |   35 +-
 .../backend/mali_kbase_debug_ktrace_defs_jm.h |    2 +-
 .../backend/mali_kbase_debug_ktrace_jm.c      |   37 +-
 .../backend/mali_kbase_debug_ktrace_jm.h      |   12 +-
 .../mali_kbase_debug_linux_ktrace_csf.h       |    3 +-
 .../mali_kbase_debug_linux_ktrace_jm.h        |   16 +-
 .../bifrost/debug/mali_kbase_debug_ktrace.c   |   34 +-
 .../debug/mali_kbase_debug_ktrace_codes.h     |   12 +-
 .../debug/mali_kbase_debug_linux_ktrace.h     |   11 +-
 .../device/backend/mali_kbase_device_csf.c    |   30 +-
 .../device/backend/mali_kbase_device_hw_csf.c |   47 +-
 .../device/backend/mali_kbase_device_hw_jm.c  |   13 +-
 .../device/backend/mali_kbase_device_jm.c     |   18 +-
 .../arm/bifrost/device/mali_kbase_device.c    |  156 +-
 .../arm/bifrost/device/mali_kbase_device.h    |    5 +-
 .../arm/bifrost/device/mali_kbase_device_hw.c |   18 +-
 .../device/mali_kbase_device_internal.h       |   10 +
 .../gpu/backend/mali_kbase_gpu_fault_csf.c    |   16 +-
 .../mali_kbase_hw_access_model_linux.c        |   12 +-
 .../backend/mali_kbase_hw_access_real_hw.c    |   36 +-
 .../bifrost/hw_access/mali_kbase_hw_access.c  |   60 +-
 .../bifrost/hw_access/mali_kbase_hw_access.h  |   56 +-
 .../hw_access/mali_kbase_hw_access_regmap.h   |   36 +-
 .../mali_kbase_hw_access_regmap_legacy.h      |    2 +-
 .../regmap/mali_kbase_regmap_csf_macros.h     |   26 +-
 .../hw_access/regmap/mali_kbase_regmap_jm.c   |   65 +-
 .../regmap/mali_kbase_regmap_jm_enums.h       |   17 +-
 .../regmap/mali_kbase_regmap_jm_macros.h      |   26 +-
 .../hwcnt/backend/mali_kbase_hwcnt_backend.h  |   20 +-
 .../backend/mali_kbase_hwcnt_backend_csf.c    |  209 ++-
 .../backend/mali_kbase_hwcnt_backend_csf.h    |   39 +-
 .../backend/mali_kbase_hwcnt_backend_csf_if.h |   26 +-
 .../mali_kbase_hwcnt_backend_csf_if_fw.c      |   42 +-
 .../backend/mali_kbase_hwcnt_backend_jm.c     |  119 +-
 .../mali_kbase_hwcnt_backend_jm_watchdog.c    |   12 +-
 .../gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt.c  |    8 +-
 .../arm/bifrost/hwcnt/mali_kbase_hwcnt_gpu.c  |  214 ++-
 .../arm/bifrost/hwcnt/mali_kbase_hwcnt_gpu.h  |  105 +-
 .../bifrost/hwcnt/mali_kbase_hwcnt_types.c    |   39 +-
 .../bifrost/hwcnt/mali_kbase_hwcnt_types.h    |   66 +-
 .../mali_kbase_ipa_counter_common_csf.c       |    6 +-
 .../mali_kbase_ipa_counter_common_jm.c        |    4 +-
 drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.c  |    4 +-
 .../arm/bifrost/ipa/mali_kbase_ipa_debugfs.c  |    6 +-
 .../arm/bifrost/ipa/mali_kbase_ipa_simple.c   |    3 -
 .../gpu/arm/bifrost/jm/mali_kbase_jm_defs.h   |  128 +-
 drivers/gpu/arm/bifrost/jm/mali_kbase_jm_js.h |   23 +-
 .../arm/bifrost/mali_base_hwconfig_features.h |  161 +++
 .../arm/bifrost/mali_base_hwconfig_issues.h   |  497 +++++++
 drivers/gpu/arm/bifrost/mali_csffw.bin        |  Bin 278528 -> 0 bytes
 drivers/gpu/arm/bifrost/mali_kbase.h          |  231 ++-
 .../arm/bifrost/mali_kbase_as_fault_debugfs.h |    4 +-
 .../gpu/arm/bifrost/mali_kbase_cache_policy.c |    3 +-
 .../gpu/arm/bifrost/mali_kbase_cache_policy.h |    3 +-
 drivers/gpu/arm/bifrost/mali_kbase_caps.h     |   96 +-
 drivers/gpu/arm/bifrost/mali_kbase_ccswe.c    |    5 +-
 drivers/gpu/arm/bifrost/mali_kbase_ccswe.h    |    1 -
 drivers/gpu/arm/bifrost/mali_kbase_config.h   |   39 +-
 .../arm/bifrost/mali_kbase_config_defaults.h  |   95 +-
 .../gpu/arm/bifrost/mali_kbase_core_linux.c   | 1269 ++++++++--------
 .../arm/bifrost/mali_kbase_cs_experimental.h  |    5 +-
 .../gpu/arm/bifrost/mali_kbase_ctx_sched.c    |   11 +-
 .../gpu/arm/bifrost/mali_kbase_ctx_sched.h    |    7 +-
 drivers/gpu/arm/bifrost/mali_kbase_debug.c    |    3 +-
 drivers/gpu/arm/bifrost/mali_kbase_debug.h    |    3 -
 .../arm/bifrost/mali_kbase_debug_job_fault.h  |    6 +-
 .../arm/bifrost/mali_kbase_debug_mem_allocs.c |    4 +-
 .../arm/bifrost/mali_kbase_debug_mem_allocs.h |    4 +-
 .../arm/bifrost/mali_kbase_debug_mem_view.c   |   16 +-
 .../arm/bifrost/mali_kbase_debug_mem_zones.h  |    4 +-
 .../arm/bifrost/mali_kbase_debugfs_helper.c   |    7 +-
 drivers/gpu/arm/bifrost/mali_kbase_defs.h     |  287 ++--
 .../arm/bifrost/mali_kbase_disjoint_events.c  |    2 +-
 .../gpu/arm/bifrost/mali_kbase_dummy_job_wa.c |   67 +-
 drivers/gpu/arm/bifrost/mali_kbase_fence.c    |   12 +-
 drivers/gpu/arm/bifrost/mali_kbase_fence.h    |   61 +-
 .../gpu/arm/bifrost/mali_kbase_fence_ops.c    |    2 +-
 .../bifrost/mali_kbase_gpu_memory_debugfs.h   |    7 +-
 .../gpu/arm/bifrost/mali_kbase_gpu_metrics.c  |  121 +-
 .../gpu/arm/bifrost/mali_kbase_gpu_metrics.h  |   19 +-
 drivers/gpu/arm/bifrost/mali_kbase_gpuprops.c |   32 +-
 drivers/gpu/arm/bifrost/mali_kbase_gpuprops.h |    5 +-
 drivers/gpu/arm/bifrost/mali_kbase_gwt.c      |   44 +-
 drivers/gpu/arm/bifrost/mali_kbase_gwt.h      |    3 +-
 drivers/gpu/arm/bifrost/mali_kbase_hw.c       |   26 +-
 drivers/gpu/arm/bifrost/mali_kbase_hw.h       |   10 +-
 .../arm/bifrost/mali_kbase_hwaccess_defs.h    |    4 +-
 .../gpu/arm/bifrost/mali_kbase_hwaccess_jm.h  |    2 +-
 .../gpu/arm/bifrost/mali_kbase_hwaccess_pm.h  |   19 +-
 .../arm/bifrost/mali_kbase_hwaccess_time.h    |   50 +-
 .../bifrost/mali_kbase_hwconfig_features.h    |  158 --
 .../arm/bifrost/mali_kbase_hwconfig_issues.h  |  609 --------
 .../arm/bifrost/mali_kbase_ioctl_helpers.h    |  542 -------
 drivers/gpu/arm/bifrost/mali_kbase_jd.c       |   90 +-
 drivers/gpu/arm/bifrost/mali_kbase_jm.c       |    6 +-
 drivers/gpu/arm/bifrost/mali_kbase_jm.h       |    2 +-
 drivers/gpu/arm/bifrost/mali_kbase_js.c       |  807 +++++++++--
 .../gpu/arm/bifrost/mali_kbase_kinstr_jm.c    |   22 +-
 .../arm/bifrost/mali_kbase_kinstr_prfcnt.c    |  109 +-
 drivers/gpu/arm/bifrost/mali_kbase_linux.h    |    7 +-
 drivers/gpu/arm/bifrost/mali_kbase_mem.c      |  564 +++-----
 drivers/gpu/arm/bifrost/mali_kbase_mem.h      |  223 ++-
 .../gpu/arm/bifrost/mali_kbase_mem_linux.c    |  344 ++---
 .../gpu/arm/bifrost/mali_kbase_mem_linux.h    |   13 +-
 .../gpu/arm/bifrost/mali_kbase_mem_lowlevel.h |   34 +-
 .../gpu/arm/bifrost/mali_kbase_mem_migrate.c  |   50 +-
 .../gpu/arm/bifrost/mali_kbase_mem_migrate.h  |   10 +-
 drivers/gpu/arm/bifrost/mali_kbase_mem_pool.c |   70 +-
 .../arm/bifrost/mali_kbase_mem_pool_group.c   |    4 +-
 .../arm/bifrost/mali_kbase_mem_pool_group.h   |    4 +-
 .../bifrost/mali_kbase_mem_profile_debugfs.h  |    2 -
 .../gpu/arm/bifrost/mali_kbase_native_mgm.c   |   97 +-
 drivers/gpu/arm/bifrost/mali_kbase_pbha.c     |   80 +-
 drivers/gpu/arm/bifrost/mali_kbase_pbha.h     |    4 +-
 .../gpu/arm/bifrost/mali_kbase_pbha_debugfs.c |   12 +-
 .../gpu/arm/bifrost/mali_kbase_pbha_debugfs.h |    4 +-
 drivers/gpu/arm/bifrost/mali_kbase_pm.c       |  117 +-
 drivers/gpu/arm/bifrost/mali_kbase_pm.h       |   39 +-
 .../arm/bifrost/mali_kbase_refcount_defs.h    |   57 +
 .../gpu/arm/bifrost/mali_kbase_reg_track.c    |   10 +-
 .../bifrost/mali_kbase_regs_history_debugfs.c |    4 +-
 .../gpu/arm/bifrost/mali_kbase_reset_gpu.h    |    4 +-
 drivers/gpu/arm/bifrost/mali_kbase_smc.h      |    8 +-
 drivers/gpu/arm/bifrost/mali_kbase_softjobs.c |   39 +-
 drivers/gpu/arm/bifrost/mali_kbase_sync.h     |   11 +-
 .../gpu/arm/bifrost/mali_kbase_sync_file.c    |   65 +-
 drivers/gpu/arm/bifrost/mmu/Kbuild            |   11 +-
 .../bifrost/mmu/backend/mali_kbase_mmu_csf.c  |  179 +--
 .../mali_kbase_mmu_faults_decoder_luts_csf.c  |  139 --
 .../mali_kbase_mmu_faults_decoder_luts_csf.h  |   50 -
 .../mali_kbase_mmu_faults_decoder_luts_jm.c   |   74 -
 .../mali_kbase_mmu_faults_decoder_luts_jm.h   |   37 -
 .../bifrost/mmu/backend/mali_kbase_mmu_jm.c   |  115 +-
 drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.c  | 1270 ++++++-----------
 drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.h  |   11 +-
 .../mmu/mali_kbase_mmu_faults_decoder.c       |  124 --
 .../mmu/mali_kbase_mmu_faults_decoder.h       |  143 --
 .../mmu/mali_kbase_mmu_faults_decoder_luts.c  |  660 ---------
 .../mmu/mali_kbase_mmu_faults_decoder_luts.h  |  119 --
 .../gpu/arm/bifrost/mmu/mali_kbase_mmu_hw.h   |   29 +-
 .../bifrost/mmu/mali_kbase_mmu_hw_direct.c    |  148 +-
 .../arm/bifrost/mmu/mali_kbase_mmu_internal.h |   15 +-
 .../bifrost/mmu/mali_kbase_mmu_mode_aarch64.c |   10 +-
 .../devicetree/mali_kbase_runtime_pm.c        |    2 +-
 .../platform/meson/mali_kbase_runtime_pm.c    |    2 +-
 .../gpu/arm/bifrost/platform/rk/custom_log.h  |    2 +-
 .../platform/rk/mali_kbase_config_rk.c        |   55 +-
 drivers/gpu/arm/bifrost/tests/Kbuild          |    2 +-
 drivers/gpu/arm/bifrost/tests/Kconfig         |    2 +-
 drivers/gpu/arm/bifrost/tests/kutf/Kbuild     |    5 +-
 drivers/gpu/arm/bifrost/tests/kutf/build.bp   |    3 +-
 .../bifrost/tests/kutf/kutf_helpers_user.c    |   32 +-
 .../gpu/arm/bifrost/tests/kutf/kutf_kprobe.c  |  354 -----
 .../gpu/arm/bifrost/tests/kutf/kutf_suite.c   |   25 +-
 .../gpu/arm/bifrost/tests/kutf/kutf_utils.c   |    4 +-
 .../kernel/mali_kutf_clk_rate_trace_test.c    |   12 +-
 .../mali_kutf_irq_test_main.c                 |   54 +-
 .../mali_kutf_mgm_integration_test_main.c     |    8 +-
 .../arm/bifrost/thirdparty/mali_kbase_mmap.c  |  273 +---
 .../tl/backend/mali_kbase_timeline_csf.c      |   10 +-
 .../tl/backend/mali_kbase_timeline_jm.c       |    2 +-
 .../gpu/arm/bifrost/tl/mali_kbase_timeline.c  |    6 +-
 .../arm/bifrost/tl/mali_kbase_timeline_io.c   |   19 +-
 .../gpu/arm/bifrost/tl/mali_kbase_tlstream.c  |    8 +-
 .../arm/bifrost/tl/mali_kbase_tracepoints.c   |   30 +-
 .../arm/bifrost/tl/mali_kbase_tracepoints.h   |  294 ++--
 drivers/gpu/arm/mali400/mali/platform/rk/rk.c |    2 +-
 .../platform/rk/mali_kbase_config_rk.c        |   19 +-
 drivers/hwtracing/coresight/mali/Makefile     |    4 +-
 drivers/hwtracing/coresight/mali/build.bp     |    2 +-
 .../mali/sources/coresight_mali_sources.c     |    6 +-
 .../itm/coresight_mali_source_itm_core.c      |    6 +-
 drivers/xen/arm/Makefile                      |    2 -
 include/linux/mali_arbiter_interface.h        |    4 +-
 include/linux/mali_hw_access.h                |   62 -
 include/linux/memory_group_manager.h          |   67 +-
 include/linux/priority_control_manager.h      |   82 +-
 include/linux/version_compat_defs.h           |  226 +--
 .../dma-buf-test-exporter.h                   |    2 +-
 .../backend/gpu/mali_kbase_model_dummy.h      |    4 +-
 .../backend/gpu/mali_kbase_model_linux.h      |    3 +-
 .../arm/bifrost/csf/mali_base_csf_kernel.h    |   63 +-
 .../arm/bifrost/csf/mali_kbase_csf_ioctl.h    |   52 +-
 .../bifrost/gpu/mali_kbase_gpu_coherency.h    |    8 +-
 .../gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h   |    2 +-
 .../gpu/arm/bifrost/jm/mali_base_jm_kernel.h  |  145 +-
 .../gpu/arm/bifrost/jm/mali_kbase_jm_ioctl.h  |   27 +-
 .../gpu/arm/bifrost/mali_base_common_kernel.h |   13 +-
 .../uapi/gpu/arm/bifrost/mali_base_kernel.h   |   48 +-
 .../uapi/gpu/arm/bifrost/mali_kbase_ioctl.h   |    6 +-
 .../mali_kbase_mem_profile_debugfs_buf_size.h |    4 +-
 277 files changed, 8105 insertions(+), 13943 deletions(-)
 delete mode 100644 Documentation/devicetree/bindings/arm/arm,coresight-mali-source.yaml
 rename drivers/gpu/arm/bifrost/{tests/include/kutf/kutf_kprobe.h => backend/gpu/mali_kbase_backend_config.h} (63%)
 rename drivers/gpu/arm/bifrost/backend/gpu/{mali_kbase_defs.h => mali_kbase_jm_defs.h} (97%)
 create mode 100644 drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_error_generator.c
 delete mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io.c
 delete mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io.h
 delete mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io_no_mali.c
 create mode 100644 drivers/gpu/arm/bifrost/mali_base_hwconfig_features.h
 create mode 100644 drivers/gpu/arm/bifrost/mali_base_hwconfig_issues.h
 delete mode 100644 drivers/gpu/arm/bifrost/mali_csffw.bin
 delete mode 100644 drivers/gpu/arm/bifrost/mali_kbase_hwconfig_features.h
 delete mode 100644 drivers/gpu/arm/bifrost/mali_kbase_hwconfig_issues.h
 delete mode 100644 drivers/gpu/arm/bifrost/mali_kbase_ioctl_helpers.h
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_refcount_defs.h
 delete mode 100644 drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.c
 delete mode 100644 drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.h
 delete mode 100644 drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.c
 delete mode 100644 drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.h
 delete mode 100644 drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder.c
 delete mode 100644 drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder.h
 delete mode 100644 drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder_luts.c
 delete mode 100644 drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder_luts.h
 delete mode 100644 drivers/gpu/arm/bifrost/tests/kutf/kutf_kprobe.c
 delete mode 100644 include/linux/mali_hw_access.h

diff --git a/Documentation/ABI/testing/sysfs-device-mali b/Documentation/ABI/testing/sysfs-device-mali
index 12a1667feeb2..1ec265c5add4 100644
--- a/Documentation/ABI/testing/sysfs-device-mali
+++ b/Documentation/ABI/testing/sysfs-device-mali
@@ -341,7 +341,8 @@ Description:
                 device-driver that supports a CSF GPU.
 
 		Used to enable firmware logs, logging levels valid values
-		are indicated using 'min' and 'max' attributes, which are read-only.
+		are indicated using 'min and 'max' attribute values
+		values that are read-only.
 
 		Log level can be set using the 'cur' read, write attribute,
 		we can use a valid log level value from min and max range values
diff --git a/Documentation/ABI/testing/sysfs-device-mali-coresight-source b/Documentation/ABI/testing/sysfs-device-mali-coresight-source
index 58d9085b8bb6..0f31a6acaa87 100644
--- a/Documentation/ABI/testing/sysfs-device-mali-coresight-source
+++ b/Documentation/ABI/testing/sysfs-device-mali-coresight-source
@@ -19,7 +19,7 @@ Description:
 
 What:		/sys/bus/coresight/devices/mali-source-etm/is_enabled
 Description:
-        Attribute used to check if Coresight Source ETM is enabled.
+        Attribute used to check if Coresight Source ITM is enabled.
 
 What:		/sys/bus/coresight/devices/mali-source-etm/trcconfigr
 Description:
diff --git a/Documentation/devicetree/bindings/arm/arm,coresight-mali-source.yaml b/Documentation/devicetree/bindings/arm/arm,coresight-mali-source.yaml
deleted file mode 100644
index d844ad10932c..000000000000
--- a/Documentation/devicetree/bindings/arm/arm,coresight-mali-source.yaml
+++ /dev/null
@@ -1,163 +0,0 @@
-# SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
-#
-# (C) COPYRIGHT 2022-2024 ARM Limited. All rights reserved.
-#
-# This program is free software and is provided to you under the terms of the
-# GNU General Public License version 2 as published by the Free Software
-# Foundation, and any use by you of this program is subject to the terms
-# of such GNU license.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU General Public License for more details.
-#
-# You should have received a copy of the GNU General Public License
-# along with this program; if not, you can access it online at
-# http://www.gnu.org/licenses/gpl-2.0.html.
-#
-#
-%YAML 1.2
----
-$id: http://devicetree.org/schemas/arm/arm,coresight-mali-source.yaml#
-$schema: http://devicetree.org/meta-schemas/core.yaml#
-
-title: ARM CoreSight Mali Source integration
-
-maintainers:
-  - ARM Ltd.
-
-description: |
-  See Documentation/trace/coresight/coresight.rst for detailed information
-  about Coresight.
-
-  This documentation will cover Mali specific devicetree integration.
-
-  References to Sink ports are given as examples. Access to Sink is specific
-  to an implementation and would require dedicated kernel modules.
-
-  Arm Mali GPU are supporting 3 different sources: ITM, ETM, ELA
-
-  ELA source configuration via SysFS entries:
-
-    The register values used by CoreSight for ELA can be configured using SysFS
-    interfaces. This implicitly includes configuring the ELA for independent or
-    shared JCN request and response channels.
-
-properties:
-  compatible:
-    enum:
-      - arm,coresight-mali-source-itm
-      - arm,coresight-mali-source-etm
-      - arm,coresight-mali-source-ela
-
-  gpu:
-    minItems: 1
-    maxItems: 1
-    description:
-      Phandle to a Mali GPU definition
-
-  port:
-    description:
-      Output connection to CoreSight Sink Trace bus.
-
-      Legacy binding between Coresight Sources and CoreSight Sink.
-      For Linux kernel < v4.20.
-    $ref: /schemas/graph.yaml#/properties/port
-
-  out-ports:
-    description:
-      Binding between Coresight Sources and CoreSight Sink.
-      For Linux kernel >= v4.20.
-    $ref: /schemas/graph.yaml#/properties/ports
-
-    properties:
-      port:
-        description: Output connection to CoreSight Sink Trace bus.
-        $ref: /schemas/graph.yaml#/properties/port
-
-required:
-  - compatible
-  - gpu
-  - port
-  - out-ports
-
-additionalProperties: false
-
-examples:
-
-# A Sink node without legacy CoreSight connections
-  - |
-    mali-source-itm {
-        compatible = "arm,coresight-mali-source-itm";
-        gpu = <&gpu>;
-
-        out-ports {
-            port {
-                mali_source_itm_out_port0: endpoint {
-                    remote-endpoint = <&mali_sink_in_port0>;
-                };
-            };
-        };
-    };
-
-    mali-source-ela {
-        compatible = "arm,coresight-mali-source-ela";
-        gpu = <&gpu>;
-
-        out-ports {
-            port {
-                mali_source_ela_out_port0: endpoint {
-                    remote-endpoint = <&mali_sink_in_port1>;
-                };
-            };
-        };
-    };
-
-    mali-source-etm {
-        compatible = "arm,coresight-mali-source-etm";
-        gpu = <&gpu>;
-
-        out-ports {
-            port {
-                mali_source_etm_out_port0: endpoint {
-                    remote-endpoint = <&mali_sink_in_port2>;
-                };
-            };
-        };
-    };
-
-# A Sink node with legacy CoreSight connections
-  - |
-    mali-source-itm {
-        compatible = "arm,coresight-mali-source-itm";
-        gpu = <&gpu>;
-
-        port {
-            mali_source_itm_out_port0: endpoint {
-                remote-endpoint = <&mali_sink_in_port0>;
-            };
-        };
-    };
-
-    mali-source-etm {
-        compatible = "arm,coresight-mali-source-etm";
-        gpu = <&gpu>;
-
-        port {
-            mali_source_etm_out_port0: endpoint {
-                remote-endpoint = <&mali_sink_in_port1>;
-            };
-        };
-    };
-
-    mali-source-ela {
-        compatible = "arm,coresight-mali-source-ela";
-        gpu = <&gpu>;
-
-        port {
-            mali_source_ela_out_port0: endpoint {
-                remote-endpoint = <&mali_sink_in_port2>;
-            };
-        };
-    };
diff --git a/Documentation/devicetree/bindings/arm/mali-bifrost.txt b/Documentation/devicetree/bindings/arm/mali-bifrost.txt
index 8ada052ebe56..85672c6c6258 100644
--- a/Documentation/devicetree/bindings/arm/mali-bifrost.txt
+++ b/Documentation/devicetree/bindings/arm/mali-bifrost.txt
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2013-2024 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2013-2023 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -111,10 +111,7 @@ for details.
 -  idvs-group-size : Override the IDVS group size value. Tasks are sent to
 		     cores in groups of N + 1, so i.e. 0xF means 16 tasks.
 		     Valid values are between 0 to 0x3F (including).
--  l2-size : Override L2 cache size on GPU that supports it. Value should be larger than the minimum
-             size 1KiB and smaller than the maximum size. Maximum size is Hardware integration dependent.
-             The value passed should be of log2(Cache Size in Bytes).
-             For example for a 1KiB of cache size, 0xa should be passed.
+-  l2-size : Override L2 cache size on GPU that supports it
 -  l2-hash : Override L2 hash function on GPU that supports it
 -  l2-hash-values : Override L2 hash function using provided hash values, on GPUs that supports it.
 		    It is mutually exclusive with 'l2-hash'. Only one or the other must be
@@ -132,10 +129,6 @@ for details.
 		   set and the setting coresponding to the SYSC_ALLOC register.
 - propagate-bits: Used to write to L2_CONFIG.PBHA_HWU. This bitset establishes which
 		   PBHA bits are propagated on the AXI bus.
-- mma-wa-id: Sets the PBHA ID to be used for the PBHA override based MMA violation workaround.
-	     The read and write allocation override bits for the PBHA are set to NONCACHEABLE
-	     and the driver encodes the PBHA ID in the PTEs where this workaround is to be applied.
-	     Valid values are from 1 to 15.
 
 
 Example for a Mali GPU with 1 clock and 1 regulator:
@@ -244,8 +237,7 @@ gpu@0xfc010000 {
     ...
     pbha {
         int-id-override = <2 0x32>, <9 0x05>, <16 0x32>;
-        propagate-bits = /bits/ 8 <0x03>;
-        mma-wa-id = <2>;
+        propagate-bits = /bits/ 4 <0x03>;
     };
     ...
 };
diff --git a/drivers/base/arm/Kconfig b/drivers/base/arm/Kconfig
index c24a377723ca..e8bb8a40d2c5 100644
--- a/drivers/base/arm/Kconfig
+++ b/drivers/base/arm/Kconfig
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2021-2024 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2021-2023 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/base/arm/Makefile b/drivers/base/arm/Makefile
index 42071f769729..4aa68f89d3d9 100644
--- a/drivers/base/arm/Makefile
+++ b/drivers/base/arm/Makefile
@@ -125,8 +125,6 @@ CFLAGS_MODULE += -Wno-sign-compare
 CFLAGS_MODULE += -Wno-shift-negative-value
 # This flag is needed to avoid build errors on older kernels
 CFLAGS_MODULE += $(call cc-option, -Wno-cast-function-type)
-# The following ensures the stack frame does not get larger than a page
-CFLAGS_MODULE += -Wframe-larger-than=4096
 
 KBUILD_CPPFLAGS += -DKBUILD_EXTRA_WARN1
 
diff --git a/drivers/base/arm/dma_buf_test_exporter/dma-buf-test-exporter.c b/drivers/base/arm/dma_buf_test_exporter/dma-buf-test-exporter.c
index 9bf7f8d2dd6d..deef790dc73b 100644
--- a/drivers/base/arm/dma_buf_test_exporter/dma-buf-test-exporter.c
+++ b/drivers/base/arm/dma_buf_test_exporter/dma-buf-test-exporter.c
@@ -182,7 +182,7 @@ static struct sg_table *dma_buf_te_map(struct dma_buf_attachment *attachment,
 			sg_set_page(iter, alloc->pages[i], PAGE_SIZE, 0);
 	}
 
-	if (!dma_map_sg(attachment->dev, sg->sgl, (int)sg->nents, direction)) {
+	if (!dma_map_sg(attachment->dev, sg->sgl, sg->nents, direction)) {
 		mutex_unlock(&alloc->lock);
 		sg_free_table(sg);
 		kfree(sg);
@@ -213,7 +213,7 @@ static void dma_buf_te_unmap(struct dma_buf_attachment *attachment, struct sg_ta
 	pa->sg = NULL;
 	mutex_unlock(&alloc->lock);
 
-	dma_unmap_sg(attachment->dev, sg->sgl, (int)sg->nents, direction);
+	dma_unmap_sg(attachment->dev, sg->sgl, sg->nents, direction);
 	sg_free_table(sg);
 	kfree(sg);
 }
@@ -273,12 +273,12 @@ static int dma_buf_te_sync(struct dma_buf *dmabuf, enum dma_data_direction direc
 			dev_dbg(te_device.this_device, "sync cpu with device %s\n",
 				dev_name(attachment->dev));
 
-			dma_sync_sg_for_cpu(attachment->dev, sg->sgl, (int)sg->nents, direction);
+			dma_sync_sg_for_cpu(attachment->dev, sg->sgl, sg->nents, direction);
 		} else {
 			dev_dbg(te_device.this_device, "sync device %s with cpu\n",
 				dev_name(attachment->dev));
 
-			dma_sync_sg_for_device(attachment->dev, sg->sgl, (int)sg->nents, direction);
+			dma_sync_sg_for_device(attachment->dev, sg->sgl, sg->nents, direction);
 		}
 	}
 
@@ -682,7 +682,7 @@ static int do_dma_buf_te_ioctl_set_failing(struct dma_buf_te_ioctl_set_failing _
 	return res;
 }
 
-static int dma_te_buf_fill(struct dma_buf *dma_buf, int value)
+static u32 dma_te_buf_fill(struct dma_buf *dma_buf, unsigned int value)
 {
 	struct dma_buf_attachment *attachment;
 	struct sg_table *sgt;
@@ -695,11 +695,7 @@ static int dma_te_buf_fill(struct dma_buf *dma_buf, int value)
 	if (IS_ERR_OR_NULL(attachment))
 		return -EBUSY;
 
-#if (KERNEL_VERSION(6, 1, 55) <= LINUX_VERSION_CODE)
-	sgt = dma_buf_map_attachment_unlocked(attachment, DMA_BIDIRECTIONAL);
-#else
 	sgt = dma_buf_map_attachment(attachment, DMA_BIDIRECTIONAL);
-#endif
 	if (IS_ERR_OR_NULL(sgt)) {
 		ret = PTR_ERR(sgt);
 		goto no_import;
@@ -733,11 +729,7 @@ static int dma_te_buf_fill(struct dma_buf *dma_buf, int value)
 no_kmap:
 	dma_buf_end_cpu_access(dma_buf, DMA_BIDIRECTIONAL);
 no_cpu_access:
-#if (KERNEL_VERSION(6, 1, 55) <= LINUX_VERSION_CODE)
-	dma_buf_unmap_attachment_unlocked(attachment, sgt, DMA_BIDIRECTIONAL);
-#else
 	dma_buf_unmap_attachment(attachment, sgt, DMA_BIDIRECTIONAL);
-#endif
 no_import:
 	dma_buf_detach(dma_buf, attachment);
 	return ret;
diff --git a/drivers/base/arm/memory_group_manager/memory_group_manager.c b/drivers/base/arm/memory_group_manager/memory_group_manager.c
index da4a0c39e63a..11dc1c2e24d8 100644
--- a/drivers/base/arm/memory_group_manager/memory_group_manager.c
+++ b/drivers/base/arm/memory_group_manager/memory_group_manager.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -51,13 +51,17 @@ static inline vm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigne
 }
 #endif
 
+#define PTE_PBHA_SHIFT (59)
+#define PTE_PBHA_MASK ((uint64_t)0xf << PTE_PBHA_SHIFT)
+#define PTE_RES_BIT_MULTI_AS_SHIFT (63)
+
 #define IMPORTED_MEMORY_ID (MEMORY_GROUP_MANAGER_NR_GROUPS - 1)
 
 /**
  * struct mgm_group - Structure to keep track of the number of allocated
  *                    pages per group
  *
- * @size:  The number of allocated small pages of PAGE_SIZE bytes
+ * @size:  The number of allocated small(4KB) pages
  * @lp_size:  The number of allocated large(2MB) pages
  * @insert_pfn: The number of calls to map pages for CPU access.
  * @update_gpu_pte: The number of calls to update GPU page table entries.
@@ -96,7 +100,7 @@ static int mgm_size_get(void *data, u64 *val)
 {
 	struct mgm_group *group = data;
 
-	*val = (u64)atomic_read(&group->size);
+	*val = atomic_read(&group->size);
 
 	return 0;
 }
@@ -104,21 +108,21 @@ static int mgm_size_get(void *data, u64 *val)
 static int mgm_lp_size_get(void *data, u64 *val)
 {
 	struct mgm_group *group = data;
-	*val = (u64)atomic_read(&group->lp_size);
+	*val = atomic_read(&group->lp_size);
 	return 0;
 }
 
 static int mgm_insert_pfn_get(void *data, u64 *val)
 {
 	struct mgm_group *group = data;
-	*val = (u64)atomic_read(&group->insert_pfn);
+	*val = atomic_read(&group->insert_pfn);
 	return 0;
 }
 
 static int mgm_update_gpu_pte_get(void *data, u64 *val)
 {
 	struct mgm_group *group = data;
-	*val = (u64)atomic_read(&group->update_gpu_pte);
+	*val = atomic_read(&group->update_gpu_pte);
 	return 0;
 }
 
@@ -208,9 +212,9 @@ static int mgm_initialize_debugfs(struct mgm_groups *mgm_data)
 #endif /* CONFIG_DEBUG_FS */
 
 #define ORDER_SMALL_PAGE 0
-#define ORDER_LARGE_PAGE (__builtin_ffs(SZ_2M / PAGE_SIZE) - 1)
+#define ORDER_LARGE_PAGE 9
 static void update_size(struct memory_group_manager_device *mgm_dev, unsigned int group_id,
-			unsigned int order, bool alloc)
+			int order, bool alloc)
 {
 	struct mgm_groups *data = mgm_dev->data;
 
@@ -234,22 +238,21 @@ static void update_size(struct memory_group_manager_device *mgm_dev, unsigned in
 		break;
 
 	default:
-		dev_err(data->dev, "Unknown order(%u)\n", order);
+		dev_err(data->dev, "Unknown order(%d)\n", order);
 		break;
 	}
 }
 
 static struct page *example_mgm_alloc_page(struct memory_group_manager_device *mgm_dev,
-					   unsigned int group_id, gfp_t gfp_mask,
-					   unsigned int order)
+					   int group_id, gfp_t gfp_mask, unsigned int order)
 {
 	struct mgm_groups *const data = mgm_dev->data;
 	struct page *p;
 
-	dev_dbg(data->dev, "%s(mgm_dev=%pK, group_id=%u gfp_mask=0x%x order=%u\n", __func__,
+	dev_dbg(data->dev, "%s(mgm_dev=%pK, group_id=%d gfp_mask=0x%x order=%u\n", __func__,
 		(void *)mgm_dev, group_id, gfp_mask, order);
 
-	if (WARN_ON(group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS))
+	if (WARN_ON(group_id < 0) || WARN_ON(group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS))
 		return NULL;
 
 	p = alloc_pages(gfp_mask, order);
@@ -259,21 +262,21 @@ static struct page *example_mgm_alloc_page(struct memory_group_manager_device *m
 	} else {
 		struct mgm_groups *data = mgm_dev->data;
 
-		dev_dbg(data->dev, "alloc_pages failed\n");
+		dev_err(data->dev, "alloc_pages failed\n");
 	}
 
 	return p;
 }
 
-static void example_mgm_free_page(struct memory_group_manager_device *mgm_dev,
-				  unsigned int group_id, struct page *page, unsigned int order)
+static void example_mgm_free_page(struct memory_group_manager_device *mgm_dev, int group_id,
+				  struct page *page, unsigned int order)
 {
 	struct mgm_groups *const data = mgm_dev->data;
 
-	dev_dbg(data->dev, "%s(mgm_dev=%pK, group_id=%u page=%pK order=%u\n", __func__,
+	dev_dbg(data->dev, "%s(mgm_dev=%pK, group_id=%d page=%pK order=%u\n", __func__,
 		(void *)mgm_dev, group_id, (void *)page, order);
 
-	if (WARN_ON(group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS))
+	if (WARN_ON(group_id < 0) || WARN_ON(group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS))
 		return;
 
 	__free_pages(page, order);
@@ -299,21 +302,17 @@ static int example_mgm_get_import_memory_id(struct memory_group_manager_device *
 }
 
 static u64 example_mgm_update_gpu_pte(struct memory_group_manager_device *const mgm_dev,
-				      unsigned int const group_id, unsigned int const pbha_id,
-				      unsigned int pte_flags, int const mmu_level, u64 pte)
+				      int const group_id, int const mmu_level, u64 pte)
 {
 	struct mgm_groups *const data = mgm_dev->data;
 
-	dev_dbg(data->dev, "%s(mgm_dev=%pK, group_id=%u, mmu_level=%d, pte=0x%llx)\n", __func__,
+	dev_dbg(data->dev, "%s(mgm_dev=%pK, group_id=%d, mmu_level=%d, pte=0x%llx)\n", __func__,
 		(void *)mgm_dev, group_id, mmu_level, pte);
 
-	if (WARN_ON(group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS))
+	if (WARN_ON(group_id < 0) || WARN_ON(group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS))
 		return pte;
 
-	if (pte_flags & BIT(MMA_VIOLATION)) {
-		pr_warn_once("MMA violation! Applying PBHA override workaround to PTE\n");
-		pte |= ((u64)pbha_id << PTE_PBHA_SHIFT) & PTE_PBHA_MASK;
-	}
+	pte |= ((u64)group_id << PTE_PBHA_SHIFT) & PTE_PBHA_MASK;
 
 	/* Address could be translated into a different bus address here */
 	pte |= ((u64)1 << PTE_RES_BIT_MULTI_AS_SHIFT);
@@ -324,8 +323,7 @@ static u64 example_mgm_update_gpu_pte(struct memory_group_manager_device *const
 }
 
 static u64 example_mgm_pte_to_original_pte(struct memory_group_manager_device *const mgm_dev,
-					   unsigned int const group_id, int const mmu_level,
-					   u64 pte)
+					   int const group_id, int const mmu_level, u64 pte)
 {
 	CSTD_UNUSED(mgm_dev);
 	CSTD_UNUSED(group_id);
@@ -340,7 +338,7 @@ static u64 example_mgm_pte_to_original_pte(struct memory_group_manager_device *c
 }
 
 static vm_fault_t example_mgm_vmf_insert_pfn_prot(struct memory_group_manager_device *const mgm_dev,
-						  unsigned int const group_id,
+						  int const group_id,
 						  struct vm_area_struct *const vma,
 						  unsigned long const addr, unsigned long const pfn,
 						  pgprot_t const prot)
@@ -349,11 +347,11 @@ static vm_fault_t example_mgm_vmf_insert_pfn_prot(struct memory_group_manager_de
 	vm_fault_t fault;
 
 	dev_dbg(data->dev,
-		"%s(mgm_dev=%pK, group_id=%u, vma=%pK, addr=0x%lx, pfn=0x%lx, prot=0x%llx)\n",
+		"%s(mgm_dev=%pK, group_id=%d, vma=%pK, addr=0x%lx, pfn=0x%lx, prot=0x%llx)\n",
 		__func__, (void *)mgm_dev, group_id, (void *)vma, addr, pfn,
 		(unsigned long long)pgprot_val(prot));
 
-	if (WARN_ON(group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS))
+	if (WARN_ON(group_id < 0) || WARN_ON(group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS))
 		return VM_FAULT_SIGBUS;
 
 	fault = vmf_insert_pfn_prot(vma, addr, pfn, prot);
@@ -366,16 +364,6 @@ static vm_fault_t example_mgm_vmf_insert_pfn_prot(struct memory_group_manager_de
 	return fault;
 }
 
-static bool example_mgm_get_import_memory_cached_access_permitted(
-	struct memory_group_manager_device *mgm_dev,
-	struct memory_group_manager_import_data *import_data)
-{
-	CSTD_UNUSED(mgm_dev);
-	CSTD_UNUSED(import_data);
-
-	return true;
-}
-
 static int mgm_initialize_data(struct mgm_groups *mgm_data)
 {
 	int i;
@@ -422,8 +410,6 @@ static int memory_group_manager_probe(struct platform_device *pdev)
 	mgm_dev->ops.mgm_vmf_insert_pfn_prot = example_mgm_vmf_insert_pfn_prot;
 	mgm_dev->ops.mgm_update_gpu_pte = example_mgm_update_gpu_pte;
 	mgm_dev->ops.mgm_pte_to_original_pte = example_mgm_pte_to_original_pte;
-	mgm_dev->ops.mgm_get_import_memory_cached_access_permitted =
-		example_mgm_get_import_memory_cached_access_permitted;
 
 	mgm_data = kzalloc(sizeof(*mgm_data), GFP_KERNEL);
 	if (!mgm_data) {
diff --git a/drivers/base/arm/protected_memory_allocator/protected_memory_allocator.c b/drivers/base/arm/protected_memory_allocator/protected_memory_allocator.c
index b27eed93fc8b..d7e0bec0978e 100644
--- a/drivers/base/arm/protected_memory_allocator/protected_memory_allocator.c
+++ b/drivers/base/arm/protected_memory_allocator/protected_memory_allocator.c
@@ -303,7 +303,7 @@ simple_pma_alloc_page(struct protected_memory_allocator_device *pma_dev, unsigne
 
 					large_granularity_alloc(epma_dev, start_idx, order, pma);
 
-					epma_dev->num_free_pages -= 1ULL << order;
+					epma_dev->num_free_pages -= 1 << order;
 					spin_unlock(&epma_dev->rmem_lock);
 					return pma;
 				}
diff --git a/drivers/gpu/arm/bifrost/Kbuild b/drivers/gpu/arm/bifrost/Kbuild
index d64c439fbabc..957f412d6547 100644
--- a/drivers/gpu/arm/bifrost/Kbuild
+++ b/drivers/gpu/arm/bifrost/Kbuild
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2012-2024 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2012-2023 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -69,7 +69,7 @@ endif
 #
 
 # Driver version string which is returned to userspace via an ioctl
-MALI_RELEASE_NAME ?= '"g25p0-00eac0"'
+MALI_RELEASE_NAME ?= '"g21p0-01eac0"'
 # Set up defaults if not defined by build system
 ifeq ($(CONFIG_MALI_BIFROST_DEBUG), y)
     MALI_UNIT_TEST = 1
@@ -104,6 +104,7 @@ endif
 #
 # Experimental features must default to disabled, e.g.:
 # MALI_EXPERIMENTAL_FEATURE ?= 0
+MALI_INCREMENTAL_RENDERING_JM ?= 0
 
 #
 # ccflags
@@ -116,6 +117,7 @@ ccflags-y = \
     -DMALI_COVERAGE=$(MALI_COVERAGE) \
     -DMALI_RELEASE_NAME=$(MALI_RELEASE_NAME) \
     -DMALI_JIT_PRESSURE_LIMIT_BASE=$(MALI_JIT_PRESSURE_LIMIT_BASE) \
+    -DMALI_INCREMENTAL_RENDERING_JM=$(MALI_INCREMENTAL_RENDERING_JM) \
     -DMALI_PLATFORM_DIR=$(MALI_PLATFORM_DIR)
 
 
@@ -210,7 +212,6 @@ endif
 
 
 INCLUDE_SUBDIR = \
-    $(src)/arbiter/Kbuild \
     $(src)/context/Kbuild \
     $(src)/debug/Kbuild \
     $(src)/device/Kbuild \
@@ -227,6 +228,9 @@ ifeq ($(CONFIG_MALI_CSF_SUPPORT),y)
     INCLUDE_SUBDIR += $(src)/csf/Kbuild
 endif
 
+ifeq ($(CONFIG_MALI_ARBITER_SUPPORT),y)
+    INCLUDE_SUBDIR += $(src)/arbiter/Kbuild
+endif
 
 ifeq ($(CONFIG_MALI_BIFROST_DEVFREQ),y)
     ifeq ($(CONFIG_DEVFREQ_THERMAL),y)
diff --git a/drivers/gpu/arm/bifrost/Kconfig b/drivers/gpu/arm/bifrost/Kconfig
index b8ceff10e250..f32b107949ba 100644
--- a/drivers/gpu/arm/bifrost/Kconfig
+++ b/drivers/gpu/arm/bifrost/Kconfig
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2012-2024 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2012-2023 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -63,8 +63,6 @@ config MALI_BIFROST_NO_MALI
 	  All calls to the simulated hardware will complete immediately as if the hardware
 	  completed the task.
 
-endchoice
-
 config MALI_NO_MALI_DEFAULT_GPU
 	string "Default GPU for No Mali"
 	depends on MALI_BIFROST_NO_MALI
@@ -72,12 +70,8 @@ config MALI_NO_MALI_DEFAULT_GPU
 	help
 	  This option sets the default GPU to identify as for No Mali builds.
 
-config MALI_IS_FPGA
-	bool "Enable build of Mali kernel driver for FPGA"
-	depends on MALI_BIFROST
-	default n
-	help
-	  This is the default HW backend.
+
+endchoice
 
 menu "Platform specific options"
 source "$(MALI_KCONFIG_EXT_PREFIX)drivers/gpu/arm/bifrost/platform/Kconfig"
@@ -127,7 +121,7 @@ config MALI_BIFROST_ENABLE_TRACE
 
 config MALI_ARBITER_SUPPORT
 	bool "Enable arbiter support for Mali"
-	depends on MALI_BIFROST
+	depends on MALI_BIFROST && !MALI_CSF_SUPPORT
 	default n
 	help
 	  Enable support for the arbiter interface in the driver.
@@ -207,6 +201,16 @@ config PAGE_MIGRATION_SUPPORT
 	  If in doubt, say Y. To strip out page migration symbols and support,
 	  say N.
 
+config MALI_MEMORY_FULLY_BACKED
+	bool "Enable memory fully physically-backed"
+	depends on MALI_BIFROST && MALI_BIFROST_EXPERT
+	default n
+	help
+	  This option enables full physical backing of all virtual
+	  memory allocations in the kernel. Notice that this build
+	  option only affects allocations of grow-on-GPU-page-fault
+	  memory.
+
 config MALI_CORESTACK
 	bool "Enable support of GPU core stack power control"
 	depends on MALI_BIFROST && MALI_BIFROST_EXPERT
@@ -220,6 +224,16 @@ config MALI_CORESTACK
 
 	  If unsure, say N.
 
+comment "Platform options"
+	depends on MALI_BIFROST && MALI_BIFROST_EXPERT
+
+config MALI_BIFROST_ERROR_INJECT
+	bool "Enable No Mali error injection"
+	depends on MALI_BIFROST && MALI_BIFROST_EXPERT && MALI_BIFROST_NO_MALI
+	default n
+	help
+	  Enables insertion of errors to test module failure and recovery mechanisms.
+
 comment "Debug options"
 	depends on MALI_BIFROST && MALI_BIFROST_EXPERT
 
@@ -300,7 +314,7 @@ endchoice
 
 config MALI_PRFCNT_SET_SELECT_VIA_DEBUG_FS
 	bool "Enable runtime selection of performance counters set via debugfs"
-	depends on MALI_BIFROST && MALI_BIFROST_EXPERT && DEBUG_FS && !MALI_CSF_SUPPORT
+	depends on MALI_BIFROST && MALI_BIFROST_EXPERT && DEBUG_FS
 	default n
 	help
 	  Select this option to make the secondary set of performance counters
@@ -347,7 +361,7 @@ config MALI_PWRSOFT_765
 	  changes have been backported say Y to avoid compilation errors.
 
 config MALI_HW_ERRATA_1485982_NOT_AFFECTED
-	bool "Disable workaround for KBASE_HW_ISSUE_GPU2017_1336"
+	bool "Disable workaround for BASE_HW_ISSUE_GPU2017_1336"
 	depends on MALI_BIFROST && MALI_BIFROST_EXPERT
 	default n
 	help
@@ -359,7 +373,7 @@ config MALI_HW_ERRATA_1485982_NOT_AFFECTED
 	  coherency mode requires the L2 to be turned off.
 
 config MALI_HW_ERRATA_1485982_USE_CLOCK_ALTERNATIVE
-	bool "Use alternative workaround for KBASE_HW_ISSUE_GPU2017_1336"
+	bool "Use alternative workaround for BASE_HW_ISSUE_GPU2017_1336"
 	depends on MALI_BIFROST && MALI_BIFROST_EXPERT && !MALI_HW_ERRATA_1485982_NOT_AFFECTED
 	default n
 	help
@@ -389,10 +403,6 @@ config MALI_TRACE_POWER_GPU_WORK_PERIOD
 
 	  If unsure, say N.
 
-config MALI_CSF_INCLUDE_FW
-	depends on MALI_BIFROST && MALI_CSF_SUPPORT
-	bool "Whether to include CSF firmware into driver"
-	default y
 
 # source "$(MALI_KCONFIG_EXT_PREFIX)drivers/gpu/arm/bifrost/tests/Kconfig"
 
diff --git a/drivers/gpu/arm/bifrost/Makefile b/drivers/gpu/arm/bifrost/Makefile
index e10033aabc57..f0f5e6072193 100644
--- a/drivers/gpu/arm/bifrost/Makefile
+++ b/drivers/gpu/arm/bifrost/Makefile
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -41,12 +41,11 @@ ifeq ($(MALI_KCONFIG_EXT_PREFIX),)
         CONFIG_MALI_BIFROST_GATOR_SUPPORT ?= y
         CONFIG_MALI_ARBITRATION ?= n
         CONFIG_MALI_PARTITION_MANAGER ?= n
-
+        CONFIG_MALI_64BIT_HW_ACCESS ?= n
 
         ifneq ($(CONFIG_MALI_BIFROST_NO_MALI),y)
-            # Prevent misuse when CONFIG_MALI_BIFROST_NO_MALI!=y
+            # Prevent misuse when CONFIG_MALI_BIFROST_NO_MALI=y
             CONFIG_MALI_REAL_HW ?= y
-        else
             CONFIG_MALI_CORESIGHT = n
         endif
 
@@ -77,6 +76,7 @@ ifeq ($(MALI_KCONFIG_EXT_PREFIX),)
             else
                 # Prevent misuse when CONFIG_MALI_BIFROST_NO_MALI=n
                 CONFIG_MALI_REAL_HW = y
+                CONFIG_MALI_BIFROST_ERROR_INJECT = n
             endif
 
 
@@ -105,9 +105,11 @@ ifeq ($(MALI_KCONFIG_EXT_PREFIX),)
             CONFIG_MALI_CORESTACK = n
             CONFIG_LARGE_PAGE_SUPPORT = y
             CONFIG_MALI_PWRSOFT_765 = n
+            CONFIG_MALI_MEMORY_FULLY_BACKED = n
             CONFIG_MALI_JOB_DUMP = n
             CONFIG_MALI_BIFROST_NO_MALI = n
             CONFIG_MALI_REAL_HW = y
+            CONFIG_MALI_BIFROST_ERROR_INJECT = n
             CONFIG_MALI_HW_ERRATA_1485982_NOT_AFFECTED = n
             CONFIG_MALI_HW_ERRATA_1485982_USE_CLOCK_ALTERNATIVE = n
             CONFIG_MALI_PRFCNT_SET_SELECT_VIA_DEBUG_FS = n
@@ -156,6 +158,7 @@ ifeq ($(MALI_KCONFIG_EXT_PREFIX),)
         CONFIG_MALI_BIFROST \
         CONFIG_MALI_CSF_SUPPORT \
         CONFIG_MALI_BIFROST_GATOR_SUPPORT \
+        CONFIG_MALI_ARBITER_SUPPORT \
         CONFIG_MALI_ARBITRATION \
         CONFIG_MALI_PARTITION_MANAGER \
         CONFIG_MALI_REAL_HW \
@@ -167,9 +170,10 @@ ifeq ($(MALI_KCONFIG_EXT_PREFIX),)
         CONFIG_MALI_CORESTACK \
         CONFIG_LARGE_PAGE_SUPPORT \
         CONFIG_MALI_PWRSOFT_765 \
+        CONFIG_MALI_MEMORY_FULLY_BACKED \
         CONFIG_MALI_JOB_DUMP \
         CONFIG_MALI_BIFROST_NO_MALI \
-        CONFIG_MALI_IS_FPGA \
+        CONFIG_MALI_BIFROST_ERROR_INJECT \
         CONFIG_MALI_HW_ERRATA_1485982_NOT_AFFECTED \
         CONFIG_MALI_HW_ERRATA_1485982_USE_CLOCK_ALTERNATIVE \
         CONFIG_MALI_PRFCNT_SET_PRIMARY \
@@ -189,7 +193,6 @@ ifeq ($(MALI_KCONFIG_EXT_PREFIX),)
         CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD
 
 
-
 endif
 
 THIS_DIR := $(dir $(lastword $(MAKEFILE_LIST)))
@@ -228,8 +231,7 @@ endif
 #
 # KBUILD_EXTRA_SYMBOLS to prevent warnings about unknown functions
 #
-BASE_SYMBOLS =
-
+BASE_SYMBOLS = $(M)/../../base/arm/Module.symvers
 
 EXTRA_SYMBOLS += \
     $(BASE_SYMBOLS)
@@ -270,8 +272,6 @@ CFLAGS_MODULE += -Wmissing-field-initializers
 CFLAGS_MODULE += -Wno-type-limits
 CFLAGS_MODULE += $(call cc-option, -Wmaybe-uninitialized)
 CFLAGS_MODULE += $(call cc-option, -Wunused-macros)
-# The following ensures the stack frame does not get larger than a page
-CFLAGS_MODULE += -Wframe-larger-than=4096
 
 KBUILD_CPPFLAGS += -DKBUILD_EXTRA_WARN2
 
diff --git a/drivers/gpu/arm/bifrost/arbiter/Kbuild b/drivers/gpu/arm/bifrost/arbiter/Kbuild
index de339ccae394..2e6b111441ca 100644
--- a/drivers/gpu/arm/bifrost/arbiter/Kbuild
+++ b/drivers/gpu/arm/bifrost/arbiter/Kbuild
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2019-2021 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -21,4 +21,3 @@
 bifrost_kbase-y += \
     arbiter/mali_kbase_arbif.o \
     arbiter/mali_kbase_arbiter_pm.o
-
diff --git a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.c b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.c
index 49b42a6ec2c0..728f013f293d 100644
--- a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.c
+++ b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -108,7 +108,6 @@ static void on_gpu_stop(struct device *dev)
 	}
 
 	KBASE_TLSTREAM_TL_ARBITER_STOP_REQUESTED(kbdev, kbdev);
-	KBASE_KTRACE_ADD(kbdev, ARB_GPU_STOP_REQUESTED, NULL, 0);
 	kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_GPU_STOP_EVT);
 }
 
@@ -134,7 +133,6 @@ static void on_gpu_granted(struct device *dev)
 	}
 
 	KBASE_TLSTREAM_TL_ARBITER_GRANTED(kbdev, kbdev);
-	KBASE_KTRACE_ADD(kbdev, ARB_GPU_GRANTED, NULL, 0);
 	kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_GPU_GRANTED_EVT);
 }
 
@@ -158,102 +156,63 @@ static void on_gpu_lost(struct device *dev)
 		dev_err(dev, "%s(): kbdev is NULL", __func__);
 		return;
 	}
-	KBASE_TLSTREAM_TL_ARBITER_LOST(kbdev, kbdev);
-	KBASE_KTRACE_ADD(kbdev, ARB_GPU_LOST, NULL, 0);
+
 	kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_GPU_LOST_EVT);
 }
 
-static int kbase_arbif_of_init(struct kbase_device *kbdev)
+/**
+ * kbase_arbif_init() - Kbase Arbiter interface initialisation.
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
+ *
+ * Initialise Kbase Arbiter interface and assign callback functions.
+ *
+ * Return:
+ * * 0			- the interface was initialized or was not specified
+ * *			in the device tree.
+ * * -EFAULT		- the interface was specified but failed to initialize.
+ * * -EPROBE_DEFER	- module dependencies are not yet available.
+ */
+int kbase_arbif_init(struct kbase_device *kbdev)
 {
+#if IS_ENABLED(CONFIG_OF)
+	struct arbiter_if_arb_vm_ops ops;
 	struct arbiter_if_dev *arb_if;
 	struct device_node *arbiter_if_node;
 	struct platform_device *pdev;
+	int err;
 
-	if (!IS_ENABLED(CONFIG_OF)) {
-		/*
-		 * Return -ENODEV in the event CONFIG_OF is not available and let the
-		 * internal AW check for suitability for arbitration.
-		 */
-		return -ENODEV;
-	}
+	dev_dbg(kbdev->dev, "%s\n", __func__);
 
-	arbiter_if_node = of_parse_phandle(kbdev->dev->of_node, "arbiter-if", 0);
-	if (!arbiter_if_node)
-		arbiter_if_node = of_parse_phandle(kbdev->dev->of_node, "arbiter_if", 0);
+	arbiter_if_node = of_parse_phandle(kbdev->dev->of_node, "arbiter_if", 0);
 	if (!arbiter_if_node) {
-		dev_dbg(kbdev->dev, "No arbiter_if in Device Tree");
+		dev_dbg(kbdev->dev, "No arbiter_if in Device Tree\n");
 		/* no arbiter interface defined in device tree */
 		kbdev->arb.arb_dev = NULL;
 		kbdev->arb.arb_if = NULL;
-		return -ENODEV;
+		return 0;
 	}
 
 	pdev = of_find_device_by_node(arbiter_if_node);
 	if (!pdev) {
-		dev_err(kbdev->dev, "Failed to find arbiter_if device");
+		dev_err(kbdev->dev, "Failed to find arbiter_if device\n");
 		return -EPROBE_DEFER;
 	}
 
 	if (!pdev->dev.driver || !try_module_get(pdev->dev.driver->owner)) {
-		dev_err(kbdev->dev, "arbiter_if driver not available");
+		dev_err(kbdev->dev, "arbiter_if driver not available\n");
 		put_device(&pdev->dev);
 		return -EPROBE_DEFER;
 	}
 	kbdev->arb.arb_dev = &pdev->dev;
 	arb_if = platform_get_drvdata(pdev);
 	if (!arb_if) {
-		dev_err(kbdev->dev, "arbiter_if driver not ready");
+		dev_err(kbdev->dev, "arbiter_if driver not ready\n");
 		module_put(pdev->dev.driver->owner);
 		put_device(&pdev->dev);
 		return -EPROBE_DEFER;
 	}
 
 	kbdev->arb.arb_if = arb_if;
-	return 0;
-}
-
-static void kbase_arbif_of_term(struct kbase_device *kbdev)
-{
-	if (!IS_ENABLED(CONFIG_OF))
-		return;
-
-	if (kbdev->arb.arb_dev) {
-		module_put(kbdev->arb.arb_dev->driver->owner);
-		put_device(kbdev->arb.arb_dev);
-	}
-	kbdev->arb.arb_dev = NULL;
-}
-
-
-/**
- * kbase_arbif_init() - Kbase Arbiter interface initialisation.
- * @kbdev: The kbase device structure for the device (must be a valid pointer)
- *
- * Initialise Kbase Arbiter interface and assign callback functions.
- *
- * Return:
- * * 0			- the interface was initialized or was not specified
- * *			in the device tree.
- * * -EFAULT		- the interface was specified but failed to initialize.
- * * -EPROBE_DEFER	- module dependencies are not yet available.
- */
-int kbase_arbif_init(struct kbase_device *kbdev)
-{
-	struct arbiter_if_arb_vm_ops ops;
-	struct arbiter_if_dev *arb_if;
-	int err = 0;
-
-	/* Tries to init with 'arbiter-if' if present in devicetree */
-	err = kbase_arbif_of_init(kbdev);
-
-	if (err == -ENODEV) {
-		/* devicetree does not support arbitration */
-		return -EPERM;
-	}
-
-	if (err)
-		return err;
-
 	ops.arb_vm_gpu_stop = on_gpu_stop;
 	ops.arb_vm_gpu_granted = on_gpu_granted;
 	ops.arb_vm_gpu_lost = on_gpu_lost;
@@ -264,35 +223,25 @@ int kbase_arbif_init(struct kbase_device *kbdev)
 	kbdev->arb.arb_freq.freq_updated = false;
 	mutex_init(&kbdev->arb.arb_freq.arb_freq_lock);
 
-	arb_if = kbdev->arb.arb_if;
-
-	if (arb_if == NULL) {
-		dev_err(kbdev->dev, "No arbiter interface present");
-		goto failure_term;
-	}
-
-	if (!arb_if->vm_ops.vm_arb_register_dev) {
-		dev_err(kbdev->dev, "arbiter_if registration callback not present");
-		goto failure_term;
-	}
-
 	/* register kbase arbiter_if callbacks */
-	err = arb_if->vm_ops.vm_arb_register_dev(arb_if, kbdev->dev, &ops);
-	if (err) {
-		dev_err(kbdev->dev, "Failed to register with arbiter. (err = %d)", err);
-		goto failure_term;
+	if (arb_if->vm_ops.vm_arb_register_dev) {
+		err = arb_if->vm_ops.vm_arb_register_dev(arb_if, kbdev->dev, &ops);
+		if (err) {
+			dev_err(&pdev->dev, "Failed to register with arbiter. (err = %d)\n", err);
+			module_put(pdev->dev.driver->owner);
+			put_device(&pdev->dev);
+			if (err != -EPROBE_DEFER)
+				err = -EFAULT;
+			return err;
+		}
 	}
 
+#else /* CONFIG_OF */
+	dev_dbg(kbdev->dev, "No arbiter without Device Tree support\n");
+	kbdev->arb.arb_dev = NULL;
+	kbdev->arb.arb_if = NULL;
+#endif
 	return 0;
-
-failure_term:
-	{
-		kbase_arbif_of_term(kbdev);
-	}
-
-	if (err != -EPROBE_DEFER)
-		err = -EFAULT;
-	return err;
 }
 
 /**
@@ -305,13 +254,16 @@ void kbase_arbif_destroy(struct kbase_device *kbdev)
 {
 	struct arbiter_if_dev *arb_if = kbdev->arb.arb_if;
 
-	if (arb_if && arb_if->vm_ops.vm_arb_unregister_dev)
+	if (arb_if && arb_if->vm_ops.vm_arb_unregister_dev) {
+		dev_dbg(kbdev->dev, "%s\n", __func__);
 		arb_if->vm_ops.vm_arb_unregister_dev(kbdev->arb.arb_if);
-
-	{
-		kbase_arbif_of_term(kbdev);
 	}
 	kbdev->arb.arb_if = NULL;
+	if (kbdev->arb.arb_dev) {
+		module_put(kbdev->arb.arb_dev->driver->owner);
+		put_device(kbdev->arb.arb_dev);
+	}
+	kbdev->arb.arb_dev = NULL;
 }
 
 /**
@@ -324,8 +276,10 @@ void kbase_arbif_get_max_config(struct kbase_device *kbdev)
 {
 	struct arbiter_if_dev *arb_if = kbdev->arb.arb_if;
 
-	if (arb_if && arb_if->vm_ops.vm_arb_get_max_config)
+	if (arb_if && arb_if->vm_ops.vm_arb_get_max_config) {
+		dev_dbg(kbdev->dev, "%s\n", __func__);
 		arb_if->vm_ops.vm_arb_get_max_config(arb_if);
+	}
 }
 
 /**
@@ -339,8 +293,8 @@ void kbase_arbif_gpu_request(struct kbase_device *kbdev)
 	struct arbiter_if_dev *arb_if = kbdev->arb.arb_if;
 
 	if (arb_if && arb_if->vm_ops.vm_arb_gpu_request) {
+		dev_dbg(kbdev->dev, "%s\n", __func__);
 		KBASE_TLSTREAM_TL_ARBITER_REQUESTED(kbdev, kbdev);
-		KBASE_KTRACE_ADD(kbdev, ARB_GPU_REQUESTED, NULL, 0);
 		arb_if->vm_ops.vm_arb_gpu_request(arb_if);
 	}
 }
@@ -356,12 +310,10 @@ void kbase_arbif_gpu_stopped(struct kbase_device *kbdev, u8 gpu_required)
 	struct arbiter_if_dev *arb_if = kbdev->arb.arb_if;
 
 	if (arb_if && arb_if->vm_ops.vm_arb_gpu_stopped) {
+		dev_dbg(kbdev->dev, "%s\n", __func__);
 		KBASE_TLSTREAM_TL_ARBITER_STOPPED(kbdev, kbdev);
-		KBASE_KTRACE_ADD(kbdev, ARB_GPU_STOPPED, NULL, 0);
-		if (gpu_required) {
+		if (gpu_required)
 			KBASE_TLSTREAM_TL_ARBITER_REQUESTED(kbdev, kbdev);
-			KBASE_KTRACE_ADD(kbdev, ARB_GPU_REQUESTED, NULL, 0);
-		}
 		arb_if->vm_ops.vm_arb_gpu_stopped(arb_if, gpu_required);
 	}
 }
@@ -376,8 +328,10 @@ void kbase_arbif_gpu_active(struct kbase_device *kbdev)
 {
 	struct arbiter_if_dev *arb_if = kbdev->arb.arb_if;
 
-	if (arb_if && arb_if->vm_ops.vm_arb_gpu_active)
+	if (arb_if && arb_if->vm_ops.vm_arb_gpu_active) {
+		dev_dbg(kbdev->dev, "%s\n", __func__);
 		arb_if->vm_ops.vm_arb_gpu_active(arb_if);
+	}
 }
 
 /**
@@ -390,6 +344,8 @@ void kbase_arbif_gpu_idle(struct kbase_device *kbdev)
 {
 	struct arbiter_if_dev *arb_if = kbdev->arb.arb_if;
 
-	if (arb_if && arb_if->vm_ops.vm_arb_gpu_idle)
+	if (arb_if && arb_if->vm_ops.vm_arb_gpu_idle) {
+		dev_dbg(kbdev->dev, "vm_arb_gpu_idle\n");
 		arb_if->vm_ops.vm_arb_gpu_idle(arb_if);
+	}
 }
diff --git a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.h b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.h
index c77792115e4d..701ffd42f6f7 100644
--- a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.h
+++ b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2021 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -50,7 +50,6 @@ enum kbase_arbif_evt {
 	KBASE_VM_OS_RESUME_EVENT,
 };
 
-
 /**
  * kbase_arbif_init() - Initialize the arbiter interface functionality.
  * @kbdev: The kbase device structure for the device (must be a valid pointer)
diff --git a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.c b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.c
index 9b8551609dc7..4498b469300e 100644
--- a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.c
+++ b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -48,7 +48,7 @@ MODULE_PARM_DESC(
 	"On a virtualized platform, if the GPU is not granted within this time(ms) kbase will defer the probe");
 
 static void kbase_arbiter_pm_vm_wait_gpu_assignment(struct kbase_device *kbdev);
-static inline bool kbase_arbiter_pm_vm_gpu_assigned_locked(struct kbase_device *kbdev);
+static inline bool kbase_arbiter_pm_vm_gpu_assigned_lockheld(struct kbase_device *kbdev);
 
 /**
  * kbase_arbiter_pm_vm_state_str() - Helper function to get string
@@ -85,6 +85,7 @@ static inline const char *kbase_arbiter_pm_vm_state_str(enum kbase_vm_state stat
 	case KBASE_VM_STATE_SUSPEND_WAIT_FOR_GRANT:
 		return "KBASE_VM_STATE_SUSPEND_WAIT_FOR_GRANT";
 	default:
+		KBASE_DEBUG_ASSERT(false);
 		return "[UnknownState]";
 	}
 }
@@ -116,13 +117,14 @@ static inline const char *kbase_arbiter_pm_vm_event_str(enum kbase_arbif_evt evt
 	case KBASE_VM_REF_EVENT:
 		return "KBASE_VM_REF_EVENT";
 	default:
+		KBASE_DEBUG_ASSERT(false);
 		return "[UnknownEvent]";
 	}
 }
 
 /**
  * kbase_arbiter_pm_vm_set_state() - Sets new kbase_arbiter_vm_state
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  * @new_state: kbase VM new state
  *
  * This function sets the new state for the VM
@@ -199,7 +201,6 @@ static void kbase_arbiter_pm_resume_wq(struct work_struct *data)
 	arb_vm_state->vm_arb_starting = false;
 	mutex_unlock(&arb_vm_state->vm_state_lock);
 	KBASE_TLSTREAM_TL_ARBITER_STARTED(kbdev, kbdev);
-	KBASE_KTRACE_ADD(kbdev, ARB_GPU_STARTED, NULL, 0);
 	dev_dbg(kbdev->dev, "<%s\n", __func__);
 }
 
@@ -228,7 +229,7 @@ static enum hrtimer_restart request_timer_callback(struct hrtimer *timer)
 
 /**
  * start_request_timer() - Start a timer after requesting GPU
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Start a timer to track when kbase is waiting for the GPU from the
  * Arbiter.  If the timer expires before GPU is granted, a warning in
@@ -244,7 +245,7 @@ static void start_request_timer(struct kbase_device *kbdev)
 
 /**
  * cancel_request_timer() - Stop the request timer
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Stops the request timer once GPU has been granted.  Safe to call
  * even if timer is no longer running.
@@ -259,7 +260,7 @@ static void cancel_request_timer(struct kbase_device *kbdev)
 /**
  * kbase_arbiter_pm_early_init() - Initialize arbiter for VM
  *                                 Paravirtualized use.
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Initialize the arbiter and other required resources during the runtime
  * and request the GPU for the VM for the first time.
@@ -271,7 +272,7 @@ int kbase_arbiter_pm_early_init(struct kbase_device *kbdev)
 	int err;
 	struct kbase_arbiter_vm_state *arb_vm_state = NULL;
 
-	arb_vm_state = kzalloc(sizeof(struct kbase_arbiter_vm_state), GFP_KERNEL);
+	arb_vm_state = kmalloc(sizeof(struct kbase_arbiter_vm_state), GFP_KERNEL);
 	if (arb_vm_state == NULL)
 		return -ENOMEM;
 
@@ -296,23 +297,21 @@ int kbase_arbiter_pm_early_init(struct kbase_device *kbdev)
 
 	err = kbase_arbif_init(kbdev);
 	if (err) {
-		if (err != -EPERM)
-			dev_err(kbdev->dev, "Failed to initialise arbif module. (err = %d)", err);
-
+		dev_err(kbdev->dev, "Failed to initialise arbif module. (err = %d)\n", err);
 		goto arbif_init_fail;
 	}
 
-	if (kbase_has_arbiter(kbdev)) {
+	if (kbdev->arb.arb_if) {
 		kbase_arbif_gpu_request(kbdev);
 		dev_dbg(kbdev->dev, "Waiting for initial GPU assignment...\n");
 
 		err = wait_event_timeout(arb_vm_state->vm_state_wait,
 					 arb_vm_state->vm_state ==
 						 KBASE_VM_STATE_INITIALIZING_WITH_GPU,
-					 msecs_to_jiffies((unsigned int)gpu_req_timeout));
+					 msecs_to_jiffies(gpu_req_timeout));
 
 		if (!err) {
-			dev_err(kbdev->dev,
+			dev_dbg(kbdev->dev,
 				"Kbase probe Deferred after waiting %d ms to receive GPU_GRANT\n",
 				gpu_req_timeout);
 
@@ -337,7 +336,7 @@ int kbase_arbiter_pm_early_init(struct kbase_device *kbdev)
 
 /**
  * kbase_arbiter_pm_early_term() - Shutdown arbiter and free resources
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Clean up all the resources
  */
@@ -345,14 +344,6 @@ void kbase_arbiter_pm_early_term(struct kbase_device *kbdev)
 {
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
 
-	if (arb_vm_state == NULL)
-		return;
-
-	if (!kbase_has_arbiter(kbdev))
-		return;
-
-	kbase_arbiter_pm_release_interrupts(kbdev);
-
 	cancel_request_timer(kbdev);
 	mutex_lock(&arb_vm_state->vm_state_lock);
 	if (arb_vm_state->vm_state > KBASE_VM_STATE_STOPPED_GPU_REQUESTED) {
@@ -367,6 +358,12 @@ void kbase_arbiter_pm_early_term(struct kbase_device *kbdev)
 	kbdev->pm.arb_vm_state = NULL;
 }
 
+/**
+ * kbase_arbiter_pm_release_interrupts() - Release the GPU interrupts
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
+ *
+ * Releases interrupts and set the interrupt flag to false
+ */
 void kbase_arbiter_pm_release_interrupts(struct kbase_device *kbdev)
 {
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
@@ -379,25 +376,29 @@ void kbase_arbiter_pm_release_interrupts(struct kbase_device *kbdev)
 	mutex_unlock(&arb_vm_state->vm_state_lock);
 }
 
+/**
+ * kbase_arbiter_pm_install_interrupts() - Install the GPU interrupts
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
+ *
+ * Install interrupts and set the interrupt_install flag to true.
+ *
+ * Return: 0 if success, or a Linux error code
+ */
 int kbase_arbiter_pm_install_interrupts(struct kbase_device *kbdev)
 {
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
-	int err = 0;
+	int err;
 
 	mutex_lock(&arb_vm_state->vm_state_lock);
-	if (arb_vm_state->interrupts_installed == false) {
-		arb_vm_state->interrupts_installed = true;
-		err = kbase_install_interrupts(kbdev);
-	} else {
-		dev_dbg(kbdev->dev, "%s: interrupts installed already", __func__);
-	}
+	arb_vm_state->interrupts_installed = true;
+	err = kbase_install_interrupts(kbdev);
 	mutex_unlock(&arb_vm_state->vm_state_lock);
 	return err;
 }
 
 /**
  * kbase_arbiter_pm_vm_stopped() - Handle stop state for the VM
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Handles a stop state for the VM
  */
@@ -415,13 +416,7 @@ void kbase_arbiter_pm_vm_stopped(struct kbase_device *kbdev)
 	dev_dbg(kbdev->dev, "%s %s\n", __func__,
 		kbase_arbiter_pm_vm_state_str(arb_vm_state->vm_state));
 
-	/*
-	 * Release the interrupts on external arb_if to address Xen requirements.
-	 * Interrupts are not released with internal arb_if as the IRQs are required
-	 * to handle messaging to/from Arbiter/Resource Group.
-	 */
-	if (arb_vm_state->interrupts_installed
-	) {
+	if (arb_vm_state->interrupts_installed) {
 		arb_vm_state->interrupts_installed = false;
 		kbase_release_interrupts(kbdev);
 	}
@@ -481,12 +476,6 @@ int kbase_arbiter_pm_gpu_assigned(struct kbase_device *kbdev)
 	if (!kbdev)
 		return result;
 
-	/* If there is no Arbiter, then there is no virtualization
-	 * and current VM always has access to GPU.
-	 */
-	if (!kbase_has_arbiter(kbdev))
-		return 1;
-
 	/* First check the GPU_LOST state */
 	kbase_pm_lock(kbdev);
 	if (kbase_pm_is_gpu_lost(kbdev)) {
@@ -518,7 +507,7 @@ int kbase_arbiter_pm_gpu_assigned(struct kbase_device *kbdev)
 
 /**
  * kbase_arbiter_pm_vm_gpu_start() - Handles the start state of the VM
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Handles the start state of the VM
  */
@@ -543,15 +532,7 @@ static void kbase_arbiter_pm_vm_gpu_start(struct kbase_device *kbdev)
 	case KBASE_VM_STATE_STOPPED_GPU_REQUESTED:
 		kbase_arbiter_pm_vm_set_state(kbdev, KBASE_VM_STATE_STARTING);
 		arb_vm_state->interrupts_installed = true;
-		/*
-		 * Re-install interrupts that were released for external arb_if to
-		 * address Xen requirements. Interrupts are not released with internal
-		 * arb_if as the IRQs are required to handle messaging to/from
-		 * Arbiter/Resource Group.
-		 */
-		{
-			kbase_install_interrupts(kbdev);
-		}
+		kbase_install_interrupts(kbdev);
 		/*
 		 * GPU GRANTED received while in stop can be a result of a
 		 * repartitioning.
@@ -580,7 +561,7 @@ static void kbase_arbiter_pm_vm_gpu_start(struct kbase_device *kbdev)
 
 /**
  * kbase_arbiter_pm_vm_gpu_stop() - Handles the stop state of the VM
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Handles the start state of the VM
  */
@@ -622,7 +603,7 @@ static void kbase_arbiter_pm_vm_gpu_stop(struct kbase_device *kbdev)
 
 /**
  * kbase_gpu_lost() - Kbase signals GPU is lost on a lost event signal
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * On GPU lost event signals GPU_LOST to the aribiter
  */
@@ -677,7 +658,7 @@ static void kbase_gpu_lost(struct kbase_device *kbdev)
 /**
  * kbase_arbiter_pm_vm_os_suspend_ready_state() - checks if VM is ready
  *			to be moved to suspended state.
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Return: True if its ready to be suspended else False.
  */
@@ -697,10 +678,10 @@ static inline bool kbase_arbiter_pm_vm_os_suspend_ready_state(struct kbase_devic
 /**
  * kbase_arbiter_pm_vm_os_prepare_suspend() - Prepare OS to be in suspend state
  *                             until it receives the grant message from arbiter
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Prepares OS to be in suspend state until it receives GRANT message
- * from Arbiter asynchronously. This function assumes there is an active Arbiter.
+ * from Arbiter asynchronously.
  */
 static void kbase_arbiter_pm_vm_os_prepare_suspend(struct kbase_device *kbdev)
 {
@@ -708,8 +689,10 @@ static void kbase_arbiter_pm_vm_os_prepare_suspend(struct kbase_device *kbdev)
 	enum kbase_vm_state prev_state;
 
 	lockdep_assert_held(&arb_vm_state->vm_state_lock);
-	if (kbdev->pm.arb_vm_state->vm_state == KBASE_VM_STATE_SUSPENDED)
-		return;
+	if (kbdev->arb.arb_if) {
+		if (kbdev->pm.arb_vm_state->vm_state == KBASE_VM_STATE_SUSPENDED)
+			return;
+	}
 	/* Block suspend OS function until we are in a stable state
 	 * with vm_state_lock
 	 */
@@ -762,7 +745,7 @@ static void kbase_arbiter_pm_vm_os_prepare_suspend(struct kbase_device *kbdev)
 /**
  * kbase_arbiter_pm_vm_os_resume() - Resume OS function once it receives
  *                                   a grant message from arbiter
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Resume OS function once it receives GRANT message
  * from Arbiter asynchronously.
@@ -791,7 +774,7 @@ static void kbase_arbiter_pm_vm_os_resume(struct kbase_device *kbdev)
 
 /**
  * kbase_arbiter_pm_vm_event() - Dispatch VM event to the state machine.
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  * @evt: VM event
  *
  * The state machine function. Receives events and transitions states
@@ -801,7 +784,7 @@ void kbase_arbiter_pm_vm_event(struct kbase_device *kbdev, enum kbase_arbif_evt
 {
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
 
-	if (!kbase_has_arbiter(kbdev))
+	if (!kbdev->arb.arb_if)
 		return;
 
 	mutex_lock(&arb_vm_state->vm_state_lock);
@@ -870,7 +853,7 @@ void kbase_arbiter_pm_vm_event(struct kbase_device *kbdev, enum kbase_arbif_evt
 		break;
 
 	default:
-		dev_err(kbdev->dev, "Got Unknown Event!");
+		dev_alert(kbdev->dev, "Got Unknown Event!");
 		break;
 	}
 	mutex_unlock(&arb_vm_state->vm_state_lock);
@@ -880,7 +863,7 @@ KBASE_EXPORT_TEST_API(kbase_arbiter_pm_vm_event);
 
 /**
  * kbase_arbiter_pm_vm_wait_gpu_assignment() - VM wait for a GPU assignment.
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * VM waits for a GPU assignment.
  */
@@ -896,14 +879,14 @@ static void kbase_arbiter_pm_vm_wait_gpu_assignment(struct kbase_device *kbdev)
 }
 
 /**
- * kbase_arbiter_pm_vm_gpu_assigned_locked() - Check if VM holds VM state lock
- * @kbdev: The kbase device structure for the device
+ * kbase_arbiter_pm_vm_gpu_assigned_lockheld() - Check if VM holds VM state lock
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Checks if the virtual machine holds VM state lock.
  *
  * Return: true if GPU is assigned, else false.
  */
-static inline bool kbase_arbiter_pm_vm_gpu_assigned_locked(struct kbase_device *kbdev)
+static inline bool kbase_arbiter_pm_vm_gpu_assigned_lockheld(struct kbase_device *kbdev)
 {
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
 
@@ -915,14 +898,13 @@ static inline bool kbase_arbiter_pm_vm_gpu_assigned_locked(struct kbase_device *
 /**
  * kbase_arbiter_pm_ctx_active_handle_suspend() - Handle suspend operation for
  *                                                arbitration mode
- * @kbdev: The kbase device structure for the device
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
  * @suspend_handler: The handler code for how to handle a suspend
  *                   that might occur
  *
  * This function handles a suspend event from the driver,
  * communicating with the arbiter and waiting synchronously for the GPU
- * to be granted again depending on the VM state. Returns immediately
- * with success if there is no Arbiter.
+ * to be granted again depending on the VM state.
  *
  * Return: 0 on success else 1 suspend handler isn not possible.
  */
@@ -932,58 +914,58 @@ int kbase_arbiter_pm_ctx_active_handle_suspend(struct kbase_device *kbdev,
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
 	int res = 0;
 
-	if (!kbase_has_arbiter(kbdev))
-		return res;
+	if (kbdev->arb.arb_if) {
+		mutex_lock(&arb_vm_state->vm_state_lock);
+		while (!kbase_arbiter_pm_vm_gpu_assigned_lockheld(kbdev)) {
+			/* Update VM state since we have GPU work to do */
+			if (arb_vm_state->vm_state == KBASE_VM_STATE_STOPPING_IDLE)
+				kbase_arbiter_pm_vm_set_state(kbdev,
+							      KBASE_VM_STATE_STOPPING_ACTIVE);
+			else if (arb_vm_state->vm_state == KBASE_VM_STATE_STOPPED) {
+				kbase_arbiter_pm_vm_set_state(kbdev,
+							      KBASE_VM_STATE_STOPPED_GPU_REQUESTED);
+				kbase_arbif_gpu_request(kbdev);
+				start_request_timer(kbdev);
+			} else if (arb_vm_state->vm_state == KBASE_VM_STATE_INITIALIZING_WITH_GPU)
+				break;
 
-	mutex_lock(&arb_vm_state->vm_state_lock);
-	while (!kbase_arbiter_pm_vm_gpu_assigned_locked(kbdev)) {
-		/* Update VM state since we have GPU work to do */
-		if (arb_vm_state->vm_state == KBASE_VM_STATE_STOPPING_IDLE)
-			kbase_arbiter_pm_vm_set_state(kbdev, KBASE_VM_STATE_STOPPING_ACTIVE);
-		else if (arb_vm_state->vm_state == KBASE_VM_STATE_STOPPED) {
-			kbase_arbiter_pm_vm_set_state(kbdev, KBASE_VM_STATE_STOPPED_GPU_REQUESTED);
-			kbase_arbif_gpu_request(kbdev);
-			start_request_timer(kbdev);
-		} else if (arb_vm_state->vm_state == KBASE_VM_STATE_INITIALIZING_WITH_GPU)
-			break;
+			if (suspend_handler != KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE) {
+				/* In case of GPU lost, even if
+				 * active_count > 0, we no longer have GPU
+				 * access
+				 */
+				if (kbase_pm_is_gpu_lost(kbdev))
+					res = 1;
 
-		if (suspend_handler != KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE) {
-			/* In case of GPU lost, even if
-			 * active_count > 0, we no longer have GPU
-			 * access
-			 */
-			if (kbase_pm_is_gpu_lost(kbdev))
-				res = 1;
-
-			switch (suspend_handler) {
-			case KBASE_PM_SUSPEND_HANDLER_DONT_INCREASE:
-				res = 1;
-				break;
-			case KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE:
-				if (kbdev->pm.active_count == 0)
+				switch (suspend_handler) {
+				case KBASE_PM_SUSPEND_HANDLER_DONT_INCREASE:
 					res = 1;
-				break;
-			case KBASE_PM_SUSPEND_HANDLER_VM_GPU_GRANTED:
-				break;
-			default:
-				WARN(1, "Unknown suspend_handler\n");
-				res = 1;
+					break;
+				case KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE:
+					if (kbdev->pm.active_count == 0)
+						res = 1;
+					break;
+				case KBASE_PM_SUSPEND_HANDLER_VM_GPU_GRANTED:
+					break;
+				default:
+					WARN(1, "Unknown suspend_handler\n");
+					res = 1;
+					break;
+				}
 				break;
 			}
-			break;
-		}
 
-		/* Need to synchronously wait for GPU assignment */
-		atomic_inc(&kbdev->pm.gpu_users_waiting);
+			/* Need to synchronously wait for GPU assignment */
+			atomic_inc(&kbdev->pm.gpu_users_waiting);
+			mutex_unlock(&arb_vm_state->vm_state_lock);
+			kbase_pm_unlock(kbdev);
+			kbase_arbiter_pm_vm_wait_gpu_assignment(kbdev);
+			kbase_pm_lock(kbdev);
+			mutex_lock(&arb_vm_state->vm_state_lock);
+			atomic_dec(&kbdev->pm.gpu_users_waiting);
+		}
 		mutex_unlock(&arb_vm_state->vm_state_lock);
-		kbase_pm_unlock(kbdev);
-		kbase_arbiter_pm_vm_wait_gpu_assignment(kbdev);
-		kbase_pm_lock(kbdev);
-		mutex_lock(&arb_vm_state->vm_state_lock);
-		atomic_dec(&kbdev->pm.gpu_users_waiting);
 	}
-	mutex_unlock(&arb_vm_state->vm_state_lock);
-
 	return res;
 }
 
diff --git a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.h b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.h
index 649f488d4f67..3734d32b6e2b 100644
--- a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.h
+++ b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -102,7 +102,7 @@ void kbase_arbiter_pm_release_interrupts(struct kbase_device *kbdev);
  *
  * Install interrupts and set the interrupt_install flag to true.
  *
- * Return: 0 if success or already installed. Otherwise a Linux error code
+ * Return: 0 if success, or a Linux error code
  */
 int kbase_arbiter_pm_install_interrupts(struct kbase_device *kbdev);
 
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/Kbuild b/drivers/gpu/arm/bifrost/backend/gpu/Kbuild
index ffec0417aa5c..c3db14217c6d 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/Kbuild
+++ b/drivers/gpu/arm/bifrost/backend/gpu/Kbuild
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2014-2022 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -47,7 +47,12 @@ endif
 bifrost_kbase-$(CONFIG_MALI_BIFROST_DEVFREQ) += \
     backend/gpu/mali_kbase_devfreq.o
 
-bifrost_kbase-$(CONFIG_MALI_BIFROST_NO_MALI) += backend/gpu/mali_kbase_model_linux.o
+ifneq ($(CONFIG_MALI_REAL_HW),y)
+    bifrost_kbase-y += backend/gpu/mali_kbase_model_linux.o
+endif
 
 # NO_MALI Dummy model interface
 bifrost_kbase-$(CONFIG_MALI_BIFROST_NO_MALI) += backend/gpu/mali_kbase_model_dummy.o
+# HW error simulation
+bifrost_kbase-$(CONFIG_MALI_BIFROST_NO_MALI) += backend/gpu/mali_kbase_model_error_generator.o
+
diff --git a/drivers/gpu/arm/bifrost/tests/include/kutf/kutf_kprobe.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_backend_config.h
similarity index 63%
rename from drivers/gpu/arm/bifrost/tests/include/kutf/kutf_kprobe.h
rename to drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_backend_config.h
index d8c3ca88166b..f0368c2e59b9 100644
--- a/drivers/gpu/arm/bifrost/tests/include/kutf/kutf_kprobe.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_backend_config.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -19,19 +19,11 @@
  *
  */
 
-#ifndef _KUTF_KPROBE_H_
-#define _KUTF_KPROBE_H_
-
-struct dentry;
-
-int kutf_kprobe_init(struct dentry *base_dir);
-void kutf_kprobe_exit(void);
-
-typedef void (*kutf_kp_handler)(int argc, char **argv);
-
-void kutf_kp_sample_handler(int argc, char **argv);
-void kutf_kp_sample_kernel_function(void);
+/*
+ * Backend specific configuration
+ */
 
-void kutf_kp_delay_handler(int argc, char **argv);
+#ifndef _KBASE_BACKEND_CONFIG_H_
+#define _KBASE_BACKEND_CONFIG_H_
 
-#endif /* _KUTF_KPROBE_H_ */
+#endif /* _KBASE_BACKEND_CONFIG_H_ */
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_cache_policy_backend.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_cache_policy_backend.h
index 7317f1449d44..d95aa37d8950 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_cache_policy_backend.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_cache_policy_backend.h
@@ -22,9 +22,8 @@
 #ifndef _KBASE_CACHE_POLICY_BACKEND_H_
 #define _KBASE_CACHE_POLICY_BACKEND_H_
 
-#include <linux/types.h>
-
-struct kbase_device;
+#include "mali_kbase.h"
+#include <uapi/gpu/arm/bifrost/mali_base_kernel.h>
 
 /**
  * kbase_cache_set_coherency_mode() - Sets the system coherency mode
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.c
index 851e6feafd30..e47dd440bff2 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -50,22 +50,14 @@ static struct kbase_clk_rate_trace_op_conf *
 get_clk_rate_trace_callbacks(__maybe_unused struct kbase_device *kbdev)
 {
 	/* base case */
-	const void *arbiter_if_node;
 	struct kbase_clk_rate_trace_op_conf *callbacks =
 		(struct kbase_clk_rate_trace_op_conf *)CLK_RATE_TRACE_OPS;
-
-	/* Nothing left to do here if there is no Arbiter/virtualization or if
-	 * CONFIG_OF is not enabled.
-	 */
-	if (!IS_ENABLED(CONFIG_OF))
-		return callbacks;
+#if defined(CONFIG_MALI_ARBITER_SUPPORT) && defined(CONFIG_OF)
+	const void *arbiter_if_node;
 
 	if (WARN_ON(!kbdev) || WARN_ON(!kbdev->dev))
 		return callbacks;
 
-	if (!kbase_has_arbiter(kbdev))
-		return callbacks;
-
 	arbiter_if_node = of_get_property(kbdev->dev->of_node, "arbiter-if", NULL);
 	if (!arbiter_if_node)
 		arbiter_if_node = of_get_property(kbdev->dev->of_node, "arbiter_if", NULL);
@@ -77,6 +69,8 @@ get_clk_rate_trace_callbacks(__maybe_unused struct kbase_device *kbdev)
 		dev_dbg(kbdev->dev,
 			"Arbitration supported but disabled by platform. Leaving clk rate callbacks as default.\n");
 
+#endif
+
 	return callbacks;
 }
 
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_debug_job_fault_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_debug_job_fault_backend.c
index af8d1e3af87c..bd3622e7d44b 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_debug_job_fault_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_debug_job_fault_backend.c
@@ -27,38 +27,34 @@
 #if IS_ENABLED(CONFIG_DEBUG_FS)
 
 /*GPU_CONTROL_REG(r)*/
-static unsigned int gpu_control_reg_snapshot[] = { GPU_CONTROL_ENUM(GPU_ID),
-						   GPU_CONTROL_ENUM(SHADER_READY),
-						   GPU_CONTROL_ENUM(TILER_READY),
-						   GPU_CONTROL_ENUM(L2_READY) };
+static int gpu_control_reg_snapshot[] = { GPU_CONTROL_ENUM(GPU_ID), GPU_CONTROL_ENUM(SHADER_READY),
+					  GPU_CONTROL_ENUM(TILER_READY),
+					  GPU_CONTROL_ENUM(L2_READY) };
 
 /* JOB_CONTROL_REG(r) */
-static unsigned int job_control_reg_snapshot[] = { JOB_CONTROL_ENUM(JOB_IRQ_MASK),
-						   JOB_CONTROL_ENUM(JOB_IRQ_STATUS) };
+static int job_control_reg_snapshot[] = { JOB_CONTROL_ENUM(JOB_IRQ_MASK),
+					  JOB_CONTROL_ENUM(JOB_IRQ_STATUS) };
 
 /* JOB_SLOT_REG(n,r) */
-static unsigned int job_slot_reg_snapshot[] = {
-	JOB_SLOT_ENUM(0, HEAD) - JOB_SLOT_BASE_ENUM(0),
-	JOB_SLOT_ENUM(0, TAIL) - JOB_SLOT_BASE_ENUM(0),
-	JOB_SLOT_ENUM(0, AFFINITY) - JOB_SLOT_BASE_ENUM(0),
-	JOB_SLOT_ENUM(0, CONFIG) - JOB_SLOT_BASE_ENUM(0),
-	JOB_SLOT_ENUM(0, STATUS) - JOB_SLOT_BASE_ENUM(0),
-	JOB_SLOT_ENUM(0, HEAD_NEXT) - JOB_SLOT_BASE_ENUM(0),
-	JOB_SLOT_ENUM(0, AFFINITY_NEXT) - JOB_SLOT_BASE_ENUM(0),
-	JOB_SLOT_ENUM(0, CONFIG_NEXT) - JOB_SLOT_BASE_ENUM(0)
-};
+static int job_slot_reg_snapshot[] = { JOB_SLOT_ENUM(0, HEAD) - JOB_SLOT_BASE_ENUM(0),
+				       JOB_SLOT_ENUM(0, TAIL) - JOB_SLOT_BASE_ENUM(0),
+				       JOB_SLOT_ENUM(0, AFFINITY) - JOB_SLOT_BASE_ENUM(0),
+				       JOB_SLOT_ENUM(0, CONFIG) - JOB_SLOT_BASE_ENUM(0),
+				       JOB_SLOT_ENUM(0, STATUS) - JOB_SLOT_BASE_ENUM(0),
+				       JOB_SLOT_ENUM(0, HEAD_NEXT) - JOB_SLOT_BASE_ENUM(0),
+				       JOB_SLOT_ENUM(0, AFFINITY_NEXT) - JOB_SLOT_BASE_ENUM(0),
+				       JOB_SLOT_ENUM(0, CONFIG_NEXT) - JOB_SLOT_BASE_ENUM(0) };
 
 /*MMU_CONTROL_REG(r)*/
-static unsigned int mmu_reg_snapshot[] = { MMU_CONTROL_ENUM(IRQ_MASK),
-					   MMU_CONTROL_ENUM(IRQ_STATUS) };
+static int mmu_reg_snapshot[] = { MMU_CONTROL_ENUM(IRQ_MASK), MMU_CONTROL_ENUM(IRQ_STATUS) };
 
 /* MMU_AS_REG(n,r) */
-static unsigned int as_reg_snapshot[] = { MMU_AS_ENUM(0, TRANSTAB) - MMU_AS_BASE_ENUM(0),
-					  MMU_AS_ENUM(0, TRANSCFG) - MMU_AS_BASE_ENUM(0),
-					  MMU_AS_ENUM(0, MEMATTR) - MMU_AS_BASE_ENUM(0),
-					  MMU_AS_ENUM(0, FAULTSTATUS) - MMU_AS_BASE_ENUM(0),
-					  MMU_AS_ENUM(0, FAULTADDRESS) - MMU_AS_BASE_ENUM(0),
-					  MMU_AS_ENUM(0, STATUS) - MMU_AS_BASE_ENUM(0) };
+static int as_reg_snapshot[] = { MMU_AS_ENUM(0, TRANSTAB) - MMU_AS_BASE_ENUM(0),
+				 MMU_AS_ENUM(0, TRANSCFG) - MMU_AS_BASE_ENUM(0),
+				 MMU_AS_ENUM(0, MEMATTR) - MMU_AS_BASE_ENUM(0),
+				 MMU_AS_ENUM(0, FAULTSTATUS) - MMU_AS_BASE_ENUM(0),
+				 MMU_AS_ENUM(0, FAULTADDRESS) - MMU_AS_BASE_ENUM(0),
+				 MMU_AS_ENUM(0, STATUS) - MMU_AS_BASE_ENUM(0) };
 
 bool kbase_debug_job_fault_reg_snapshot_init(struct kbase_context *kctx, int reg_range)
 {
@@ -136,7 +132,7 @@ bool kbase_debug_job_fault_reg_snapshot_init(struct kbase_context *kctx, int reg
 bool kbase_job_fault_get_reg_snapshot(struct kbase_context *kctx)
 {
 	int offset = 0;
-	u32 reg_enum;
+	int reg_enum;
 	u64 val64;
 
 	if (kctx->reg_dump == NULL)
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_devfreq.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_devfreq.c
index e223535d01f7..0712439e37a5 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_devfreq.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_devfreq.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -197,7 +197,7 @@ static int kbase_devfreq_status(struct device *dev, struct devfreq_dev_status *s
 static int kbase_devfreq_init_freq_table(struct kbase_device *kbdev, struct devfreq_dev_profile *dp)
 {
 	int count;
-	unsigned int i = 0;
+	int i = 0;
 	unsigned long freq;
 	struct dev_pm_opp *opp;
 
@@ -211,14 +211,14 @@ static int kbase_devfreq_init_freq_table(struct kbase_device *kbdev, struct devf
 	if (count < 0)
 		return count;
 
-	dp->freq_table = kmalloc_array((size_t)count, sizeof(dp->freq_table[0]), GFP_KERNEL);
+	dp->freq_table = kmalloc_array(count, sizeof(dp->freq_table[0]), GFP_KERNEL);
 	if (!dp->freq_table)
 		return -ENOMEM;
 
 #if KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE
 	rcu_read_lock();
 #endif
-	for (i = 0, freq = ULONG_MAX; i < (unsigned int)count; i++, freq--) {
+	for (i = 0, freq = ULONG_MAX; i < count; i++, freq--) {
 		opp = dev_pm_opp_find_freq_floor(kbdev->dev, &freq);
 		if (IS_ERR(opp))
 			break;
@@ -232,8 +232,8 @@ static int kbase_devfreq_init_freq_table(struct kbase_device *kbdev, struct devf
 	rcu_read_unlock();
 #endif
 
-	if ((unsigned int)count != i)
-		dev_warn(kbdev->dev, "Unable to enumerate all OPPs (%d!=%u\n", count, i);
+	if (count != i)
+		dev_warn(kbdev->dev, "Unable to enumerate all OPPs (%d!=%d\n", count, i);
 
 	dp->max_state = i;
 
@@ -335,8 +335,7 @@ static int kbase_devfreq_init_core_mask_table(struct kbase_device *kbdev)
 		return 0;
 
 	count = dev_pm_opp_get_opp_count(kbdev->dev);
-	kbdev->devfreq_table =
-		kmalloc_array((size_t)count, sizeof(struct kbase_devfreq_opp), GFP_KERNEL);
+	kbdev->devfreq_table = kmalloc_array(count, sizeof(struct kbase_devfreq_opp), GFP_KERNEL);
 	if (!kbdev->devfreq_table)
 		return -ENOMEM;
 
@@ -366,7 +365,7 @@ static int kbase_devfreq_init_core_mask_table(struct kbase_device *kbdev)
 		err = of_property_read_u64(node, "opp-hz-real", real_freqs);
 #endif
 		if (err < 0) {
-			dev_warn(kbdev->dev, "Failed to read opp-hz-real property with error %d",
+			dev_warn(kbdev->dev, "Failed to read opp-hz-real property with error %d\n",
 				 err);
 			continue;
 		}
@@ -374,8 +373,8 @@ static int kbase_devfreq_init_core_mask_table(struct kbase_device *kbdev)
 		err = of_property_read_u32_array(node, "opp-microvolt", opp_volts,
 						 kbdev->nr_regulators);
 		if (err < 0) {
-			dev_warn(kbdev->dev, "Failed to read opp-microvolt property with error %d",
-				 err);
+			dev_warn(kbdev->dev,
+				 "Failed to read opp-microvolt property with error %d\n", err);
 			continue;
 		}
 #endif
@@ -386,12 +385,11 @@ static int kbase_devfreq_init_core_mask_table(struct kbase_device *kbdev)
 
 			dev_warn(
 				kbdev->dev,
-				"Ignoring OPP %llu - Dynamic Core Scaling not supported on this GPU",
+				"Ignoring OPP %llu - Dynamic Core Scaling not supported on this GPU\n",
 				opp_freq);
 			continue;
 		}
 
-
 		core_count_p = of_get_property(node, "opp-core-count", NULL);
 		if (core_count_p) {
 			u64 remaining_core_mask = kbdev->gpu_props.shader_present;
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_gpuprops_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_gpuprops_backend.c
index a9b629ad7ea5..443a1466e0e0 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_gpuprops_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_gpuprops_backend.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -30,7 +30,7 @@
 
 int kbase_backend_gpuprops_get(struct kbase_device *kbdev, struct kbasep_gpuprops_regdump *regdump)
 {
-	uint i;
+	int i;
 
 	/* regdump is zero intiialized, individual entries do not need to be explicitly set */
 	regdump->gpu_id = KBASE_REG_READ(kbdev, GPU_CONTROL_ENUM(GPU_ID));
@@ -48,7 +48,7 @@ int kbase_backend_gpuprops_get(struct kbase_device *kbdev, struct kbasep_gpuprop
 	/* Not a valid register on TMIX */
 
 	/* TGOx specific register */
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_THREAD_TLS_ALLOC))
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_THREAD_TLS_ALLOC))
 		regdump->thread_tls_alloc =
 			kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(THREAD_TLS_ALLOC));
 #endif /* !MALI_USE_CSF */
@@ -64,7 +64,7 @@ int kbase_backend_gpuprops_get(struct kbase_device *kbdev, struct kbasep_gpuprop
 	/* AMBA_FEATURES enum is mapped to COHERENCY_FEATURES enum */
 	regdump->coherency_features = KBASE_REG_READ(kbdev, GPU_CONTROL_ENUM(COHERENCY_FEATURES));
 
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_CORE_FEATURES))
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_CORE_FEATURES))
 		regdump->core_features = KBASE_REG_READ(kbdev, GPU_CONTROL_ENUM(CORE_FEATURES));
 
 #if MALI_USE_CSF
@@ -116,13 +116,13 @@ int kbase_backend_gpuprops_get_curr_config(struct kbase_device *kbdev,
 int kbase_backend_gpuprops_get_l2_features(struct kbase_device *kbdev,
 					   struct kbasep_gpuprops_regdump *regdump)
 {
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_L2_CONFIG)) {
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_L2_CONFIG)) {
 		regdump->l2_features = KBASE_REG_READ(kbdev, GPU_CONTROL_ENUM(L2_FEATURES));
 		regdump->l2_config = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(L2_CONFIG));
 
 #if MALI_USE_CSF
 		if (kbase_hw_has_l2_slice_hash_feature(kbdev)) {
-			uint i;
+			int i;
 			for (i = 0; i < GPU_L2_SLICE_HASH_COUNT; i++)
 				regdump->l2_slice_hash[i] =
 					kbase_reg_read32(kbdev, GPU_L2_SLICE_HASH_OFFSET(i));
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_backend.c
index 07960713f75a..3d61081e0f84 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_backend.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -29,21 +29,19 @@
 #include <device/mali_kbase_device.h>
 #include <backend/gpu/mali_kbase_instr_internal.h>
 
-#define WAIT_FOR_DUMP_TIMEOUT_MS 5000
-
 static int wait_prfcnt_ready(struct kbase_device *kbdev)
 {
-	u32 val;
-	const u32 timeout_us =
-		kbase_get_timeout_ms(kbdev, KBASE_PRFCNT_ACTIVE_TIMEOUT) * USEC_PER_MSEC;
-	const int err = kbase_reg_poll32_timeout(kbdev, GPU_CONTROL_ENUM(GPU_STATUS), val,
-						 !(val & GPU_STATUS_PRFCNT_ACTIVE), 0, timeout_us,
-						 false);
-	if (err) {
-		dev_err(kbdev->dev, "PRFCNT_ACTIVE bit stuck\n");
-		return -EBUSY;
+	u32 loops;
+
+	for (loops = 0; loops < KBASE_PRFCNT_ACTIVE_MAX_LOOPS; loops++) {
+		const u32 prfcnt_active = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_STATUS)) &
+					  GPU_STATUS_PRFCNT_ACTIVE;
+		if (!prfcnt_active)
+			return 0;
 	}
-	return 0;
+
+	dev_err(kbdev->dev, "PRFCNT_ACTIVE bit stuck\n");
+	return -EBUSY;
 }
 
 int kbase_instr_hwcnt_enable_internal(struct kbase_device *kbdev, struct kbase_context *kctx,
@@ -88,12 +86,11 @@ int kbase_instr_hwcnt_enable_internal(struct kbase_device *kbdev, struct kbase_c
 	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
 
 	/* Configure */
-	prfcnt_config = (u32)kctx->as_nr << PRFCNT_CONFIG_AS_SHIFT;
+	prfcnt_config = kctx->as_nr << PRFCNT_CONFIG_AS_SHIFT;
 #ifdef CONFIG_MALI_PRFCNT_SET_SELECT_VIA_DEBUG_FS
-	prfcnt_config |= (u32)kbdev->hwcnt.backend.override_counter_set
-			 << PRFCNT_CONFIG_SETSELECT_SHIFT;
+	prfcnt_config |= kbdev->hwcnt.backend.override_counter_set << PRFCNT_CONFIG_SETSELECT_SHIFT;
 #else
-	prfcnt_config |= (u32)enable->counter_set << PRFCNT_CONFIG_SETSELECT_SHIFT;
+	prfcnt_config |= enable->counter_set << PRFCNT_CONFIG_SETSELECT_SHIFT;
 #endif
 
 	/* Wait until prfcnt config register can be written */
@@ -165,7 +162,6 @@ int kbase_instr_hwcnt_disable_internal(struct kbase_context *kctx)
 {
 	unsigned long flags, pm_flags;
 	struct kbase_device *kbdev = kctx->kbdev;
-	const unsigned long timeout = msecs_to_jiffies(WAIT_FOR_DUMP_TIMEOUT_MS);
 
 	while (1) {
 		spin_lock_irqsave(&kbdev->hwaccess_lock, pm_flags);
@@ -202,8 +198,7 @@ int kbase_instr_hwcnt_disable_internal(struct kbase_context *kctx)
 		spin_unlock_irqrestore(&kbdev->hwaccess_lock, pm_flags);
 
 		/* Ongoing dump/setup - wait for its completion */
-		wait_event_timeout(kbdev->hwcnt.backend.wait, kbdev->hwcnt.backend.triggered != 0,
-				   timeout);
+		wait_event(kbdev->hwcnt.backend.wait, kbdev->hwcnt.backend.triggered != 0);
 	}
 
 	kbdev->hwcnt.backend.state = KBASE_INSTR_STATE_DISABLED;
@@ -323,19 +318,8 @@ int kbase_instr_hwcnt_wait_for_dump(struct kbase_context *kctx)
 	unsigned long flags;
 	int err;
 
-	unsigned long remaining;
-	const unsigned long timeout = msecs_to_jiffies(WAIT_FOR_DUMP_TIMEOUT_MS);
-
 	/* Wait for dump & cache clean to complete */
-	remaining = wait_event_timeout(kbdev->hwcnt.backend.wait,
-				       kbdev->hwcnt.backend.triggered != 0, timeout);
-	if (remaining == 0) {
-		err = -ETIME;
-		/* Set the backend state so it's clear things have gone bad (could be a HW issue)
-		 */
-		kbdev->hwcnt.backend.state = KBASE_INSTR_STATE_UNRECOVERABLE_ERROR;
-		goto timed_out;
-	}
+	wait_event(kbdev->hwcnt.backend.wait, kbdev->hwcnt.backend.triggered != 0);
 
 	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
 
@@ -351,7 +335,7 @@ int kbase_instr_hwcnt_wait_for_dump(struct kbase_context *kctx)
 	}
 
 	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
-timed_out:
+
 	return err;
 }
 
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_internal.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_internal.h
index feb76757f955..1429c01a1bd2 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_internal.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_internal.h
@@ -26,29 +26,8 @@
 #ifndef _KBASE_IRQ_INTERNAL_H_
 #define _KBASE_IRQ_INTERNAL_H_
 
-/* GPU IRQ Tags */
-#define JOB_IRQ_TAG 0
-#define MMU_IRQ_TAG 1
-#define GPU_IRQ_TAG 2
-
-/**
- * kbase_install_interrupts - Install IRQs handlers.
- *
- * @kbdev: The kbase device
- *
- * This function must be called once only when a kbase device is initialized.
- *
- * Return: 0 on success. Error code (negative) on failure.
- */
 int kbase_install_interrupts(struct kbase_device *kbdev);
 
-/**
- * kbase_release_interrupts - Uninstall IRQs handlers.
- *
- * @kbdev: The kbase device
- *
- * This function needs to be called when a kbase device is terminated.
- */
 void kbase_release_interrupts(struct kbase_device *kbdev);
 
 /**
@@ -58,52 +37,10 @@ void kbase_release_interrupts(struct kbase_device *kbdev);
  */
 void kbase_synchronize_irqs(struct kbase_device *kbdev);
 
-#ifdef CONFIG_MALI_BIFROST_DEBUG
-#if IS_ENABLED(CONFIG_MALI_REAL_HW)
-/**
- * kbase_validate_interrupts - Validate interrupts
- *
- * @kbdev: The kbase device
- *
- * This function will be called once when a kbase device is initialized
- * to check whether interrupt handlers are configured appropriately.
- * If interrupt numbers and/or flags defined in the device tree are
- * incorrect, then the validation might fail.
- * The whold device initialization will fail if it returns error code.
- *
- * Return: 0 on success. Error code (negative) on failure.
- */
-int kbase_validate_interrupts(struct kbase_device *const kbdev);
-#endif /* IS_ENABLED(CONFIG_MALI_REAL_HW) */
-#endif /* CONFIG_MALI_BIFROST_DEBUG */
-
-/**
- * kbase_get_interrupt_handler - Return default interrupt handler
- * @kbdev:   Kbase device
- * @irq_tag: Tag to choose the handler
- *
- * If single interrupt line is used the combined interrupt handler
- * will be returned regardless of irq_tag. Otherwise the corresponding
- * interrupt handler will be returned.
- *
- * Return: Interrupt handler corresponding to the tag. NULL on failure.
- */
-irq_handler_t kbase_get_interrupt_handler(struct kbase_device *kbdev, u32 irq_tag);
+int kbasep_common_test_interrupt_handlers(struct kbase_device *const kbdev);
 
-/**
- * kbase_set_custom_irq_handler - Set a custom IRQ handler
- *
- * @kbdev: The kbase device for which the handler is to be registered
- * @custom_handler: Handler to be registered
- * @irq_tag: Interrupt tag
- *
- * Register given interrupt handler for requested interrupt tag
- * In the case where irq handler is not specified, the default handler shall be
- * registered
- *
- * Return: 0 case success, error code otherwise
- */
+irqreturn_t kbase_gpu_irq_test_handler(int irq, void *data, u32 val);
 int kbase_set_custom_irq_handler(struct kbase_device *kbdev, irq_handler_t custom_handler,
-				 u32 irq_tag);
+				 int irq_type);
 
 #endif /* _KBASE_IRQ_INTERNAL_H_ */
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_linux.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_linux.c
index 152b140b5381..6474f27dafcc 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_linux.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_linux.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -23,19 +23,23 @@
 #include <device/mali_kbase_device.h>
 #include <backend/gpu/mali_kbase_irq_internal.h>
 
-
 #include <linux/interrupt.h>
 
 #if IS_ENABLED(CONFIG_MALI_REAL_HW)
+
+/* GPU IRQ Tags */
+#define JOB_IRQ_TAG 0
+#define MMU_IRQ_TAG 1
+#define GPU_IRQ_TAG 2
+
 static void *kbase_tag(void *ptr, u32 tag)
 {
 	return (void *)(((uintptr_t)ptr) | tag);
 }
-#endif
 
 static void *kbase_untag(void *ptr)
 {
-	return (void *)(((uintptr_t)ptr) & ~(uintptr_t)3);
+	return (void *)(((uintptr_t)ptr) & ~3);
 }
 
 static irqreturn_t kbase_job_irq_handler(int irq, void *data)
@@ -54,6 +58,12 @@ static irqreturn_t kbase_job_irq_handler(int irq, void *data)
 
 	val = kbase_reg_read32(kbdev, JOB_CONTROL_ENUM(JOB_IRQ_STATUS));
 
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+	if (!kbdev->pm.backend.driver_ready_for_irqs)
+		dev_warn(kbdev->dev, "%s: irq %d irqstatus 0x%x before driver is ready\n", __func__,
+			 irq, val);
+#endif /* CONFIG_MALI_BIFROST_DEBUG */
+
 	if (!val) {
 		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 		return IRQ_NONE;
@@ -91,6 +101,11 @@ static irqreturn_t kbase_mmu_irq_handler(int irq, void *data)
 
 	val = kbase_reg_read32(kbdev, MMU_CONTROL_ENUM(IRQ_STATUS));
 
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+	if (!kbdev->pm.backend.driver_ready_for_irqs)
+		dev_warn(kbdev->dev, "%s: irq %d irqstatus 0x%x before driver is ready\n", __func__,
+			 irq, val);
+#endif /* CONFIG_MALI_BIFROST_DEBUG */
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 	if (!val) {
@@ -107,8 +122,7 @@ static irqreturn_t kbase_mmu_irq_handler(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
-
-static irqreturn_t kbase_gpuonly_irq_handler(int irq, void *data)
+static irqreturn_t kbase_gpu_irq_handler(int irq, void *data)
 {
 	unsigned long flags;
 	struct kbase_device *kbdev = kbase_untag(data);
@@ -125,6 +139,13 @@ static irqreturn_t kbase_gpuonly_irq_handler(int irq, void *data)
 
 	gpu_irq_status = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_STATUS));
 
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+	if (!kbdev->pm.backend.driver_ready_for_irqs) {
+		dev_dbg(kbdev->dev, "%s: irq %d irqstatus 0x%x before driver is ready\n", __func__,
+			irq, gpu_irq_status);
+	}
+#endif /* CONFIG_MALI_BIFROST_DEBUG */
+
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 	if (gpu_irq_status) {
@@ -137,86 +158,82 @@ static irqreturn_t kbase_gpuonly_irq_handler(int irq, void *data)
 	return irq_state;
 }
 
-/**
- * kbase_gpu_irq_handler - GPU interrupt handler
- * @irq:  IRQ number
- * @data: Data associated with this IRQ (i.e. kbdev)
- *
- * Return: IRQ_HANDLED if any interrupt request has been successfully handled.
- *         IRQ_NONE otherwise.
- */
-static irqreturn_t kbase_gpu_irq_handler(int irq, void *data)
-{
-	irqreturn_t irq_state = kbase_gpuonly_irq_handler(irq, data);
-	return irq_state;
-}
+static irq_handler_t kbase_handler_table[] = {
+	[JOB_IRQ_TAG] = kbase_job_irq_handler,
+	[MMU_IRQ_TAG] = kbase_mmu_irq_handler,
+	[GPU_IRQ_TAG] = kbase_gpu_irq_handler,
+};
+
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+#define JOB_IRQ_HANDLER JOB_IRQ_TAG
+#define GPU_IRQ_HANDLER GPU_IRQ_TAG
 
 /**
- * kbase_combined_irq_handler - Combined interrupt handler for all interrupts
+ * kbase_gpu_irq_test_handler - Variant (for test) of kbase_gpu_irq_handler()
  * @irq:  IRQ number
  * @data: Data associated with this IRQ (i.e. kbdev)
+ * @val:  Value of the GPU_CONTROL_ENUM(GPU_IRQ_STATUS)
  *
- * This handler will be used for the GPU with single interrupt line.
+ * Handle the GPU device interrupt source requests reflected in the
+ * given source bit-pattern. The test code caller is responsible for
+ * undertaking the required device power maintenace.
  *
- * Return: IRQ_HANDLED if any interrupt request has been successfully handled.
- *         IRQ_NONE otherwise.
+ * Return: IRQ_HANDLED if the requests are from the GPU device,
+ *         IRQ_NONE otherwise
  */
-static irqreturn_t kbase_combined_irq_handler(int irq, void *data)
+irqreturn_t kbase_gpu_irq_test_handler(int irq, void *data, u32 val)
 {
-	irqreturn_t irq_state = IRQ_NONE;
-	irq_state |= kbase_job_irq_handler(irq, data);
-	irq_state |= kbase_mmu_irq_handler(irq, data);
-	irq_state |= kbase_gpu_irq_handler(irq, data);
+	struct kbase_device *kbdev = kbase_untag(data);
 
-	return irq_state;
-}
+	if (!val)
+		return IRQ_NONE;
 
-static irq_handler_t kbase_handler_table[] = {
-	[JOB_IRQ_TAG] = kbase_job_irq_handler,
-	[MMU_IRQ_TAG] = kbase_mmu_irq_handler,
-	[GPU_IRQ_TAG] = kbase_gpu_irq_handler,
-};
+	dev_dbg(kbdev->dev, "%s: irq %d irqstatus 0x%x\n", __func__, irq, val);
 
-irq_handler_t kbase_get_interrupt_handler(struct kbase_device *kbdev, u32 irq_tag)
-{
-	if (kbdev->nr_irqs == 1)
-		return kbase_combined_irq_handler;
-	else if (irq_tag < ARRAY_SIZE(kbase_handler_table))
-		return kbase_handler_table[irq_tag];
-	else
-		return NULL;
+	kbase_gpu_interrupt(kbdev, val);
+
+	return IRQ_HANDLED;
 }
 
-#if IS_ENABLED(CONFIG_MALI_REAL_HW)
-#ifdef CONFIG_MALI_BIFROST_DEBUG
+KBASE_EXPORT_TEST_API(kbase_gpu_irq_test_handler);
+
+/**
+ * kbase_set_custom_irq_handler - Set a custom IRQ handler
+ * @kbdev: Device for which the handler is to be registered
+ * @custom_handler: Handler to be registered
+ * @irq_type: Interrupt type
+ *
+ * Registers given interrupt handler for requested interrupt type
+ * In the case where irq handler is not specified, the default handler shall be
+ * registered
+ *
+ * Return: 0 case success, error code otherwise
+ */
 int kbase_set_custom_irq_handler(struct kbase_device *kbdev, irq_handler_t custom_handler,
-				 u32 irq_tag)
+				 int irq_type)
 {
 	int result = 0;
-	irq_handler_t handler = custom_handler;
-	const int irq = (kbdev->nr_irqs == 1) ? 0 : irq_tag;
+	irq_handler_t requested_irq_handler = NULL;
 
-	if (unlikely(!((irq_tag >= JOB_IRQ_TAG) && (irq_tag <= GPU_IRQ_TAG)))) {
-		dev_err(kbdev->dev, "Invalid irq_tag (%d)\n", irq_tag);
-		return -EINVAL;
-	}
+	KBASE_DEBUG_ASSERT((irq_type >= JOB_IRQ_HANDLER) && (irq_type <= GPU_IRQ_HANDLER));
 
 	/* Release previous handler */
-	if (kbdev->irqs[irq].irq)
-		free_irq(kbdev->irqs[irq].irq, kbase_tag(kbdev, irq));
+	if (kbdev->irqs[irq_type].irq)
+		free_irq(kbdev->irqs[irq_type].irq, kbase_tag(kbdev, irq_type));
 
-	/* If a custom handler isn't provided use the default handler */
-	if (!handler)
-		handler = kbase_get_interrupt_handler(kbdev, irq_tag);
+	requested_irq_handler = (custom_handler != NULL) ? custom_handler :
+								 kbase_handler_table[irq_type];
 
-	if (request_irq(kbdev->irqs[irq].irq, handler, kbdev->irqs[irq].flags | IRQF_SHARED,
-			dev_name(kbdev->dev), kbase_tag(kbdev, irq)) != 0) {
+	if (request_irq(kbdev->irqs[irq_type].irq, requested_irq_handler,
+			kbdev->irqs[irq_type].flags | IRQF_SHARED, dev_name(kbdev->dev),
+			kbase_tag(kbdev, irq_type)) != 0) {
 		result = -EINVAL;
-		dev_err(kbdev->dev, "Can't request interrupt %u (index %u)\n", kbdev->irqs[irq].irq,
-			irq_tag);
-		if (IS_ENABLED(CONFIG_SPARSE_IRQ))
-			dev_err(kbdev->dev,
-				"CONFIG_SPARSE_IRQ enabled - is the interrupt number correct for this config?\n");
+		dev_err(kbdev->dev, "Can't request interrupt %d (index %d)\n",
+			kbdev->irqs[irq_type].irq, irq_type);
+#if IS_ENABLED(CONFIG_SPARSE_IRQ)
+		dev_err(kbdev->dev,
+			"You have CONFIG_SPARSE_IRQ support enabled - is the interrupt number correct for this configuration?\n");
+#endif /* CONFIG_SPARSE_IRQ */
 	}
 
 	return result;
@@ -308,43 +325,30 @@ static enum hrtimer_restart kbasep_test_interrupt_timeout(struct hrtimer *timer)
 	return HRTIMER_NORESTART;
 }
 
-/**
- * validate_interrupt - Validate an interrupt
- * @kbdev: Kbase device
- * @tag:   Tag to choose the interrupt
- *
- * To validate the settings for the interrupt, write a value on RAWSTAT
- * register to trigger interrupt. Then with custom interrupt handler
- * check whether the interrupt happens within reasonable time.
- *
- * Return: 0 if validating interrupt succeeds.
- */
-static int validate_interrupt(struct kbase_device *const kbdev, u32 tag)
+static int kbasep_common_test_interrupt(struct kbase_device *const kbdev, u32 tag)
 {
 	int err = 0;
-	irq_handler_t handler;
-	const int irq = (kbdev->nr_irqs == 1) ? 0 : tag;
+	irq_handler_t test_handler;
+
 	u32 old_mask_val;
 	u16 mask_offset;
 	u16 rawstat_offset;
 
 	switch (tag) {
 	case JOB_IRQ_TAG:
-		handler = kbase_job_irq_test_handler;
+		test_handler = kbase_job_irq_test_handler;
 		rawstat_offset = JOB_CONTROL_ENUM(JOB_IRQ_RAWSTAT);
 		mask_offset = JOB_CONTROL_ENUM(JOB_IRQ_MASK);
 		break;
 	case MMU_IRQ_TAG:
-		handler = kbase_mmu_irq_test_handler;
+		test_handler = kbase_mmu_irq_test_handler;
 		rawstat_offset = MMU_CONTROL_ENUM(IRQ_RAWSTAT);
 		mask_offset = MMU_CONTROL_ENUM(IRQ_MASK);
 		break;
 	case GPU_IRQ_TAG:
 		/* already tested by pm_driver - bail out */
-		return 0;
 	default:
-		dev_err(kbdev->dev, "Invalid tag (%d)\n", tag);
-		return -EINVAL;
+		return 0;
 	}
 
 	/* store old mask */
@@ -352,9 +356,9 @@ static int validate_interrupt(struct kbase_device *const kbdev, u32 tag)
 	/* mask interrupts */
 	kbase_reg_write32(kbdev, mask_offset, 0x0);
 
-	if (kbdev->irqs[irq].irq) {
+	if (kbdev->irqs[tag].irq) {
 		/* release original handler and install test handler */
-		if (kbase_set_custom_irq_handler(kbdev, handler, tag) != 0) {
+		if (kbase_set_custom_irq_handler(kbdev, test_handler, tag) != 0) {
 			err = -EINVAL;
 		} else {
 			kbasep_irq_test_data.timeout = 0;
@@ -372,12 +376,12 @@ static int validate_interrupt(struct kbase_device *const kbdev, u32 tag)
 			wait_event(kbasep_irq_test_data.wait, kbasep_irq_test_data.triggered != 0);
 
 			if (kbasep_irq_test_data.timeout != 0) {
-				dev_err(kbdev->dev, "Interrupt %u (index %u) didn't reach CPU.\n",
-					kbdev->irqs[irq].irq, irq);
+				dev_err(kbdev->dev, "Interrupt %d (index %d) didn't reach CPU.\n",
+					kbdev->irqs[tag].irq, tag);
 				err = -EINVAL;
 			} else {
-				dev_dbg(kbdev->dev, "Interrupt %u (index %u) reached CPU.\n",
-					kbdev->irqs[irq].irq, irq);
+				dev_dbg(kbdev->dev, "Interrupt %d (index %d) reached CPU.\n",
+					kbdev->irqs[tag].irq, tag);
 			}
 
 			hrtimer_cancel(&kbasep_irq_test_data.timer);
@@ -387,15 +391,15 @@ static int validate_interrupt(struct kbase_device *const kbdev, u32 tag)
 			kbase_reg_write32(kbdev, mask_offset, 0x0);
 
 			/* release test handler */
-			free_irq(kbdev->irqs[irq].irq, kbase_tag(kbdev, irq));
+			free_irq(kbdev->irqs[tag].irq, kbase_tag(kbdev, tag));
 		}
 
 		/* restore original interrupt */
-		if (request_irq(kbdev->irqs[irq].irq, kbase_get_interrupt_handler(kbdev, tag),
-				kbdev->irqs[irq].flags | IRQF_SHARED, dev_name(kbdev->dev),
-				kbase_tag(kbdev, irq))) {
-			dev_err(kbdev->dev, "Can't restore original interrupt %u (index %u)\n",
-				kbdev->irqs[irq].irq, tag);
+		if (request_irq(kbdev->irqs[tag].irq, kbase_handler_table[tag],
+				kbdev->irqs[tag].flags | IRQF_SHARED, dev_name(kbdev->dev),
+				kbase_tag(kbdev, tag))) {
+			dev_err(kbdev->dev, "Can't restore original interrupt %d (index %d)\n",
+				kbdev->irqs[tag].irq, tag);
 			err = -EINVAL;
 		}
 	}
@@ -405,8 +409,7 @@ static int validate_interrupt(struct kbase_device *const kbdev, u32 tag)
 	return err;
 }
 
-#if IS_ENABLED(CONFIG_MALI_REAL_HW)
-int kbase_validate_interrupts(struct kbase_device *const kbdev)
+int kbasep_common_test_interrupt_handlers(struct kbase_device *const kbdev)
 {
 	int err;
 
@@ -416,14 +419,14 @@ int kbase_validate_interrupts(struct kbase_device *const kbdev)
 	/* A suspend won't happen during startup/insmod */
 	kbase_pm_context_active(kbdev);
 
-	err = validate_interrupt(kbdev, JOB_IRQ_TAG);
+	err = kbasep_common_test_interrupt(kbdev, JOB_IRQ_TAG);
 	if (err) {
 		dev_err(kbdev->dev,
 			"Interrupt JOB_IRQ didn't reach CPU. Check interrupt assignments.\n");
 		goto out;
 	}
 
-	err = validate_interrupt(kbdev, MMU_IRQ_TAG);
+	err = kbasep_common_test_interrupt(kbdev, MMU_IRQ_TAG);
 	if (err) {
 		dev_err(kbdev->dev,
 			"Interrupt MMU_IRQ didn't reach CPU. Check interrupt assignments.\n");
@@ -437,21 +440,25 @@ int kbase_validate_interrupts(struct kbase_device *const kbdev)
 
 	return err;
 }
-#endif /* CONFIG_MALI_REAL_HW */
 #endif /* CONFIG_MALI_BIFROST_DEBUG */
 
 int kbase_install_interrupts(struct kbase_device *kbdev)
 {
+	u32 nr = ARRAY_SIZE(kbase_handler_table);
+	int err;
 	u32 i;
 
-	for (i = 0; i < kbdev->nr_irqs; i++) {
-		const int result = request_irq(kbdev->irqs[i].irq,
-					       kbase_get_interrupt_handler(kbdev, i),
-					       kbdev->irqs[i].flags | IRQF_SHARED,
-					       dev_name(kbdev->dev), kbase_tag(kbdev, i));
-		if (result) {
-			dev_err(kbdev->dev, "Can't request interrupt %u (index %u)\n",
+	for (i = 0; i < nr; i++) {
+		err = request_irq(kbdev->irqs[i].irq, kbase_handler_table[i],
+				  kbdev->irqs[i].flags | IRQF_SHARED, dev_name(kbdev->dev),
+				  kbase_tag(kbdev, i));
+		if (err) {
+			dev_err(kbdev->dev, "Can't request interrupt %d (index %d)\n",
 				kbdev->irqs[i].irq, i);
+#if IS_ENABLED(CONFIG_SPARSE_IRQ)
+			dev_err(kbdev->dev,
+				"You have CONFIG_SPARSE_IRQ support enabled - is the interrupt number correct for this configuration?\n");
+#endif /* CONFIG_SPARSE_IRQ */
 			goto release;
 		}
 	}
@@ -459,21 +466,18 @@ int kbase_install_interrupts(struct kbase_device *kbdev)
 	return 0;
 
 release:
-	if (IS_ENABLED(CONFIG_SPARSE_IRQ))
-		dev_err(kbdev->dev,
-			"CONFIG_SPARSE_IRQ enabled - is the interrupt number correct for this config?\n");
-
 	while (i-- > 0)
 		free_irq(kbdev->irqs[i].irq, kbase_tag(kbdev, i));
 
-	return -EINVAL;
+	return err;
 }
 
 void kbase_release_interrupts(struct kbase_device *kbdev)
 {
+	u32 nr = ARRAY_SIZE(kbase_handler_table);
 	u32 i;
 
-	for (i = 0; i < kbdev->nr_irqs; i++) {
+	for (i = 0; i < nr; i++) {
 		if (kbdev->irqs[i].irq)
 			free_irq(kbdev->irqs[i].irq, kbase_tag(kbdev, i));
 	}
@@ -481,9 +485,10 @@ void kbase_release_interrupts(struct kbase_device *kbdev)
 
 void kbase_synchronize_irqs(struct kbase_device *kbdev)
 {
+	u32 nr = ARRAY_SIZE(kbase_handler_table);
 	u32 i;
 
-	for (i = 0; i < kbdev->nr_irqs; i++) {
+	for (i = 0; i < nr; i++) {
 		if (kbdev->irqs[i].irq)
 			synchronize_irq(kbdev->irqs[i].irq);
 	}
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_defs.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_defs.h
similarity index 97%
rename from drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_defs.h
rename to drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_defs.h
index bff0b7235736..e9dbe825d47f 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_defs.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_defs.h
@@ -50,7 +50,7 @@ struct rb_entry {
  *			u64 for serving as tagged value.
  * @kctx: Pointer to kbase context.
  */
-#define SLOT_RB_TAG_KCTX(kctx) ((u64)(uintptr_t)(kctx))
+#define SLOT_RB_TAG_KCTX(kctx) (u64)((uintptr_t)(kctx))
 /**
  * struct slot_rb - Slot ringbuffer
  * @entries:		Ringbuffer entries
@@ -116,7 +116,7 @@ struct kbase_backend_data {
  * within the timeout period
  */
 #define KBASE_RESET_GPU_COMMITTED 2
-/* The GPU reset process is currently occurring (timeout has expired or
+/* The GPU reset process is currently occuring (timeout has expired or
  * kbasep_try_reset_gpu_early was called)
  */
 #define KBASE_RESET_GPU_HAPPENING 3
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_hw.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_hw.c
index b251de4fc23e..8f06058bbdb4 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_hw.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_hw.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -98,17 +98,96 @@ static u64 kbase_job_write_affinity(struct kbase_device *kbdev, base_jd_core_req
 	return affinity;
 }
 
+/**
+ * select_job_chain() - Select which job chain to submit to the GPU
+ * @katom: Pointer to the atom about to be submitted to the GPU
+ *
+ * Selects one of the fragment job chains attached to the special atom at the
+ * end of a renderpass, or returns the address of the single job chain attached
+ * to any other type of atom.
+ *
+ * Which job chain is selected depends upon whether the tiling phase of the
+ * renderpass completed normally or was soft-stopped because it used too
+ * much memory. It also depends upon whether one of the fragment job chains
+ * has already been run as part of the same renderpass.
+ *
+ * Return: GPU virtual address of the selected job chain
+ */
+static u64 select_job_chain(struct kbase_jd_atom *katom)
+{
+	struct kbase_context *const kctx = katom->kctx;
+	u64 jc = katom->jc;
+	struct kbase_jd_renderpass *rp;
+
+	lockdep_assert_held(&kctx->kbdev->hwaccess_lock);
+
+	if (!(katom->core_req & BASE_JD_REQ_END_RENDERPASS))
+		return jc;
+
+	compiletime_assert((1ull << (sizeof(katom->renderpass_id) * 8)) <=
+				   ARRAY_SIZE(kctx->jctx.renderpasses),
+			   "Should check invalid access to renderpasses");
+
+	rp = &kctx->jctx.renderpasses[katom->renderpass_id];
+	/* We can read a subset of renderpass state without holding
+	 * higher-level locks (but not end_katom, for example).
+	 * If the end-of-renderpass atom is running with as-yet indeterminate
+	 * OOM state then assume that the start atom was not soft-stopped.
+	 */
+	switch (rp->state) {
+	case KBASE_JD_RP_OOM:
+		/* Tiling ran out of memory.
+		 * Start of incremental rendering, used once.
+		 */
+		jc = katom->jc_fragment.norm_read_forced_write;
+		break;
+	case KBASE_JD_RP_START:
+	case KBASE_JD_RP_PEND_OOM:
+		/* Tiling completed successfully first time.
+		 * Single-iteration rendering, used once.
+		 */
+		jc = katom->jc_fragment.norm_read_norm_write;
+		break;
+	case KBASE_JD_RP_RETRY_OOM:
+		/* Tiling ran out of memory again.
+		 * Continuation of incremental rendering, used as
+		 * many times as required.
+		 */
+		jc = katom->jc_fragment.forced_read_forced_write;
+		break;
+	case KBASE_JD_RP_RETRY:
+	case KBASE_JD_RP_RETRY_PEND_OOM:
+		/* Tiling completed successfully this time.
+		 * End of incremental rendering, used once.
+		 */
+		jc = katom->jc_fragment.forced_read_norm_write;
+		break;
+	default:
+		WARN_ON(1);
+		break;
+	}
+
+	dev_dbg(kctx->kbdev->dev, "Selected job chain 0x%llx for end atom %pK in state %d\n", jc,
+		(void *)katom, (int)rp->state);
+
+	katom->jc = jc;
+	return jc;
+}
+
 static inline bool kbasep_jm_wait_js_free(struct kbase_device *kbdev, unsigned int js,
 					  struct kbase_context *kctx)
 {
-	u32 val;
-	const u32 timeout_us = kbdev->js_data.js_free_wait_time_ms * USEC_PER_MSEC;
+	const ktime_t wait_loop_start = ktime_get_raw();
+	const s64 max_timeout = (s64)kbdev->js_data.js_free_wait_time_ms;
+	s64 diff = 0;
+
 	/* wait for the JS_COMMAND_NEXT register to reach the given status value */
-	const int err = kbase_reg_poll32_timeout(kbdev, JOB_SLOT_OFFSET(js, COMMAND_NEXT), val,
-						 !val, 0, timeout_us, false);
+	do {
+		if (!kbase_reg_read32(kbdev, JOB_SLOT_OFFSET(js, COMMAND_NEXT)))
+			return true;
 
-	if (!err)
-		return true;
+		diff = ktime_to_ms(ktime_sub(ktime_get_raw(), wait_loop_start));
+	} while (diff < max_timeout);
 
 	dev_err(kbdev->dev, "Timeout in waiting for job slot %u to become free for ctx %d_%u", js,
 		kctx->tgid, kctx->id);
@@ -120,7 +199,7 @@ int kbase_job_hw_submit(struct kbase_device *kbdev, struct kbase_jd_atom *katom,
 {
 	struct kbase_context *kctx;
 	u32 cfg;
-	u64 jc_head = katom->jc;
+	u64 const jc_head = select_job_chain(katom);
 	u64 affinity;
 	struct slot_rb *ptr_slot_rb = &kbdev->hwaccess.backend.slot_rb[js];
 
@@ -142,23 +221,23 @@ int kbase_job_hw_submit(struct kbase_device *kbdev, struct kbase_jd_atom *katom,
 	/* start MMU, medium priority, cache clean/flush on end, clean/flush on
 	 * start
 	 */
-	cfg = (u32)kctx->as_nr;
+	cfg = kctx->as_nr;
 
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_FLUSH_REDUCTION) &&
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_FLUSH_REDUCTION) &&
 	    !(kbdev->serialize_jobs & KBASE_SERIALIZE_RESET))
 		cfg |= JS_CONFIG_ENABLE_FLUSH_REDUCTION;
 
 	if (0 != (katom->core_req & BASE_JD_REQ_SKIP_CACHE_START)) {
 		/* Force a cache maintenance operation if the newly submitted
 		 * katom to the slot is from a different kctx. For a JM GPU
-		 * that has the feature KBASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER,
+		 * that has the feature BASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER,
 		 * applies a FLUSH_INV_SHADER_OTHER. Otherwise, do a
 		 * FLUSH_CLEAN_INVALIDATE.
 		 */
 		u64 tagged_kctx = ptr_slot_rb->last_kctx_tagged;
 
 		if (tagged_kctx != SLOT_RB_NULL_TAG_VAL && tagged_kctx != SLOT_RB_TAG_KCTX(kctx)) {
-			if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER))
+			if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER))
 				cfg |= JS_CONFIG_START_FLUSH_INV_SHADER_OTHER;
 			else
 				cfg |= JS_CONFIG_START_FLUSH_CLEAN_INVALIDATE;
@@ -170,14 +249,15 @@ int kbase_job_hw_submit(struct kbase_device *kbdev, struct kbase_jd_atom *katom,
 	if (0 != (katom->core_req & BASE_JD_REQ_SKIP_CACHE_END) &&
 	    !(kbdev->serialize_jobs & KBASE_SERIALIZE_RESET))
 		cfg |= JS_CONFIG_END_FLUSH_NO_ACTION;
-	else if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_CLEAN_ONLY_SAFE))
+	else if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_CLEAN_ONLY_SAFE))
 		cfg |= JS_CONFIG_END_FLUSH_CLEAN;
 	else
 		cfg |= JS_CONFIG_END_FLUSH_CLEAN_INVALIDATE;
 
 	cfg |= JS_CONFIG_THREAD_PRI(8);
 
-	if (katom->atom_flags & KBASE_KATOM_FLAG_PROTECTED)
+	if ((katom->atom_flags & KBASE_KATOM_FLAG_PROTECTED) ||
+	    (katom->core_req & BASE_JD_REQ_END_RENDERPASS))
 		cfg |= JS_CONFIG_DISABLE_DESCRIPTOR_WR_BK;
 
 	if (!ptr_slot_rb->job_chain_flag) {
@@ -191,7 +271,7 @@ int kbase_job_hw_submit(struct kbase_device *kbdev, struct kbase_jd_atom *katom,
 
 	kbase_reg_write32(kbdev, JOB_SLOT_OFFSET(js, CONFIG_NEXT), cfg);
 
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_FLUSH_REDUCTION))
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_FLUSH_REDUCTION))
 		kbase_reg_write32(kbdev, JOB_SLOT_OFFSET(js, FLUSH_ID_NEXT), katom->flush_id);
 
 	/* Write an approximate start timestamp.
@@ -306,10 +386,10 @@ void kbase_job_done(struct kbase_device *kbdev, u32 done)
 		/* Note: This is inherently unfair, as we always check for lower
 		 * numbered interrupts before the higher numbered ones.
 		 */
-		i = (unsigned int)ffs((int)finished) - 1u;
+		i = ffs(finished) - 1;
 
 		do {
-			u32 nr_done;
+			int nr_done;
 			u32 active;
 			u32 completion_code = BASE_JD_EVENT_DONE; /* assume OK */
 			u64 job_tail = 0;
@@ -363,7 +443,7 @@ void kbase_job_done(struct kbase_device *kbdev, u32 done)
 				 * jobs to hang. Reset GPU before allowing
 				 * any other jobs on the slot to continue.
 				 */
-				if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TTRX_3076)) {
+				if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TTRX_3076)) {
 					if (completion_code == BASE_JD_EVENT_JOB_BUS_FAULT) {
 						if (kbase_prepare_to_reset_gpu_locked(
 							    kbdev, RESET_FLAGS_NONE))
@@ -373,7 +453,7 @@ void kbase_job_done(struct kbase_device *kbdev, u32 done)
 			}
 
 			kbase_reg_write32(kbdev, JOB_CONTROL_ENUM(JOB_IRQ_CLEAR),
-					  done & ((1u << i) | (1u << (i + 16))));
+					  done & ((1 << i) | (1 << (i + 16))));
 			active = kbase_reg_read32(kbdev, JOB_CONTROL_ENUM(JOB_IRQ_JS_STATE));
 
 			if (((active >> i) & 1) == 0 && (((done >> (i + 16)) & 1) == 0)) {
@@ -434,8 +514,8 @@ void kbase_job_done(struct kbase_device *kbdev, u32 done)
 			nr_done -= (active >> i) & 1;
 			nr_done -= (active >> (i + 16)) & 1;
 
-			if (nr_done == 0 || nr_done > SLOT_RB_SIZE) {
-				dev_warn(kbdev->dev, "Spurious interrupt on slot %u", i);
+			if (nr_done <= 0) {
+				dev_warn(kbdev->dev, "Spurious interrupt on slot %d", i);
 
 				goto spurious;
 			}
@@ -476,7 +556,7 @@ void kbase_job_done(struct kbase_device *kbdev, u32 done)
 			finished = (done & 0xFFFF) | failed;
 			if (done)
 				end_timestamp = ktime_get_raw();
-		} while (finished & (1u << i));
+		} while (finished & (1 << i));
 
 		kbasep_job_slot_update_head_start_timestamp(kbdev, i, end_timestamp);
 	}
@@ -500,7 +580,7 @@ void kbasep_job_slot_soft_or_hard_stop_do_action(struct kbase_device *kbdev, uns
 	u64 job_in_head_before;
 	u32 status_reg_after;
 
-	WARN_ON(action & (~(u32)JS_COMMAND_MASK));
+	WARN_ON(action & (~JS_COMMAND_MASK));
 
 	/* Check the head pointer */
 	job_in_head_before = kbase_reg_read64(kbdev, JOB_SLOT_OFFSET(js, HEAD));
@@ -663,17 +743,76 @@ void kbase_job_slot_ctx_priority_check_locked(struct kbase_context *kctx,
 	}
 }
 
+static int softstop_start_rp_nolock(struct kbase_context *kctx, struct kbase_va_region *reg)
+{
+	struct kbase_device *const kbdev = kctx->kbdev;
+	struct kbase_jd_atom *katom;
+	struct kbase_jd_renderpass *rp;
+
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	katom = kbase_gpu_inspect(kbdev, 1, 0);
+
+	if (!katom) {
+		dev_dbg(kctx->kbdev->dev, "No atom on job slot\n");
+		return -ESRCH;
+	}
+
+	if (!(katom->core_req & BASE_JD_REQ_START_RENDERPASS)) {
+		dev_dbg(kctx->kbdev->dev, "Atom %pK on job slot is not start RP\n", (void *)katom);
+		return -EPERM;
+	}
+
+	compiletime_assert((1ull << (sizeof(katom->renderpass_id) * 8)) <=
+				   ARRAY_SIZE(kctx->jctx.renderpasses),
+			   "Should check invalid access to renderpasses");
+
+	rp = &kctx->jctx.renderpasses[katom->renderpass_id];
+	if (WARN_ON(rp->state != KBASE_JD_RP_START && rp->state != KBASE_JD_RP_RETRY))
+		return -EINVAL;
+
+	dev_dbg(kctx->kbdev->dev, "OOM in state %d with region %pK\n", (int)rp->state, (void *)reg);
+
+	if (WARN_ON(katom != rp->start_katom))
+		return -EINVAL;
+
+	dev_dbg(kctx->kbdev->dev, "Adding region %pK to list %pK\n", (void *)reg,
+		(void *)&rp->oom_reg_list);
+	list_move_tail(&reg->link, &rp->oom_reg_list);
+	dev_dbg(kctx->kbdev->dev, "Added region to list\n");
+
+	rp->state = (rp->state == KBASE_JD_RP_START ? KBASE_JD_RP_PEND_OOM :
+							    KBASE_JD_RP_RETRY_PEND_OOM);
+
+	kbase_job_slot_softstop(kbdev, 1, katom);
+
+	return 0;
+}
+
+int kbase_job_slot_softstop_start_rp(struct kbase_context *const kctx,
+				     struct kbase_va_region *const reg)
+{
+	struct kbase_device *const kbdev = kctx->kbdev;
+	int err;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	err = softstop_start_rp_nolock(kctx, reg);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	return err;
+}
+
 void kbase_jm_wait_for_zero_jobs(struct kbase_context *kctx)
 {
 	struct kbase_device *kbdev = kctx->kbdev;
 	unsigned long timeout = msecs_to_jiffies(ZAP_TIMEOUT);
 
-	timeout = wait_event_timeout(kctx->jctx.zero_jobs_wait, kctx->jctx.job_nr == 0,
-				     (long)timeout);
+	timeout = wait_event_timeout(kctx->jctx.zero_jobs_wait, kctx->jctx.job_nr == 0, timeout);
 
 	if (timeout != 0)
 		timeout = wait_event_timeout(kctx->jctx.sched_info.ctx.is_scheduled_wait,
-					     !kbase_ctx_flag(kctx, KCTX_SCHEDULED), (long)timeout);
+					     !kbase_ctx_flag(kctx, KCTX_SCHEDULED), timeout);
 
 	/* Neither wait timed out; all done! */
 	if (timeout != 0)
@@ -702,7 +841,7 @@ u32 kbase_backend_get_current_flush_id(struct kbase_device *kbdev)
 {
 	u32 flush_id = 0;
 
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_FLUSH_REDUCTION)) {
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_FLUSH_REDUCTION)) {
 		mutex_lock(&kbdev->pm.lock);
 		if (kbdev->pm.backend.gpu_powered)
 			flush_id = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(LATEST_FLUSH));
@@ -760,8 +899,7 @@ void kbase_job_slot_softstop_swflags(struct kbase_device *kbdev, unsigned int js
 					  JS_COMMAND_SOFT_STOP | sw_flags);
 }
 
-void kbase_job_slot_softstop(struct kbase_device *kbdev, unsigned int js,
-			     struct kbase_jd_atom *target_katom)
+void kbase_job_slot_softstop(struct kbase_device *kbdev, int js, struct kbase_jd_atom *target_katom)
 {
 	kbase_job_slot_softstop_swflags(kbdev, js, target_katom, 0u);
 }
@@ -841,7 +979,7 @@ void kbase_reset_gpu_assert_failed_or_prevented(struct kbase_device *kbdev)
 
 static void kbase_debug_dump_registers(struct kbase_device *kbdev)
 {
-	unsigned int i;
+	int i;
 
 	kbase_io_history_dump(kbdev);
 
@@ -853,7 +991,7 @@ static void kbase_debug_dump_registers(struct kbase_device *kbdev)
 		kbase_reg_read32(kbdev, JOB_CONTROL_ENUM(JOB_IRQ_RAWSTAT)),
 		kbase_reg_read32(kbdev, JOB_CONTROL_ENUM(JOB_IRQ_JS_STATE)));
 	for (i = 0; i < 3; i++) {
-		dev_err(kbdev->dev, "  JS%u_STATUS=0x%08x      JS%u_HEAD=0x%016llx", i,
+		dev_err(kbdev->dev, "  JS%d_STATUS=0x%08x      JS%d_HEAD=0x%016llx", i,
 			kbase_reg_read32(kbdev, JOB_SLOT_OFFSET(i, STATUS)), i,
 			kbase_reg_read64(kbdev, JOB_SLOT_OFFSET(i, HEAD)));
 	}
@@ -882,6 +1020,7 @@ static void kbasep_reset_timeout_worker(struct work_struct *data)
 	ktime_t end_timestamp = ktime_get_raw();
 	struct kbasep_js_device_data *js_devdata;
 	bool silent = false;
+	u32 max_loops = KBASE_CLEAN_CACHE_MAX_LOOPS;
 
 	kbdev = container_of(data, struct kbase_device, hwaccess.backend.reset_work);
 
@@ -948,16 +1087,14 @@ static void kbasep_reset_timeout_worker(struct work_struct *data)
 	/* The flush has completed so reset the active indicator */
 	kbdev->irq_reset_flush = false;
 
-	if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TMIX_8463)) {
-		u64 val;
-		const u32 timeout_us =
-			kbase_get_timeout_ms(kbdev, KBASE_CLEAN_CACHE_TIMEOUT) * USEC_PER_MSEC;
-		/* Ensure that L2 is not transitioning when we send the reset command */
-		const int err = read_poll_timeout_atomic(kbase_pm_get_trans_cores, val, !val, 0,
-							 timeout_us, false, kbdev,
-							 KBASE_PM_CORE_L2);
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TMIX_8463)) {
+		/* Ensure that L2 is not transitioning when we send the reset
+		 * command
+		 */
+		while (--max_loops && kbase_pm_get_trans_cores(kbdev, KBASE_PM_CORE_L2))
+			;
 
-		WARN(err, "L2 power transition timed out while trying to reset\n");
+		WARN(!max_loops, "L2 power transition timed out while trying to reset\n");
 	}
 
 	mutex_lock(&kbdev->pm.lock);
@@ -1075,7 +1212,7 @@ static enum hrtimer_restart kbasep_reset_timer_callback(struct hrtimer *timer)
 static void kbasep_try_reset_gpu_early_locked(struct kbase_device *kbdev)
 {
 	unsigned int i;
-	u32 pending_jobs = 0;
+	int pending_jobs = 0;
 
 	/* Count the number of jobs */
 	for (i = 0; i < kbdev->gpu_props.num_job_slots; i++)
@@ -1129,14 +1266,16 @@ static void kbasep_try_reset_gpu_early(struct kbase_device *kbdev)
  */
 bool kbase_prepare_to_reset_gpu_locked(struct kbase_device *kbdev, unsigned int flags)
 {
-	unsigned int i;
+	int i;
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	if (kbase_pm_is_gpu_lost(kbdev)) {
 		/* GPU access has been removed, reset will be done by
 		 * Arbiter instead
 		 */
 		return false;
 	}
+#endif
 
 	if (flags & RESET_FLAGS_HWC_UNRECOVERABLE_ERROR)
 		kbase_instr_hwcnt_on_unrecoverable_error(kbdev);
@@ -1189,7 +1328,7 @@ void kbase_reset_gpu(struct kbase_device *kbdev)
 
 	if (!kbase_is_quick_reset_enabled(kbdev))
 		dev_err(kbdev->dev,
-			"Preparing to soft-reset GPU: Waiting (up to %d ms) for all jobs to complete soft-stop\n",
+			"Preparing to soft-reset GPU: Waiting (upto %d ms) for all jobs to complete soft-stop\n",
 			kbdev->reset_timeout_ms);
 
 	hrtimer_start(&kbdev->hwaccess.backend.reset_timer,
@@ -1211,7 +1350,7 @@ void kbase_reset_gpu_locked(struct kbase_device *kbdev)
 
 	if (!kbase_is_quick_reset_enabled(kbdev))
 		dev_err(kbdev->dev,
-			"Preparing to soft-reset GPU: Waiting (up to %d ms) for all jobs to complete soft-stop\n",
+			"Preparing to soft-reset GPU: Waiting (upto %d ms) for all jobs to complete soft-stop\n",
 			kbdev->reset_timeout_ms);
 	hrtimer_start(&kbdev->hwaccess.backend.reset_timer,
 		      HR_TIMER_DELAY_MSEC(kbdev->reset_timeout_ms), HRTIMER_MODE_REL);
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_rb.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_rb.c
index a4a640a0fb92..50cf19d876c5 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_rb.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_rb.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -159,9 +159,9 @@ bool kbase_gpu_atoms_submitted_any(struct kbase_device *kbdev)
 	return false;
 }
 
-u32 kbase_backend_nr_atoms_submitted(struct kbase_device *kbdev, unsigned int js)
+int kbase_backend_nr_atoms_submitted(struct kbase_device *kbdev, unsigned int js)
 {
-	u32 nr = 0;
+	int nr = 0;
 	int i;
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
@@ -425,7 +425,7 @@ static void kbase_gpu_release_atom(struct kbase_device *kbdev, struct kbase_jd_a
 			}
 		}
 
-		if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TGOX_R1_1234)) {
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TGOX_R1_1234)) {
 			if (katom->atom_flags & KBASE_KATOM_FLAG_HOLDING_L2_REF_PROT) {
 				kbase_pm_protected_l2_override(kbdev, false);
 				katom->atom_flags &= ~KBASE_KATOM_FLAG_HOLDING_L2_REF_PROT;
@@ -539,7 +539,7 @@ static int kbase_gpu_protected_mode_reset(struct kbase_device *kbdev)
 }
 
 static int kbase_jm_protected_entry(struct kbase_device *kbdev, struct kbase_jd_atom **katom,
-				    int idx, unsigned int js)
+				    int idx, int js)
 {
 	int err = 0;
 
@@ -594,7 +594,7 @@ static int kbase_jm_protected_entry(struct kbase_device *kbdev, struct kbase_jd_
 }
 
 static int kbase_jm_enter_protected_mode(struct kbase_device *kbdev, struct kbase_jd_atom **katom,
-					 int idx, unsigned int js)
+					 int idx, int js)
 {
 	int err = 0;
 
@@ -698,7 +698,7 @@ static int kbase_jm_enter_protected_mode(struct kbase_device *kbdev, struct kbas
 
 		kbase_pm_protected_entry_override_disable(kbdev);
 
-		if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TGOX_R1_1234)) {
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TGOX_R1_1234)) {
 			/*
 			 * Power on L2 caches; this will also result in the
 			 * correct value written to coherency enable register.
@@ -714,13 +714,13 @@ static int kbase_jm_enter_protected_mode(struct kbase_device *kbdev, struct kbas
 
 		katom[idx]->protected_state.enter = KBASE_ATOM_ENTER_PROTECTED_FINISHED;
 
-		if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TGOX_R1_1234))
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TGOX_R1_1234))
 			return -EAGAIN;
 
 		/* ***TRANSITION TO HIGHER STATE*** */
 		fallthrough;
 	case KBASE_ATOM_ENTER_PROTECTED_FINISHED:
-		if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TGOX_R1_1234)) {
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TGOX_R1_1234)) {
 			/*
 			 * Check that L2 caches are powered and, if so,
 			 * enter protected mode.
@@ -758,7 +758,7 @@ static int kbase_jm_enter_protected_mode(struct kbase_device *kbdev, struct kbas
 }
 
 static int kbase_jm_exit_protected_mode(struct kbase_device *kbdev, struct kbase_jd_atom **katom,
-					int idx, unsigned int js)
+					int idx, int js)
 {
 	int err = 0;
 
@@ -864,7 +864,11 @@ void kbase_backend_slot_update(struct kbase_device *kbdev)
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
-	if (kbase_reset_gpu_is_active(kbdev) || (kbase_is_gpu_removed(kbdev)))
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbase_reset_gpu_is_active(kbdev) || kbase_is_gpu_removed(kbdev))
+#else
+	if (kbase_reset_gpu_is_active(kbdev))
+#endif
 		return;
 
 	for (js = 0; js < kbdev->gpu_props.num_job_slots; js++) {
@@ -892,7 +896,7 @@ void kbase_backend_slot_update(struct kbase_device *kbdev)
 				break;
 
 			case KBASE_ATOM_GPU_RB_WAITING_BLOCKED:
-				if (katom[idx]->atom_flags & KBASE_KATOM_FLAG_X_DEP_BLOCKED)
+				if (kbase_js_atom_blocked_on_x_dep(katom[idx]))
 					break;
 
 				katom[idx]->gpu_rb_state =
@@ -1232,7 +1236,7 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, unsigned int js, u32 comp
 	 * When a hard-stop is followed close after a soft-stop, the completion
 	 * code may be set to STOPPED, even though the job is terminated
 	 */
-	if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TMIX_8438)) {
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TMIX_8438)) {
 		if (completion_code == BASE_JD_EVENT_STOPPED &&
 		    (katom->atom_flags & KBASE_KATOM_FLAG_BEEN_HARD_STOPPED)) {
 			completion_code = BASE_JD_EVENT_TERMINATED;
@@ -1270,8 +1274,7 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, unsigned int js, u32 comp
 		struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
 		unsigned int i;
 
-		if (!kbase_ctx_flag(katom->kctx, KCTX_DYING) &&
-		    !kbase_ctx_flag(katom->kctx, KCTX_PAGE_FAULT_REPORT_SKIP)) {
+		if (!kbase_ctx_flag(katom->kctx, KCTX_DYING)) {
 			dev_warn(kbdev->dev, "error detected from slot %d, job status 0x%08x (%s)",
 				 js, completion_code, kbase_gpu_exception_name(completion_code));
 
@@ -1327,9 +1330,6 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, unsigned int js, u32 comp
 		dev_dbg(kbdev->dev, "Update job chain address of atom %pK to resume from 0x%llx\n",
 			(void *)katom, job_tail);
 
-		/* Some of the job has been executed, so we update the job chain address to where
-		 *  we should resume from
-		 */
 		katom->jc = job_tail;
 		KBASE_KTRACE_ADD_JM_SLOT(kbdev, JM_UPDATE_HEAD, katom->kctx, katom, job_tail, js);
 	}
@@ -1380,12 +1380,10 @@ void kbase_gpu_complete_hw(struct kbase_device *kbdev, unsigned int js, u32 comp
 		dev_dbg(kbdev->dev, "Cross-slot dependency %pK has become runnable.\n",
 			(void *)katom);
 
-		/* Cross-slot dependency has now become runnable. Try to submit it. */
-
 		/* Check if there are lower priority jobs to soft stop */
 		kbase_job_slot_ctx_priority_check_locked(kctx, katom);
 
-		kbase_jm_try_kick(kbdev, 1UL << katom->slot_nr);
+		kbase_jm_try_kick(kbdev, 1 << katom->slot_nr);
 	}
 
 	/* For partial shader core off L2 cache flush */
@@ -1438,7 +1436,7 @@ void kbase_backend_reset(struct kbase_device *kbdev, ktime_t *end_timestamp)
 			 * then leave it in the RB and next time we're kicked
 			 * it will be processed again from the starting state.
 			 */
-			if (!kbase_is_gpu_removed(kbdev) && keep_in_jm_rb) {
+			if (keep_in_jm_rb) {
 				katom->protected_state.exit = KBASE_ATOM_EXIT_PROTECTED_CHECK;
 				/* As the atom was not removed, increment the
 				 * index so that we read the correct atom in the
@@ -1537,7 +1535,7 @@ static int should_stop_x_dep_slot(struct kbase_jd_atom *katom)
 
 		if (dep_atom->gpu_rb_state != KBASE_ATOM_GPU_RB_NOT_IN_SLOT_RB &&
 		    dep_atom->gpu_rb_state != KBASE_ATOM_GPU_RB_RETURN_TO_JS)
-			return (int)dep_atom->slot_nr;
+			return dep_atom->slot_nr;
 	}
 	return -1;
 }
@@ -1716,12 +1714,10 @@ bool kbase_backend_soft_hard_stop_slot(struct kbase_device *kbdev, struct kbase_
 	}
 
 	if (stop_x_dep_idx0 != -1)
-		kbase_backend_soft_hard_stop_slot(kbdev, kctx, (unsigned int)stop_x_dep_idx0, NULL,
-						  action);
+		kbase_backend_soft_hard_stop_slot(kbdev, kctx, stop_x_dep_idx0, NULL, action);
 
 	if (stop_x_dep_idx1 != -1)
-		kbase_backend_soft_hard_stop_slot(kbdev, kctx, (unsigned int)stop_x_dep_idx1, NULL,
-						  action);
+		kbase_backend_soft_hard_stop_slot(kbdev, kctx, stop_x_dep_idx1, NULL, action);
 
 	return ret;
 }
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_js_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_js_backend.c
index 99037c25bf08..c40ffbf9e089 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_js_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_js_backend.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -46,7 +46,7 @@ static inline bool timer_callback_should_run(struct kbase_device *kbdev, int nr_
 	}
 #endif /* CONFIG_MALI_BIFROST_DEBUG */
 
-	if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_9435)) {
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_9435)) {
 		/* Timeouts would have to be 4x longer (due to micro-
 		 * architectural design) to support OpenCL conformance tests, so
 		 * only run the timer when there's:
@@ -100,7 +100,7 @@ static enum hrtimer_restart timer_callback(struct hrtimer *timer)
 			/* The current version of the model doesn't support
 			 * Soft-Stop
 			 */
-			if (!kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_5736)) {
+			if (!kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_5736)) {
 				u32 ticks = atom->ticks++;
 
 #if !defined(CONFIG_MALI_JOB_DUMP) && !defined(CONFIG_MALI_VECTOR_DUMP)
@@ -174,12 +174,12 @@ static enum hrtimer_restart timer_callback(struct hrtimer *timer)
 					 * now. Hard stop the slot.
 					 */
 #if !KBASE_DISABLE_SCHEDULING_HARD_STOPS
-					u32 ms = js_devdata->scheduling_period_ns / 1000000u;
+					int ms = js_devdata->scheduling_period_ns / 1000000u;
 					if (!kbase_is_quick_reset_enabled(kbdev))
 						dev_warn(
 							kbdev->dev,
-							"JS: Job Hard-Stopped (took more than %u ticks at %u ms/tick)",
-							ticks, ms);
+							"JS: Job Hard-Stopped (took more than %lu ticks at %lu ms/tick)",
+							(unsigned long)ticks, (unsigned long)ms);
 					kbase_job_slot_hardstop(atom->kctx, s, atom);
 #endif
 				} else if (ticks == gpu_reset_ticks) {
@@ -210,11 +210,11 @@ static enum hrtimer_restart timer_callback(struct hrtimer *timer)
 					 * ticks. Hard stop the slot.
 					 */
 #if !KBASE_DISABLE_SCHEDULING_HARD_STOPS
-					u32 ms = js_devdata->scheduling_period_ns / 1000000u;
+					int ms = js_devdata->scheduling_period_ns / 1000000u;
 					dev_warn(
 						kbdev->dev,
-						"JS: Job Hard-Stopped (took more than %u ticks at %u ms/tick)",
-						ticks, ms);
+						"JS: Job Hard-Stopped (took more than %lu ticks at %lu ms/tick)",
+						(unsigned long)ticks, (unsigned long)ms);
 					kbase_job_slot_hardstop(atom->kctx, s, atom);
 #endif
 				} else if (ticks == js_devdata->gpu_reset_ticks_dumping) {
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.c
index 0f4a8cd096bb..4c6bb912105e 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -25,8 +25,42 @@
  *   insmod'ing mali_kbase.ko with no arguments after a build with "scons
  *   gpu=tXYZ" will yield the expected GPU ID for tXYZ. This can always be
  *   overridden by passing the 'no_mali_gpu' argument to insmod.
+ *
+ * - if CONFIG_MALI_BIFROST_ERROR_INJECT is defined the error injection system is
+ *   activated.
  */
 
+/* Implementation of failure injection system:
+ *
+ * Error conditions are generated by gpu_generate_error().
+ * According to CONFIG_MALI_BIFROST_ERROR_INJECT definition gpu_generate_error() either
+ * generates an error HW condition randomly (CONFIG_MALI_ERROR_INJECT_RANDOM) or
+ * checks if there is (in error_track_list) an error configuration to be set for
+ * the current job chain (CONFIG_MALI_ERROR_INJECT_RANDOM not defined).
+ * Each error condition will trigger a specific "state" for a certain set of
+ * registers as per Midgard Architecture Specifications doc.
+ *
+ * According to Midgard Architecture Specifications doc the following registers
+ * are always affected by error conditions:
+ *
+ * JOB Exception:
+ *				JOB_IRQ_RAWSTAT
+ *				JOB<n> STATUS AREA
+ *
+ * MMU Exception:
+ *				MMU_IRQ_RAWSTAT
+ *				AS<n>_FAULTSTATUS
+ *				AS<n>_FAULTADDRESS
+ *
+ * GPU Exception:
+ *				GPU_IRQ_RAWSTAT
+ *				GPU_FAULTSTATUS
+ *				GPU_FAULTADDRESS
+ *
+ *	For further clarification on the model behaviour upon specific error
+ *      conditions the user may refer to the Midgard Architecture Specification
+ *      document
+ */
 #include <mali_kbase.h>
 #include <device/mali_kbase_device.h>
 #include <hw_access/mali_kbase_hw_access_regmap.h>
@@ -34,8 +68,6 @@
 #include <backend/gpu/mali_kbase_model_linux.h>
 #include <mali_kbase_mem_linux.h>
 
-#include <asm/arch_timer.h>
-
 #if MALI_USE_CSF
 #include <csf/mali_kbase_csf_firmware.h>
 
@@ -46,7 +78,7 @@
 
 /* Array for storing the value of SELECT register for each type of core */
 static u64 ipa_ctl_select_config[KBASE_IPA_CORE_TYPE_NUM];
-static u32 ipa_control_timer_enabled;
+static bool ipa_control_timer_enabled;
 #endif
 
 #if MALI_USE_CSF
@@ -92,7 +124,7 @@ struct error_status_t hw_error_status;
  */
 struct control_reg_values_t {
 	const char *name;
-	u64 gpu_id;
+	u32 gpu_id;
 	u32 as_present;
 	u32 thread_max_threads;
 	u32 thread_max_workgroup_size;
@@ -110,8 +142,8 @@ struct control_reg_values_t {
 struct job_slot {
 	int job_active;
 	int job_queued;
-	u32 job_complete_irq_asserted;
-	u32 job_irq_mask;
+	int job_complete_irq_asserted;
+	int job_irq_mask;
 	int job_disabled;
 };
 
@@ -480,7 +512,7 @@ void *gpu_device_get_data(void *model)
 	return dummy->kbdev;
 }
 
-#define signal_int(m, s) m->slots[(s)].job_complete_irq_asserted = 1u
+#define signal_int(m, s) m->slots[(s)].job_complete_irq_asserted = 1
 
 static char *no_mali_gpu = CONFIG_MALI_NO_MALI_DEFAULT_GPU;
 module_param(no_mali_gpu, charp, 0000);
@@ -490,7 +522,7 @@ MODULE_PARM_DESC(no_mali_gpu, "GPU to identify as");
 static u32 gpu_model_get_prfcnt_value(enum kbase_ipa_core_type core_type, u32 cnt_idx,
 				      bool is_low_word)
 {
-	u64 *counters_data = NULL;
+	u64 *counters_data;
 	u32 core_count = 0;
 	u32 event_index;
 	u64 value = 0;
@@ -546,9 +578,6 @@ static u32 gpu_model_get_prfcnt_value(enum kbase_ipa_core_type core_type, u32 cn
 		break;
 	}
 
-	if (unlikely(counters_data == NULL))
-		return 0;
-
 	for (core = 0; core < core_count; core++) {
 		value += counters_data[event_index];
 		event_index += KBASE_DUMMY_MODEL_COUNTER_PER_CORE;
@@ -621,7 +650,7 @@ static void gpu_model_dump_prfcnt_blocks(u64 *values, u32 *out_index, u32 block_
 
 	for (block_idx = 0; block_idx < block_count; block_idx++) {
 		/* only dump values if core is present */
-		if (!(blocks_present & (1U << block_idx))) {
+		if (!(blocks_present & (1 << block_idx))) {
 #if MALI_USE_CSF
 			/* if CSF dump zeroed out block */
 			memset(&prfcnt_base[*out_index], 0, KBASE_DUMMY_MODEL_BLOCK_SIZE);
@@ -683,11 +712,6 @@ static void gpu_model_dump_nolock(void)
 	performance_counters.time += 10;
 }
 
-static void gpu_model_raise_irq(void *model, u32 irq)
-{
-		gpu_device_raise_irq(model, irq);
-}
-
 #if !MALI_USE_CSF
 static void midgard_model_dump_prfcnt(void)
 {
@@ -719,7 +743,7 @@ void gpu_model_glb_request_job_irq(void *model)
 	spin_lock_irqsave(&hw_error_status.access_lock, flags);
 	hw_error_status.job_irq_status |= JOB_IRQ_GLOBAL_IF;
 	spin_unlock_irqrestore(&hw_error_status.access_lock, flags);
-	gpu_model_raise_irq(model, MODEL_LINUX_JOB_IRQ);
+	gpu_device_raise_irq(model, MODEL_LINUX_JOB_IRQ);
 }
 #endif /* !MALI_USE_CSF */
 
@@ -743,13 +767,13 @@ static void init_register_statuses(struct dummy_model_t *dummy)
 	for (i = 0; i < NUM_MMU_AS; i++) {
 		hw_error_status.as_command[i] = 0;
 		hw_error_status.as_faultstatus[i] = 0;
-		hw_error_status.mmu_irq_mask |= (1u << i);
+		hw_error_status.mmu_irq_mask |= 1 << i;
 	}
 
 	performance_counters.time = 0;
 }
 
-static void update_register_statuses(struct dummy_model_t *dummy, u32 job_slot)
+static void update_register_statuses(struct dummy_model_t *dummy, unsigned int job_slot)
 {
 	lockdep_assert_held(&hw_error_status.access_lock);
 
@@ -891,7 +915,7 @@ static void update_register_statuses(struct dummy_model_t *dummy, u32 job_slot)
 	} /* end of job register statuses */
 
 	if (hw_error_status.errors_mask & IS_A_MMU_ERROR) {
-		u32 i;
+		int i;
 
 		for (i = 0; i < NUM_MMU_AS; i++) {
 			if (i == hw_error_status.faulty_mmu_as) {
@@ -941,10 +965,9 @@ static void update_register_statuses(struct dummy_model_t *dummy, u32 job_slot)
 
 				if (hw_error_status.errors_mask & KBASE_TRANSTAB_BUS_FAULT)
 					hw_error_status.mmu_irq_rawstat |=
-						1u << (16 + i); /* bus error */
+						1 << (16 + i); /* bus error */
 				else
-					hw_error_status.mmu_irq_rawstat |=
-						(1u << i); /* page fault */
+					hw_error_status.mmu_irq_rawstat |= 1 << i; /* page fault */
 			}
 		}
 	} /*end of mmu register statuses */
@@ -956,11 +979,11 @@ static void update_register_statuses(struct dummy_model_t *dummy, u32 job_slot)
 			hw_error_status.gpu_error_irq |= 1;
 			switch (hw_error_status.errors_mask & IS_A_GPU_ERROR) {
 			case KBASE_DELAYED_BUS_FAULT:
-				hw_error_status.gpu_fault_status = (1u << 7);
+				hw_error_status.gpu_fault_status = (1 << 7);
 				break;
 
 			case KBASE_SHAREABILITY_FAULT:
-				hw_error_status.gpu_fault_status = (1u << 7) | (1u << 3);
+				hw_error_status.gpu_fault_status = (1 << 7) | (1 << 3);
 				break;
 
 			default:
@@ -1096,7 +1119,7 @@ static void midgard_model_get_outputs(void *h)
 	lockdep_assert_held(&hw_error_status.access_lock);
 
 	if (hw_error_status.job_irq_status)
-		gpu_model_raise_irq(dummy, MODEL_LINUX_JOB_IRQ);
+		gpu_device_raise_irq(dummy, MODEL_LINUX_JOB_IRQ);
 
 	if ((dummy->power_changed && dummy->power_changed_mask) ||
 	    (dummy->reset_completed & dummy->reset_completed_mask) ||
@@ -1107,16 +1130,16 @@ static void midgard_model_get_outputs(void *h)
 	    (dummy->flush_pa_range_completed && dummy->flush_pa_range_completed_irq_enabled) ||
 #endif
 	    (dummy->clean_caches_completed && dummy->clean_caches_completed_irq_enabled))
-		gpu_model_raise_irq(dummy, MODEL_LINUX_GPU_IRQ);
+		gpu_device_raise_irq(dummy, MODEL_LINUX_GPU_IRQ);
 
 	if (hw_error_status.mmu_irq_rawstat & hw_error_status.mmu_irq_mask)
-		gpu_model_raise_irq(dummy, MODEL_LINUX_MMU_IRQ);
+		gpu_device_raise_irq(dummy, MODEL_LINUX_MMU_IRQ);
 }
 
 static void midgard_model_update(void *h)
 {
 	struct dummy_model_t *dummy = (struct dummy_model_t *)h;
-	u32 i;
+	int i;
 
 	lockdep_assert_held(&hw_error_status.access_lock);
 
@@ -1134,16 +1157,19 @@ static void midgard_model_update(void *h)
 		 * as we will overwrite the register status of the job in
 		 * the head registers - which has not yet been read
 		 */
-		if ((hw_error_status.job_irq_rawstat & (1u << (i + 16))) ||
-		    (hw_error_status.job_irq_rawstat & (1u << i))) {
+		if ((hw_error_status.job_irq_rawstat & (1 << (i + 16))) ||
+		    (hw_error_status.job_irq_rawstat & (1 << i))) {
 			continue;
 		}
 
 		/*this job is done assert IRQ lines */
 		signal_int(dummy, i);
+#ifdef CONFIG_MALI_BIFROST_ERROR_INJECT
+		midgard_set_error(i);
+#endif /* CONFIG_MALI_BIFROST_ERROR_INJECT */
 		update_register_statuses(dummy, i);
 		/*if this job slot returned failures we cannot use it */
-		if (hw_error_status.job_irq_rawstat & (1u << (i + 16))) {
+		if (hw_error_status.job_irq_rawstat & (1 << (i + 16))) {
 			dummy->slots[i].job_active = 0;
 			continue;
 		}
@@ -1151,7 +1177,7 @@ static void midgard_model_update(void *h)
 		dummy->slots[i].job_active = dummy->slots[i].job_queued;
 		dummy->slots[i].job_queued = 0;
 		if (dummy->slots[i].job_active) {
-			if (hw_error_status.job_irq_rawstat & (1u << (i + 16)))
+			if (hw_error_status.job_irq_rawstat & (1 << (i + 16)))
 				model_error_log(KBASE_CORE,
 						"\natom %lld running a job on a dirty slot",
 						hw_error_status.current_jc);
@@ -1167,7 +1193,7 @@ static void invalidate_active_jobs(struct dummy_model_t *dummy)
 
 	for (i = 0; i < NUM_SLOTS; i++) {
 		if (dummy->slots[i].job_active) {
-			hw_error_status.job_irq_rawstat |= (1u << (16 + i));
+			hw_error_status.job_irq_rawstat |= (1 << (16 + i));
 
 			hw_error_status.js_status[i] = 0x7f; /*UNKNOWN*/
 		}
@@ -1230,7 +1256,7 @@ void midgard_model_write_reg(void *h, u32 addr, u32 value)
 		int i;
 
 		for (i = 0; i < NUM_SLOTS; i++) {
-			if (value & ((1u << i) | (1u << (i + 16))))
+			if (value & ((1 << i) | (1 << (i + 16))))
 				dummy->slots[i].job_complete_irq_asserted = 0;
 			/* hw_error_status.js_status[i] is cleared in
 			 * update_job_irq_js_state
@@ -1253,9 +1279,6 @@ void midgard_model_write_reg(void *h, u32 addr, u32 value)
 
 		hw_error_status.job_irq_rawstat &= ~(value);
 		hw_error_status.job_irq_status &= ~(value);
-	} else if (addr == JOB_CONTROL_REG(JOB_IRQ_RAWSTAT)) {
-		hw_error_status.job_irq_rawstat |= value;
-		hw_error_status.job_irq_status |= value;
 	} else if (addr == JOB_CONTROL_REG(JOB_IRQ_MASK)) {
 		/* ignore JOB_IRQ_MASK as it is handled by CSFFW */
 #endif /* !MALI_USE_CSF */
@@ -1355,7 +1378,7 @@ void midgard_model_write_reg(void *h, u32 addr, u32 value)
 		pr_debug("Received IPA_CONTROL command");
 	} else if (addr == IPA_CONTROL_REG(TIMER)
 	) {
-		ipa_control_timer_enabled = value ? 1U : 0U;
+		ipa_control_timer_enabled = value ? true : false;
 	} else if ((addr >= IPA_CONTROL_REG(SELECT_CSHW_LO)) &&
 		   (addr <= IPA_CONTROL_REG(SELECT_SHADER_HI))) {
 		enum kbase_ipa_core_type core_type =
@@ -1376,7 +1399,7 @@ void midgard_model_write_reg(void *h, u32 addr, u32 value)
 		hw_error_status.mmu_irq_rawstat &= (~value);
 	} else if ((addr >= MMU_STAGE1_REG(MMU_AS_REG(0, AS_TRANSTAB_LO))) &&
 		   (addr <= MMU_STAGE1_REG(MMU_AS_REG(15, AS_STATUS)))) {
-		u32 mem_addr_space = (addr - MMU_STAGE1_REG(MMU_AS_REG(0, AS_TRANSTAB_LO))) >> 6;
+		int mem_addr_space = (addr - MMU_STAGE1_REG(MMU_AS_REG(0, AS_TRANSTAB_LO))) >> 6;
 
 		switch (addr & 0x3F) {
 		case AS_COMMAND:
@@ -1530,7 +1553,6 @@ void midgard_model_write_reg(void *h, u32 addr, u32 value)
 		case L2_PWROFF_HI:
 		case PWR_KEY:
 		case PWR_OVERRIDE0:
-		case PWR_OVERRIDE1:
 #if MALI_USE_CSF
 		case SHADER_PWRFEATURES:
 		case CSF_CONFIG:
@@ -1574,7 +1596,8 @@ void midgard_model_read_reg(void *h, u32 addr, u32 *const value)
 #else /* !MALI_USE_CSF */
 	if (addr == GPU_CONTROL_REG(GPU_ID)) {
 #endif /* !MALI_USE_CSF */
-		*value = dummy->control_reg_values->gpu_id & U32_MAX;
+
+		*value = dummy->control_reg_values->gpu_id;
 	} else if (addr == JOB_CONTROL_REG(JOB_IRQ_RAWSTAT)) {
 		*value = hw_error_status.job_irq_rawstat;
 		pr_debug("%s", "JS_IRQ_RAWSTAT being read");
@@ -1599,7 +1622,7 @@ void midgard_model_read_reg(void *h, u32 addr, u32 *const value)
 #if MALI_USE_CSF
 			 ((dummy->flush_pa_range_completed_irq_enabled ? 1u : 0u) << 20) |
 #endif
-			 (dummy->power_changed_mask << 9) | (1u << 7) | 1u;
+			 (dummy->power_changed_mask << 9) | (1 << 7) | 1;
 		pr_debug("GPU_IRQ_MASK read %x", *value);
 	} else if (addr == GPU_CONTROL_REG(GPU_IRQ_RAWSTAT)) {
 		*value = ((dummy->clean_caches_completed ? 1u : 0u) << 17) |
@@ -1868,7 +1891,7 @@ void midgard_model_read_reg(void *h, u32 addr, u32 *const value)
 		*value = 0;
 	} else if (addr >= MMU_STAGE1_REG(MMU_AS_REG(0, AS_TRANSTAB_LO)) &&
 		   addr <= MMU_STAGE1_REG(MMU_AS_REG(15, AS_STATUS))) {
-		u32 mem_addr_space = (addr - MMU_STAGE1_REG(MMU_AS_REG(0, AS_TRANSTAB_LO))) >> 6;
+		int mem_addr_space = (addr - MMU_STAGE1_REG(MMU_AS_REG(0, AS_TRANSTAB_LO))) >> 6;
 
 		switch (addr & 0x3F) {
 		case AS_TRANSTAB_LO:
@@ -1904,7 +1927,7 @@ void midgard_model_read_reg(void *h, u32 addr, u32 *const value)
 		default:
 			model_error_log(
 				KBASE_CORE,
-				"Dummy model register access: Reading unsupported MMU #%u register 0x%x. Returning 0\n",
+				"Dummy model register access: Reading unsupported MMU #%d register 0x%x. Returning 0\n",
 				mem_addr_space, addr);
 			*value = 0;
 			break;
@@ -1953,8 +1976,7 @@ void midgard_model_read_reg(void *h, u32 addr, u32 *const value)
 		*value = dummy->control_reg_values->gpu_features_lo;
 	} else if (addr == GPU_CONTROL_REG(GPU_FEATURES_HI)) {
 		*value = dummy->control_reg_values->gpu_features_hi;
-	}
-	else {
+	} else {
 		model_error_log(
 			KBASE_CORE,
 			"Dummy model register access: Reading unsupported register 0x%x. Returning 0\n",
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.h
index d4d2160a4438..3c6561a2f7b9 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.h
@@ -134,10 +134,10 @@ struct error_status_t {
 
 	u32 errors_mask;
 	u32 mmu_table_level;
-	u32 faulty_mmu_as;
+	int faulty_mmu_as;
 
 	u64 current_jc;
-	u32 current_job_slot;
+	int current_job_slot;
 
 	u32 job_irq_rawstat;
 	u32 job_irq_status;
@@ -168,7 +168,7 @@ struct gpu_model_prfcnt_en {
 	u32 shader;
 };
 
-void midgard_set_error(u32 job_slot);
+void midgard_set_error(int job_slot);
 int job_atom_inject_error(struct kbase_error_params *params);
 int gpu_model_control(void *h, struct kbase_model_control_params *params);
 
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_error_generator.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_error_generator.c
new file mode 100644
index 000000000000..072ad5bb01a0
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_error_generator.c
@@ -0,0 +1,172 @@
+// SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
+/*
+ *
+ * (C) COPYRIGHT 2014-2015, 2018-2023 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#include <mali_kbase.h>
+#include <linux/random.h>
+#include "backend/gpu/mali_kbase_model_linux.h"
+
+static struct kbase_error_atom *error_track_list;
+
+#ifdef CONFIG_MALI_ERROR_INJECT_RANDOM
+
+/** Kernel 6.1.0 has dropped prandom_u32(), use get_random_u32() */
+#if (KERNEL_VERSION(6, 1, 0) <= LINUX_VERSION_CODE)
+#define prandom_u32 get_random_u32
+#endif
+
+/*following error probability are set quite high in order to stress the driver*/
+static unsigned int error_probability = 50; /* to be set between 0 and 100 */
+/* probability to have multiple error give that there is an error */
+static unsigned int multiple_error_probability = 50;
+
+/* all the error conditions supported by the model */
+#define TOTAL_FAULTS 27
+/* maximum number of levels in the MMU translation table tree */
+#define MAX_MMU_TABLE_LEVEL 4
+/* worst case scenario is <1 MMU fault + 1 job fault + 2 GPU faults> */
+#define MAX_CONCURRENT_FAULTS 3
+
+/**
+ * gpu_generate_error - Generate GPU error
+ */
+static void gpu_generate_error(void)
+{
+	unsigned int errors_num = 0;
+
+	/*is there at least one error? */
+	if ((prandom_u32() % 100) < error_probability) {
+		/* pick up a faulty mmu address space */
+		hw_error_status.faulty_mmu_as = prandom_u32() % NUM_MMU_AS;
+		/* pick up an mmu table level */
+		hw_error_status.mmu_table_level = 1 + (prandom_u32() % MAX_MMU_TABLE_LEVEL);
+		hw_error_status.errors_mask = (u32)(1 << (prandom_u32() % TOTAL_FAULTS));
+
+		/*is there also one or more errors? */
+		if ((prandom_u32() % 100) < multiple_error_probability) {
+			errors_num = 1 + (prandom_u32() % (MAX_CONCURRENT_FAULTS - 1));
+			while (errors_num-- > 0) {
+				u32 temp_mask;
+
+				temp_mask = (u32)(1 << (prandom_u32() % TOTAL_FAULTS));
+				/* below we check that no bit of the same error
+				 * type is set again in the error mask
+				 */
+				if ((temp_mask & IS_A_JOB_ERROR) &&
+				    (hw_error_status.errors_mask & IS_A_JOB_ERROR)) {
+					errors_num++;
+					continue;
+				}
+				if ((temp_mask & IS_A_MMU_ERROR) &&
+				    (hw_error_status.errors_mask & IS_A_MMU_ERROR)) {
+					errors_num++;
+					continue;
+				}
+				if ((temp_mask & IS_A_GPU_ERROR) &&
+				    (hw_error_status.errors_mask & IS_A_GPU_ERROR)) {
+					errors_num++;
+					continue;
+				}
+				/* this error mask is already set */
+				if ((hw_error_status.errors_mask | temp_mask) ==
+				    hw_error_status.errors_mask) {
+					errors_num++;
+					continue;
+				}
+				hw_error_status.errors_mask |= temp_mask;
+			}
+		}
+	}
+}
+#endif
+
+int job_atom_inject_error(struct kbase_error_params *params)
+{
+	struct kbase_error_atom *new_elem;
+
+	KBASE_DEBUG_ASSERT(params);
+
+	new_elem = kzalloc(sizeof(*new_elem), GFP_KERNEL);
+
+	if (!new_elem) {
+		model_error_log(KBASE_CORE,
+				"\njob_atom_inject_error: kzalloc failed for new_elem\n");
+		return -ENOMEM;
+	}
+	new_elem->params.jc = params->jc;
+	new_elem->params.errors_mask = params->errors_mask;
+	new_elem->params.mmu_table_level = params->mmu_table_level;
+	new_elem->params.faulty_mmu_as = params->faulty_mmu_as;
+
+	/*circular list below */
+	if (error_track_list == NULL) { /*no elements */
+		error_track_list = new_elem;
+		new_elem->next = error_track_list;
+	} else {
+		struct kbase_error_atom *walker = error_track_list;
+
+		while (walker->next != error_track_list)
+			walker = walker->next;
+
+		new_elem->next = error_track_list;
+		walker->next = new_elem;
+	}
+	return 0;
+}
+
+void midgard_set_error(int job_slot)
+{
+#ifdef CONFIG_MALI_ERROR_INJECT_RANDOM
+	gpu_generate_error();
+#else
+	struct kbase_error_atom *walker, *auxiliar;
+
+	if (error_track_list != NULL) {
+		walker = error_track_list->next;
+		auxiliar = error_track_list;
+		do {
+			if (walker->params.jc == hw_error_status.current_jc) {
+				/* found a faulty atom matching with the
+				 * current one
+				 */
+				hw_error_status.errors_mask = walker->params.errors_mask;
+				hw_error_status.mmu_table_level = walker->params.mmu_table_level;
+				hw_error_status.faulty_mmu_as = walker->params.faulty_mmu_as;
+				hw_error_status.current_job_slot = job_slot;
+
+				if (walker->next == walker) {
+					/* only one element */
+					kfree(error_track_list);
+					error_track_list = NULL;
+				} else {
+					auxiliar->next = walker->next;
+					if (walker == error_track_list)
+						error_track_list = walker->next;
+
+					kfree(walker);
+				}
+				break;
+			}
+			auxiliar = walker;
+			walker = walker->next;
+		} while (auxiliar->next != error_track_list);
+	}
+#endif /* CONFIG_MALI_ERROR_INJECT_RANDOM */
+}
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.c
index fa12e52143f2..098b60d4f4a2 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.c
@@ -37,39 +37,68 @@ struct model_irq_data {
 	struct work_struct work;
 };
 
-#define DEFINE_SERVE_IRQ(irq_handler)                                                          \
-	static void serve_##irq_handler(struct work_struct *work)                              \
-	{                                                                                      \
-		struct model_irq_data *data = container_of(work, struct model_irq_data, work); \
-		struct kbase_device *kbdev = data->kbdev;                                      \
-		irq_handler(kbdev);                                                            \
-		kmem_cache_free(kbdev->irq_slab, data);                                        \
-	}
-
-static void job_irq(struct kbase_device *kbdev)
+static void serve_job_irq(struct work_struct *work)
 {
+	struct model_irq_data *data = container_of(work, struct model_irq_data, work);
+	struct kbase_device *kbdev = data->kbdev;
+
 	/* Make sure no worker is already serving this IRQ */
-	while (atomic_cmpxchg(&kbdev->serving_job_irq, 1, 0) == 1)
-		kbase_get_interrupt_handler(kbdev, JOB_IRQ_TAG)(0, kbdev);
+	while (atomic_cmpxchg(&kbdev->serving_job_irq, 1, 0) == 1) {
+		u32 val;
+
+		while ((val = kbase_reg_read32(kbdev, JOB_CONTROL_ENUM(JOB_IRQ_STATUS)))) {
+			unsigned long flags;
+
+			/* Handle the IRQ */
+			spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+#if MALI_USE_CSF
+			kbase_csf_interrupt(kbdev, val);
+#else
+			kbase_job_done(kbdev, val);
+#endif
+			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+		}
+	}
+
+	kmem_cache_free(kbdev->irq_slab, data);
 }
-DEFINE_SERVE_IRQ(job_irq)
 
-static void gpu_irq(struct kbase_device *kbdev)
+static void serve_gpu_irq(struct work_struct *work)
 {
+	struct model_irq_data *data = container_of(work, struct model_irq_data, work);
+	struct kbase_device *kbdev = data->kbdev;
+
 	/* Make sure no worker is already serving this IRQ */
-	while (atomic_cmpxchg(&kbdev->serving_gpu_irq, 1, 0) == 1)
-		kbase_get_interrupt_handler(kbdev, GPU_IRQ_TAG)(0, kbdev);
+	while (atomic_cmpxchg(&kbdev->serving_gpu_irq, 1, 0) == 1) {
+		u32 val;
+
+		while ((val = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_STATUS)))) {
+			/* Handle the GPU_IRQ */
+			kbase_gpu_interrupt(kbdev, val);
+		}
+
+	}
+
+	kmem_cache_free(kbdev->irq_slab, data);
 }
-DEFINE_SERVE_IRQ(gpu_irq)
 
-static void mmu_irq(struct kbase_device *kbdev)
+static void serve_mmu_irq(struct work_struct *work)
 {
+	struct model_irq_data *data = container_of(work, struct model_irq_data, work);
+	struct kbase_device *kbdev = data->kbdev;
+
 	/* Make sure no worker is already serving this IRQ */
-	while (atomic_cmpxchg(&kbdev->serving_mmu_irq, 1, 0) == 1)
-		kbase_get_interrupt_handler(kbdev, MMU_IRQ_TAG)(0, kbdev);
-}
-DEFINE_SERVE_IRQ(mmu_irq)
+	if (atomic_cmpxchg(&kbdev->serving_mmu_irq, 1, 0) == 1) {
+		u32 val;
+
+		while ((val = kbase_reg_read32(kbdev, MMU_CONTROL_ENUM(IRQ_STATUS)))) {
+			/* Handle the IRQ */
+			kbase_mmu_interrupt(kbdev, val);
+		}
+	}
 
+	kmem_cache_free(kbdev->irq_slab, data);
+}
 
 void gpu_device_raise_irq(void *model, u32 irq)
 {
@@ -127,9 +156,6 @@ int kbase_install_interrupts(struct kbase_device *kbdev)
 		return -ENOMEM;
 	}
 
-	kbdev->nr_irqs = 3;
-
-
 	return 0;
 }
 
@@ -149,13 +175,23 @@ void kbase_synchronize_irqs(struct kbase_device *kbdev)
 KBASE_EXPORT_TEST_API(kbase_synchronize_irqs);
 
 int kbase_set_custom_irq_handler(struct kbase_device *kbdev, irq_handler_t custom_handler,
-				 u32 irq_tag)
+				 int irq_type)
 {
 	return 0;
 }
 
 KBASE_EXPORT_TEST_API(kbase_set_custom_irq_handler);
 
+irqreturn_t kbase_gpu_irq_test_handler(int irq, void *data, u32 val)
+{
+	if (!val)
+		return IRQ_NONE;
+
+	return IRQ_HANDLED;
+}
+
+KBASE_EXPORT_TEST_API(kbase_gpu_irq_test_handler);
+
 int kbase_gpu_device_create(struct kbase_device *kbdev)
 {
 	kbdev->model = midgard_model_create(kbdev);
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.h
index d38bb8891be1..8f09afe3d1cc 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2022 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -48,8 +48,12 @@
 /*
  * Include Model definitions
  */
+
+#if IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
 #include <backend/gpu/mali_kbase_model_dummy.h>
+#endif /* IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI) */
 
+#if !IS_ENABLED(CONFIG_MALI_REAL_HW)
 /**
  * kbase_gpu_device_create() - Generic create function.
  *
@@ -142,5 +146,6 @@ void gpu_device_set_data(void *model, void *data);
  * Return: Pointer to the data carried by model.
  */
 void *gpu_device_get_data(void *model);
+#endif /* !IS_ENABLED(CONFIG_MALI_REAL_HW) */
 
 #endif /* _KBASE_MODEL_LINUX_H_ */
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_backend.c
index e1941d50133a..61b756855060 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_backend.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -33,10 +33,8 @@
 #include <backend/gpu/mali_kbase_js_internal.h>
 #include <backend/gpu/mali_kbase_jm_internal.h>
 #else
-#include <linux/version_compat_defs.h>
 #include <linux/pm_runtime.h>
 #include <mali_kbase_reset_gpu.h>
-#include <csf/mali_kbase_csf_scheduler.h>
 #endif /* !MALI_USE_CSF */
 #include <hwcnt/mali_kbase_hwcnt_context.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
@@ -44,7 +42,6 @@
 #include <mali_kbase_dummy_job_wa.h>
 #include <backend/gpu/mali_kbase_irq_internal.h>
 
-
 static void kbase_pm_gpu_poweroff_wait_wq(struct work_struct *data);
 static void kbase_pm_hwcnt_disable_worker(struct work_struct *data);
 static void kbase_pm_gpu_clock_control_worker(struct work_struct *data);
@@ -54,8 +51,6 @@ int kbase_pm_runtime_init(struct kbase_device *kbdev)
 	struct kbase_pm_callback_conf *callbacks;
 
 	callbacks = (struct kbase_pm_callback_conf *)POWER_MANAGEMENT_CALLBACKS;
-
-
 	if (callbacks) {
 		kbdev->pm.backend.callback_power_on = callbacks->power_on_callback;
 		kbdev->pm.backend.callback_power_off = callbacks->power_off_callback;
@@ -98,8 +93,10 @@ void kbase_pm_register_access_enable(struct kbase_device *kbdev)
 	if (callbacks)
 		callbacks->power_on_callback(kbdev);
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	if (WARN_ON(kbase_pm_is_gpu_lost(kbdev)))
 		dev_err(kbdev->dev, "Attempting to power on while GPU lost\n");
+#endif
 
 	kbdev->pm.backend.gpu_powered = true;
 }
@@ -132,7 +129,9 @@ int kbase_hwaccess_pm_init(struct kbase_device *kbdev)
 	INIT_WORK(&kbdev->pm.backend.gpu_poweroff_wait_work, kbase_pm_gpu_poweroff_wait_wq);
 
 	kbdev->pm.backend.ca_cores_enabled = ~0ull;
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	kbase_pm_set_gpu_lost(kbdev, false);
+#endif
 	init_waitqueue_head(&kbdev->pm.backend.gpu_in_desired_state_wait);
 
 #if !MALI_USE_CSF
@@ -174,18 +173,11 @@ int kbase_hwaccess_pm_init(struct kbase_device *kbdev)
 	kbase_hwcnt_context_disable(kbdev->hwcnt_gpu_ctx);
 
 #if MALI_USE_CSF && defined(KBASE_PM_RUNTIME)
-	kbdev->pm.backend.gpu_sleep_allowed = 0;
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_GPU_SLEEP) &&
-	    !kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TURSEHW_1997) &&
-	    kbdev->pm.backend.callback_power_runtime_gpu_active &&
-	    kbdev->pm.backend.callback_power_runtime_gpu_idle)
-		set_bit(KBASE_GPU_SUPPORTS_GPU_SLEEP, &kbdev->pm.backend.gpu_sleep_allowed);
-
-	kbdev->pm.backend.apply_hw_issue_TITANHW_2938_wa =
-		kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TITANHW_2938) &&
-		test_bit(KBASE_GPU_SUPPORTS_GPU_SLEEP, &kbdev->pm.backend.gpu_sleep_allowed);
-
-	/* FW Sleep-on-Idle is feature is kept disabled */
+	kbdev->pm.backend.gpu_sleep_supported =
+		kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_GPU_SLEEP) &&
+		!kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TURSEHW_1997) &&
+		kbdev->pm.backend.callback_power_runtime_gpu_active &&
+		kbdev->pm.backend.callback_power_runtime_gpu_idle;
 #endif
 
 	if (IS_ENABLED(CONFIG_MALI_HW_ERRATA_1485982_NOT_AFFECTED))
@@ -193,14 +185,14 @@ int kbase_hwaccess_pm_init(struct kbase_device *kbdev)
 
 	/* WA1: L2 always_on for GPUs being affected by GPU2017-1336 */
 	if (!IS_ENABLED(CONFIG_MALI_HW_ERRATA_1485982_USE_CLOCK_ALTERNATIVE)) {
-		if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_GPU2017_1336))
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_GPU2017_1336))
 			kbdev->pm.backend.l2_always_on = true;
 
 		return 0;
 	}
 
 	/* WA3: Clock slow down for GPUs being affected by GPU2017-1336 */
-	if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_GPU2017_1336)) {
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_GPU2017_1336)) {
 		kbdev->pm.backend.gpu_clock_slow_down_wa = true;
 		kbdev->pm.backend.gpu_clock_slow_down_desired = true;
 		INIT_WORK(&kbdev->pm.backend.gpu_clock_control_work,
@@ -247,58 +239,6 @@ void kbase_pm_do_poweron(struct kbase_device *kbdev, bool is_resume)
 	 */
 }
 
-#if MALI_USE_CSF
-static bool wait_cond_mmu_fault_handling_in_gpu_poweroff_wait_wq(struct kbase_device *kbdev,
-								 int faults_pending)
-{
-	struct kbase_pm_backend_data *backend = &kbdev->pm.backend;
-	bool cond = false;
-
-	kbase_pm_lock(kbdev);
-	cond = backend->poweron_required || (faults_pending == 0);
-	kbase_pm_unlock(kbdev);
-
-	return cond;
-}
-#endif
-
-static void wait_for_mmu_fault_handling_in_gpu_poweroff_wait_wq(struct kbase_device *kbdev)
-{
-#if MALI_USE_CSF
-	bool reset_triggered = false;
-	int ret = 0;
-
-	lockdep_assert_held(&kbdev->pm.lock);
-
-	do {
-		const u64 timeout_us = kbase_get_timeout_ms(kbdev, CSF_PM_TIMEOUT) * USEC_PER_MSEC;
-		const unsigned long delay_us = 10;
-		int faults_pending = 0;
-
-		kbase_pm_unlock(kbdev);
-		ret = read_poll_timeout_atomic(
-			atomic_read, faults_pending,
-			wait_cond_mmu_fault_handling_in_gpu_poweroff_wait_wq(kbdev, faults_pending),
-			delay_us, timeout_us, false, &kbdev->faults_pending);
-		kbase_pm_lock(kbdev);
-
-		if (ret && !reset_triggered) {
-			dev_err(kbdev->dev,
-				"Wait for fault handling timed-out in gpu_poweroff_wait_wq");
-			if (kbase_prepare_to_reset_gpu(kbdev,
-						       RESET_FLAGS_HWC_UNRECOVERABLE_ERROR)) {
-				kbase_reset_gpu(kbdev);
-				reset_triggered = true;
-			}
-		}
-	} while (ret);
-#else
-	kbase_pm_unlock(kbdev);
-	kbase_flush_mmu_wqs(kbdev);
-	kbase_pm_lock(kbdev);
-#endif
-}
-
 static void pm_handle_power_off(struct kbase_device *kbdev)
 {
 	struct kbase_pm_backend_data *backend = &kbdev->pm.backend;
@@ -343,13 +283,17 @@ static void pm_handle_power_off(struct kbase_device *kbdev)
 		 * process.  Interrupts are disabled so no more faults
 		 * should be generated at this point.
 		 */
-		wait_for_mmu_fault_handling_in_gpu_poweroff_wait_wq(kbdev);
+		kbase_pm_unlock(kbdev);
+		kbase_flush_mmu_wqs(kbdev);
+		kbase_pm_lock(kbdev);
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 		/* poweron_required may have changed while pm lock
 		 * was released.
 		 */
 		if (kbase_pm_is_gpu_lost(kbdev))
 			backend->poweron_required = false;
+#endif
 
 		/* Turn off clock now that fault have been handled. We
 		 * dropped locks so poweron_required may have changed -
@@ -391,7 +335,7 @@ static void kbase_pm_gpu_poweroff_wait_wq(struct work_struct *data)
 		backend->poweron_required = false;
 		kbdev->pm.backend.l2_desired = true;
 #if MALI_USE_CSF
-		kbdev->pm.backend.mcu_desired = kbdev->pm.backend.mcu_poweron_required;
+		kbdev->pm.backend.mcu_desired = true;
 #endif
 		kbase_pm_update_state(kbdev);
 		kbase_pm_update_cores_state_nolock(kbdev);
@@ -754,6 +698,12 @@ int kbase_hwaccess_pm_powerup(struct kbase_device *kbdev, unsigned int flags)
 	kbdev->pm.backend.gpu_cycle_counter_requests = 0;
 	spin_unlock_irqrestore(&kbdev->pm.backend.gpu_cycle_counter_requests_lock, irq_flags);
 
+	/* We are ready to receive IRQ's now as power policy is set up, so
+	 * enable them now.
+	 */
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+	kbdev->pm.backend.driver_ready_for_irqs = true;
+#endif
 	kbase_pm_enable_interrupts(kbdev);
 
 	WARN_ON(!kbdev->pm.backend.gpu_powered);
@@ -858,11 +808,9 @@ void kbase_pm_set_debug_core_mask(struct kbase_device *kbdev, u64 new_core_mask)
 }
 KBASE_EXPORT_TEST_API(kbase_pm_set_debug_core_mask);
 #else
-void kbase_pm_set_debug_core_mask(struct kbase_device *kbdev, u64 *new_core_mask,
-				  size_t new_core_mask_size)
+void kbase_pm_set_debug_core_mask(struct kbase_device *kbdev, u64 new_core_mask_js0,
+				  u64 new_core_mask_js1, u64 new_core_mask_js2)
 {
-	size_t i;
-
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 	lockdep_assert_held(&kbdev->pm.lock);
 
@@ -870,14 +818,13 @@ void kbase_pm_set_debug_core_mask(struct kbase_device *kbdev, u64 *new_core_mask
 		dev_warn_once(
 			kbdev->dev,
 			"Change of core mask not supported for slot 0 as dummy job WA is enabled");
-		new_core_mask[0] = kbdev->pm.debug_core_mask[0];
+		new_core_mask_js0 = kbdev->pm.debug_core_mask[0];
 	}
 
-	kbdev->pm.debug_core_mask_all = 0;
-	for (i = 0; i < new_core_mask_size; i++) {
-		kbdev->pm.debug_core_mask[i] = new_core_mask[i];
-		kbdev->pm.debug_core_mask_all |= new_core_mask[i];
-	}
+	kbdev->pm.debug_core_mask[0] = new_core_mask_js0;
+	kbdev->pm.debug_core_mask[1] = new_core_mask_js1;
+	kbdev->pm.debug_core_mask[2] = new_core_mask_js2;
+	kbdev->pm.debug_core_mask_all = new_core_mask_js0 | new_core_mask_js1 | new_core_mask_js2;
 
 	kbase_pm_update_dynamic_cores_onoff(kbdev);
 }
@@ -943,11 +890,13 @@ void kbase_hwaccess_pm_resume(struct kbase_device *kbdev)
 	/* System resume callback has begun */
 	kbdev->pm.resuming = true;
 	kbdev->pm.suspending = false;
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	if (kbase_pm_is_gpu_lost(kbdev)) {
 		dev_dbg(kbdev->dev, "%s: GPU lost in progress\n", __func__);
 		kbase_pm_unlock(kbdev);
 		return;
 	}
+#endif
 	kbase_pm_do_poweron(kbdev, true);
 
 #if !MALI_USE_CSF
@@ -957,20 +906,15 @@ void kbase_hwaccess_pm_resume(struct kbase_device *kbdev)
 	kbase_pm_unlock(kbdev);
 }
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 void kbase_pm_handle_gpu_lost(struct kbase_device *kbdev)
 {
 	unsigned long flags;
-#if MALI_USE_CSF
-	unsigned long flags_sched;
-#else
 	ktime_t end_timestamp = ktime_get_raw();
-#endif
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
 
-	if (!kbase_has_arbiter(kbdev)) {
-		dev_warn(kbdev->dev, "%s called with no active arbiter!\n", __func__);
+	if (!kbdev->arb.arb_if)
 		return;
-	}
 
 	mutex_lock(&kbdev->pm.lock);
 	mutex_lock(&arb_vm_state->vm_state_lock);
@@ -983,35 +927,11 @@ void kbase_pm_handle_gpu_lost(struct kbase_device *kbdev)
 		 */
 		WARN(!kbase_is_gpu_removed(kbdev), "GPU is still available after GPU lost event\n");
 
-#if MALI_USE_CSF
-		/* Full GPU reset will have been done by hypervisor, so cancel */
-		if (kbase_reset_gpu_prevent_and_wait(kbdev))
-			dev_warn(kbdev->dev, "Failed to prevent GPU reset.");
-
-		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-		kbase_csf_scheduler_spin_lock(kbdev, &flags_sched);
-		atomic_set(&kbdev->hwaccess.backend.reset_gpu, KBASE_RESET_GPU_NOT_PENDING);
-		kbase_csf_scheduler_spin_unlock(kbdev, flags_sched);
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-
-		kbase_synchronize_irqs(kbdev);
-
-		/* Scheduler reset happens outside of spinlock due to the mutex it acquires */
-		kbase_csf_scheduler_reset(kbdev);
-
-		/* Update kbase status */
-		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-		kbdev->protected_mode = false;
-		kbase_pm_update_state(kbdev);
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-
-		/* Cancel any pending HWC dumps */
-		kbase_hwcnt_backend_csf_on_unrecoverable_error(&kbdev->hwcnt_gpu_iface);
-#else
-		/* Full GPU reset will have been done by hypervisor, so cancel */
+		/* Full GPU reset will have been done by hypervisor, so
+		 * cancel
+		 */
 		atomic_set(&kbdev->hwaccess.backend.reset_gpu, KBASE_RESET_GPU_NOT_PENDING);
 		hrtimer_cancel(&kbdev->hwaccess.backend.reset_timer);
-
 		kbase_synchronize_irqs(kbdev);
 
 		/* Clear all jobs running on the GPU */
@@ -1031,12 +951,13 @@ void kbase_pm_handle_gpu_lost(struct kbase_device *kbdev)
 			wake_up(&kbdev->hwcnt.backend.wait);
 		}
 		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
-#endif /* MALI_USE_CSF */
 	}
 	mutex_unlock(&arb_vm_state->vm_state_lock);
 	mutex_unlock(&kbdev->pm.lock);
 }
 
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
+
 #if MALI_USE_CSF && defined(KBASE_PM_RUNTIME)
 int kbase_pm_force_mcu_wakeup_after_sleep(struct kbase_device *kbdev)
 {
@@ -1047,7 +968,6 @@ int kbase_pm_force_mcu_wakeup_after_sleep(struct kbase_device *kbdev)
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	/* Set the override flag to force the power up of L2 cache */
 	kbdev->pm.backend.gpu_wakeup_override = true;
-	kbdev->pm.backend.runtime_suspend_abort_reason = ABORT_REASON_NONE;
 	kbase_pm_update_state(kbdev);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
@@ -1083,14 +1003,12 @@ static int pm_handle_mcu_sleep_on_runtime_suspend(struct kbase_device *kbdev)
 		return ret;
 	}
 
-	/* Check if a Doorbell mirror interrupt occurred meanwhile.
-	 */
+	/* Check if a Doorbell mirror interrupt occurred meanwhile */
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	if (kbdev->pm.backend.gpu_sleep_mode_active && kbdev->pm.backend.exit_gpu_sleep_mode) {
 		dev_dbg(kbdev->dev,
 			"DB mirror interrupt occurred during runtime suspend after L2 power up");
 		kbdev->pm.backend.gpu_wakeup_override = false;
-		kbdev->pm.backend.runtime_suspend_abort_reason = ABORT_REASON_DB_MIRROR_IRQ;
 		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 		return -EBUSY;
 	}
@@ -1247,5 +1165,4 @@ int kbase_pm_handle_runtime_suspend(struct kbase_device *kbdev)
 
 	return ret;
 }
-
 #endif
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.c
index 6522e5ca66e9..b16d8d99ad7e 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2013-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2013-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -55,18 +55,11 @@ void kbase_devfreq_set_core_mask(struct kbase_device *kbdev, u64 core_mask)
 	unsigned long flags;
 #if MALI_USE_CSF
 	u64 old_core_mask = 0;
-	bool mmu_sync_needed = false;
-
-	if (!IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI) &&
-	    kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_GPU2019_3901)) {
-		mmu_sync_needed = true;
-		down_write(&kbdev->csf.mmu_sync_sem);
-	}
 #endif
+
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
 #if MALI_USE_CSF
-
 	if (!(core_mask & kbdev->pm.debug_core_mask)) {
 		dev_err(kbdev->dev,
 			"OPP core mask 0x%llX does not intersect with debug mask 0x%llX\n",
@@ -105,9 +98,6 @@ void kbase_devfreq_set_core_mask(struct kbase_device *kbdev, u64 core_mask)
 				 old_core_mask, core_mask);
 		}
 	}
-
-	if (mmu_sync_needed)
-		up_write(&kbdev->csf.mmu_sync_sem);
 #endif
 
 	dev_dbg(kbdev->dev, "Devfreq policy : new core mask=%llX\n", pm_backend->ca_cores_enabled);
@@ -115,27 +105,17 @@ void kbase_devfreq_set_core_mask(struct kbase_device *kbdev, u64 core_mask)
 	return;
 unlock:
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-#if MALI_USE_CSF
-	if (mmu_sync_needed)
-		up_write(&kbdev->csf.mmu_sync_sem);
-#endif
 }
 KBASE_EXPORT_TEST_API(kbase_devfreq_set_core_mask);
 #endif
 
-u64 kbase_pm_ca_get_debug_core_mask(struct kbase_device *kbdev)
+u64 kbase_pm_ca_get_core_mask(struct kbase_device *kbdev)
 {
 #if MALI_USE_CSF
-	return kbdev->pm.debug_core_mask;
+	u64 debug_core_mask = kbdev->pm.debug_core_mask;
 #else
-	return kbdev->pm.debug_core_mask_all;
+	u64 debug_core_mask = kbdev->pm.debug_core_mask_all;
 #endif
-}
-KBASE_EXPORT_TEST_API(kbase_pm_ca_get_debug_core_mask);
-
-u64 kbase_pm_ca_get_core_mask(struct kbase_device *kbdev)
-{
-	u64 debug_core_mask = kbase_pm_ca_get_debug_core_mask(kbdev);
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.h
index 37d1020e28ff..95ec1dfb0739 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.h
@@ -57,17 +57,6 @@ void kbase_pm_ca_term(struct kbase_device *kbdev);
  */
 u64 kbase_pm_ca_get_core_mask(struct kbase_device *kbdev);
 
-/**
- * kbase_pm_ca_get_debug_core_mask - Get debug core mask.
- *
- * @kbdev: The kbase device structure for the device (must be a valid pointer)
- *
- * Returns a mask of the currently selected shader cores.
- *
- * Return: The bit mask of user-selected cores
- */
-u64 kbase_pm_ca_get_debug_core_mask(struct kbase_device *kbdev);
-
 /**
  * kbase_pm_ca_update_core_status - Update core status
  *
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_defs.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_defs.h
index a25fe6bdc912..e5ae92a23c90 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_defs.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_defs.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -29,8 +29,6 @@
 #include "mali_kbase_pm_always_on.h"
 #include "mali_kbase_pm_coarse_demand.h"
 
-#include <hw_access/mali_kbase_hw_access_regmap.h>
-
 #if defined(CONFIG_PM_RUNTIME) || defined(CONFIG_PM)
 #define KBASE_PM_RUNTIME 1
 #endif
@@ -99,42 +97,6 @@ enum kbase_shader_core_state {
 #undef KBASEP_SHADER_STATE
 };
 
-/**
- * enum kbase_pm_runtime_suspend_abort_reason - Reason why runtime suspend was aborted
- *                                              after the wake up of MCU.
- *
- * @ABORT_REASON_NONE:          Not aborted
- * @ABORT_REASON_DB_MIRROR_IRQ: Runtime suspend was aborted due to DB_MIRROR irq.
- * @ABORT_REASON_NON_IDLE_CGS:  Runtime suspend was aborted as CSGs were detected as non-idle after
- *                              their suspension.
- */
-enum kbase_pm_runtime_suspend_abort_reason {
-	ABORT_REASON_NONE,
-	ABORT_REASON_DB_MIRROR_IRQ,
-	ABORT_REASON_NON_IDLE_CGS
-};
-
-/* The following indices point to the corresponding bits stored in
- * &kbase_pm_backend_data.gpu_sleep_allowed. They denote the conditions that
- * would be checked against to determine the level of support for GPU sleep
- * and firmware sleep-on-idle.
- */
-#define KBASE_GPU_SUPPORTS_GPU_SLEEP ((uint8_t)0)
-#define KBASE_GPU_SUPPORTS_FW_SLEEP_ON_IDLE ((uint8_t)1)
-#define KBASE_GPU_PERF_COUNTERS_COLLECTION_ENABLED ((uint8_t)2)
-#define KBASE_GPU_IGNORE_IDLE_EVENT ((uint8_t)3)
-#define KBASE_GPU_NON_IDLE_OFF_SLOT_GROUPS_AVAILABLE ((uint8_t)4)
-
-/* FW sleep-on-idle could be enabled if
- * &kbase_pm_backend_data.gpu_sleep_allowed is equal to this value.
- */
-#define KBASE_GPU_FW_SLEEP_ON_IDLE_ALLOWED                             \
-	((uint8_t)((1 << KBASE_GPU_SUPPORTS_GPU_SLEEP) |               \
-		   (1 << KBASE_GPU_SUPPORTS_FW_SLEEP_ON_IDLE) |        \
-		   (0 << KBASE_GPU_PERF_COUNTERS_COLLECTION_ENABLED) | \
-		   (0 << KBASE_GPU_IGNORE_IDLE_EVENT) |                \
-		   (0 << KBASE_GPU_NON_IDLE_OFF_SLOT_GROUPS_AVAILABLE)))
-
 /**
  * struct kbasep_pm_metrics - Metrics data collected for use by the power
  *                            management framework.
@@ -286,6 +248,9 @@ union kbase_pm_policy_data {
  *                     states and transitions.
  * @cg1_disabled:      Set if the policy wants to keep the second core group
  *                     powered off
+ * @driver_ready_for_irqs: Debug state indicating whether sufficient
+ *                         initialization of the driver has occurred to handle
+ *                         IRQs
  * @metrics:           Structure to hold metrics for the GPU
  * @shader_tick_timer: Structure to hold the shader poweroff tick timer state
  * @poweroff_wait_in_progress: true if a wait for GPU power off is in progress.
@@ -325,8 +290,6 @@ union kbase_pm_policy_data {
  *                                     called previously.
  *                                     See &struct kbase_pm_callback_conf.
  * @ca_cores_enabled: Cores that are currently available
- * @apply_hw_issue_TITANHW_2938_wa: Indicates if the workaround for KBASE_HW_ISSUE_TITANHW_2938
- *                                  needs to be applied when unmapping memory from GPU.
  * @mcu_state: The current state of the micro-control unit, only applicable
  *             to GPUs that have such a component
  * @l2_state:     The current state of the L2 cache state machine. See
@@ -353,11 +316,7 @@ union kbase_pm_policy_data {
  *                   cores may be different, but there should be transitions in
  *                   progress that will eventually achieve this state (assuming
  *                   that the policy doesn't change its mind in the mean time).
- * @mcu_desired: True if the micro-control unit should be powered on by the MCU state
- *               machine. Updated as per the value of @mcu_poweron_required.
- * @mcu_poweron_required: Boolean flag updated mainly by the CSF Scheduler code,
- *                        before updating the PM active count, to indicate to the
- *                        PM code that micro-control unit needs to be powered up/down.
+ * @mcu_desired: True if the micro-control unit should be powered on
  * @policy_change_clamp_state_to_off: Signaling the backend is in PM policy
  *                change transition, needs the mcu/L2 to be brought back to the
  *                off state and remain in that state until the flag is cleared.
@@ -371,9 +330,10 @@ union kbase_pm_policy_data {
  * @core_idle_work: Work item used to wait for undesired cores to become inactive.
  *                  The work item is enqueued when Host controls the power for
  *                  shader cores and down scaling of cores is performed.
- * @gpu_sleep_allowed: Bitmask to indicate the conditions that would be
- *                     used to determine what support for GPU sleep is
- *                     available.
+ * @gpu_sleep_supported: Flag to indicate that if GPU sleep feature can be
+ *                       supported by the kernel driver or not. If this
+ *                       flag is not set, then HW state is directly saved
+ *                       when GPU idle notification is received.
  * @gpu_sleep_mode_active: Flag to indicate that the GPU needs to be in sleep
  *                         mode. It is set when the GPU idle notification is
  *                         received and is cleared when HW state has been
@@ -398,12 +358,6 @@ union kbase_pm_policy_data {
  *                       mode for the saving the HW state before power down.
  * @db_mirror_interrupt_enabled: Flag tracking if the Doorbell mirror interrupt
  *                               is enabled or not.
- * @runtime_suspend_abort_reason: Tracks if the runtime suspend was aborted,
- *                                after the wake up of MCU, due to the DB_MIRROR irq
- *                                or non-idle CSGs. Tracking is done to avoid
- *                                redundant transition of MCU to sleep state after the
- *                                abort of runtime suspend and before the resumption
- *                                of scheduling.
  * @l2_force_off_after_mcu_halt: Flag to indicate that L2 cache power down is
  *				 must after performing the MCU halt. Flag is set
  *				 immediately after the MCU halt and cleared
@@ -473,6 +427,10 @@ struct kbase_pm_backend_data {
 
 	bool cg1_disabled;
 
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+	bool driver_ready_for_irqs;
+#endif /* CONFIG_MALI_BIFROST_DEBUG */
+
 	struct kbasep_pm_metrics_state metrics;
 
 	struct kbasep_pm_tick_timer_state shader_tick_timer;
@@ -500,7 +458,6 @@ struct kbase_pm_backend_data {
 	u64 ca_cores_enabled;
 
 #if MALI_USE_CSF
-	bool apply_hw_issue_TITANHW_2938_wa;
 	enum kbase_mcu_state mcu_state;
 #endif
 	enum kbase_l2_core_state l2_state;
@@ -509,7 +466,6 @@ struct kbase_pm_backend_data {
 	u64 shaders_desired_mask;
 #if MALI_USE_CSF
 	bool mcu_desired;
-	bool mcu_poweron_required;
 	bool policy_change_clamp_state_to_off;
 	unsigned int csf_pm_sched_flags;
 	struct mutex policy_change_lock;
@@ -517,13 +473,12 @@ struct kbase_pm_backend_data {
 	struct work_struct core_idle_work;
 
 #ifdef KBASE_PM_RUNTIME
-	unsigned long gpu_sleep_allowed;
+	bool gpu_sleep_supported;
 	bool gpu_sleep_mode_active;
 	bool exit_gpu_sleep_mode;
 	bool gpu_idled;
 	bool gpu_wakeup_override;
 	bool db_mirror_interrupt_enabled;
-	enum kbase_pm_runtime_suspend_abort_reason runtime_suspend_abort_reason;
 #endif
 
 	bool l2_force_off_after_mcu_halt;
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_driver.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_driver.c
index c6b6f3a8668a..b77e46b35422 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_driver.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_driver.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -47,7 +47,9 @@
 #include <backend/gpu/mali_kbase_pm_internal.h>
 #include <backend/gpu/mali_kbase_l2_mmu_config.h>
 #include <mali_kbase_dummy_job_wa.h>
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 #include <arbiter/mali_kbase_arbiter_pm.h>
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 #if MALI_USE_CSF
 #include <linux/delay.h>
@@ -68,19 +70,6 @@ MODULE_PARM_DESC(corestack_driver_control,
 		 "to the Mali GPU is known to be problematic.");
 KBASE_EXPORT_TEST_API(corestack_driver_control);
 
-/**
- * enum kbase_gpu_state - The state of data in the GPU.
- *
- * @GPU_STATE_INTACT: The GPU state is intact
- * @GPU_STATE_LOST: The GPU state is lost
- * @GPU_STATE_IN_RESET: The GPU is in reset state
- *
- * This enumeration is private to the file. It is used as
- * the return values of platform specific PM
- * callback (*power_on_callback).
- */
-enum kbase_gpu_state { GPU_STATE_INTACT = 0, GPU_STATE_LOST, GPU_STATE_IN_RESET };
-
 /**
  * enum kbasep_pm_action - Actions that can be performed on a core.
  *
@@ -121,24 +110,21 @@ bool kbase_pm_is_mcu_desired(struct kbase_device *kbdev)
 	if (kbdev->pm.backend.l2_force_off_after_mcu_halt)
 		return false;
 
-	/* Check if policy changing transition needs MCU to be off. */
-	if (unlikely(kbdev->pm.backend.policy_change_clamp_state_to_off))
-		return false;
-
-	if (kbdev->pm.backend.mcu_desired)
-		return true;
-
-	/* For always_on policy, the MCU needs to be kept on */
-	if (kbase_pm_no_mcu_core_pwroff(kbdev))
+	if (kbdev->csf.scheduler.pm_active_count && kbdev->pm.backend.mcu_desired)
 		return true;
 
 #ifdef KBASE_PM_RUNTIME
-	if (kbdev->pm.backend.gpu_wakeup_override ||
-	    kbdev->pm.backend.runtime_suspend_abort_reason != ABORT_REASON_NONE)
+	if (kbdev->pm.backend.gpu_wakeup_override)
 		return true;
 #endif
 
-	return false;
+	/* MCU is supposed to be ON, only when scheduler.pm_active_count is
+	 * non zero. But for always_on policy, the MCU needs to be kept on,
+	 * unless policy changing transition needs it off.
+	 */
+
+	return (kbdev->pm.backend.mcu_desired && kbase_pm_no_mcu_core_pwroff(kbdev) &&
+		!kbdev->pm.backend.policy_change_clamp_state_to_off);
 }
 #endif
 
@@ -365,14 +351,12 @@ __pure static u32 map_core_type_to_tl_pm_state(struct kbase_device *kbdev,
 }
 #endif
 
-#if IS_ENABLED(CONFIG_ARM64) && !MALI_USE_CSF
-
+#if IS_ENABLED(CONFIG_ARM64)
 static void mali_cci_flush_l2(struct kbase_device *kbdev)
 {
-	u32 val;
 	const u32 mask = CLEAN_CACHES_COMPLETED | RESET_COMPLETED;
-	const u32 timeout_us =
-		kbase_get_timeout_ms(kbdev, KBASE_CLEAN_CACHE_TIMEOUT) * USEC_PER_MSEC;
+	u32 loops = KBASE_CLEAN_CACHE_MAX_LOOPS;
+	u32 raw;
 
 	/*
 	 * Note that we don't take the cache flush mutex here since
@@ -384,11 +368,14 @@ static void mali_cci_flush_l2(struct kbase_device *kbdev)
 
 	kbase_reg_write32(kbdev, GPU_CONTROL_ENUM(GPU_COMMAND), GPU_COMMAND_CACHE_CLN_INV_L2);
 
+	raw = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_RAWSTAT));
+
 	/* Wait for cache flush to complete before continuing, exit on
 	 * gpu resets or loop expiry.
 	 */
-	kbase_reg_poll32_timeout(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_RAWSTAT), val, val & mask, 0,
-				 timeout_us, false);
+	while (((raw & mask) == 0) && --loops) {
+		raw = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_RAWSTAT));
+	}
 }
 #endif
 
@@ -613,11 +600,11 @@ static void kbase_pm_l2_config_override(struct kbase_device *kbdev)
 	/*
 	 * Skip if it is not supported
 	 */
-	if (!kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_L2_CONFIG))
+	if (!kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_L2_CONFIG))
 		return;
 
 #if MALI_USE_CSF
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_PBHA_HWU)) {
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_PBHA_HWU)) {
 		val = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(L2_CONFIG));
 		kbase_reg_write32(kbdev, GPU_CONTROL_ENUM(L2_CONFIG),
 				  L2_CONFIG_PBHA_HWU_SET(val, kbdev->pbha_propagate_bits));
@@ -645,7 +632,7 @@ static void kbase_pm_l2_config_override(struct kbase_device *kbdev)
 		val |= (kbdev->l2_hash_override << L2_CONFIG_HASH_SHIFT);
 	} else if (kbdev->l2_hash_values_override) {
 #if MALI_USE_CSF
-		uint i;
+		int i;
 
 		WARN_ON(!kbase_hw_has_l2_slice_hash_feature(kbdev));
 
@@ -653,7 +640,7 @@ static void kbase_pm_l2_config_override(struct kbase_device *kbdev)
 		val |= (0x1 << L2_CONFIG_L2_SLICE_HASH_ENABLE_SHIFT);
 		for (i = 0; i < GPU_L2_SLICE_HASH_COUNT; i++) {
 			/* L2_SLICE_HASH and ASN_HASH alias each other */
-			dev_dbg(kbdev->dev, "Program 0x%x to ASN_HASH[%u]\n",
+			dev_dbg(kbdev->dev, "Program 0x%x to ASN_HASH[%d]\n",
 				kbdev->l2_hash_values[i], i);
 			kbase_reg_write32(kbdev, GPU_L2_SLICE_HASH_OFFSET(i),
 					  kbdev->l2_hash_values[i]);
@@ -741,8 +728,16 @@ bool kbase_pm_is_mcu_inactive(struct kbase_device *kbdev, enum kbase_mcu_state s
 }
 
 #ifdef KBASE_PM_RUNTIME
-
-void kbase_pm_enable_mcu_db_notification(struct kbase_device *kbdev)
+/**
+ * kbase_pm_enable_mcu_db_notification - Enable the Doorbell notification on
+ *                                       MCU side
+ *
+ * @kbdev: Pointer to the device.
+ *
+ * This function is called to re-enable the Doorbell notification on MCU side
+ * when MCU needs to beome active again.
+ */
+static void kbase_pm_enable_mcu_db_notification(struct kbase_device *kbdev)
 {
 	u32 val = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(MCU_CONTROL));
 
@@ -762,20 +757,19 @@ void kbase_pm_enable_mcu_db_notification(struct kbase_device *kbdev)
  */
 static void wait_mcu_as_inactive(struct kbase_device *kbdev)
 {
-	u32 val;
-	int err;
-	const u32 timeout_us =
-		kbase_get_timeout_ms(kbdev, KBASE_AS_INACTIVE_TIMEOUT) * USEC_PER_MSEC;
+	unsigned int max_loops = KBASE_AS_INACTIVE_MAX_LOOPS;
+
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
-	if (!kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TURSEHW_2716))
+	if (!kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TURSEHW_2716))
 		return;
 
 	/* Wait for the AS_ACTIVE_INT bit to become 0 for the AS used by MCU FW */
-	err = kbase_reg_poll32_timeout(kbdev, MMU_AS_OFFSET(MCU_AS_NR, STATUS), val,
-				       !(val & AS_STATUS_AS_ACTIVE_INT_MASK), 10, timeout_us,
-				       false);
-	if (!WARN_ON_ONCE(err == -ETIMEDOUT))
+	while (--max_loops && kbase_reg_read32(kbdev, MMU_AS_OFFSET(MCU_AS_NR, STATUS)) &
+				      AS_STATUS_AS_ACTIVE_INT_MASK)
+		;
+
+	if (!WARN_ON_ONCE(max_loops == 0))
 		return;
 
 	dev_err(kbdev->dev, "AS_ACTIVE_INT bit stuck for AS %d used by MCU FW", MCU_AS_NR);
@@ -917,18 +911,6 @@ static int kbase_pm_mcu_update_state(struct kbase_device *kbdev)
 			if (kbase_pm_is_mcu_desired(kbdev) &&
 			    !backend->policy_change_clamp_state_to_off &&
 			    backend->l2_state == KBASE_L2_ON) {
-				kbdev->csf.mcu_halted = false;
-
-				/* Ensure that FW would not go to sleep immediately after
-				 * resumption.
-				 */
-				kbase_csf_firmware_global_input_mask(&kbdev->csf.global_iface,
-								     GLB_REQ,
-								     GLB_REQ_REQ_IDLE_DISABLE,
-								     GLB_REQ_IDLE_DISABLE_MASK);
-				atomic_set(&kbdev->csf.scheduler.gpu_idle_timer_enabled, false);
-				atomic_set(&kbdev->csf.scheduler.fw_soi_enabled, false);
-
 				kbase_csf_firmware_trigger_reload(kbdev);
 				backend->mcu_state = KBASE_MCU_PEND_ON_RELOAD;
 			}
@@ -996,8 +978,8 @@ static int kbase_pm_mcu_update_state(struct kbase_device *kbdev)
 				kbase_hwcnt_backend_csf_set_hw_availability(
 					&kbdev->hwcnt_gpu_iface,
 					kbdev->gpu_props.curr_config.l2_slices,
-					kbdev->gpu_props.curr_config.shader_present,
-					kbdev->pm.debug_core_mask);
+					kbdev->gpu_props.curr_config.shader_present &
+						kbdev->pm.debug_core_mask);
 				kbase_hwcnt_context_enable(kbdev->hwcnt_gpu_ctx);
 				kbase_csf_scheduler_spin_unlock(kbdev, flags);
 				backend->hwcnt_disabled = false;
@@ -1007,6 +989,7 @@ static int kbase_pm_mcu_update_state(struct kbase_device *kbdev)
 
 		case KBASE_MCU_ON:
 			backend->shaders_desired_mask = kbase_pm_ca_get_core_mask(kbdev);
+
 			if (!kbase_pm_is_mcu_desired(kbdev))
 				backend->mcu_state = KBASE_MCU_ON_HWCNT_DISABLE;
 			else if (kbdev->csf.firmware_hctl_core_pwr) {
@@ -1186,7 +1169,7 @@ static int kbase_pm_mcu_update_state(struct kbase_device *kbdev)
 			break;
 
 		case KBASE_MCU_POWER_DOWN:
-			if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TITANHW_2922)) {
+			if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TITANHW_2922)) {
 				if (!kbdev->csf.firmware_hctl_core_pwr)
 					kbasep_pm_toggle_power_interrupt(kbdev, true);
 				backend->mcu_state = KBASE_MCU_OFF;
@@ -1207,20 +1190,7 @@ static int kbase_pm_mcu_update_state(struct kbase_device *kbdev)
 #ifdef KBASE_PM_RUNTIME
 		case KBASE_MCU_ON_SLEEP_INITIATE:
 			if (!kbase_pm_is_mcu_desired(kbdev)) {
-				bool db_notif_disabled = false;
-
-				if (likely(test_bit(KBASE_GPU_SUPPORTS_FW_SLEEP_ON_IDLE,
-						    &kbdev->pm.backend.gpu_sleep_allowed)))
-					db_notif_disabled =
-						kbase_reg_read32(kbdev,
-								 GPU_CONTROL_ENUM(MCU_CONTROL)) &
-						MCU_CNTRL_DOORBELL_DISABLE_MASK;
-
-				/* If DB notification is enabled on FW side then send a sleep
-				 * request to FW.
-				 */
-				if (!db_notif_disabled)
-					kbase_csf_firmware_trigger_mcu_sleep(kbdev);
+				kbase_csf_firmware_trigger_mcu_sleep(kbdev);
 				backend->mcu_state = KBASE_MCU_ON_PEND_SLEEP;
 			} else
 				backend->mcu_state = KBASE_MCU_ON_HWCNT_ENABLE;
@@ -1254,16 +1224,6 @@ static int kbase_pm_mcu_update_state(struct kbase_device *kbdev)
 		case KBASE_MCU_IN_SLEEP:
 			if (kbase_pm_is_mcu_desired(kbdev) && backend->l2_state == KBASE_L2_ON) {
 				wait_mcu_as_inactive(kbdev);
-				/* Ensure that FW would not go to sleep immediately after
-				 * resumption.
-				 */
-				kbase_csf_firmware_global_input_mask(&kbdev->csf.global_iface,
-								     GLB_REQ,
-								     GLB_REQ_REQ_IDLE_DISABLE,
-								     GLB_REQ_IDLE_DISABLE_MASK);
-				atomic_set(&kbdev->csf.scheduler.gpu_idle_timer_enabled, false);
-				atomic_set(&kbdev->csf.scheduler.fw_soi_enabled, false);
-
 				KBASE_TLSTREAM_TL_KBASE_CSFFW_FW_REQUEST_WAKEUP(
 					kbdev, kbase_backend_get_cycle_cnt(kbdev));
 				kbase_pm_enable_mcu_db_notification(kbdev);
@@ -1381,10 +1341,15 @@ static void kbase_pm_l2_clear_backend_slot_submit_kctx(struct kbase_device *kbde
 
 static bool can_power_down_l2(struct kbase_device *kbdev)
 {
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	/* Defer the power-down if MMU is in process of page migration. */
+#if MALI_USE_CSF
+	/* Due to the HW issue GPU2019-3878, need to prevent L2 power off
+	 * whilst MMU command is in progress.
+	 * Also defer the power-down if MMU is in process of page migration.
+	 */
+	return !kbdev->mmu_hw_operation_in_progress && !kbdev->mmu_page_migrate_in_progress;
+#else
 	return !kbdev->mmu_page_migrate_in_progress;
+#endif
 }
 
 static bool can_power_up_l2(struct kbase_device *kbdev)
@@ -1408,6 +1373,20 @@ static bool need_tiler_control(struct kbase_device *kbdev)
 #endif
 }
 
+/**
+ * hctl_l2_power_down - Initiate power down of L2 cache
+ *
+ * @kbdev: The kbase device structure for the device.
+ *
+ * This function initiates the power down of L2 cache when Host controls the power
+ * for Tiler block. The function expects that power down of Tiler to already have
+ * been initiated and it triggers the L2 power down only after the power down for
+ * Tiler is complete.
+ * The function shall be called only if L2 is in ready state.
+ */
+static void hctl_l2_power_down(struct kbase_device *kbdev)
+{
+}
 
 /**
  * hctl_tiler_power_up_done - Check and/or initiate power up of Tiler
@@ -1454,6 +1433,7 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 		u64 l2_trans = kbase_pm_get_trans_cores(kbdev, KBASE_PM_CORE_L2);
 		u64 l2_ready = kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_L2);
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 		/*
 		 * kbase_pm_get_ready_cores and kbase_pm_get_trans_cores
 		 * are vulnerable to corruption if gpu is lost
@@ -1482,6 +1462,7 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 			}
 			break;
 		}
+#endif
 
 		/* mask off ready from trans in case transitions finished
 		 * between the register reads
@@ -1527,7 +1508,7 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 				 * must power them on explicitly.
 				 */
 				if (l2_present != 1)
-					kbase_pm_invoke(kbdev, KBASE_PM_CORE_L2, l2_present & ~1ULL,
+					kbase_pm_invoke(kbdev, KBASE_PM_CORE_L2, l2_present & ~1,
 							ACTION_PWRON);
 				/* Clear backend slot submission kctx */
 				kbase_pm_l2_clear_backend_slot_submit_kctx(kbdev);
@@ -1582,7 +1563,7 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 
 		case KBASE_L2_RESTORE_CLOCKS:
 			/* We always assume only GPUs being affected by
-			 * KBASE_HW_ISSUE_GPU2017_1336 fall into this state
+			 * BASE_HW_ISSUE_GPU2017_1336 fall into this state
 			 */
 			WARN_ON_ONCE(!kbdev->pm.backend.gpu_clock_slow_down_wa);
 
@@ -1684,7 +1665,7 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 
 		case KBASE_L2_SLOW_DOWN_CLOCKS:
 			/* We always assume only GPUs being affected by
-			 * KBASE_HW_ISSUE_GPU2017_1336 fall into this state
+			 * BASE_HW_ISSUE_GPU2017_1336 fall into this state
 			 */
 			WARN_ON_ONCE(!kbdev->pm.backend.gpu_clock_slow_down_wa);
 
@@ -1733,6 +1714,11 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 
 		case KBASE_L2_PEND_OFF:
 			if (likely(!backend->l2_always_on)) {
+				if (need_tiler_control(kbdev) && l2_ready) {
+					hctl_l2_power_down(kbdev);
+					break;
+				}
+
 				if (l2_trans || l2_ready)
 					break;
 			} else if (kbdev->cache_clean_in_progress)
@@ -1747,10 +1733,11 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 			}
 #endif
 			/* Disabling MCU after L2 cache power down is to address
-			 * KBASE_HW_ISSUE_TITANHW_2922 hardware issue.
+			 * BASE_HW_ISSUE_TITANHW_2922 hardware issue.
 			 */
 			if (backend->l2_force_off_after_mcu_halt) {
-				kbase_csf_stop_firmware_and_wait(kbdev);
+				kbase_csf_firmware_disable_mcu(kbdev);
+				kbase_csf_firmware_disable_mcu_wait(kbdev);
 				WARN_ON_ONCE(backend->mcu_state != KBASE_MCU_OFF);
 				backend->l2_force_off_after_mcu_halt = false;
 			}
@@ -1897,7 +1884,12 @@ static int kbase_pm_shaders_update_state(struct kbase_device *kbdev)
 		 * kbase_pm_get_ready_cores and kbase_pm_get_trans_cores
 		 * are vulnerable to corruption if gpu is lost
 		 */
-		if (kbase_is_gpu_removed(kbdev) || kbase_pm_is_gpu_lost(kbdev)) {
+		if (kbase_is_gpu_removed(kbdev)
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+		    || kbase_pm_is_gpu_lost(kbdev)) {
+#else
+		) {
+#endif
 			backend->shaders_state = KBASE_SHADERS_OFF_CORESTACK_OFF;
 			dev_dbg(kbdev->dev, "GPU lost has occurred - shaders off\n");
 			break;
@@ -2002,8 +1994,9 @@ static int kbase_pm_shaders_update_state(struct kbase_device *kbdev)
 						kbdev, KBASE_PM_POLICY_EVENT_IDLE);
 
 				if (kbdev->pm.backend.protected_transition_override ||
-				    (kbase_has_arbiter(kbdev) && (kbase_pm_is_suspending(kbdev) ||
-								  kbase_pm_is_gpu_lost(kbdev))) ||
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+				    kbase_pm_is_suspending(kbdev) || kbase_pm_is_gpu_lost(kbdev) ||
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 				    !stt->configured_ticks || WARN_ON(stt->cancel_queued)) {
 					backend->shaders_state =
 						KBASE_SHADERS_WAIT_FINISHED_CORESTACK_ON;
@@ -2070,9 +2063,10 @@ static int kbase_pm_shaders_update_state(struct kbase_device *kbdev)
 						kbdev, KBASE_PM_POLICY_EVENT_TIMER_MISS);
 
 				backend->shaders_state = KBASE_SHADERS_WAIT_FINISHED_CORESTACK_ON;
-			} else if (kbase_has_arbiter(kbdev) &&
-				   (kbase_pm_is_suspending(kbdev) || kbase_pm_is_gpu_lost(kbdev))) {
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+			} else if (kbase_pm_is_suspending(kbdev) || kbase_pm_is_gpu_lost(kbdev)) {
 				backend->shaders_state = KBASE_SHADERS_WAIT_FINISHED_CORESTACK_ON;
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 			}
 			break;
 
@@ -2091,7 +2085,7 @@ static int kbase_pm_shaders_update_state(struct kbase_device *kbdev)
 			if (!backend->partial_shaderoff)
 				shader_poweroff_timer_queue_cancel(kbdev);
 
-			if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TTRX_921)) {
+			if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TTRX_921)) {
 				kbase_gpu_start_cache_clean_nolock(kbdev,
 								   GPU_COMMAND_CACHE_CLN_INV_L2);
 				backend->shaders_state = KBASE_SHADERS_L2_FLUSHING_CORESTACK_ON;
@@ -2441,9 +2435,6 @@ void kbase_pm_reset_complete(struct kbase_device *kbdev)
 	backend->in_reset = false;
 #if MALI_USE_CSF && defined(KBASE_PM_RUNTIME)
 	backend->gpu_wakeup_override = false;
-	backend->db_mirror_interrupt_enabled = false;
-	backend->gpu_sleep_mode_active = false;
-	backend->exit_gpu_sleep_mode = false;
 #endif
 	kbase_pm_update_state(kbdev);
 
@@ -2510,8 +2501,7 @@ int kbase_pm_wait_for_l2_powered(struct kbase_device *kbdev)
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 #if MALI_USE_CSF
-	timeout = (unsigned long)kbase_csf_timeout_in_jiffies(
-		kbase_get_timeout_ms(kbdev, CSF_PM_TIMEOUT));
+	timeout = kbase_csf_timeout_in_jiffies(kbase_get_timeout_ms(kbdev, CSF_PM_TIMEOUT));
 #else
 	timeout = msecs_to_jiffies(PM_TIMEOUT_MS);
 #endif
@@ -2520,11 +2510,11 @@ int kbase_pm_wait_for_l2_powered(struct kbase_device *kbdev)
 #if KERNEL_VERSION(4, 13, 1) <= LINUX_VERSION_CODE
 	remaining = wait_event_killable_timeout(kbdev->pm.backend.gpu_in_desired_state_wait,
 						kbase_pm_is_in_desired_state_with_l2_powered(kbdev),
-						(long)timeout);
+						timeout);
 #else
 	remaining = wait_event_timeout(kbdev->pm.backend.gpu_in_desired_state_wait,
 				       kbase_pm_is_in_desired_state_with_l2_powered(kbdev),
-				       (long)timeout);
+				       timeout);
 #endif
 
 	if (!remaining) {
@@ -2545,7 +2535,7 @@ static int pm_wait_for_desired_state(struct kbase_device *kbdev, bool killable_w
 #if MALI_USE_CSF
 	long timeout = kbase_csf_timeout_in_jiffies(kbase_get_timeout_ms(kbdev, CSF_PM_TIMEOUT));
 #else
-	long timeout = (long)msecs_to_jiffies(PM_TIMEOUT_MS);
+	long timeout = msecs_to_jiffies(PM_TIMEOUT_MS);
 #endif
 	int err = 0;
 
@@ -2668,9 +2658,12 @@ static int pm_wait_for_poweroff_work_complete(struct kbase_device *kbdev, bool k
 	const long timeout = kbase_csf_timeout_in_jiffies(
 		kbase_get_timeout_ms(kbdev, CSF_PM_TIMEOUT) + extra_wait_time_ms);
 #else
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	/* Handling of timeout error isn't supported for arbiter builds */
-	const long timeout = kbase_has_arbiter(kbdev) ? MAX_SCHEDULE_TIMEOUT :
-							      (long)msecs_to_jiffies(PM_TIMEOUT_MS);
+	const long timeout = MAX_SCHEDULE_TIMEOUT;
+#else
+	const long timeout = msecs_to_jiffies(PM_TIMEOUT_MS);
+#endif
 #endif
 	int err = 0;
 
@@ -2791,8 +2784,7 @@ static void update_user_reg_page_mapping(struct kbase_device *kbdev)
 		 * when the context (user process) needs to access to the page.
 		 */
 		unmap_mapping_range(kbdev->csf.user_reg.filp->f_inode->i_mapping,
-				    (loff_t)kctx->csf.user_reg.file_offset << PAGE_SHIFT, PAGE_SIZE,
-				    1);
+				    kctx->csf.user_reg.file_offset << PAGE_SHIFT, PAGE_SIZE, 1);
 		list_del_init(&kctx->csf.user_reg.link);
 		dev_dbg(kbdev->dev, "Updated USER Reg page mapping of ctx %d_%d", kctx->tgid,
 			kctx->id);
@@ -2810,7 +2802,7 @@ static void update_user_reg_page_mapping(struct kbase_device *kbdev)
 void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 {
 	struct kbase_pm_backend_data *backend = &kbdev->pm.backend;
-	int ret = is_resume;
+	bool reset_required = is_resume;
 	unsigned long flags;
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
@@ -2819,10 +2811,12 @@ void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 #endif /* !MALI_USE_CSF */
 	lockdep_assert_held(&kbdev->pm.lock);
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	if (WARN_ON(kbase_pm_is_gpu_lost(kbdev))) {
 		dev_err(kbdev->dev, "%s: Cannot power up while GPU lost", __func__);
 		return;
 	}
+#endif
 
 	if (backend->gpu_powered) {
 #if MALI_USE_CSF && defined(KBASE_PM_RUNTIME)
@@ -2847,7 +2841,7 @@ void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 		backend->callback_power_resume(kbdev);
 		return;
 	} else if (backend->callback_power_on) {
-		ret = backend->callback_power_on(kbdev);
+		reset_required = backend->callback_power_on(kbdev);
 	}
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
@@ -2860,18 +2854,15 @@ void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 #endif
 
 
-	if (ret == GPU_STATE_IN_RESET) {
-		/* GPU is already in reset state after power on and no
-		 * soft-reset needed. Just reconfiguration is needed.
-		 */
-		kbase_pm_init_hw(kbdev, PM_ENABLE_IRQS | PM_NO_RESET);
-	} else if (ret == GPU_STATE_LOST) {
+	if (reset_required) {
 		/* GPU state was lost, reset GPU to ensure it is in a
 		 * consistent state
 		 */
 		kbase_pm_init_hw(kbdev, PM_ENABLE_IRQS);
-	} else {
-		if (kbase_has_arbiter(kbdev)) {
+	}
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	else {
+		if (kbdev->arb.arb_if) {
 			struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
 
 			/* In the case that the GPU has just been granted by
@@ -2887,8 +2878,8 @@ void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 	 * that a repartitioning occurred. In this case the current config
 	 * should be read again.
 	 */
-	if (kbase_has_arbiter(kbdev))
-		kbase_gpuprops_get_curr_config_props(kbdev, &kbdev->gpu_props.curr_config);
+	kbase_gpuprops_get_curr_config_props(kbdev, &kbdev->gpu_props.curr_config);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 	mutex_lock(&kbdev->mmu_hw_mutex);
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
@@ -2912,7 +2903,7 @@ void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 	backend->l2_desired = true;
 #if MALI_USE_CSF
 	{
-		if (ret != GPU_STATE_INTACT) {
+		if (reset_required) {
 			/* GPU reset was done after the power on, so send the post
 			 * reset event instead. This is okay as GPU power off event
 			 * is same as pre GPU reset event.
@@ -2980,7 +2971,12 @@ bool kbase_pm_clock_off(struct kbase_device *kbdev)
 	}
 #endif
 
-	if (kbase_is_gpu_removed(kbdev) || kbase_pm_is_gpu_lost(kbdev)) {
+	if (kbase_is_gpu_removed(kbdev)
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	    || kbase_pm_is_gpu_lost(kbdev)) {
+#else
+	) {
+#endif
 		/* Ensure we unblock any threads that are stuck waiting
 		 * for the GPU
 		 */
@@ -2998,7 +2994,10 @@ bool kbase_pm_clock_off(struct kbase_device *kbdev)
 	/* GPU is about to be turned off, switch to dummy page */
 	update_user_reg_page_mapping(kbdev);
 #endif
+
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_GPU_IDLE_EVENT);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 	if (kbdev->pm.backend.callback_power_off)
 		kbdev->pm.backend.callback_power_off(kbdev);
@@ -3052,7 +3051,6 @@ static enum hrtimer_restart kbasep_reset_timeout(struct hrtimer *timer)
 	return HRTIMER_NORESTART;
 }
 
-
 static int kbase_set_gpu_quirks(struct kbase_device *kbdev)
 {
 #if MALI_USE_CSF
@@ -3082,8 +3080,8 @@ static int kbase_set_gpu_quirks(struct kbase_device *kbdev)
 	kbdev->hw_quirks_gpu = hw_quirks_gpu;
 
 #endif /* !MALI_USE_CSF */
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_IDVS_GROUP_SIZE)) {
-		u32 default_idvs_group_size = 0xF;
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_IDVS_GROUP_SIZE)) {
+		int default_idvs_group_size = 0xF;
 		u32 group_size = 0;
 
 		if (of_property_read_u32(kbdev->dev->of_node, "idvs-group-size", &group_size))
@@ -3116,10 +3114,10 @@ static int kbase_set_sc_quirks(struct kbase_device *kbdev)
 	if (kbase_is_gpu_removed(kbdev))
 		return -EIO;
 
-	if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TTRX_2968_TTRX_3162))
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TTRX_2968_TTRX_3162))
 		hw_quirks_sc |= SC_VAR_ALGORITHM;
 
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_TLS_HASHING))
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_TLS_HASHING))
 		hw_quirks_sc |= SC_TLS_HASH_ENABLE;
 
 	kbdev->hw_quirks_sc = hw_quirks_sc;
@@ -3138,7 +3136,7 @@ static int kbase_set_tiler_quirks(struct kbase_device *kbdev)
 		return -EIO;
 
 	/* Set tiler clock gate override if required */
-	if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_T76X_3953))
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_T76X_3953))
 		hw_quirks_tiler |= TC_CLOCK_GATE_OVERRIDE;
 
 	kbdev->hw_quirks_tiler = hw_quirks_tiler;
@@ -3146,7 +3144,6 @@ static int kbase_set_tiler_quirks(struct kbase_device *kbdev)
 	return 0;
 }
 
-
 static int kbase_pm_hw_issues_detect(struct kbase_device *kbdev)
 {
 	struct device_node *np = kbdev->dev->of_node;
@@ -3199,7 +3196,6 @@ static int kbase_pm_hw_issues_detect(struct kbase_device *kbdev)
 		error = kbase_set_mmu_quirks(kbdev);
 	}
 
-
 	return error;
 }
 
@@ -3219,12 +3215,10 @@ static void kbase_pm_hw_issues_apply(struct kbase_device *kbdev)
 #else
 	kbase_reg_write32(kbdev, GPU_CONTROL_ENUM(JM_CONFIG), kbdev->hw_quirks_gpu);
 #endif
-
 }
 
 void kbase_pm_cache_snoop_enable(struct kbase_device *kbdev)
 {
-#if !MALI_USE_CSF
 	if ((kbdev->current_gpu_coherency_mode == COHERENCY_ACE) && !kbdev->cci_snoop_enabled) {
 #if IS_ENABLED(CONFIG_ARM64)
 		if (kbdev->snoop_enable_smc != 0)
@@ -3233,12 +3227,10 @@ void kbase_pm_cache_snoop_enable(struct kbase_device *kbdev)
 		dev_dbg(kbdev->dev, "MALI - CCI Snoops - Enabled\n");
 		kbdev->cci_snoop_enabled = true;
 	}
-#endif /* !MALI_USE_CSF */
 }
 
 void kbase_pm_cache_snoop_disable(struct kbase_device *kbdev)
 {
-#if !MALI_USE_CSF
 	if (kbdev->cci_snoop_enabled) {
 #if IS_ENABLED(CONFIG_ARM64)
 		if (kbdev->snoop_disable_smc != 0) {
@@ -3249,7 +3241,6 @@ void kbase_pm_cache_snoop_disable(struct kbase_device *kbdev)
 		dev_dbg(kbdev->dev, "MALI - CCI Snoops Disabled\n");
 		kbdev->cci_snoop_enabled = false;
 	}
-#endif /* !MALI_USE_CSF */
 }
 
 #if !MALI_USE_CSF
@@ -3267,10 +3258,16 @@ static void reenable_protected_mode_hwcnt(struct kbase_device *kbdev)
 }
 #endif
 
-static int kbase_pm_do_reset_soft(struct kbase_device *kbdev)
+static int kbase_pm_do_reset(struct kbase_device *kbdev)
 {
+	struct kbasep_reset_timeout_data rtdata;
+	u32 reg_offset, reg_val;
 	int ret;
 
+	KBASE_KTRACE_ADD(kbdev, CORE_GPU_SOFT_RESET, NULL, 0);
+
+	KBASE_TLSTREAM_JD_GPU_SOFT_RESET(kbdev, kbdev);
+
 	if (kbdev->pm.backend.callback_soft_reset) {
 		ret = kbdev->pm.backend.callback_soft_reset(kbdev);
 		if (ret < 0)
@@ -3283,30 +3280,12 @@ static int kbase_pm_do_reset_soft(struct kbase_device *kbdev)
 					  GPU_COMMAND_SOFT_RESET);
 		}
 	}
-	return 0;
-}
-
-static int kbase_pm_do_reset(struct kbase_device *kbdev)
-{
-	struct kbasep_reset_timeout_data rtdata;
-	u32 reg_offset, reg_val;
-	int ret;
 
-	KBASE_KTRACE_ADD(kbdev, CORE_GPU_SOFT_RESET, NULL, 0);
-
-	KBASE_TLSTREAM_JD_GPU_SOFT_RESET(kbdev, kbdev);
-
-	{
-		ret = kbase_pm_do_reset_soft(kbdev);
-		if (ret)
-			return ret;
-
-		reg_offset = GPU_CONTROL_ENUM(GPU_IRQ_MASK);
-		reg_val = RESET_COMPLETED;
+	reg_offset = GPU_CONTROL_ENUM(GPU_IRQ_MASK);
+	reg_val = RESET_COMPLETED;
 
-		/* Unmask the reset complete interrupt only */
-		kbase_reg_write32(kbdev, reg_offset, reg_val);
-	}
+	/* Unmask the reset complete interrupt only */
+	kbase_reg_write32(kbdev, reg_offset, reg_val);
 
 	/* Initialize a structure for tracking the status of the reset */
 	rtdata.kbdev = kbdev;
@@ -3355,8 +3334,9 @@ static int kbase_pm_do_reset(struct kbase_device *kbdev)
 	/* The GPU doesn't seem to be responding to the reset so try a hard
 	 * reset, but only when NOT in arbitration mode.
 	 */
-
-	if (!kbase_has_arbiter(kbdev)) {
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (!kbdev->arb.arb_if) {
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 		dev_err(kbdev->dev,
 			"Failed to soft-reset GPU (timed out after %d ms), now attempting a hard reset\n",
 			RESET_TIMEOUT);
@@ -3386,7 +3366,9 @@ static int kbase_pm_do_reset(struct kbase_device *kbdev)
 
 		dev_err(kbdev->dev, "Failed to hard-reset the GPU (timed out after %d ms)\n",
 			RESET_TIMEOUT);
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	}
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 	return -EINVAL;
 }
@@ -3437,7 +3419,9 @@ int kbase_pm_init_hw(struct kbase_device *kbdev, unsigned int flags)
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, irq_flags);
 
 	/* Soft reset the GPU */
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	if (!(flags & PM_NO_RESET))
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 		err = kbdev->protected_ops->protected_mode_disable(kbdev->protected_dev);
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, irq_flags);
@@ -3458,6 +3442,7 @@ int kbase_pm_init_hw(struct kbase_device *kbdev, unsigned int flags)
 	if (err)
 		goto exit;
 
+
 	if (flags & PM_HW_ISSUES_DETECT) {
 		err = kbase_pm_hw_issues_detect(kbdev);
 		if (err)
@@ -3467,10 +3452,6 @@ int kbase_pm_init_hw(struct kbase_device *kbdev, unsigned int flags)
 	kbase_pm_hw_issues_apply(kbdev);
 	kbase_cache_set_coherency_mode(kbdev, kbdev->system_coherency);
 	kbase_amba_set_shareable_cache_support(kbdev);
-#if MALI_USE_CSF
-	kbase_backend_update_gpu_timestamp_offset(kbdev);
-	kbdev->csf.compute_progress_timeout_cc = 0;
-#endif
 
 	/* Sanity check protected mode was left after reset */
 	WARN_ON(kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_STATUS)) &
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_internal.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_internal.h
index a7fa191b89d1..851d56141e53 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_internal.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_internal.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -821,21 +821,6 @@ bool kbase_pm_is_mcu_desired(struct kbase_device *kbdev);
  */
 bool kbase_pm_is_mcu_inactive(struct kbase_device *kbdev, enum kbase_mcu_state state);
 
-#ifdef KBASE_PM_RUNTIME
-
-/**
- * kbase_pm_enable_mcu_db_notification - Enable the Doorbell notification on
- *                                       MCU side
- *
- * @kbdev: Pointer to the device.
- *
- * This function is called to re-enable the Doorbell notification on MCU side
- * when MCU needs to beome active again.
- */
-void kbase_pm_enable_mcu_db_notification(struct kbase_device *kbdev);
-
-#endif /* KBASE_PM_RUNTIME */
-
 /**
  * kbase_pm_idle_groups_sched_suspendable - Check whether the scheduler can be
  *                                        suspended to low power state when all
@@ -978,29 +963,11 @@ static inline bool kbase_pm_gpu_sleep_allowed(struct kbase_device *kbdev)
 	 * A high positive value of autosuspend_delay can be used to keep the
 	 * GPU in sleep state for a long time.
 	 */
-	if (unlikely(kbdev->dev->power.autosuspend_delay <= 0))
+	if (unlikely(!kbdev->dev->power.autosuspend_delay ||
+		     (kbdev->dev->power.autosuspend_delay < 0)))
 		return false;
 
-	return test_bit(KBASE_GPU_SUPPORTS_GPU_SLEEP, &kbdev->pm.backend.gpu_sleep_allowed);
-}
-
-/**
- * kbase_pm_fw_sleep_on_idle_allowed - Check if FW sleep-on-idle could be enabled
- *
- * @kbdev: Device pointer
- *
- * This function should be called whenever the conditions that impact
- * FW sleep-on-idle support change so that it could be enabled/disabled
- * accordingly.
- *
- * Return: true if FW sleep-on-idle is allowed
- */
-static inline bool kbase_pm_fw_sleep_on_idle_allowed(struct kbase_device *kbdev)
-{
-	if (unlikely(kbdev->dev->power.autosuspend_delay <= 0))
-		return false;
-
-	return kbdev->pm.backend.gpu_sleep_allowed == KBASE_GPU_FW_SLEEP_ON_IDLE_ALLOWED;
+	return kbdev->pm.backend.gpu_sleep_supported;
 }
 
 /**
@@ -1074,5 +1041,4 @@ static inline bool kbase_pm_l2_allow_mmu_page_migration(struct kbase_device *kbd
 	return (backend->l2_state != KBASE_L2_PEND_ON && backend->l2_state != KBASE_L2_PEND_OFF);
 }
 
-
 #endif /* _KBASE_BACKEND_PM_INTERNAL_H_ */
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_metrics.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_metrics.c
index 833a8216928e..5e6e9f058da9 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_metrics.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_metrics.c
@@ -454,7 +454,7 @@ static void kbase_pm_metrics_active_calc(struct kbase_device *kbdev)
 
 		if (katom && katom->gpu_rb_state == KBASE_ATOM_GPU_RB_SUBMITTED) {
 			if (katom->core_req & BASE_JD_REQ_ONLY_COMPUTE) {
-				u32 device_nr =
+				int device_nr =
 					(katom->core_req & BASE_JD_REQ_SPECIFIC_COHERENT_GROUP) ?
 						      katom->device_nr :
 						      0;
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_policy.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_policy.c
index 457e91a0a978..59d38cad0031 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_policy.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_policy.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -77,16 +77,7 @@ void kbase_pm_policy_init(struct kbase_device *kbdev)
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	kbdev->pm.backend.pm_current_policy = default_policy;
 	kbdev->pm.backend.csf_pm_sched_flags = default_policy->pm_sched_flags;
-
-#ifdef KBASE_PM_RUNTIME
-	if (kbase_pm_idle_groups_sched_suspendable(kbdev))
-		clear_bit(KBASE_GPU_IGNORE_IDLE_EVENT, &kbdev->pm.backend.gpu_sleep_allowed);
-	else
-		set_bit(KBASE_GPU_IGNORE_IDLE_EVENT, &kbdev->pm.backend.gpu_sleep_allowed);
-#endif /* KBASE_PM_RUNTIME */
-
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-
 #else
 	CSTD_UNUSED(flags);
 	kbdev->pm.backend.pm_current_policy = default_policy;
@@ -136,7 +127,7 @@ void kbase_pm_update_active(struct kbase_device *kbdev)
 			pm->backend.poweroff_wait_in_progress = false;
 			pm->backend.l2_desired = true;
 #if MALI_USE_CSF
-			pm->backend.mcu_desired = pm->backend.mcu_poweron_required;
+			pm->backend.mcu_desired = true;
 #endif
 
 			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
@@ -237,8 +228,7 @@ void kbase_pm_update_cores_state(struct kbase_device *kbdev)
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 }
 
-size_t kbase_pm_list_policies(struct kbase_device *kbdev,
-			      const struct kbase_pm_policy *const **list)
+int kbase_pm_list_policies(struct kbase_device *kbdev, const struct kbase_pm_policy *const **list)
 {
 	CSTD_UNUSED(kbdev);
 	if (list)
@@ -304,7 +294,7 @@ void kbase_pm_set_policy(struct kbase_device *kbdev, const struct kbase_pm_polic
 	bool reset_gpu = false;
 	bool reset_op_prevented = true;
 	struct kbase_csf_scheduler *scheduler = NULL;
-	u64 pwroff_ns;
+	u32 pwroff;
 	bool switching_to_always_on;
 #endif
 
@@ -314,9 +304,9 @@ void kbase_pm_set_policy(struct kbase_device *kbdev, const struct kbase_pm_polic
 	KBASE_KTRACE_ADD(kbdev, PM_SET_POLICY, NULL, new_policy->id);
 
 #if MALI_USE_CSF
-	pwroff_ns = kbase_csf_firmware_get_mcu_core_pwroff_time(kbdev);
+	pwroff = kbase_csf_firmware_get_mcu_core_pwroff_time(kbdev);
 	switching_to_always_on = new_policy == &kbase_pm_always_on_policy_ops;
-	if (pwroff_ns == 0 && !switching_to_always_on) {
+	if (pwroff == 0 && !switching_to_always_on) {
 		dev_warn(
 			kbdev->dev,
 			"power_policy: cannot switch away from always_on with mcu_shader_pwroff_timeout set to 0\n");
@@ -409,13 +399,6 @@ void kbase_pm_set_policy(struct kbase_device *kbdev, const struct kbase_pm_polic
 	/* New policy in place, release the clamping on mcu/L2 off state */
 	kbdev->pm.backend.policy_change_clamp_state_to_off = false;
 	kbase_pm_update_state(kbdev);
-
-#ifdef KBASE_PM_RUNTIME
-	if (kbase_pm_idle_groups_sched_suspendable(kbdev))
-		clear_bit(KBASE_GPU_IGNORE_IDLE_EVENT, &kbdev->pm.backend.gpu_sleep_allowed);
-	else
-		set_bit(KBASE_GPU_IGNORE_IDLE_EVENT, &kbdev->pm.backend.gpu_sleep_allowed);
-#endif /* KBASE_PM_RUNTIME */
 #endif
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_time.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_time.c
index d3715d97d23c..f4ff61ff5eb6 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_time.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_time.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -22,6 +22,7 @@
 #include <mali_kbase.h>
 #include <mali_kbase_hwaccess_time.h>
 #if MALI_USE_CSF
+#include <asm/arch_timer.h>
 #include <linux/gcd.h>
 #include <csf/mali_kbase_csf_timeout.h>
 #endif
@@ -29,8 +30,6 @@
 #include <backend/gpu/mali_kbase_pm_internal.h>
 #include <mali_kbase_config_defaults.h>
 #include <linux/version_compat_defs.h>
-#include <asm/arch_timer.h>
-#include <linux/mali_hw_access.h>
 
 struct kbase_timeout_info {
 	char *selector_str;
@@ -38,15 +37,12 @@ struct kbase_timeout_info {
 };
 
 #if MALI_USE_CSF
-
-#define GPU_TIMESTAMP_OFFSET_INVALID S64_MAX
-
 static struct kbase_timeout_info timeout_info[KBASE_TIMEOUT_SELECTOR_COUNT] = {
 	[CSF_FIRMWARE_TIMEOUT] = { "CSF_FIRMWARE_TIMEOUT", MIN(CSF_FIRMWARE_TIMEOUT_CYCLES,
 							       CSF_FIRMWARE_PING_TIMEOUT_CYCLES) },
 	[CSF_PM_TIMEOUT] = { "CSF_PM_TIMEOUT", CSF_PM_TIMEOUT_CYCLES },
 	[CSF_GPU_RESET_TIMEOUT] = { "CSF_GPU_RESET_TIMEOUT", CSF_GPU_RESET_TIMEOUT_CYCLES },
-	[CSF_CSG_TERM_TIMEOUT] = { "CSF_CSG_TERM_TIMEOUT", CSF_CSG_TERM_TIMEOUT_CYCLES },
+	[CSF_CSG_SUSPEND_TIMEOUT] = { "CSF_CSG_SUSPEND_TIMEOUT", CSF_CSG_SUSPEND_TIMEOUT_CYCLES },
 	[CSF_FIRMWARE_BOOT_TIMEOUT] = { "CSF_FIRMWARE_BOOT_TIMEOUT",
 					CSF_FIRMWARE_BOOT_TIMEOUT_CYCLES },
 	[CSF_FIRMWARE_PING_TIMEOUT] = { "CSF_FIRMWARE_PING_TIMEOUT",
@@ -57,15 +53,6 @@ static struct kbase_timeout_info timeout_info[KBASE_TIMEOUT_SELECTOR_COUNT] = {
 					   MMU_AS_INACTIVE_WAIT_TIMEOUT_CYCLES },
 	[KCPU_FENCE_SIGNAL_TIMEOUT] = { "KCPU_FENCE_SIGNAL_TIMEOUT",
 					KCPU_FENCE_SIGNAL_TIMEOUT_CYCLES },
-	[KBASE_PRFCNT_ACTIVE_TIMEOUT] = { "KBASE_PRFCNT_ACTIVE_TIMEOUT",
-					  KBASE_PRFCNT_ACTIVE_TIMEOUT_CYCLES },
-	[KBASE_CLEAN_CACHE_TIMEOUT] = { "KBASE_CLEAN_CACHE_TIMEOUT",
-					KBASE_CLEAN_CACHE_TIMEOUT_CYCLES },
-	[KBASE_AS_INACTIVE_TIMEOUT] = { "KBASE_AS_INACTIVE_TIMEOUT",
-					KBASE_AS_INACTIVE_TIMEOUT_CYCLES },
-	[IPA_INACTIVE_TIMEOUT] = { "IPA_INACTIVE_TIMEOUT", IPA_INACTIVE_TIMEOUT_CYCLES },
-	[CSF_FIRMWARE_STOP_TIMEOUT] = { "CSF_FIRMWARE_STOP_TIMEOUT",
-					CSF_FIRMWARE_STOP_TIMEOUT_CYCLES },
 };
 #else
 static struct kbase_timeout_info timeout_info[KBASE_TIMEOUT_SELECTOR_COUNT] = {
@@ -73,77 +60,9 @@ static struct kbase_timeout_info timeout_info[KBASE_TIMEOUT_SELECTOR_COUNT] = {
 					   MMU_AS_INACTIVE_WAIT_TIMEOUT_CYCLES },
 	[JM_DEFAULT_JS_FREE_TIMEOUT] = { "JM_DEFAULT_JS_FREE_TIMEOUT",
 					 JM_DEFAULT_JS_FREE_TIMEOUT_CYCLES },
-	[KBASE_PRFCNT_ACTIVE_TIMEOUT] = { "KBASE_PRFCNT_ACTIVE_TIMEOUT",
-					  KBASE_PRFCNT_ACTIVE_TIMEOUT_CYCLES },
-	[KBASE_CLEAN_CACHE_TIMEOUT] = { "KBASE_CLEAN_CACHE_TIMEOUT",
-					KBASE_CLEAN_CACHE_TIMEOUT_CYCLES },
-	[KBASE_AS_INACTIVE_TIMEOUT] = { "KBASE_AS_INACTIVE_TIMEOUT",
-					KBASE_AS_INACTIVE_TIMEOUT_CYCLES },
 };
 #endif
 
-#if MALI_USE_CSF
-void kbase_backend_invalidate_gpu_timestamp_offset(struct kbase_device *kbdev)
-{
-	kbdev->backend_time.gpu_timestamp_offset = GPU_TIMESTAMP_OFFSET_INVALID;
-}
-KBASE_EXPORT_TEST_API(kbase_backend_invalidate_gpu_timestamp_offset);
-
-/**
- * kbase_backend_compute_gpu_ts_offset() - Compute GPU TS offset.
- *
- * @kbdev:	Kbase device.
- *
- * This function compute the value of GPU and CPU TS offset:
- *   - set to zero current TIMESTAMP_OFFSET register
- *   - read CPU TS and convert it to ticks
- *   - read GPU TS
- *   - calculate diff between CPU and GPU ticks
- *   - cache the diff as the GPU TS offset
- *
- * To reduce delays, preemption must be disabled during reads of both CPU and GPU TS
- * this function require access to GPU register to be enabled
- */
-static inline void kbase_backend_compute_gpu_ts_offset(struct kbase_device *kbdev)
-{
-	s64 cpu_ts_ticks = 0;
-	s64 gpu_ts_ticks = 0;
-
-	if (kbdev->backend_time.gpu_timestamp_offset != GPU_TIMESTAMP_OFFSET_INVALID)
-		return;
-
-	kbase_reg_write64(kbdev, GPU_CONTROL_ENUM(TIMESTAMP_OFFSET), 0);
-
-	gpu_ts_ticks = kbase_reg_read64_coherent(kbdev, GPU_CONTROL_ENUM(TIMESTAMP));
-	cpu_ts_ticks = ktime_get_raw_ns();
-	cpu_ts_ticks = div64_u64(cpu_ts_ticks * kbdev->backend_time.divisor,
-				 kbdev->backend_time.multiplier);
-	kbdev->backend_time.gpu_timestamp_offset = cpu_ts_ticks - gpu_ts_ticks;
-}
-
-void kbase_backend_update_gpu_timestamp_offset(struct kbase_device *kbdev)
-{
-	lockdep_assert_held(&kbdev->pm.lock);
-
-	kbase_backend_compute_gpu_ts_offset(kbdev);
-
-	dev_dbg(kbdev->dev, "Setting GPU timestamp offset register to %lld (%lld ns)",
-		kbdev->backend_time.gpu_timestamp_offset,
-		div64_s64(kbdev->backend_time.gpu_timestamp_offset *
-				  (s64)kbdev->backend_time.multiplier,
-			  (s64)kbdev->backend_time.divisor));
-	kbase_reg_write64(kbdev, GPU_CONTROL_ENUM(TIMESTAMP_OFFSET),
-			  kbdev->backend_time.gpu_timestamp_offset);
-}
-#if MALI_UNIT_TEST
-u64 kbase_backend_read_gpu_timestamp_offset_reg(struct kbase_device *kbdev)
-{
-	return kbase_reg_read64_coherent(kbdev, GPU_CONTROL_ENUM(TIMESTAMP_OFFSET));
-}
-KBASE_EXPORT_TEST_API(kbase_backend_read_gpu_timestamp_offset_reg);
-#endif
-#endif
-
 void kbase_backend_get_gpu_time_norequest(struct kbase_device *kbdev, u64 *cycle_counter,
 					  u64 *system_time, struct timespec64 *ts)
 {
@@ -162,7 +81,6 @@ void kbase_backend_get_gpu_time_norequest(struct kbase_device *kbdev, u64 *cycle
 		ktime_get_raw_ts64(ts);
 #endif
 }
-KBASE_EXPORT_TEST_API(kbase_backend_get_gpu_time_norequest);
 
 #if !MALI_USE_CSF
 /**
@@ -206,7 +124,6 @@ void kbase_backend_get_gpu_time(struct kbase_device *kbdev, u64 *cycle_counter,
 	kbase_pm_release_gpu_cycle_counter(kbdev);
 #endif
 }
-KBASE_EXPORT_TEST_API(kbase_backend_get_gpu_time);
 
 static u64 kbase_device_get_scaling_frequency(struct kbase_device *kbdev)
 {
@@ -235,15 +152,6 @@ void kbase_device_set_timeout_ms(struct kbase_device *kbdev, enum kbase_timeout_
 	}
 	selector_str = timeout_info[selector].selector_str;
 
-#if MALI_USE_CSF
-	if (IS_ENABLED(CONFIG_MALI_REAL_HW) && !IS_ENABLED(CONFIG_MALI_IS_FPGA) &&
-	    unlikely(timeout_ms >= MAX_TIMEOUT_MS)) {
-		dev_warn(kbdev->dev, "%s is capped from %dms to %dms\n",
-			 timeout_info[selector].selector_str, timeout_ms, MAX_TIMEOUT_MS);
-		timeout_ms = MAX_TIMEOUT_MS;
-	}
-#endif
-
 	kbdev->backend_time.device_scaled_timeouts[selector] = timeout_ms;
 	dev_dbg(kbdev->dev, "\t%-35s: %ums\n", selector_str, timeout_ms);
 }
@@ -355,29 +263,41 @@ u64 __maybe_unused kbase_backend_time_convert_gpu_to_cpu(struct kbase_device *kb
 	if (WARN_ON(!kbdev))
 		return 0;
 
-	return div64_u64(gpu_ts * kbdev->backend_time.multiplier, kbdev->backend_time.divisor);
+	return div64_u64(gpu_ts * kbdev->backend_time.multiplier, kbdev->backend_time.divisor) +
+	       kbdev->backend_time.offset;
 }
-KBASE_EXPORT_TEST_API(kbase_backend_time_convert_gpu_to_cpu);
-#endif
 
-u64 kbase_arch_timer_get_cntfrq(struct kbase_device *kbdev)
+/**
+ * get_cpu_gpu_time() - Get current CPU and GPU timestamps.
+ *
+ * @kbdev:	Kbase device.
+ * @cpu_ts:	Output CPU timestamp.
+ * @gpu_ts:	Output GPU timestamp.
+ * @gpu_cycle:  Output GPU cycle counts.
+ */
+static void get_cpu_gpu_time(struct kbase_device *kbdev, u64 *cpu_ts, u64 *gpu_ts, u64 *gpu_cycle)
 {
-	u64 freq = mali_arch_timer_get_cntfrq();
+	struct timespec64 ts;
 
-	dev_dbg(kbdev->dev, "System Timer Freq = %lluHz", freq);
+	kbase_backend_get_gpu_time(kbdev, gpu_cycle, gpu_ts, &ts);
 
-	return freq;
+	if (cpu_ts)
+		*cpu_ts = ts.tv_sec * NSEC_PER_SEC + ts.tv_nsec;
 }
+#endif
 
 int kbase_backend_time_init(struct kbase_device *kbdev)
 {
 	int err = 0;
 #if MALI_USE_CSF
+	u64 cpu_ts = 0;
+	u64 gpu_ts = 0;
 	u64 freq;
 	u64 common_factor;
 
 	kbase_pm_register_access_enable(kbdev);
-	freq = kbase_arch_timer_get_cntfrq(kbdev);
+	get_cpu_gpu_time(kbdev, &cpu_ts, &gpu_ts, NULL);
+	freq = arch_timer_get_cntfrq();
 
 	if (!freq) {
 		dev_warn(kbdev->dev, "arch_timer_get_rate() is zero!");
@@ -396,8 +316,8 @@ int kbase_backend_time_init(struct kbase_device *kbdev)
 		goto disable_registers;
 	}
 
-	kbase_backend_invalidate_gpu_timestamp_offset(
-		kbdev); /* force computation of GPU Timestamp offset */
+	kbdev->backend_time.offset = cpu_ts - div64_u64(gpu_ts * kbdev->backend_time.multiplier,
+							kbdev->backend_time.divisor);
 #endif
 
 	if (kbase_timeout_scaling_init(kbdev)) {
diff --git a/drivers/gpu/arm/bifrost/build.bp b/drivers/gpu/arm/bifrost/build.bp
index 2ae771f5b546..72dd15f2d8cc 100644
--- a/drivers/gpu/arm/bifrost/build.bp
+++ b/drivers/gpu/arm/bifrost/build.bp
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2017-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2017-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -65,12 +65,27 @@ bob_defaults {
     large_page_support: {
         kbuild_options: ["CONFIG_LARGE_PAGE_SUPPORT=y"],
     },
+    mali_memory_fully_backed: {
+        kbuild_options: ["CONFIG_MALI_MEMORY_FULLY_BACKED=y"],
+    },
     mali_corestack: {
         kbuild_options: ["CONFIG_MALI_CORESTACK=y"],
     },
     mali_real_hw: {
         kbuild_options: ["CONFIG_MALI_REAL_HW=y"],
     },
+    mali_error_inject_none: {
+        kbuild_options: ["CONFIG_MALI_ERROR_INJECT_NONE=y"],
+    },
+    mali_error_inject_track_list: {
+        kbuild_options: ["CONFIG_MALI_ERROR_INJECT_TRACK_LIST=y"],
+    },
+    mali_error_inject_random: {
+        kbuild_options: ["CONFIG_MALI_ERROR_INJECT_RANDOM=y"],
+    },
+    mali_error_inject: {
+        kbuild_options: ["CONFIG_MALI_BIFROST_ERROR_INJECT=y"],
+    },
     mali_debug: {
         kbuild_options: [
             "CONFIG_MALI_BIFROST_DEBUG=y",
@@ -113,7 +128,7 @@ bob_defaults {
     mali_hw_errata_1485982_use_clock_alternative: {
         kbuild_options: ["CONFIG_MALI_HW_ERRATA_1485982_USE_CLOCK_ALTERNATIVE=y"],
     },
-    mali_is_fpga: {
+    platform_is_fpga: {
         kbuild_options: ["CONFIG_MALI_IS_FPGA=y"],
     },
     mali_coresight: {
@@ -148,6 +163,7 @@ bob_defaults {
         // is an umbrella feature that would be open for inappropriate use
         // (catch-all for experimental CS code without separating it into
         // different features).
+        "MALI_INCREMENTAL_RENDERING_JM={{.incremental_rendering_jm}}",
         "MALI_BASE_CSF_PERFORMANCE_TESTS={{.base_csf_performance_tests}}",
     ],
 }
@@ -161,9 +177,6 @@ bob_kernel_module {
         "*.c",
         "*.h",
         "Kbuild",
-        "arbiter/*.c",
-        "arbiter/*.h",
-        "arbiter/Kbuild",
         "backend/gpu/*.c",
         "backend/gpu/*.h",
         "backend/gpu/Kbuild",
@@ -229,7 +242,6 @@ bob_kernel_module {
             "jm/*.h",
             "tl/backend/*_jm.c",
             "mmu/backend/*_jm.c",
-            "mmu/backend/*_jm.h",
             "ipa/backend/*_jm.c",
             "ipa/backend/*_jm.h",
         ],
@@ -254,11 +266,17 @@ bob_kernel_module {
             "hwcnt/backend/*_csf_*.h",
             "tl/backend/*_csf.c",
             "mmu/backend/*_csf.c",
-            "mmu/backend/*_csf.h",
             "ipa/backend/*_csf.c",
             "ipa/backend/*_csf.h",
         ],
     },
+    mali_arbiter_support: {
+        srcs: [
+            "arbiter/*.c",
+            "arbiter/*.h",
+            "arbiter/Kbuild",
+        ],
+    },
     kbuild_options: [
         "CONFIG_MALI_BIFROST=m",
         "CONFIG_MALI_KUTF=n",
diff --git a/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_csf.c b/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_csf.c
index fe1dbfaca872..8b1410886b05 100644
--- a/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_csf.c
+++ b/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_csf.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -116,7 +116,8 @@ static void kbase_context_term_partial(struct kbase_context *kctx, unsigned int
 
 struct kbase_context *kbase_create_context(struct kbase_device *kbdev, bool is_compat,
 					   base_context_create_flags const flags,
-					   unsigned long const api_version, struct file *const filp)
+					   unsigned long const api_version,
+					   struct kbase_file *const kfile)
 {
 	struct kbase_context *kctx;
 	unsigned int i = 0;
@@ -135,7 +136,7 @@ struct kbase_context *kbase_create_context(struct kbase_device *kbdev, bool is_c
 
 	kctx->kbdev = kbdev;
 	kctx->api_version = api_version;
-	kctx->filp = filp;
+	kctx->kfile = kfile;
 	kctx->create_flags = flags;
 
 	memcpy(kctx->comm, current->comm, sizeof(current->comm));
@@ -186,15 +187,11 @@ void kbase_destroy_context(struct kbase_context *kctx)
 	 * Customer side that a hang could occur if context termination is
 	 * not blocked until the resume of GPU device.
 	 */
-	if (kbase_has_arbiter(kbdev))
-		atomic_inc(&kbdev->pm.gpu_users_waiting);
 	while (kbase_pm_context_active_handle_suspend(kbdev,
 						      KBASE_PM_SUSPEND_HANDLER_DONT_INCREASE)) {
-		dev_dbg(kbdev->dev, "Suspend in progress when destroying context");
+		dev_info(kbdev->dev, "Suspend in progress when destroying context");
 		wait_event(kbdev->pm.resume_wait, !kbase_pm_is_suspending(kbdev));
 	}
-	if (kbase_has_arbiter(kbdev))
-		atomic_dec(&kbdev->pm.gpu_users_waiting);
 
 	/* Have synchronized against the System suspend and incremented the
 	 * pm.active_count. So any subsequent invocation of System suspend
diff --git a/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_jm.c b/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_jm.c
index ef474f625f63..f2eefe9ddcd0 100644
--- a/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_jm.c
+++ b/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_jm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -168,7 +168,8 @@ static void kbase_context_term_partial(struct kbase_context *kctx, unsigned int
 
 struct kbase_context *kbase_create_context(struct kbase_device *kbdev, bool is_compat,
 					   base_context_create_flags const flags,
-					   unsigned long const api_version, struct file *const filp)
+					   unsigned long const api_version,
+					   struct kbase_file *const kfile)
 {
 	struct kbase_context *kctx;
 	unsigned int i = 0;
@@ -187,7 +188,7 @@ struct kbase_context *kbase_create_context(struct kbase_device *kbdev, bool is_c
 
 	kctx->kbdev = kbdev;
 	kctx->api_version = api_version;
-	kctx->filp = filp;
+	kctx->kfile = kfile;
 	kctx->create_flags = flags;
 
 	if (is_compat)
@@ -231,13 +232,14 @@ void kbase_destroy_context(struct kbase_context *kctx)
 	if (WARN_ON(!kbdev))
 		return;
 
-	/* Context termination could happen whilst the system suspend of
+		/* Context termination could happen whilst the system suspend of
 	 * the GPU device is ongoing or has completed. It has been seen on
 	 * Customer side that a hang could occur if context termination is
 	 * not blocked until the resume of GPU device.
 	 */
-	if (kbase_has_arbiter(kbdev))
-		atomic_inc(&kbdev->pm.gpu_users_waiting);
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	atomic_inc(&kbdev->pm.gpu_users_waiting);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 	while (kbase_pm_context_active_handle_suspend(kbdev,
 						      KBASE_PM_SUSPEND_HANDLER_DONT_INCREASE)) {
 		dev_dbg(kbdev->dev, "Suspend in progress when destroying context");
@@ -254,8 +256,9 @@ void kbase_destroy_context(struct kbase_context *kctx)
 	 */
 	wait_event(kbdev->pm.resume_wait, !kbase_pm_is_resuming(kbdev));
 
-	if (kbase_has_arbiter(kbdev))
-		atomic_dec(&kbdev->pm.gpu_users_waiting);
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	atomic_dec(&kbdev->pm.gpu_users_waiting);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 	kbase_mem_pool_group_mark_dying(&kctx->mem_pools);
 
diff --git a/drivers/gpu/arm/bifrost/context/mali_kbase_context.c b/drivers/gpu/arm/bifrost/context/mali_kbase_context.c
index 2c7417bd6506..41f129624245 100644
--- a/drivers/gpu/arm/bifrost/context/mali_kbase_context.c
+++ b/drivers/gpu/arm/bifrost/context/mali_kbase_context.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -141,7 +141,7 @@ int kbase_context_common_init(struct kbase_context *kctx)
 	kctx->pid = task_pid_vnr(current);
 
 	/* Check if this is a Userspace created context */
-	if (likely(kctx->filp)) {
+	if (likely(kctx->kfile)) {
 		struct pid *pid_struct;
 
 		rcu_read_lock();
@@ -184,20 +184,18 @@ int kbase_context_common_init(struct kbase_context *kctx)
 	spin_lock_init(&kctx->waiting_soft_jobs_lock);
 	INIT_LIST_HEAD(&kctx->waiting_soft_jobs);
 
-	init_waitqueue_head(&kctx->event_queue);
-
 	kbase_gpu_vm_lock(kctx);
 	bitmap_copy(kctx->cookies, &cookies_mask, BITS_PER_LONG);
 	kbase_gpu_vm_unlock(kctx);
 
-	kctx->id = (u32)atomic_add_return(1, &(kctx->kbdev->ctx_num)) - 1;
+	kctx->id = atomic_add_return(1, &(kctx->kbdev->ctx_num)) - 1;
 
 	mutex_lock(&kctx->kbdev->kctx_list_lock);
 	err = kbase_insert_kctx_to_process(kctx);
 	mutex_unlock(&kctx->kbdev->kctx_list_lock);
 	if (err) {
 		dev_err(kctx->kbdev->dev, "(err:%d) failed to insert kctx to kbase_process", err);
-		if (likely(kctx->filp)) {
+		if (likely(kctx->kfile)) {
 			mmdrop(kctx->process_mm);
 			put_task_struct(kctx->task);
 		}
@@ -286,7 +284,7 @@ void kbase_context_common_term(struct kbase_context *kctx)
 	kbase_remove_kctx_from_process(kctx);
 	mutex_unlock(&kctx->kbdev->kctx_list_lock);
 
-	if (likely(kctx->filp)) {
+	if (likely(kctx->kfile)) {
 		mmdrop(kctx->process_mm);
 		put_task_struct(kctx->task);
 	}
diff --git a/drivers/gpu/arm/bifrost/context/mali_kbase_context.h b/drivers/gpu/arm/bifrost/context/mali_kbase_context.h
index 07c235fab11e..939eb9bbd65e 100644
--- a/drivers/gpu/arm/bifrost/context/mali_kbase_context.h
+++ b/drivers/gpu/arm/bifrost/context/mali_kbase_context.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2011-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -56,9 +56,9 @@ void kbase_context_debugfs_term(struct kbase_context *const kctx);
  *               BASEP_CONTEXT_CREATE_KERNEL_FLAGS.
  * @api_version: Application program interface version, as encoded in
  *               a single integer by the KBASE_API_VERSION macro.
- * @filp:        Pointer to the struct file corresponding to device file
- *               /dev/malixx instance, passed to the file's open method.
- *               Shall be passed as NULL for internally created contexts.
+ * @kfile:       Pointer to the object representing the /dev/malixx device
+ *               file instance. Shall be passed as NULL for internally created
+ *               contexts.
  *
  * Up to one context can be created for each client that opens the device file
  * /dev/malixx. Context creation is deferred until a special ioctl() system call
@@ -68,7 +68,8 @@ void kbase_context_debugfs_term(struct kbase_context *const kctx);
  */
 struct kbase_context *kbase_create_context(struct kbase_device *kbdev, bool is_compat,
 					   base_context_create_flags const flags,
-					   unsigned long api_version, struct file *filp);
+					   unsigned long api_version,
+					   struct kbase_file *const kfile);
 
 /**
  * kbase_destroy_context - Destroy a kernel base context.
@@ -87,7 +88,7 @@ void kbase_destroy_context(struct kbase_context *kctx);
  */
 static inline bool kbase_ctx_flag(struct kbase_context *kctx, enum kbase_context_flags flag)
 {
-	return atomic_read(&kctx->flags) & (int)flag;
+	return atomic_read(&kctx->flags) & flag;
 }
 
 /**
diff --git a/drivers/gpu/arm/bifrost/csf/Kbuild b/drivers/gpu/arm/bifrost/csf/Kbuild
index 8159bc9d10e8..5df35864efc7 100644
--- a/drivers/gpu/arm/bifrost/csf/Kbuild
+++ b/drivers/gpu/arm/bifrost/csf/Kbuild
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -48,10 +48,8 @@ bifrost_kbase-y += \
 
 ifeq ($(CONFIG_MALI_BIFROST_NO_MALI),y)
 bifrost_kbase-y += csf/mali_kbase_csf_firmware_no_mali.o
-bifrost_kbase-y += csf/mali_kbase_csf_fw_io_no_mali.o
 else
 bifrost_kbase-y += csf/mali_kbase_csf_firmware.o
-bifrost_kbase-y += csf/mali_kbase_csf_fw_io.o
 endif
 
 bifrost_kbase-$(CONFIG_DEBUG_FS) += csf/mali_kbase_debug_csf_fault.o
diff --git a/drivers/gpu/arm/bifrost/csf/ipa_control/mali_kbase_csf_ipa_control.c b/drivers/gpu/arm/bifrost/csf/ipa_control/mali_kbase_csf_ipa_control.c
index ec47b88fac53..1489d8c1971b 100644
--- a/drivers/gpu/arm/bifrost/csf/ipa_control/mali_kbase_csf_ipa_control.c
+++ b/drivers/gpu/arm/bifrost/csf/ipa_control/mali_kbase_csf_ipa_control.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -45,6 +45,11 @@
  */
 #define TIMER_EVENTS_PER_SECOND ((u32)1000 / IPA_CONTROL_TIMER_DEFAULT_VALUE_MS)
 
+/*
+ * Maximum number of loops polling the GPU before we assume the GPU has hung.
+ */
+#define IPA_INACTIVE_MAX_LOOPS (8000000U)
+
 /*
  * Number of bits used to configure a performance counter in SELECT registers.
  */
@@ -81,16 +86,16 @@ static u32 timer_value(u32 gpu_rate)
 
 static int wait_status(struct kbase_device *kbdev, u32 flags)
 {
-	u32 val;
-	const u32 timeout_us = kbase_get_timeout_ms(kbdev, IPA_INACTIVE_TIMEOUT) * USEC_PER_MSEC;
+	unsigned int max_loops = IPA_INACTIVE_MAX_LOOPS;
+	u32 status = kbase_reg_read32(kbdev, IPA_CONTROL_ENUM(STATUS));
+
 	/*
 	 * Wait for the STATUS register to indicate that flags have been
 	 * cleared, in case a transition is pending.
 	 */
-	const int err = kbase_reg_poll32_timeout(kbdev, IPA_CONTROL_ENUM(STATUS), val,
-						 !(val & flags), 0, timeout_us, false);
-
-	if (err) {
+	while (--max_loops && (status & flags))
+		status = kbase_reg_read32(kbdev, IPA_CONTROL_ENUM(STATUS));
+	if (max_loops == 0) {
 		dev_err(kbdev->dev, "IPA_CONTROL STATUS register stuck");
 		return -EBUSY;
 	}
@@ -121,7 +126,7 @@ static int apply_select_config(struct kbase_device *kbdev, u64 *select)
 	return ret;
 }
 
-static u64 read_value_cnt(struct kbase_device *kbdev, u8 type, u8 select_idx)
+static u64 read_value_cnt(struct kbase_device *kbdev, u8 type, int select_idx)
 {
 	switch (type) {
 	case KBASE_IPA_CORE_TYPE_CSHW:
@@ -943,8 +948,6 @@ void kbase_ipa_control_protm_entered(struct kbase_device *kbdev)
 	struct kbase_ipa_control *ipa_ctrl = &kbdev->csf.ipa_control;
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-
 	ipa_ctrl->protm_start = ktime_get_raw_ns();
 }
 
@@ -957,7 +960,6 @@ void kbase_ipa_control_protm_exited(struct kbase_device *kbdev)
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
-
 	for (i = 0; i < KBASE_IPA_CONTROL_MAX_SESSIONS; i++) {
 		struct kbase_ipa_control_session *session = &ipa_ctrl->sessions[i];
 
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.c
index d3300ea8dcde..2cd4b201173d 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -45,9 +45,6 @@
 #define CS_RING_BUFFER_MAX_SIZE ((uint32_t)(1 << 31)) /* 2GiB */
 #define CS_RING_BUFFER_MIN_SIZE ((uint32_t)4096)
 
-/* 0.2 second assuming 600 MHz GPU clock, which is double of iterator disabling timeout */
-#define MAX_PROGRESS_TIMEOUT_EVENT_DELAY ((u32)120000000)
-
 #define PROTM_ALLOC_MAX_RETRIES ((u8)5)
 
 const u8 kbasep_csf_queue_group_priority_to_relative[BASE_QUEUE_GROUP_PRIORITY_COUNT] = {
@@ -194,14 +191,14 @@ static int kernel_map_user_io_pages(struct kbase_context *kctx, struct kbase_que
 	struct page *page_list[2];
 	pgprot_t cpu_map_prot;
 	unsigned long flags;
-	u64 *user_io_addr;
+	uint64_t *user_io_addr;
 	int ret = 0;
 	size_t i;
 
 	kbase_gpu_vm_lock(kctx);
 
 	if (ARRAY_SIZE(page_list) > (KBASE_PERMANENTLY_MAPPED_MEM_LIMIT_PAGES -
-				     (unsigned int)atomic_read(&kctx->permanent_mapped_pages))) {
+				     atomic_read(&kctx->permanent_mapped_pages))) {
 		ret = -ENOMEM;
 		goto unlock;
 	}
@@ -542,8 +539,6 @@ static int csf_queue_register_internal(struct kbase_context *kctx,
 
 	queue->blocked_reason = CS_STATUS_BLOCKED_REASON_REASON_UNBLOCKED;
 
-	queue->clear_faults = true;
-
 	INIT_LIST_HEAD(&queue->link);
 	atomic_set(&queue->pending_kick, 0);
 	INIT_LIST_HEAD(&queue->pending_kick_link);
@@ -558,7 +553,7 @@ static int csf_queue_register_internal(struct kbase_context *kctx,
 	 * enabled, otherwise leave them as default zeros.
 	 */
 	if (reg_ex && reg_ex->ex_buffer_size) {
-		u32 cfg = CS_INSTR_CONFIG_EVENT_SIZE_SET(0U, reg_ex->ex_event_size);
+		u32 cfg = CS_INSTR_CONFIG_EVENT_SIZE_SET(0, reg_ex->ex_event_size);
 		cfg = CS_INSTR_CONFIG_EVENT_STATE_SET(cfg, reg_ex->ex_event_state);
 
 		queue->trace_cfg = cfg;
@@ -594,19 +589,11 @@ int kbase_csf_queue_register_ex(struct kbase_context *kctx,
 	u32 const glb_version = iface->version;
 	u32 instr = iface->instr_features;
 	u8 max_size = GLB_INSTR_FEATURES_EVENT_SIZE_MAX_GET(instr);
-	const u8 event_size = reg->ex_event_size;
-	u64 min_buf_size;
+	u32 min_buf_size =
+		(1u << reg->ex_event_size) * GLB_INSTR_FEATURES_OFFSET_UPDATE_RATE_GET(instr);
 
 	/* If cs_trace_command not supported, the call fails */
 	if (glb_version < kbase_csf_interface_version(1, 1, 0))
-		return -EPERM;
-
-	/* Sanity check to avoid shift-out-of-bounds */
-	if (event_size >= 32)
-		return -EINVAL;
-
-	min_buf_size = ((u64)1 << event_size) * GLB_INSTR_FEATURES_OFFSET_UPDATE_RATE_GET(instr);
-	if (min_buf_size > UINT32_MAX)
 		return -EINVAL;
 
 	/* Validate the ring buffer configuration parameters */
@@ -618,8 +605,8 @@ int kbase_csf_queue_register_ex(struct kbase_context *kctx,
 
 	/* Validate the cs_trace configuration parameters */
 	if (reg->ex_buffer_size &&
-	    ((event_size > max_size) || (reg->ex_buffer_size & (reg->ex_buffer_size - 1)) ||
-	     (reg->ex_buffer_size < (u32)min_buf_size)))
+	    ((reg->ex_event_size > max_size) || (reg->ex_buffer_size & (reg->ex_buffer_size - 1)) ||
+	     (reg->ex_buffer_size < min_buf_size)))
 		return -EINVAL;
 
 	return csf_queue_register_internal(kctx, NULL, reg);
@@ -737,7 +724,7 @@ int kbase_csf_queue_bind(struct kbase_context *kctx, union kbase_ioctl_cs_queue_
 	group->bound_queues[bind->in.csi_index] = queue;
 	queue->group = group;
 	queue->group_priority = group->priority;
-	queue->csi_index = (s8)bind->in.csi_index;
+	queue->csi_index = bind->in.csi_index;
 	queue->bind_state = KBASE_CSF_QUEUE_BIND_IN_PROGRESS;
 
 out:
@@ -747,7 +734,7 @@ int kbase_csf_queue_bind(struct kbase_context *kctx, union kbase_ioctl_cs_queue_
 }
 
 /**
- * get_bound_queue_group() - Get the group to which a queue was bound
+ * get_bound_queue_group - Get the group to which a queue was bound
  *
  * @queue: Pointer to the queue for this group
  *
@@ -853,54 +840,13 @@ void kbase_csf_ring_cs_kernel_doorbell(struct kbase_device *kbdev, int csi_index
 	dmb(osh);
 
 	value = kbase_csf_firmware_csg_output(ginfo, CSG_DB_ACK);
-	value ^= (1U << csi_index);
-	kbase_csf_firmware_csg_input_mask(ginfo, CSG_DB_REQ, value, 1U << csi_index);
+	value ^= (1 << csi_index);
+	kbase_csf_firmware_csg_input_mask(ginfo, CSG_DB_REQ, value, 1 << csi_index);
 
 	if (likely(ring_csg_doorbell))
 		kbase_csf_ring_csg_doorbell(kbdev, csg_nr);
 }
 
-int kbase_csf_queue_group_clear_faults(struct kbase_context *kctx,
-				       struct kbase_ioctl_queue_group_clear_faults *faults)
-{
-	void __user *user_bufs = u64_to_user_ptr(faults->addr);
-	u32 i;
-	struct kbase_device *kbdev = kctx->kbdev;
-	const u32 nr_queues = faults->nr_queues;
-
-	if (unlikely(nr_queues > kbdev->csf.global_iface.groups[0].stream_num)) {
-		dev_warn(kbdev->dev, "Invalid nr_queues %u", nr_queues);
-		return -EINVAL;
-	}
-
-	for (i = 0; i < nr_queues; ++i) {
-		u64 buf_gpu_addr;
-		struct kbase_va_region *region;
-
-		if (copy_from_user(&buf_gpu_addr, user_bufs, sizeof(buf_gpu_addr)))
-			return -EFAULT;
-		mutex_lock(&kctx->csf.lock);
-		kbase_gpu_vm_lock(kctx);
-		region = kbase_region_tracker_find_region_enclosing_address(kctx, buf_gpu_addr);
-		if (likely(!kbase_is_region_invalid_or_free(region))) {
-			struct kbase_queue *queue = region->user_data;
-
-			queue->clear_faults = true;
-		} else {
-			dev_warn(kbdev->dev, "GPU queue %u without a valid command buffer region",
-				 i);
-			kbase_gpu_vm_unlock(kctx);
-			mutex_unlock(&kctx->csf.lock);
-			return -EFAULT;
-		}
-		kbase_gpu_vm_unlock(kctx);
-		mutex_unlock(&kctx->csf.lock);
-		user_bufs = (void __user *)((uintptr_t)user_bufs + sizeof(buf_gpu_addr));
-	}
-
-	return 0;
-}
-
 int kbase_csf_queue_kick(struct kbase_context *kctx, struct kbase_ioctl_cs_queue_kick *kick)
 {
 	struct kbase_device *kbdev = kctx->kbdev;
@@ -922,7 +868,7 @@ int kbase_csf_queue_kick(struct kbase_context *kctx, struct kbase_ioctl_cs_queue
 		struct kbase_queue *queue = region->user_data;
 
 		if (queue && (queue->bind_state == KBASE_CSF_QUEUE_BOUND)) {
-			spin_lock(&kbdev->csf.pending_gpuq_kick_queues_lock);
+			spin_lock(&kbdev->csf.pending_gpuq_kicks_lock);
 			if (list_empty(&queue->pending_kick_link)) {
 				/* Queue termination shall block until this
 				 * kick has been handled.
@@ -930,12 +876,10 @@ int kbase_csf_queue_kick(struct kbase_context *kctx, struct kbase_ioctl_cs_queue
 				atomic_inc(&queue->pending_kick);
 				list_add_tail(
 					&queue->pending_kick_link,
-					&kbdev->csf.pending_gpuq_kick_queues[queue->group_priority]);
-				if (atomic_cmpxchg(&kbdev->csf.pending_gpuq_kicks, false, true) ==
-				    false)
-					complete(&kbdev->csf.scheduler.kthread_signal);
+					&kbdev->csf.pending_gpuq_kicks[queue->group_priority]);
+				complete(&kbdev->csf.scheduler.kthread_signal);
 			}
-			spin_unlock(&kbdev->csf.pending_gpuq_kick_queues_lock);
+			spin_unlock(&kbdev->csf.pending_gpuq_kicks_lock);
 		}
 	} else {
 		dev_dbg(kbdev->dev,
@@ -958,7 +902,7 @@ static void unbind_stopped_queue(struct kbase_context *kctx, struct kbase_queue
 		unsigned long flags;
 
 		kbase_csf_scheduler_spin_lock(kctx->kbdev, &flags);
-		bitmap_clear(queue->group->protm_pending_bitmap, (unsigned int)queue->csi_index, 1);
+		bitmap_clear(queue->group->protm_pending_bitmap, queue->csi_index, 1);
 		KBASE_KTRACE_ADD_CSF_GRP_Q(kctx->kbdev, CSI_PROTM_PEND_CLEAR, queue->group, queue,
 					   queue->group->protm_pending_bitmap[0]);
 		queue->group->bound_queues[queue->csi_index] = NULL;
@@ -1151,11 +1095,12 @@ static int create_normal_suspend_buffer(struct kbase_context *const kctx,
 }
 
 static void timer_event_worker(struct work_struct *data);
+static void protm_event_worker(struct work_struct *data);
 static void term_normal_suspend_buffer(struct kbase_context *const kctx,
 				       struct kbase_normal_suspend_buffer *s_buf);
 
 /**
- * create_suspend_buffers() - Setup normal and protected mode
+ * create_suspend_buffers - Setup normal and protected mode
  *				suspend buffers.
  *
  * @kctx:	Address of the kbase context within which the queue group
@@ -1254,8 +1199,6 @@ static int create_queue_group(struct kbase_context *const kctx,
 			group->deschedule_deferred_cnt = 0;
 #endif
 
-			group->cs_fault_report_enable = create->in.cs_fault_report_enable;
-
 			group->group_uid = generate_group_uid();
 			create->out.group_uid = group->group_uid;
 
@@ -1263,9 +1206,7 @@ static int create_queue_group(struct kbase_context *const kctx,
 			INIT_LIST_HEAD(&group->link_to_schedule);
 			INIT_LIST_HEAD(&group->error_fatal.link);
 			INIT_WORK(&group->timer_event_work, timer_event_worker);
-			INIT_LIST_HEAD(&group->protm_event_work);
-			group->progress_timer_state = 0;
-			atomic_set(&group->pending_protm_event_work, 0);
+			INIT_WORK(&group->protm_event_work, protm_event_worker);
 			bitmap_zero(group->protm_pending_bitmap, MAX_SUPPORTED_STREAMS_PER_GROUP);
 
 			group->run_state = KBASE_CSF_GROUP_INACTIVE;
@@ -1310,6 +1251,14 @@ int kbase_csf_queue_group_create(struct kbase_context *const kctx,
 	const u32 tiler_count = hweight64(create->in.tiler_mask);
 	const u32 fragment_count = hweight64(create->in.fragment_mask);
 	const u32 compute_count = hweight64(create->in.compute_mask);
+	size_t i;
+
+	for (i = 0; i < ARRAY_SIZE(create->in.padding); i++) {
+		if (create->in.padding[i] != 0) {
+			dev_warn(kctx->kbdev->dev, "Invalid padding not 0 in queue group create\n");
+			return -EINVAL;
+		}
+	}
 
 	mutex_lock(&kctx->csf.lock);
 
@@ -1430,7 +1379,7 @@ void kbase_csf_term_descheduled_queue_group(struct kbase_queue_group *group)
 }
 
 /**
- * term_queue_group() - Terminate a GPU command queue group.
+ * term_queue_group - Terminate a GPU command queue group.
  *
  * @group: Pointer to GPU command queue group data.
  *
@@ -1458,8 +1407,8 @@ static void term_queue_group(struct kbase_queue_group *group)
 }
 
 /**
- * wait_group_deferred_deschedule_completion() - Wait for refcount of the group
- *     to become 0 that was taken when the group deschedule had to be deferred.
+ * wait_group_deferred_deschedule_completion - Wait for refcount of the group to
+ *         become 0 that was taken when the group deschedule had to be deferred.
  *
  * @group: Pointer to GPU command queue group that is being deleted.
  *
@@ -1488,10 +1437,7 @@ static void wait_group_deferred_deschedule_completion(struct kbase_queue_group *
 static void cancel_queue_group_events(struct kbase_queue_group *group)
 {
 	cancel_work_sync(&group->timer_event_work);
-
-	/* Drain a pending protected mode request if any */
-	kbase_csf_scheduler_wait_for_kthread_pending_work(group->kctx->kbdev,
-							  &group->pending_protm_event_work);
+	cancel_work_sync(&group->protm_event_work);
 }
 
 static void remove_pending_group_fatal_error(struct kbase_queue_group *group)
@@ -1646,7 +1592,6 @@ int kbase_csf_ctx_init(struct kbase_context *kctx)
 
 	INIT_LIST_HEAD(&kctx->csf.queue_list);
 	INIT_LIST_HEAD(&kctx->csf.link);
-	atomic_set(&kctx->csf.pending_sync_update, 0);
 
 	kbase_csf_event_init(kctx);
 
@@ -1882,7 +1827,7 @@ void kbase_csf_ctx_term(struct kbase_context *kctx)
 }
 
 /**
- * handle_oom_event() - Handle the OoM event generated by the firmware for the
+ * handle_oom_event - Handle the OoM event generated by the firmware for the
  *                    CSI.
  *
  * @group:  Pointer to the CSG group the oom-event belongs to.
@@ -1957,7 +1902,7 @@ static int handle_oom_event(struct kbase_queue_group *const group,
 }
 
 /**
- * report_tiler_oom_error() - Report a CSG error due to a tiler heap OOM event
+ * report_tiler_oom_error - Report a CSG error due to a tiler heap OOM event
  *
  * @group: Pointer to the GPU command queue group that encountered the error
  */
@@ -1988,8 +1933,8 @@ static void flush_gpu_cache_on_fatal_error(struct kbase_device *kbdev)
 	 */
 	if (kbdev->pm.backend.gpu_powered) {
 		kbase_gpu_start_cache_clean(kbdev, GPU_COMMAND_CACHE_CLN_INV_L2_LSC);
-		if (kbase_gpu_wait_cache_clean_timeout(
-			    kbdev, kbase_get_timeout_ms(kbdev, MMU_AS_INACTIVE_WAIT_TIMEOUT)))
+		if (kbase_gpu_wait_cache_clean_timeout(kbdev,
+						       kbdev->mmu_or_gpu_cache_op_wait_time_ms))
 			dev_warn(
 				kbdev->dev,
 				"[%llu] Timeout waiting for CACHE_CLN_INV_L2_LSC to complete after fatal error",
@@ -2000,7 +1945,7 @@ static void flush_gpu_cache_on_fatal_error(struct kbase_device *kbdev)
 }
 
 /**
- * kbase_queue_oom_event() - Handle tiler out-of-memory for a GPU command queue.
+ * kbase_queue_oom_event - Handle tiler out-of-memory for a GPU command queue.
  *
  * @queue: Pointer to queue for which out-of-memory event was received.
  *
@@ -2088,7 +2033,7 @@ static void kbase_queue_oom_event(struct kbase_queue *const queue)
 }
 
 /**
- * oom_event_worker() - Tiler out-of-memory handler called from a workqueue.
+ * oom_event_worker - Tiler out-of-memory handler called from a workqueue.
  *
  * @data: Pointer to a work_struct embedded in GPU command queue data.
  *
@@ -2116,8 +2061,7 @@ static void oom_event_worker(struct work_struct *data)
 }
 
 /**
- * report_group_timeout_error() - Report the timeout error for the group to
- *                                userspace.
+ * report_group_timeout_error - Report the timeout error for the group to userspace.
  *
  * @group: Pointer to the group for which timeout error occurred
  */
@@ -2141,7 +2085,7 @@ static void report_group_timeout_error(struct kbase_queue_group *const group)
 }
 
 /**
- * timer_event_worker() - Handle the progress timeout error for the group
+ * timer_event_worker - Handle the progress timeout error for the group
  *
  * @data: Pointer to a work_struct embedded in GPU command queue group data.
  *
@@ -2176,74 +2120,19 @@ static void timer_event_worker(struct work_struct *data)
 }
 
 /**
- * handle_progress_timer_events() - Progress timer timeout events handler.
+ * handle_progress_timer_event - Progress timer timeout event handler.
  *
- * @kbdev:     Instance of a GPU platform device that implements a CSF interface.
- * @slot_mask: Bitmap reflecting the slots on which progress timer timeouts happen.
+ * @group: Pointer to GPU queue group for which the timeout event is received.
  *
  * Notify a waiting user space client of the timeout.
  * Enqueue a work item to terminate the group and notify the event notification
  * thread of progress timeout fault for the GPU command queue group.
- * Ignore fragment timeout if it is following a compute timeout.
  */
-static void handle_progress_timer_events(struct kbase_device *const kbdev, unsigned long *slot_mask)
+static void handle_progress_timer_event(struct kbase_queue_group *const group)
 {
-	u32 max_csg_slots = kbdev->csf.global_iface.group_num;
-	u32 csg_nr;
-	struct kbase_queue_group *group = NULL;
-	struct kbase_csf_cmd_stream_group_info *ginfo;
-
-	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
-	if (likely(bitmap_empty(slot_mask, MAX_SUPPORTED_CSGS)))
-		return;
-
-	/* Log each timeout and Update timestamp of compute progress timeout */
-	for_each_set_bit(csg_nr, slot_mask, max_csg_slots) {
-		group = kbdev->csf.scheduler.csg_slots[csg_nr].resident_group;
-		ginfo = &kbdev->csf.global_iface.groups[csg_nr];
-		group->progress_timer_state =
-			kbase_csf_firmware_csg_output(ginfo, CSG_PROGRESS_TIMER_STATE);
-
-		dev_info(
-			kbdev->dev,
-			"[%llu] Iterator PROGRESS_TIMER timeout notification received for group %u of ctx %d_%d on slot %u with state %x",
-			kbase_backend_get_cycle_cnt(kbdev), group->handle, group->kctx->tgid,
-			group->kctx->id, csg_nr, group->progress_timer_state);
+	kbase_debug_csf_fault_notify(group->kctx->kbdev, group->kctx, DF_PROGRESS_TIMER_TIMEOUT);
 
-		if (CSG_PROGRESS_TIMER_STATE_GET(group->progress_timer_state) ==
-		    CSG_PROGRESS_TIMER_STATE_COMPUTE)
-			kbdev->csf.compute_progress_timeout_cc = kbase_backend_get_cycle_cnt(kbdev);
-	}
-
-	/* Ignore fragment timeout if it is following a compute timeout.
-	 * Otherwise, terminate the command stream group.
-	 */
-	for_each_set_bit(csg_nr, slot_mask, max_csg_slots) {
-		group = kbdev->csf.scheduler.csg_slots[csg_nr].resident_group;
-
-		/* Check if it is a fragment timeout right after another compute timeout.
-		 * In such case, kill compute CSG and give fragment CSG a second chance
-		 */
-		if (CSG_PROGRESS_TIMER_STATE_GET(group->progress_timer_state) ==
-		    CSG_PROGRESS_TIMER_STATE_FRAGMENT) {
-			u64 cycle_counter = kbase_backend_get_cycle_cnt(kbdev);
-			u64 compute_progress_timeout_cc = kbdev->csf.compute_progress_timeout_cc;
-
-			if (compute_progress_timeout_cc <= cycle_counter &&
-			    cycle_counter <= compute_progress_timeout_cc +
-						     MAX_PROGRESS_TIMEOUT_EVENT_DELAY) {
-				dev_info(
-					kbdev->dev,
-					"Ignored Fragment iterator timeout for group %d on slot %d",
-					group->handle, group->csg_nr);
-				continue;
-			}
-		}
-
-		kbase_debug_csf_fault_notify(group->kctx->kbdev, group->kctx,
-					     DF_PROGRESS_TIMER_TIMEOUT);
-		queue_work(group->kctx->csf.wq, &group->timer_event_work);
-	}
+	queue_work(group->kctx->csf.wq, &group->timer_event_work);
 }
 
 /**
@@ -2322,7 +2211,41 @@ static void report_group_fatal_error(struct kbase_queue_group *const group)
 }
 
 /**
- * handle_fault_event() - Handler for CS fault.
+ * protm_event_worker - Protected mode switch request event handler
+ *			called from a workqueue.
+ *
+ * @data: Pointer to a work_struct embedded in GPU command queue group data.
+ *
+ * Request to switch to protected mode.
+ */
+static void protm_event_worker(struct work_struct *data)
+{
+	struct kbase_queue_group *const group =
+		container_of(data, struct kbase_queue_group, protm_event_work);
+	struct kbase_protected_suspend_buffer *sbuf = &group->protected_suspend_buf;
+	int err = 0;
+
+	KBASE_KTRACE_ADD_CSF_GRP(group->kctx->kbdev, PROTM_EVENT_WORKER_START, group, 0u);
+
+	err = alloc_grp_protected_suspend_buffer_pages(group);
+	if (!err) {
+		kbase_csf_scheduler_group_protm_enter(group);
+	} else if (err == -ENOMEM && sbuf->alloc_retries <= PROTM_ALLOC_MAX_RETRIES) {
+		sbuf->alloc_retries++;
+		/* try again to allocate pages */
+		queue_work(group->kctx->csf.wq, &group->protm_event_work);
+	} else if (sbuf->alloc_retries >= PROTM_ALLOC_MAX_RETRIES || err != -ENOMEM) {
+		dev_err(group->kctx->kbdev->dev,
+			"Failed to allocate physical pages for Protected mode suspend buffer for the group %d of context %d_%d",
+			group->handle, group->kctx->tgid, group->kctx->id);
+		report_group_fatal_error(group);
+	}
+
+	KBASE_KTRACE_ADD_CSF_GRP(group->kctx->kbdev, PROTM_EVENT_WORKER_END, group, 0u);
+}
+
+/**
+ * handle_fault_event - Handler for CS fault.
  *
  * @queue:  Pointer to queue for which fault event was received.
  * @cs_ack: Value of the CS_ACK register in the CS kernel input page used for
@@ -2344,14 +2267,13 @@ static void handle_fault_event(struct kbase_queue *const queue, const u32 cs_ack
 	const u8 cs_fault_exception_type = CS_FAULT_EXCEPTION_TYPE_GET(cs_fault);
 	const u32 cs_fault_exception_data = CS_FAULT_EXCEPTION_DATA_GET(cs_fault);
 	const u64 cs_fault_info_exception_data = CS_FAULT_INFO_EXCEPTION_DATA_GET(cs_fault_info);
-	bool has_trace_info = false;
-	bool skip_fault_report = kbase_ctx_flag(queue->kctx, KCTX_PAGE_FAULT_REPORT_SKIP);
+	bool use_old_log_format = true;
 
 
 	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
 
 
-	if (!has_trace_info && !skip_fault_report)
+	if (use_old_log_format)
 		dev_warn(kbdev->dev,
 			 "Ctx %d_%d Group %d CSG %d CSI: %d\n"
 			 "CS_FAULT.EXCEPTION_TYPE: 0x%x (%s)\n"
@@ -2363,32 +2285,47 @@ static void handle_fault_event(struct kbase_queue *const queue, const u32 cs_ack
 			 cs_fault_info_exception_data);
 
 
-	/* If dump-on-fault daemon is waiting for a fault, wake up the daemon.
-	 * Acknowledging the fault is deferred to the bottom-half until the wait
-	 * of the dump completion is done.
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+	/* CS_RESOURCE_TERMINATED type fault event can be ignored from the
+	 * standpoint of dump on error. It is used to report fault for the CSIs
+	 * that are associated with the same CSG as the CSI for which the actual
+	 * fault was reported by the Iterator.
+	 * Dumping would be triggered when the actual fault is reported.
 	 *
-	 * Otherwise acknowledge the fault and ring the doorbell for the faulty queue
-	 * to enter into recoverable state.
+	 * CS_INHERIT_FAULT can also be ignored. It could happen due to the error
+	 * in other types of queues (cpu/kcpu). If a fault had occurred in some
+	 * other GPU queue then the dump would have been performed anyways when
+	 * that fault was reported.
 	 */
-	if (likely(!kbase_debug_csf_fault_notify(kbdev, queue->kctx, DF_CS_FAULT))) {
-		kbase_csf_firmware_cs_input_mask(stream, CS_REQ, cs_ack, CS_REQ_FAULT_MASK);
-		kbase_csf_ring_cs_kernel_doorbell(kbdev, queue->csi_index, queue->group->csg_nr,
-						  true);
-		queue->cs_error_acked = true;
-	} else
-		queue->cs_error_acked = false;
+	if ((cs_fault_exception_type != CS_FAULT_EXCEPTION_TYPE_CS_INHERIT_FAULT) &&
+	    (cs_fault_exception_type != CS_FAULT_EXCEPTION_TYPE_CS_RESOURCE_TERMINATED)) {
+		if (unlikely(kbase_debug_csf_fault_notify(kbdev, queue->kctx, DF_CS_FAULT))) {
+			queue->cs_error = cs_fault;
+			queue->cs_error_info = cs_fault_info;
+			queue->cs_error_fatal = false;
+			queue_work(queue->kctx->csf.wq, &queue->cs_error_work);
+			return;
+		}
+	}
+#endif
 
-	queue->cs_error = cs_fault;
-	queue->cs_error_info = cs_fault_info;
-	queue->cs_error_fatal = false;
-	if (!queue_work(queue->kctx->csf.wq, &queue->cs_error_work))
-		dev_warn(kbdev->dev, "%s: failed to enqueue a work", __func__);
+	kbase_csf_firmware_cs_input_mask(stream, CS_REQ, cs_ack, CS_REQ_FAULT_MASK);
+	kbase_csf_ring_cs_kernel_doorbell(kbdev, queue->csi_index, queue->group->csg_nr, true);
 }
 
-static void report_queue_error(struct kbase_queue *const queue, u32 cs_error, u64 cs_error_info,
-			       struct kbase_queue_group *group, bool fatal)
+static void report_queue_fatal_error(struct kbase_queue *const queue, u32 cs_fatal,
+				     u64 cs_fatal_info, struct kbase_queue_group *group)
 {
-	struct base_csf_notification error = { .type = BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR };
+	struct base_csf_notification
+		error = { .type = BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR,
+			  .payload = {
+				  .csg_error = {
+					  .error = { .error_type =
+							     BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL,
+						     .payload = { .fatal_queue = {
+									  .sideband = cs_fatal_info,
+									  .status = cs_fatal,
+								  } } } } } };
 
 	if (!queue)
 		return;
@@ -2397,30 +2334,17 @@ static void report_queue_error(struct kbase_queue *const queue, u32 cs_error, u6
 		return;
 
 	error.payload.csg_error.handle = group->handle;
-	if (fatal) {
-		error.payload.csg_error.error.error_type = BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL;
-		error.payload.csg_error.error.payload.fatal_queue.sideband = cs_error_info;
-		error.payload.csg_error.error.payload.fatal_queue.status = cs_error;
-		error.payload.csg_error.error.payload.fatal_queue.csi_index = queue->csi_index;
-	} else {
-		error.payload.csg_error.error.error_type = BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FAULT;
-		error.payload.csg_error.error.payload.fault_queue.sideband = cs_error_info;
-		error.payload.csg_error.error.payload.fault_queue.status = cs_error;
-		error.payload.csg_error.error.payload.fault_queue.csi_index = queue->csi_index;
-	}
+	error.payload.csg_error.error.payload.fatal_queue.csi_index = queue->csi_index;
 	kbase_csf_event_add_error(queue->kctx, &group->error_fatal, &error);
 	kbase_event_wakeup(queue->kctx);
-
-	if (!fatal)
-		queue->clear_faults = false;
 }
 
 /**
- * cs_error_worker() - Handle the CS_FATAL/CS_FAULT error for the GPU queue
+ * cs_error_worker - Handle the CS_FATAL/CS_FAULT error for the GPU queue
  *
  * @data: Pointer to a work_struct embedded in GPU command queue.
  *
- * Terminate the CSG for CS_FATAL and report the error to userspace.
+ * Terminate the CSG and report the error to userspace.
  */
 static void cs_error_worker(struct work_struct *const data)
 {
@@ -2431,7 +2355,6 @@ static void cs_error_worker(struct work_struct *const data)
 	struct kbase_queue_group *group;
 	bool reset_prevented = false;
 	int err;
-	const bool cs_fatal = queue->cs_error_fatal;
 
 	kbase_debug_csf_fault_wait_completion(kbdev);
 	err = kbase_reset_gpu_prevent_and_wait(kbdev);
@@ -2447,57 +2370,45 @@ static void cs_error_worker(struct work_struct *const data)
 
 	group = get_bound_queue_group(queue);
 	if (!group) {
-		dev_warn(kbdev->dev, "queue not bound when handling an error event");
+		dev_warn(kbdev->dev, "queue not bound when handling fatal event");
 		goto unlock;
 	}
 
-	if (!cs_fatal) {
-		if (group->cs_fault_report_enable && queue->clear_faults)
-			report_queue_error(queue, queue->cs_error, queue->cs_error_info, group,
-					   false);
-		if (unlikely(!queue->cs_error_acked)) {
-			unsigned long flags;
-			int slot_num;
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+	if (!queue->cs_error_fatal) {
+		unsigned long flags;
+		int slot_num;
 
-			kbase_csf_scheduler_spin_lock(kbdev, &flags);
-			slot_num = kbase_csf_scheduler_group_get_slot_locked(group);
-			if (likely(slot_num >= 0)) {
-				struct kbase_csf_cmd_stream_group_info const *ginfo =
-					&kbdev->csf.global_iface.groups[slot_num];
-				struct kbase_csf_cmd_stream_info const *stream =
-					&ginfo->streams[queue->csi_index];
-				u32 const cs_ack = kbase_csf_firmware_cs_output(stream, CS_ACK);
-				u32 const cs_req = kbase_csf_firmware_cs_input_read(stream, CS_REQ);
-
-				/* Acknowledge the fault and ring the doorbell for the queue
-				 * if it hasn't yet done.
-				 */
-				if ((cs_ack & CS_ACK_FAULT_MASK) != (cs_req & CS_REQ_FAULT_MASK)) {
-					kbase_csf_firmware_cs_input_mask(stream, CS_REQ, cs_ack,
-									 CS_REQ_FAULT_MASK);
-					kbase_csf_ring_cs_kernel_doorbell(kbdev, queue->csi_index,
-									  slot_num, true);
-				}
-			}
-			kbase_csf_scheduler_spin_unlock(kbdev, flags);
+		kbase_csf_scheduler_spin_lock(kbdev, &flags);
+		slot_num = kbase_csf_scheduler_group_get_slot_locked(group);
+		if (slot_num >= 0) {
+			struct kbase_csf_cmd_stream_group_info const *ginfo =
+				&kbdev->csf.global_iface.groups[slot_num];
+			struct kbase_csf_cmd_stream_info const *stream =
+				&ginfo->streams[queue->csi_index];
+			u32 const cs_ack = kbase_csf_firmware_cs_output(stream, CS_ACK);
+
+			kbase_csf_firmware_cs_input_mask(stream, CS_REQ, cs_ack, CS_REQ_FAULT_MASK);
+			kbase_csf_ring_cs_kernel_doorbell(kbdev, queue->csi_index, slot_num, true);
 		}
+		kbase_csf_scheduler_spin_unlock(kbdev, flags);
+		goto unlock;
+	}
+#endif
+
+	term_queue_group(group);
+	flush_gpu_cache_on_fatal_error(kbdev);
+	/* For an invalid GPU page fault, CS_BUS_FAULT fatal error is expected after the
+	 * page fault handler disables the AS of faulty context. Need to skip reporting the
+	 * CS_BUS_FAULT fatal error to the Userspace as it doesn't have the full fault info.
+	 * Page fault handler will report the fatal error with full page fault info.
+	 */
+	if ((cs_fatal_exception_type == CS_FATAL_EXCEPTION_TYPE_CS_BUS_FAULT) && group->faulted) {
+		dev_dbg(kbdev->dev,
+			"Skipped reporting CS_BUS_FAULT for queue %d of group %d of ctx %d_%d",
+			queue->csi_index, group->handle, kctx->tgid, kctx->id);
 	} else {
-		term_queue_group(group);
-		flush_gpu_cache_on_fatal_error(kbdev);
-		/* For an invalid GPU page fault, CS_BUS_FAULT fatal error is expected after the
-		 * page fault handler disables the AS of faulty context. Need to skip reporting the
-		 * CS_BUS_FAULT fatal error to the Userspace as it doesn't have the full fault info.
-		 * Page fault handler will report the fatal error with full page fault info.
-		 */
-		if ((cs_fatal_exception_type == CS_FATAL_EXCEPTION_TYPE_CS_BUS_FAULT) &&
-		    group->faulted) {
-			dev_dbg(kbdev->dev,
-				"Skipped reporting CS_BUS_FAULT for queue %d of group %d of ctx %d_%d",
-				queue->csi_index, group->handle, kctx->tgid, kctx->id);
-		} else {
-			report_queue_error(queue, queue->cs_error, queue->cs_error_info, group,
-					   true);
-		}
+		report_queue_fatal_error(queue, queue->cs_error, queue->cs_error_info, group);
 	}
 
 unlock:
@@ -2507,7 +2418,7 @@ static void cs_error_worker(struct work_struct *const data)
 }
 
 /**
- * handle_fatal_event() - Handler for CS fatal.
+ * handle_fatal_event - Handler for CS fatal.
  *
  * @queue:    Pointer to queue for which fatal event was received.
  * @stream:   Pointer to the structure containing info provided by the
@@ -2531,13 +2442,12 @@ static void handle_fatal_event(struct kbase_queue *const queue,
 	const u32 cs_fatal_exception_type = CS_FATAL_EXCEPTION_TYPE_GET(cs_fatal);
 	const u32 cs_fatal_exception_data = CS_FATAL_EXCEPTION_DATA_GET(cs_fatal);
 	const u64 cs_fatal_info_exception_data = CS_FATAL_INFO_EXCEPTION_DATA_GET(cs_fatal_info);
-	bool has_trace_info = false;
-	bool skip_fault_report = kbase_ctx_flag(queue->kctx, KCTX_PAGE_FAULT_REPORT_SKIP);
+	bool use_old_log_format = true;
 
 
 	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
 
-	if (!has_trace_info && !skip_fault_report)
+	if (use_old_log_format)
 		dev_warn(kbdev->dev,
 			 "Ctx %d_%d Group %d CSG %d CSI: %d\n"
 			 "CS_FATAL.EXCEPTION_TYPE: 0x%x (%s)\n"
@@ -2569,7 +2479,7 @@ static void handle_fatal_event(struct kbase_queue *const queue,
 }
 
 /**
- * process_cs_interrupts() - Process interrupts for a CS.
+ * process_cs_interrupts - Process interrupts for a CS.
  *
  * @group:  Pointer to GPU command queue group data.
  * @ginfo:  The CSG interface provided by the firmware.
@@ -2598,17 +2508,17 @@ static void process_cs_interrupts(struct kbase_queue_group *const group,
 	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
 
 	while (remaining != 0) {
-		unsigned int const i = (unsigned int)ffs((int)remaining) - 1;
+		int const i = ffs(remaining) - 1;
 		struct kbase_queue *const queue = group->bound_queues[i];
 
-		remaining &= ~(1U << i);
+		remaining &= ~(1 << i);
 
 		/* The queue pointer can be NULL, but if it isn't NULL then it
 		 * cannot disappear since scheduler spinlock is held and before
 		 * freeing a bound queue it has to be first unbound which
 		 * requires scheduler spinlock.
 		 */
-		if (queue && !WARN_ON(queue->csi_index != (s8)i)) {
+		if (queue && !WARN_ON(queue->csi_index != i)) {
 			struct kbase_csf_cmd_stream_info const *const stream = &ginfo->streams[i];
 			u32 const cs_req = kbase_csf_firmware_cs_input_read(stream, CS_REQ);
 			u32 const cs_ack = kbase_csf_firmware_cs_output(stream, CS_ACK);
@@ -2683,7 +2593,7 @@ static void process_cs_interrupts(struct kbase_queue_group *const group,
 		}
 
 		if (!group->protected_suspend_buf.pma)
-			kbase_csf_scheduler_enqueue_protm_event_work(group);
+			queue_work(group->kctx->csf.wq, &group->protm_event_work);
 
 		if (test_bit(group->csg_nr, scheduler->csg_slots_idle_mask)) {
 			clear_bit(group->csg_nr, scheduler->csg_slots_idle_mask);
@@ -2696,14 +2606,12 @@ static void process_cs_interrupts(struct kbase_queue_group *const group,
 }
 
 /**
- * process_csg_interrupts() - Process interrupts for a CSG.
+ * process_csg_interrupts - Process interrupts for a CSG.
  *
  * @kbdev: Instance of a GPU platform device that implements a CSF interface.
  * @csg_nr: CSG number.
  * @track: Pointer that tracks the highest idle CSG and the newly possible viable
  *         protected mode requesting group, in current IRQ context.
- * @progress_timeout_slot_mask: slot mask to indicate on which slot progress timeout
- *         happens.
  *
  * Handles interrupts for a CSG and for CSs within it.
  *
@@ -2714,9 +2622,8 @@ static void process_cs_interrupts(struct kbase_queue_group *const group,
  *
  * See process_cs_interrupts() for details of per-stream interrupt handling.
  */
-static void process_csg_interrupts(struct kbase_device *const kbdev, u32 const csg_nr,
-				   struct irq_idle_and_protm_track *track,
-				   unsigned long *progress_timeout_slot_mask)
+static void process_csg_interrupts(struct kbase_device *const kbdev, int const csg_nr,
+				   struct irq_idle_and_protm_track *track)
 {
 	struct kbase_csf_cmd_stream_group_info *ginfo;
 	struct kbase_queue_group *group = NULL;
@@ -2724,7 +2631,7 @@ static void process_csg_interrupts(struct kbase_device *const kbdev, u32 const c
 
 	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
 
-	if (WARN_ON(csg_nr >= kbdev->csf.global_iface.group_num))
+	if (WARN_ON((u32)csg_nr >= kbdev->csf.global_iface.group_num))
 		return;
 
 	ginfo = &kbdev->csf.global_iface.groups[csg_nr];
@@ -2758,10 +2665,10 @@ static void process_csg_interrupts(struct kbase_device *const kbdev, u32 const c
 	if (!group)
 		return;
 
-	if (WARN_ON((u32)kbase_csf_scheduler_group_get_slot_locked(group) != csg_nr))
+	if (WARN_ON(kbase_csf_scheduler_group_get_slot_locked(group) != csg_nr))
 		return;
 
-	KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_INTERRUPT_PROCESS_START, group, (u64)csg_nr);
+	KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_INTERRUPT_PROCESS_START, group, csg_nr);
 
 	kbase_csf_handle_csg_sync_update(kbdev, ginfo, group, req, ack);
 
@@ -2776,7 +2683,7 @@ static void process_csg_interrupts(struct kbase_device *const kbdev, u32 const c
 		KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_IDLE_SET, group,
 					 scheduler->csg_slots_idle_mask[0]);
 		KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_INTERRUPT_IDLE, group, req ^ ack);
-		dev_dbg(kbdev->dev, "Idle notification received for Group %u on slot %u\n",
+		dev_dbg(kbdev->dev, "Idle notification received for Group %u on slot %d\n",
 			group->handle, csg_nr);
 
 		if (atomic_read(&scheduler->non_idle_offslot_grps)) {
@@ -2793,7 +2700,7 @@ static void process_csg_interrupts(struct kbase_device *const kbdev, u32 const c
 
 		if (group->scan_seq_num < track->idle_seq) {
 			track->idle_seq = group->scan_seq_num;
-			track->idle_slot = (s8)csg_nr;
+			track->idle_slot = csg_nr;
 		}
 	}
 
@@ -2803,9 +2710,13 @@ static void process_csg_interrupts(struct kbase_device *const kbdev, u32 const c
 
 		KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_INTERRUPT_PROGRESS_TIMER_EVENT, group,
 					 req ^ ack);
+		dev_info(
+			kbdev->dev,
+			"[%llu] Iterator PROGRESS_TIMER timeout notification received for group %u of ctx %d_%d on slot %d\n",
+			kbase_backend_get_cycle_cnt(kbdev), group->handle, group->kctx->tgid,
+			group->kctx->id, csg_nr);
 
-		set_bit(csg_nr, progress_timeout_slot_mask);
-
+		handle_progress_timer_event(group);
 	}
 
 	process_cs_interrupts(group, ginfo, irqreq, irqack, track);
@@ -2815,7 +2726,7 @@ static void process_csg_interrupts(struct kbase_device *const kbdev, u32 const c
 }
 
 /**
- * process_prfcnt_interrupts() - Process performance counter interrupts.
+ * process_prfcnt_interrupts - Process performance counter interrupts.
  *
  * @kbdev:   Instance of a GPU platform device that implements a CSF interface.
  * @glb_req: Global request register value.
@@ -2887,7 +2798,7 @@ static void process_prfcnt_interrupts(struct kbase_device *kbdev, u32 glb_req, u
 }
 
 /**
- * check_protm_enter_req_complete() - Check if PROTM_ENTER request completed
+ * check_protm_enter_req_complete - Check if PROTM_ENTER request completed
  *
  * @kbdev: Instance of a GPU platform device that implements a CSF interface.
  * @glb_req: Global request register value.
@@ -2915,14 +2826,13 @@ static inline void check_protm_enter_req_complete(struct kbase_device *kbdev, u3
 	dev_dbg(kbdev->dev, "Protected mode entry interrupt received");
 
 	kbdev->protected_mode = true;
-
 	kbase_ipa_protection_mode_switch_event(kbdev);
 	kbase_ipa_control_protm_entered(kbdev);
 	kbase_hwcnt_backend_csf_protm_entered(&kbdev->hwcnt_gpu_iface);
 }
 
 /**
- * process_protm_exit() - Handle the protected mode exit interrupt
+ * process_protm_exit - Handle the protected mode exit interrupt
  *
  * @kbdev: Instance of a GPU platform device that implements a CSF interface.
  * @glb_ack: Global acknowledge register value.
@@ -3011,7 +2921,7 @@ static inline void process_tracked_info_for_protm(struct kbase_device *kbdev,
 		if (!tock_triggered) {
 			dev_dbg(kbdev->dev, "Group-%d on slot-%d start protm work\n", group->handle,
 				group->csg_nr);
-			kbase_csf_scheduler_enqueue_protm_event_work(group);
+			queue_work(group->kctx->csf.wq, &group->protm_event_work);
 		}
 	}
 }
@@ -3040,46 +2950,6 @@ static void order_job_irq_clear_with_iface_mem_read(void)
 	dmb(osh);
 }
 
-static const char *const glb_fatal_status_errors[GLB_FATAL_STATUS_VALUE_COUNT] = {
-	[GLB_FATAL_STATUS_VALUE_OK] = "OK",
-	[GLB_FATAL_STATUS_VALUE_ASSERT] = "Firmware assert triggered",
-	[GLB_FATAL_STATUS_VALUE_UNEXPECTED_EXCEPTION] =
-		"Hardware raised an exception firmware did not expect",
-	[GLB_FATAL_STATUS_VALUE_HANG] = "Firmware hangs and watchdog timer expired",
-};
-
-/**
- * handle_glb_fatal_event() - Handle the GLB fatal event
- *
- * @kbdev:        Instance of GPU device.
- * @global_iface: CSF global interface
- */
-static void handle_glb_fatal_event(struct kbase_device *kbdev,
-				   const struct kbase_csf_global_iface *const global_iface)
-{
-	const char *error_string = NULL;
-	const u32 fatal_status = kbase_csf_firmware_global_output(global_iface, GLB_FATAL_STATUS);
-
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
-	dev_warn(kbdev->dev, "MCU encountered unrecoverable error");
-
-	if (fatal_status < GLB_FATAL_STATUS_VALUE_COUNT)
-		error_string = glb_fatal_status_errors[fatal_status];
-	else {
-		dev_err(kbdev->dev, "Invalid GLB_FATAL_STATUS (%u)", fatal_status);
-		return;
-	}
-
-	if (fatal_status == GLB_FATAL_STATUS_VALUE_OK)
-		dev_err(kbdev->dev, "GLB_FATAL_STATUS(OK) must be set with proper reason");
-	else {
-		dev_warn(kbdev->dev, "GLB_FATAL_STATUS: %s", error_string);
-		if (kbase_prepare_to_reset_gpu_locked(kbdev, RESET_FLAGS_NONE))
-			kbase_reset_gpu_locked(kbdev);
-	}
-}
-
 void kbase_csf_interrupt(struct kbase_device *kbdev, u32 val)
 {
 	bool deferred_handling_glb_idle_irq = false;
@@ -3100,25 +2970,18 @@ void kbase_csf_interrupt(struct kbase_device *kbdev, u32 val)
 			struct irq_idle_and_protm_track track = { .protm_grp = NULL,
 								  .idle_seq = U32_MAX,
 								  .idle_slot = S8_MAX };
-			DECLARE_BITMAP(progress_timeout_csgs, MAX_SUPPORTED_CSGS) = { 0 };
 
 			kbase_csf_scheduler_spin_lock(kbdev, &flags);
-			/* Looping through and track the highest idle and protm groups.
-			 * Also track the groups for which progress timer timeout happened.
-			 */
+			/* Looping through and track the highest idle and protm groups */
 			while (csg_interrupts != 0) {
-				u32 const csg_nr = (u32)ffs((int)csg_interrupts) - 1;
+				int const csg_nr = ffs(csg_interrupts) - 1;
 
-				process_csg_interrupts(kbdev, csg_nr, &track,
-						       progress_timeout_csgs);
-				csg_interrupts &= ~(1U << csg_nr);
+				process_csg_interrupts(kbdev, csg_nr, &track);
+				csg_interrupts &= ~(1 << csg_nr);
 			}
 
 			/* Handle protm from the tracked information */
 			process_tracked_info_for_protm(kbdev, &track);
-			/* Handle pending progress timeout(s) */
-			handle_progress_timer_events(kbdev, progress_timeout_csgs);
-
 			kbase_csf_scheduler_spin_unlock(kbdev, flags);
 		}
 
@@ -3147,28 +3010,11 @@ void kbase_csf_interrupt(struct kbase_device *kbdev, u32 val)
 
 				/* Handle IDLE Hysteresis notification event */
 				if ((glb_req ^ glb_ack) & GLB_REQ_IDLE_EVENT_MASK) {
-					u32 const glb_idle_timer_cfg =
-						kbase_csf_firmware_global_input_read(
-							global_iface, GLB_IDLE_TIMER_CONFIG);
-
 					dev_dbg(kbdev->dev, "Idle-hysteresis event flagged");
 					kbase_csf_firmware_global_input_mask(
 						global_iface, GLB_REQ, glb_ack,
 						GLB_REQ_IDLE_EVENT_MASK);
 
-					if (glb_idle_timer_cfg &
-					    GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_MASK) {
-						/* The FW is going to sleep, we shall:
-						 * - Enable fast GPU idle handling to avoid
-						 *   confirming CSGs status in gpu_idle_worker().
-						 * - Enable doorbell mirroring to minimise the
-						 *   chance of KBase raising kernel doorbells which
-						 *   would cause the FW to be woken up.
-						 */
-						kbdev->csf.scheduler.fast_gpu_idle_handling = true;
-						kbase_pm_enable_db_mirror_interrupt(kbdev);
-					}
-
 					glb_idle_irq_received = true;
 					/* Defer handling this IRQ to account for a race condition
 					 * where the idle worker could be executed before we have
@@ -3178,9 +3024,6 @@ void kbase_csf_interrupt(struct kbase_device *kbdev, u32 val)
 					deferred_handling_glb_idle_irq = true;
 				}
 
-				if (glb_ack & GLB_ACK_FATAL_MASK)
-					handle_glb_fatal_event(kbdev, global_iface);
-
 				process_prfcnt_interrupts(kbdev, glb_req, glb_ack);
 
 				kbase_csf_scheduler_spin_unlock(kbdev, flags);
@@ -3239,11 +3082,6 @@ void kbase_csf_doorbell_mapping_term(struct kbase_device *kbdev)
 	if (kbdev->csf.db_filp) {
 		struct page *page = as_page(kbdev->csf.dummy_db_page);
 
-		/* This is a shared dummy sink page for avoiding potential segmentation fault
-		 * to user-side library when a csi is off slot. Additionally, the call is on
-		 * module unload path, so the page can be left uncleared before returning it
-		 * back to kbdev memory pool.
-		 */
 		kbase_mem_pool_free(&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW], page, false);
 
 		fput(kbdev->csf.db_filp);
@@ -3275,27 +3113,26 @@ int kbase_csf_doorbell_mapping_init(struct kbase_device *kbdev)
 	return 0;
 }
 
-void kbase_csf_pending_gpuq_kick_queues_init(struct kbase_device *kbdev)
+void kbase_csf_pending_gpuq_kicks_init(struct kbase_device *kbdev)
 {
 	size_t i;
 
-	atomic_set(&kbdev->csf.pending_gpuq_kicks, false);
-	for (i = 0; i != ARRAY_SIZE(kbdev->csf.pending_gpuq_kick_queues); ++i)
-		INIT_LIST_HEAD(&kbdev->csf.pending_gpuq_kick_queues[i]);
-	spin_lock_init(&kbdev->csf.pending_gpuq_kick_queues_lock);
+	for (i = 0; i != ARRAY_SIZE(kbdev->csf.pending_gpuq_kicks); ++i)
+		INIT_LIST_HEAD(&kbdev->csf.pending_gpuq_kicks[i]);
+	spin_lock_init(&kbdev->csf.pending_gpuq_kicks_lock);
 }
 
-void kbase_csf_pending_gpuq_kick_queues_term(struct kbase_device *kbdev)
+void kbase_csf_pending_gpuq_kicks_term(struct kbase_device *kbdev)
 {
 	size_t i;
 
-	spin_lock(&kbdev->csf.pending_gpuq_kick_queues_lock);
-	for (i = 0; i != ARRAY_SIZE(kbdev->csf.pending_gpuq_kick_queues); ++i) {
-		if (!list_empty(&kbdev->csf.pending_gpuq_kick_queues[i]))
+	spin_lock(&kbdev->csf.pending_gpuq_kicks_lock);
+	for (i = 0; i != ARRAY_SIZE(kbdev->csf.pending_gpuq_kicks); ++i) {
+		if (!list_empty(&kbdev->csf.pending_gpuq_kicks[i]))
 			dev_warn(kbdev->dev,
 				 "Some GPU queue kicks for priority %zu were not handled", i);
 	}
-	spin_unlock(&kbdev->csf.pending_gpuq_kick_queues_lock);
+	spin_unlock(&kbdev->csf.pending_gpuq_kicks_lock);
 }
 
 void kbase_csf_free_dummy_user_reg_page(struct kbase_device *kbdev)
@@ -3303,11 +3140,6 @@ void kbase_csf_free_dummy_user_reg_page(struct kbase_device *kbdev)
 	if (kbdev->csf.user_reg.filp) {
 		struct page *page = as_page(kbdev->csf.user_reg.dummy_page);
 
-		/* This is a shared dummy page in place of the real USER Register page just
-		 * before the GPU is powered down. Additionally, the call is on module unload
-		 * path, so the page can be left uncleared before returning it back to kbdev
-		 * memory pool.
-		 */
 		kbase_mem_pool_free(&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW], page, false);
 		fput(kbdev->csf.user_reg.filp);
 	}
@@ -3390,17 +3222,17 @@ void kbase_csf_process_queue_kick(struct kbase_queue *queue)
 		if (err == -EBUSY) {
 			retry_kick = true;
 
-			spin_lock(&kbdev->csf.pending_gpuq_kick_queues_lock);
+			spin_lock(&kbdev->csf.pending_gpuq_kicks_lock);
 			if (list_empty(&queue->pending_kick_link)) {
 				/* A failed queue kick shall be pushed to the
 				 * back of the queue to avoid potential abuse.
 				 */
 				list_add_tail(
 					&queue->pending_kick_link,
-					&kbdev->csf.pending_gpuq_kick_queues[queue->group_priority]);
-				spin_unlock(&kbdev->csf.pending_gpuq_kick_queues_lock);
+					&kbdev->csf.pending_gpuq_kicks[queue->group_priority]);
+				spin_unlock(&kbdev->csf.pending_gpuq_kicks_lock);
 			} else {
-				spin_unlock(&kbdev->csf.pending_gpuq_kick_queues_lock);
+				spin_unlock(&kbdev->csf.pending_gpuq_kicks_lock);
 				WARN_ON(atomic_read(&queue->pending_kick) == 0);
 			}
 
@@ -3423,27 +3255,3 @@ void kbase_csf_process_queue_kick(struct kbase_queue *queue)
 	WARN_ON(atomic_read(&queue->pending_kick) == 0);
 	atomic_dec(&queue->pending_kick);
 }
-
-void kbase_csf_process_protm_event_request(struct kbase_queue_group *group)
-{
-	struct kbase_protected_suspend_buffer *sbuf = &group->protected_suspend_buf;
-	int err = 0;
-
-	KBASE_KTRACE_ADD_CSF_GRP(group->kctx->kbdev, PROTM_EVENT_WORKER_START, group, 0u);
-
-	err = alloc_grp_protected_suspend_buffer_pages(group);
-	if (!err) {
-		kbase_csf_scheduler_group_protm_enter(group);
-	} else if (err == -ENOMEM && sbuf->alloc_retries <= PROTM_ALLOC_MAX_RETRIES) {
-		sbuf->alloc_retries++;
-		/* try again to allocate pages */
-		kbase_csf_scheduler_enqueue_protm_event_work(group);
-	} else if (sbuf->alloc_retries >= PROTM_ALLOC_MAX_RETRIES || err != -ENOMEM) {
-		dev_err(group->kctx->kbdev->dev,
-			"Failed to allocate physical pages for Protected mode suspend buffer for the group %d of context %d_%d",
-			group->handle, group->kctx->tgid, group->kctx->id);
-		report_group_fatal_error(group);
-	}
-
-	KBASE_KTRACE_ADD_CSF_GRP(group->kctx->kbdev, PROTM_EVENT_WORKER_END, group, 0u);
-}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.h
index 566136342a06..b2f6ab2c4a27 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.h
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.h
@@ -243,19 +243,6 @@ struct kbase_queue_group *kbase_csf_find_queue_group(struct kbase_context *kctx,
  */
 int kbase_csf_queue_group_handle_is_valid(struct kbase_context *kctx, u8 group_handle);
 
-/**
- * kbase_csf_queue_group_clear_faults - Re-enable CS Fault reporting.
- *
- * @kctx:	Pointer to the kbase context within which the
- *		CS Faults for the queues has to be re-enabled.
- * @clear_faults:	Pointer to the structure which contains details of the
- *		queues for which the CS Fault reporting has to be re-enabled.
- *
- * Return:	0 on success, or negative on failure.
- */
-int kbase_csf_queue_group_clear_faults(struct kbase_context *kctx,
-				       struct kbase_ioctl_queue_group_clear_faults *clear_faults);
-
 /**
  * kbase_csf_queue_group_create - Create a GPU command queue group.
  *
@@ -392,20 +379,20 @@ int kbase_csf_setup_dummy_user_reg_page(struct kbase_device *kbdev);
 void kbase_csf_free_dummy_user_reg_page(struct kbase_device *kbdev);
 
 /**
- * kbase_csf_pending_gpuq_kick_queues_init - Initialize the data used for handling
- *                                           GPU queue kicks.
+ * kbase_csf_pending_gpuq_kicks_init - Initialize the data used for handling
+ *                                     GPU queue kicks.
  *
  * @kbdev: Instance of a GPU platform device that implements a CSF interface.
  */
-void kbase_csf_pending_gpuq_kick_queues_init(struct kbase_device *kbdev);
+void kbase_csf_pending_gpuq_kicks_init(struct kbase_device *kbdev);
 
 /**
- * kbase_csf_pending_gpuq_kick_queues_term - De-initialize the data used for handling
- *                                           GPU queue kicks.
+ * kbase_csf_pending_gpuq_kicks_term - De-initialize the data used for handling
+ *                                     GPU queue kicks.
  *
  * @kbdev: Instance of a GPU platform device that implements a CSF interface.
  */
-void kbase_csf_pending_gpuq_kick_queues_term(struct kbase_device *kbdev);
+void kbase_csf_pending_gpuq_kicks_term(struct kbase_device *kbdev);
 
 /**
  * kbase_csf_ring_csg_doorbell - ring the doorbell for a CSG interface.
@@ -559,13 +546,4 @@ static inline u64 kbase_csf_ktrace_gpu_cycle_cnt(struct kbase_device *kbdev)
  */
 void kbase_csf_process_queue_kick(struct kbase_queue *queue);
 
-/**
- * kbase_csf_process_protm_event_request - Handle protected mode switch request
- *
- * @group: The group to handle protected mode request
- *
- * Request to switch to protected mode.
- */
-void kbase_csf_process_protm_event_request(struct kbase_queue_group *group);
-
 #endif /* _KBASE_CSF_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.c
index 32f33a58a6f7..736545c86c99 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -28,8 +28,6 @@
 #include <mali_kbase.h>
 #include <linux/seq_file.h>
 #include <linux/version_compat_defs.h>
-#include <mali_kbase_reset_gpu.h>
-#include <mali_kbase_config_defaults.h>
 
 #define MAX_SCHED_STATE_STRING_LEN (16)
 /**
@@ -238,6 +236,7 @@ static ssize_t kbase_csf_debugfs_scheduler_state_set(struct file *file, const ch
 {
 	struct kbase_device *kbdev = file->private_data;
 	char buf[MAX_SCHED_STATE_STRING_LEN];
+	ssize_t ret = count;
 
 	CSTD_UNUSED(ppos);
 
@@ -257,10 +256,10 @@ static ssize_t kbase_csf_debugfs_scheduler_state_set(struct file *file, const ch
 		kbase_csf_scheduler_force_wakeup(kbdev);
 	else {
 		dev_dbg(kbdev->dev, "Bad scheduler state %s", buf);
-		return -EINVAL;
+		ret = -EINVAL;
 	}
 
-	return (ssize_t)count;
+	return ret;
 }
 
 static const struct file_operations kbasep_csf_debugfs_scheduler_state_fops = {
@@ -270,87 +269,6 @@ static const struct file_operations kbasep_csf_debugfs_scheduler_state_fops = {
 	.open = simple_open,
 	.llseek = default_llseek,
 };
-static int kbasep_csf_debugfs_eviction_timeout_get(void *data, u64 *val)
-{
-	struct kbase_device *const kbdev = data;
-	unsigned long flags;
-
-	kbase_csf_scheduler_spin_lock(kbdev, &flags);
-	*val = kbdev->csf.csg_suspend_timeout_ms - CSG_SUSPEND_TIMEOUT_HOST_ADDED_MS;
-	kbase_csf_scheduler_spin_unlock(kbdev, flags);
-
-	return 0;
-}
-
-static int kbasep_csf_debugfs_eviction_timeout_set(void *data, u64 val)
-{
-	struct kbase_device *const kbdev = data;
-	unsigned long flags_schd, flags_hw;
-	u64 dur_ms = val;
-	int ret = 0;
-
-	if (unlikely(dur_ms < CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MIN ||
-		     dur_ms > CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MAX)) {
-		dev_err(kbdev->dev, "Invalid CSG suspend timeout input (%llu)", dur_ms);
-		return -EFAULT;
-	}
-	dur_ms = dur_ms + CSG_SUSPEND_TIMEOUT_HOST_ADDED_MS;
-
-	/* The 'fw_load_lock' is taken to synchronize against the deferred
-	 * loading of FW, update will take effect after firmware gets loaded.
-	 */
-	mutex_lock(&kbdev->fw_load_lock);
-	if (unlikely(!kbdev->csf.firmware_inited)) {
-		kbase_csf_scheduler_spin_lock(kbdev, &flags_schd);
-		kbdev->csf.csg_suspend_timeout_ms = (unsigned int)dur_ms;
-		kbase_csf_scheduler_spin_unlock(kbdev, flags_schd);
-		mutex_unlock(&kbdev->fw_load_lock);
-		dev_info(kbdev->dev, "CSF set csg suspend timeout deferred till fw is loaded");
-		goto end;
-	}
-	mutex_unlock(&kbdev->fw_load_lock);
-
-	/* Firmware reloading is triggered by silent reset, and then update will take effect.
-	 */
-	kbase_csf_scheduler_pm_active(kbdev);
-	if (kbase_csf_scheduler_killable_wait_mcu_active(kbdev)) {
-		dev_err(kbdev->dev,
-			"Unable to activate the MCU, the csg suspend timeout value shall remain unchanged");
-		kbase_csf_scheduler_pm_idle(kbdev);
-		ret = -EFAULT;
-		goto exit;
-	}
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags_hw);
-	if (kbase_reset_gpu_silent(kbdev)) {
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags_hw);
-		dev_err(kbdev->dev, "CSF set csg suspend timeout pending reset, try again");
-		kbase_csf_scheduler_pm_idle(kbdev);
-		ret = -EFAULT;
-		goto exit;
-	}
-	/* GPU reset is placed and it will take place only after hwaccess_lock is released,
-	 * update on host side should be done after GPU reset is placed and before it takes place.
-	 */
-	kbase_csf_scheduler_spin_lock(kbdev, &flags_schd);
-	kbdev->csf.csg_suspend_timeout_ms = (unsigned int)dur_ms;
-	kbase_csf_scheduler_spin_unlock(kbdev, flags_schd);
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags_hw);
-	/* Keep PM active until reset finished to allow FW reloading to take place,
-	 * and then update request will be sent to FW during initialization.
-	 */
-	kbase_reset_gpu_wait(kbdev);
-	kbase_csf_scheduler_pm_idle(kbdev);
-
-end:
-	dev_info(kbdev->dev, "CSF set csg suspend timeout: %u ms", (unsigned int)dur_ms);
-
-exit:
-	return ret;
-}
-
-DEFINE_DEBUGFS_ATTRIBUTE(kbasep_csf_debugfs_eviction_timeout_fops,
-			 &kbasep_csf_debugfs_eviction_timeout_get,
-			 &kbasep_csf_debugfs_eviction_timeout_set, "%llu\n");
 
 void kbase_csf_debugfs_init(struct kbase_device *kbdev)
 {
@@ -363,8 +281,6 @@ void kbase_csf_debugfs_init(struct kbase_device *kbdev)
 			    &kbasep_csf_debugfs_scheduling_timer_kick_fops);
 	debugfs_create_file("scheduler_state", 0644, kbdev->mali_debugfs_directory, kbdev,
 			    &kbasep_csf_debugfs_scheduler_state_fops);
-	debugfs_create_file("eviction_timeout_ms", 0644, kbdev->mali_debugfs_directory, kbdev,
-			    &kbasep_csf_debugfs_eviction_timeout_fops);
 
 	kbase_csf_tl_reader_debugfs_init(kbdev);
 }
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_defs.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_defs.h
index 38e7cb940d97..c90b531d36b7 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_defs.h
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_defs.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -26,16 +26,14 @@
 #ifndef _KBASE_CSF_DEFS_H_
 #define _KBASE_CSF_DEFS_H_
 
+#include <linux/types.h>
+#include <linux/wait.h>
+
 #include <hw_access/mali_kbase_hw_access.h>
 #include "mali_kbase_csf_firmware.h"
+#include "mali_kbase_refcount_defs.h"
 #include "mali_kbase_csf_event.h"
 #include <uapi/gpu/arm/bifrost/csf/mali_kbase_csf_errors_dumpfault.h>
-#include "mali_kbase_csf_fw_io.h"
-
-#include <linux/version_compat_defs.h>
-
-#include <linux/types.h>
-#include <linux/wait.h>
 
 #if IS_ENABLED(CONFIG_MALI_CORESIGHT)
 #include <debug/backend/mali_kbase_debug_coresight_internal_csf.h>
@@ -247,11 +245,11 @@ enum kbase_csf_scheduler_state {
 /**
  * enum kbase_queue_group_priority - Kbase internal relative priority list.
  *
- * @KBASE_QUEUE_GROUP_PRIORITY_REALTIME:              The realtime queue group priority.
- * @KBASE_QUEUE_GROUP_PRIORITY_HIGH:                  The high queue group priority.
- * @KBASE_QUEUE_GROUP_PRIORITY_MEDIUM:                The medium queue group priority.
- * @KBASE_QUEUE_GROUP_PRIORITY_LOW:                   The low queue group priority.
- * @KBASE_QUEUE_GROUP_PRIORITY_COUNT:                 The number of priority levels.
+ * @KBASE_QUEUE_GROUP_PRIORITY_REALTIME:  The realtime queue group priority.
+ * @KBASE_QUEUE_GROUP_PRIORITY_HIGH:      The high queue group priority.
+ * @KBASE_QUEUE_GROUP_PRIORITY_MEDIUM:    The medium queue group priority.
+ * @KBASE_QUEUE_GROUP_PRIORITY_LOW:       The low queue group priority.
+ * @KBASE_QUEUE_GROUP_PRIORITY_COUNT:     The number of priority levels.
  */
 enum kbase_queue_group_priority {
 	KBASE_QUEUE_GROUP_PRIORITY_REALTIME = 0,
@@ -268,19 +266,14 @@ enum kbase_queue_group_priority {
  * @CSF_PM_TIMEOUT: Timeout for GPU Power Management to reach the desired
  *                  Shader, L2 and MCU state.
  * @CSF_GPU_RESET_TIMEOUT: Waiting timeout for GPU reset to complete.
- * @CSF_CSG_TERM_TIMEOUT: Timeout given for a CSG to be terminated.
+ * @CSF_CSG_SUSPEND_TIMEOUT: Timeout given for a CSG to be suspended.
  * @CSF_FIRMWARE_BOOT_TIMEOUT: Maximum time to wait for firmware to boot.
  * @CSF_FIRMWARE_PING_TIMEOUT: Maximum time to wait for firmware to respond
  *                             to a ping from KBase.
  * @CSF_SCHED_PROTM_PROGRESS_TIMEOUT: Timeout used to prevent protected mode execution hang.
  * @MMU_AS_INACTIVE_WAIT_TIMEOUT: Maximum waiting time in ms for the completion
  *                                of a MMU operation.
- * @KCPU_FENCE_SIGNAL_TIMEOUT: Waiting time in ms for triggering a KCPU queue sync state dump.
- * @KBASE_PRFCNT_ACTIVE_TIMEOUT: Waiting time for prfcnt to be ready.
- * @KBASE_CLEAN_CACHE_TIMEOUT: Waiting time for cache flush to complete.
- * @KBASE_AS_INACTIVE_TIMEOUT: Waiting time for MCU address space to become inactive.
- * @IPA_INACTIVE_TIMEOUT: Waiting time for IPA_CONTROL_STATUS flags to be cleared.
- * @CSF_FIRMWARE_STOP_TIMEOUT: Waiting time for the firmware to stop.
+ * @KCPU_FENCE_SIGNAL_TIMEOUT: Waiting time in ms for triggering a KCPU queue sync state dump
  * @KBASE_TIMEOUT_SELECTOR_COUNT: Number of timeout selectors. Must be last in
  *                                the enum.
  * @KBASE_DEFAULT_TIMEOUT: Default timeout used when an invalid selector is passed
@@ -290,17 +283,12 @@ enum kbase_timeout_selector {
 	CSF_FIRMWARE_TIMEOUT,
 	CSF_PM_TIMEOUT,
 	CSF_GPU_RESET_TIMEOUT,
-	CSF_CSG_TERM_TIMEOUT,
+	CSF_CSG_SUSPEND_TIMEOUT,
 	CSF_FIRMWARE_BOOT_TIMEOUT,
 	CSF_FIRMWARE_PING_TIMEOUT,
 	CSF_SCHED_PROTM_PROGRESS_TIMEOUT,
 	MMU_AS_INACTIVE_WAIT_TIMEOUT,
 	KCPU_FENCE_SIGNAL_TIMEOUT,
-	KBASE_PRFCNT_ACTIVE_TIMEOUT,
-	KBASE_CLEAN_CACHE_TIMEOUT,
-	KBASE_AS_INACTIVE_TIMEOUT,
-	IPA_INACTIVE_TIMEOUT,
-	CSF_FIRMWARE_STOP_TIMEOUT,
 
 	/* Must be the last in the enum */
 	KBASE_TIMEOUT_SELECTOR_COUNT,
@@ -399,10 +387,6 @@ struct kbase_csf_notification {
  * @cs_error:         Records information about the CS fatal event or
  *                    about CS fault event if dump on fault is enabled.
  * @cs_error_fatal:   Flag to track if the CS fault or CS fatal event occurred.
- * @cs_error_acked:   Flag to indicate that acknowledging the fault has been done
- *                    at top-half of fault handler.
- * @clear_faults:     Flag to track if the CS fault reporting is enabled for this queue.
- *                    It's protected by &kbase_context.csf.lock.
  * @extract_ofs: The current EXTRACT offset, this is only updated when handling
  *               the GLB IDLE IRQ if the idle timeout value is non-0 in order
  *               to help detect a queue's true idle status.
@@ -446,8 +430,6 @@ struct kbase_queue {
 	u64 cs_error_info;
 	u32 cs_error;
 	bool cs_error_fatal;
-	bool cs_error_acked;
-	bool clear_faults;
 	u64 extract_ofs;
 	u64 saved_cmd_ptr;
 };
@@ -508,8 +490,6 @@ struct kbase_protected_suspend_buffer {
  * @compute_max:    Maximum number of compute endpoints the group is
  *                  allowed to use.
  * @csi_handlers:   Requested CSI exception handler flags for the group.
- * @cs_fault_report_enable:	Indicated if reporting of CS_FAULTs to
- *				userspace is enabled.
  * @tiler_mask:     Mask of tiler endpoints the group is allowed to use.
  * @fragment_mask:  Mask of fragment endpoints the group is allowed to use.
  * @compute_mask:   Mask of compute endpoints the group is allowed to use.
@@ -525,9 +505,9 @@ struct kbase_protected_suspend_buffer {
  * @prepared_seq_num: Indicates the position of queue group in the list of
  *                    prepared groups to be scheduled.
  * @scan_seq_num:     Scan out sequence number before adjusting for dynamic
- *                    idle conditions. It represents a group's global priority
- *                    for a running tick/tock. It could differ from
- *                    prepared_seq_number when there are idle groups.
+ *                    idle conditions. It is used for setting a group's
+ *                    onslot priority. It could differ from prepared_seq_number
+ *                    when there are idle groups.
  * @faulted:          Indicates that a GPU fault occurred for the queue group.
  *                    This flag persists until the fault has been queued to be
  *                    reported to userspace.
@@ -540,21 +520,14 @@ struct kbase_protected_suspend_buffer {
  * @bound_queues:   Array of registered queues bound to this queue group.
  * @doorbell_nr:    Index of the hardware doorbell page assigned to the
  *                  group.
- * @protm_event_work: List item corresponding to the protected mode entry
- *                    event for this queue. This would be handled by
- *                    kbase_csf_scheduler_kthread().
- * @pending_protm_event_work: Indicates that kbase_csf_scheduler_kthread() should
- *                            handle PROTM request for this group. This would
- *                            be set to false when the work is done. This is used
- *                            mainly for synchronisation with group termination.
+ * @protm_event_work:   Work item corresponding to the protected mode entry
+ *                      event for this queue.
  * @protm_pending_bitmap:  Bit array to keep a track of CSs that
  *                         have pending protected mode entry requests.
  * @error_fatal: An error of type BASE_GPU_QUEUE_GROUP_ERROR_FATAL to be
  *               returned to userspace if such an error has occurred.
  * @timer_event_work: Work item to handle the progress timeout fatal event
  *                    for the group.
- * @progress_timer_state: Value of CSG_PROGRESS_TIMER_STATE register when progress
- *                        timer timeout is reported for the group.
  * @deschedule_deferred_cnt: Counter keeping a track of the number of threads
  *                           that tried to deschedule the group and had to defer
  *                           the descheduling due to the dump on fault.
@@ -565,12 +538,6 @@ struct kbase_protected_suspend_buffer {
  *                  It is accumulated on consecutive mapping attempt failures. On
  *                  reaching a preset limit, the group is regarded as suffered
  *                  a fatal error and triggers a fatal error notification.
- * @sched_act_seq_num: Scheduling action sequence number to determine the CSG slot
- *                    priority in tick/tock scheduling action. It could differ from
- *                    scan_seq_num. The field is only meaningful if the CSG is on the
- *                    Scheduler's schedulable list for a tick/tock, and used for
- *                    determining the CSG slot priority when the group is to be placed
- *                    on the CSG slot.
  */
 struct kbase_queue_group {
 	struct kbase_context *kctx;
@@ -585,7 +552,7 @@ struct kbase_queue_group {
 	u8 compute_max;
 	u8 csi_handlers;
 
-	__u8 cs_fault_report_enable;
+
 	u64 tiler_mask;
 	u64 fragment_mask;
 	u64 compute_mask;
@@ -604,14 +571,12 @@ struct kbase_queue_group {
 	struct kbase_queue *bound_queues[MAX_SUPPORTED_STREAMS_PER_GROUP];
 
 	int doorbell_nr;
-	struct list_head protm_event_work;
-	atomic_t pending_protm_event_work;
+	struct work_struct protm_event_work;
 	DECLARE_BITMAP(protm_pending_bitmap, MAX_SUPPORTED_STREAMS_PER_GROUP);
 
 	struct kbase_csf_notification error_fatal;
 
 	struct work_struct timer_event_work;
-	u32 progress_timer_state;
 
 	/**
 	 * @dvs_buf: Address and size of scratch memory.
@@ -624,7 +589,6 @@ struct kbase_queue_group {
 #endif
 	void *csg_reg;
 	u8 csg_reg_bind_retries;
-	u32 sched_act_seq_num;
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
 	/**
 	 * @prev_act: Previous CSG activity transition in a GPU metrics.
@@ -643,9 +607,6 @@ struct kbase_queue_group {
  * @cmd_seq_num:        The sequence number assigned to an enqueued command,
  *                      in incrementing order (older commands shall have a
  *                      smaller number).
- * @kcpu_wq: Work queue to process KCPU commands for all queues in this
- *           context. This would be used if the context is not prioritised,
- *           otherwise it would be handled by kbase_csf_scheduler_kthread().
  * @jit_lock:           Lock to serialise JIT operations.
  * @jit_cmds_head:      A list of the just-in-time memory commands, both
  *                      allocate & free, in submission order, protected
@@ -661,8 +622,6 @@ struct kbase_csf_kcpu_queue_context {
 	DECLARE_BITMAP(in_use, KBASEP_MAX_KCPU_QUEUES);
 	atomic64_t cmd_seq_num;
 
-	struct workqueue_struct *kcpu_wq;
-
 	struct mutex jit_lock;
 	struct list_head jit_cmds_head;
 	struct list_head jit_blocked_queues;
@@ -770,7 +729,13 @@ struct kbase_csf_ctx_heap_reclaim_info {
  *                      GPU command queues are idle and at least one of them
  *                      is blocked on a sync wait operation.
  * @num_idle_wait_grps: Length of the @idle_wait_groups list.
- * @sync_update_work:   List item to process the SYNC_UPDATE event.
+ * @sync_update_wq:     Dedicated workqueue to process work items corresponding
+ *                      to the sync_update events by sync_set/sync_add
+ *                      instruction execution on CSs bound to groups
+ *                      of @idle_wait_groups list.
+ * @sync_update_work:   work item to process the sync_update events by
+ *                      sync_set / sync_add instruction execution on command
+ *                      streams bound to groups of @idle_wait_groups list.
  * @ngrp_to_schedule:	Number of groups added for the context to the
  *                      'groups_to_schedule' list of scheduler instance.
  * @heap_info:          Heap reclaim information data of the kctx. As the
@@ -783,7 +748,8 @@ struct kbase_csf_scheduler_context {
 	u32 num_runnable_grps;
 	struct list_head idle_wait_groups;
 	u32 num_idle_wait_grps;
-	struct list_head sync_update_work;
+	struct workqueue_struct *sync_update_wq;
+	struct work_struct sync_update_work;
 	u32 ngrp_to_schedule;
 	struct kbase_csf_ctx_heap_reclaim_info heap_info;
 };
@@ -884,10 +850,6 @@ struct kbase_csf_user_reg_context {
  * @cpu_queue:        CPU queue information. Only be available when DEBUG_FS
  *                    is enabled.
  * @user_reg:         Collective information to support mapping to USER Register page.
- * @pending_sync_update: Indicates that kbase_csf_scheduler_kthread() should
- *                       handle SYNC_UPDATE event for this context. This would
- *                       be set to false when the work is done. This is used
- *                       mainly for synchronisation with context termination.
  */
 struct kbase_csf_context {
 	struct list_head event_pages_head;
@@ -904,7 +866,6 @@ struct kbase_csf_context {
 	struct kbase_csf_scheduler_context sched;
 	struct kbase_csf_cpu_queue_context cpu_queue;
 	struct kbase_csf_user_reg_context user_reg;
-	atomic_t pending_sync_update;
 };
 
 /**
@@ -933,11 +894,13 @@ struct kbase_csf_reset_gpu {
  *                             of CSG slots.
  * @resident_group:   pointer to the queue group that is resident on the CSG slot.
  * @state:            state of the slot as per enum @kbase_csf_csg_slot_state.
+ * @trigger_jiffies:  value of jiffies when change in slot state is recorded.
  * @priority:         dynamic priority assigned to CSG slot.
  */
 struct kbase_csf_csg_slot {
 	struct kbase_queue_group *resident_group;
 	atomic_t state;
+	unsigned long trigger_jiffies;
 	u8 priority;
 };
 
@@ -945,15 +908,14 @@ struct kbase_csf_csg_slot {
  * struct kbase_csf_sched_heap_reclaim_mgr - Object for managing tiler heap reclaim
  *                                           kctx lists inside the CSF device's scheduler.
  *
- * @heap_reclaim:   Defines Tiler heap reclaim shrinker object.
+ * @heap_reclaim:   Tiler heap reclaim shrinker object.
  * @ctx_lists:      Array of kctx lists, size matching CSG defined priorities. The
  *                  lists track the kctxs attached to the reclaim manager.
  * @unused_pages:   Estimated number of unused pages from the @ctxlist array. The
  *                  number is indicative for use with reclaim shrinker's count method.
  */
 struct kbase_csf_sched_heap_reclaim_mgr {
-	DEFINE_KBASE_SHRINKER heap_reclaim;
-
+	struct shrinker heap_reclaim;
 	struct list_head ctx_lists[KBASE_QUEUE_GROUP_PRIORITY_COUNT];
 	atomic_t unused_pages;
 };
@@ -1052,29 +1014,10 @@ struct kbase_csf_mcu_shared_regions {
  *                          workqueue items (kernel-provided delayed_work
  *                          items do not use hrtimer and for some reason do
  *                          not provide sufficiently reliable periodicity).
- * @pending_sync_update_works:  Indicates that kbase_csf_scheduler_kthread()
- *                              should handle SYNC_UPDATE events.
- * @sync_update_work_ctxs_lock: Lock protecting the list of contexts that
- *                              require handling SYNC_UPDATE events.
- * @sync_update_work_ctxs:      The list of contexts that require handling
- *                              SYNC_UPDATE events.
- * @pending_protm_event_works:  Indicates that kbase_csf_scheduler_kthread()
- *                              should handle PROTM requests.
- * @protm_event_work_grps_lock: Lock protecting the list of groups that
- *                              have requested protected mode.
- * @protm_event_work_grps:      The list of groups that have requested
- *                              protected mode.
- * @pending_kcpuq_works:    Indicates that kbase_csf_scheduler_kthread()
- *                          should process pending KCPU queue works.
- * @kcpuq_work_queues_lock: Lock protecting the list of KCPU queues that
- *                          need to be processed.
- * @kcpuq_work_queues:      The list of KCPU queue that need to be processed
- * @pending_tick_work:      Indicates that kbase_csf_scheduler_kthread() should
- *                          perform a scheduling tick.
- * @pending_tock_work:      Indicates that kbase_csf_scheduler_kthread() should
- *                          perform a scheduling tock.
- * @pending_gpu_idle_work:  Indicates that kbase_csf_scheduler_kthread() should
- *                          handle the GPU IDLE event.
+ * @pending_tick_work:      Indicates that kbase_csf_scheduler_kthread() should perform
+ *                          a scheduling tick.
+ * @pending_tock_work:      Indicates that kbase_csf_scheduler_kthread() should perform
+ *                          a scheduling tock.
  * @ping_work:              Work item that would ping the firmware at regular
  *                          intervals, only if there is a single active CSG
  *                          slot, to check if firmware is alive and would
@@ -1092,6 +1035,10 @@ struct kbase_csf_mcu_shared_regions {
  *                          This pointer being set doesn't necessarily indicates
  *                          that GPU is in protected mode, kbdev->protected_mode
  *                          needs to be checked for that.
+ * @idle_wq:                Workqueue for executing GPU idle notification
+ *                          handler.
+ * @gpu_idle_work:          Work item for facilitating the scheduler to bring
+ *                          the GPU to a low-power mode on becoming idle.
  * @fast_gpu_idle_handling: Indicates whether to relax many of the checks
  *                          normally done in the GPU idle worker. This is
  *                          set to true when handling the GLB IDLE IRQ if the
@@ -1122,9 +1069,6 @@ struct kbase_csf_mcu_shared_regions {
  *                          protected mode execution compared to other such
  *                          groups. It is updated on every tick/tock.
  *                          @interrupt_lock is used to serialize the access.
- * @csg_scan_sched_count:   Scheduling action counter used to assign the sched_act_seq_num
- *                          for each group added to Scheduler's schedulable list in a
- *                          tick/tock.
  * @protm_enter_time:       GPU protected mode enter time.
  * @reclaim_mgr:            CSGs tiler heap manager object.
  * @mcu_regs_data:          Scheduler MCU shared regions data for managing the
@@ -1134,11 +1078,8 @@ struct kbase_csf_mcu_shared_regions {
  *                          thread when a queue needs attention.
  * @kthread_running:        Whether the GPU queue submission thread should keep
  *                          executing.
- * @gpuq_kthread:           Dedicated thread primarily used to handle
- *                          latency-sensitive tasks such as GPU queue
+ * @gpuq_kthread:           High-priority thread used to handle GPU queue
  *                          submissions.
- * @gpu_idle_timer_enabled: Tracks whether the GPU idle timer is enabled or disabled.
- * @fw_soi_enabled:         True if FW Sleep-on-Idle is currently enabled.
  */
 struct kbase_csf_scheduler {
 	struct mutex lock;
@@ -1162,22 +1103,14 @@ struct kbase_csf_scheduler {
 	unsigned long last_schedule;
 	atomic_t timer_enabled;
 	struct hrtimer tick_timer;
-	atomic_t pending_sync_update_works;
-	spinlock_t sync_update_work_ctxs_lock;
-	struct list_head sync_update_work_ctxs;
-	atomic_t pending_protm_event_works;
-	spinlock_t protm_event_work_grps_lock;
-	struct list_head protm_event_work_grps;
-	atomic_t pending_kcpuq_works;
-	spinlock_t kcpuq_work_queues_lock;
-	struct list_head kcpuq_work_queues;
 	atomic_t pending_tick_work;
 	atomic_t pending_tock_work;
-	atomic_t pending_gpu_idle_work;
 	struct delayed_work ping_work;
 	struct kbase_context *top_kctx;
 	struct kbase_queue_group *top_grp;
 	struct kbase_queue_group *active_protm_grp;
+	struct workqueue_struct *idle_wq;
+	struct work_struct gpu_idle_work;
 	bool fast_gpu_idle_handling;
 	atomic_t gpu_no_longer_idle;
 	atomic_t non_idle_offslot_grps;
@@ -1185,7 +1118,6 @@ struct kbase_csf_scheduler {
 	u32 pm_active_count;
 	unsigned int csg_scheduling_period_ms;
 	u32 tick_protm_pending_seq;
-	u32 csg_scan_sched_count;
 	ktime_t protm_enter_time;
 	struct kbase_csf_sched_heap_reclaim_mgr reclaim_mgr;
 	struct kbase_csf_mcu_shared_regions mcu_regs_data;
@@ -1216,8 +1148,6 @@ struct kbase_csf_scheduler {
 	 */
 	spinlock_t gpu_metrics_lock;
 #endif /* CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD */
-	atomic_t gpu_idle_timer_enabled;
-	atomic_t fw_soi_enabled;
 };
 
 /*
@@ -1271,8 +1201,6 @@ enum kbase_ipa_core_type {
 	KBASE_IPA_CORE_TYPE_MEMSYS,
 	KBASE_IPA_CORE_TYPE_TILER,
 	KBASE_IPA_CORE_TYPE_SHADER,
-
-	/* Must be the last in the enum */
 	KBASE_IPA_CORE_TYPE_NUM
 };
 
@@ -1400,7 +1328,7 @@ struct kbase_ipa_control {
  * @num_pages: Number of entries in @phys and @pma (and length of the interface)
  * @num_pages_aligned: Same as @num_pages except for the case when @is_small_page
  *                     is false and @reuse_pages is false and therefore will be
- *                     aligned to NUM_PAGES_IN_2MB_LARGE_PAGE.
+ *                     aligned to NUM_4K_PAGES_IN_2MB_PAGE.
  * @virtual: Starting GPU virtual address this interface is mapped at
  * @flags: bitmask of CSF_FIRMWARE_ENTRY_* conveying the interface attributes
  * @data_start: Offset into firmware image at which the interface data starts
@@ -1665,9 +1593,9 @@ struct kbase_csf_user_reg {
  * @mcu_core_pwroff_dur_count: The counterpart of the glb_pwroff timeout input
  *                             in interface required format, ready to be used
  *                             directly in the firmware.
- * @mcu_core_pwroff_dur_count_no_modifier: Update csffw_glb_req_cfg_pwroff_timer
- *                                         to make the shr(10) modifier conditional
- *                                         on new flag in GLB_PWROFF_TIMER_CONFIG
+ * @mcu_core_pwroff_dur_count_modifier: Update csffw_glb_req_cfg_pwroff_timer
+ *                                      to make the shr(10) modifier conditional
+ *                                      on new flag in GLB_PWROFF_TIMER_CONFIG
  * @mcu_core_pwroff_reg_shadow: The actual value that has been programed into
  *                              the glb_pwoff register. This is separated from
  *                              the @p mcu_core_pwroff_dur_count as an update
@@ -1678,10 +1606,10 @@ struct kbase_csf_user_reg {
  * @gpu_idle_dur_count:     The counterpart of the hysteresis time window in
  *                          interface required format, ready to be used
  *                          directly in the firmware.
- * @gpu_idle_dur_count_no_modifier: Update csffw_glb_req_idle_enable to make the shr(10)
- *                                  modifier conditional on the new flag
- *                                  in GLB_IDLE_TIMER_CONFIG.
- * @csg_suspend_timeout_ms: Timeout given for a CSG to be suspended.
+ * @gpu_idle_dur_count_modifier: Update csffw_glb_req_idle_enable to make the shr(10)
+ *                               modifier conditional on the new flag
+ *                               in GLB_IDLE_TIMER_CONFIG.
+ * @fw_timeout_ms:          Timeout value (in milliseconds) used when waiting
  *                          for any request sent to the firmware.
  * @hwcnt:                  Contain members required for handling the dump of
  *                          HW counters.
@@ -1692,29 +1620,12 @@ struct kbase_csf_user_reg {
  * @dof:                    Structure for dump on fault.
  * @user_reg:               Collective information to support the mapping to
  *                          USER Register page for user processes.
- * @pending_gpuq_kicks:            Indicates that kbase_csf_scheduler_kthread()
- *                                 should handle GPU queue kicks.
- * @pending_gpuq_kick_queues:      Lists of GPU queued that have been kicked but not
- *                                 yet processed, categorised by queue group's priority.
- * @pending_gpuq_kick_queues_lock: Protect @pending_gpuq_kick_queues and
- *                                 kbase_queue.pending_kick_link.
+ * @pending_gpuq_kicks:     Lists of GPU queue that have been kicked but not
+ *                          yet processed, categorised by queue group's priority.
+ * @pending_gpuq_kicks_lock: Protect @pending_gpu_kicks and
+ *                           kbase_queue.pending_kick_link.
  * @quirks_ext:             Pointer to an allocated buffer containing the firmware
  *                          workarounds configuration.
- * @mmu_sync_sem:           RW Semaphore to defer MMU operations till the P.Mode entrance
- *                          or DCS request has been completed.
- * @pmode_sync_sem:         RW Semaphore to prevent MMU operations during P.Mode entrance.
- * @page_fault_cnt_ptr_address: GPU VA of the location in FW data memory, extracted from the
- *                              FW image header, that will store the GPU VA of FW visible
- *                              memory location where the @page_fault_cnt value will be written to.
- * @page_fault_cnt_ptr:         CPU VA of the FW visible memory location where the @page_fault_cnt
- *                              value will be written to.
- * @page_fault_cnt:             Counter that is incremented on every GPU page fault, just before the
- *                              MMU is unblocked to retry the memory transaction that caused the GPU
- *                              page fault. The access to counter is serialized appropriately.
- * @mcu_halted:             Flag to inform MCU FSM that the MCU has already halted.
- * @fw_io:                  Firmware I/O interface.
- * @compute_progress_timeout_cc: Value of GPU cycle count register when progress
- *                               timer timeout is reported for the compute iterator.
  */
 struct kbase_csf_device {
 	struct kbase_mmu_table mcu_mmu;
@@ -1745,14 +1656,14 @@ struct kbase_csf_device {
 	bool glb_init_request_pending;
 	struct work_struct fw_error_work;
 	struct kbase_ipa_control ipa_control;
-	u64 mcu_core_pwroff_dur_ns;
+	u32 mcu_core_pwroff_dur_ns;
 	u32 mcu_core_pwroff_dur_count;
-	u32 mcu_core_pwroff_dur_count_no_modifier;
+	u32 mcu_core_pwroff_dur_count_modifier;
 	u32 mcu_core_pwroff_reg_shadow;
-	u64 gpu_idle_hysteresis_ns;
+	u32 gpu_idle_hysteresis_ns;
 	u32 gpu_idle_dur_count;
-	u32 gpu_idle_dur_count_no_modifier;
-	u32 csg_suspend_timeout_ms;
+	u32 gpu_idle_dur_count_modifier;
+	unsigned int fw_timeout_ms;
 	struct kbase_csf_hwcnt hwcnt;
 	struct kbase_csf_mcu_fw fw;
 	struct kbase_csf_firmware_log fw_log;
@@ -1767,18 +1678,9 @@ struct kbase_csf_device {
 	struct kbase_debug_coresight_device coresight;
 #endif /* IS_ENABLED(CONFIG_MALI_CORESIGHT) */
 	struct kbase_csf_user_reg user_reg;
-	atomic_t pending_gpuq_kicks;
-	struct list_head pending_gpuq_kick_queues[KBASE_QUEUE_GROUP_PRIORITY_COUNT];
-	spinlock_t pending_gpuq_kick_queues_lock;
+	struct list_head pending_gpuq_kicks[KBASE_QUEUE_GROUP_PRIORITY_COUNT];
+	spinlock_t pending_gpuq_kicks_lock;
 	u32 *quirks_ext;
-	struct rw_semaphore mmu_sync_sem;
-	struct rw_semaphore pmode_sync_sem;
-	u32 page_fault_cnt_ptr_address;
-	u32 *page_fault_cnt_ptr;
-	u32 page_fault_cnt;
-	bool mcu_halted;
-	struct kbase_csf_fw_io fw_io;
-	u64 compute_progress_timeout_cc;
 };
 
 /**
@@ -1797,7 +1699,7 @@ struct kbase_csf_device {
  * @current_setup:     Stores the MMU configuration for this address space.
  */
 struct kbase_as {
-	unsigned int number;
+	int number;
 	struct workqueue_struct *pf_wq;
 	struct work_struct work_pagefault;
 	struct work_struct work_busfault;
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.c
index 2d8f96641181..f2362d58cfa8 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -40,7 +40,6 @@
 #include "backend/gpu/mali_kbase_clk_rate_trace_mgr.h"
 #include <csf/ipa_control/mali_kbase_csf_ipa_control.h>
 #include <csf/mali_kbase_csf_registers.h>
-#include <csf/mali_kbase_csf_fw_io.h>
 #include <linux/list.h>
 #include <linux/slab.h>
 #include <linux/firmware.h>
@@ -56,12 +55,9 @@
 #include <linux/delay.h>
 #include <linux/version_compat_defs.h>
 
-#include <mali_kbase_config_defaults.h>
-#define MALI_MAX_DEFAULT_FIRMWARE_NAME_LEN ((size_t)64)
+#define MALI_MAX_DEFAULT_FIRMWARE_NAME_LEN ((size_t)20)
 
-#define DEFAULT_FW_NAME MALI_RELEASE_NAME".mali_csffw.bin"
-
-static char default_fw_name[MALI_MAX_DEFAULT_FIRMWARE_NAME_LEN] = DEFAULT_FW_NAME;
+static char default_fw_name[MALI_MAX_DEFAULT_FIRMWARE_NAME_LEN] = "mali_csffw.bin";
 module_param_string(fw_name, default_fw_name, sizeof(default_fw_name), 0644);
 MODULE_PARM_DESC(fw_name, "firmware image");
 
@@ -70,8 +66,6 @@ static unsigned int csf_firmware_boot_timeout_ms;
 module_param(csf_firmware_boot_timeout_ms, uint, 0444);
 MODULE_PARM_DESC(csf_firmware_boot_timeout_ms, "Maximum time to wait for firmware to boot.");
 
-static bool kbase_iter_trace_enable;
-
 #ifdef CONFIG_MALI_BIFROST_DEBUG
 /* Makes Driver wait indefinitely for an acknowledgment for the different
  * requests it sends to firmware. Otherwise the timeouts interfere with the
@@ -100,7 +94,6 @@ MODULE_PARM_DESC(fw_debug, "Enables effective use of a debugger for debugging fi
 #define CSF_FIRMWARE_ENTRY_TYPE_TIMELINE_METADATA (4)
 #define CSF_FIRMWARE_ENTRY_TYPE_BUILD_INFO_METADATA (6)
 #define CSF_FIRMWARE_ENTRY_TYPE_FUNC_CALL_LIST (7)
-#define CSF_FIRMWARE_ENTRY_TYPE_PAGE_FAULT_CNT (8)
 #define CSF_FIRMWARE_ENTRY_TYPE_CORE_DUMP (9)
 
 #define CSF_FIRMWARE_CACHE_MODE_NONE (0ul << 3)
@@ -117,10 +110,11 @@ MODULE_PARM_DESC(fw_debug, "Enables effective use of a debugger for debugging fi
 #define BUILD_INFO_GIT_DIRTY_LEN (1U)
 #define BUILD_INFO_GIT_SHA_PATTERN "git_sha: "
 
+#define CSF_MAX_FW_STOP_LOOPS (100000)
+
 #define CSF_GLB_REQ_CFG_MASK                                           \
 	(GLB_REQ_CFG_ALLOC_EN_MASK | GLB_REQ_CFG_PROGRESS_TIMER_MASK | \
-	 GLB_REQ_CFG_PWROFF_TIMER_MASK | GLB_REQ_IDLE_ENABLE_MASK |    \
-	 GLB_REQ_CFG_EVICTION_TIMER_MASK | GLB_REQ_ITER_TRACE_ENABLE_MASK)
+	 GLB_REQ_CFG_PWROFF_TIMER_MASK | GLB_REQ_IDLE_ENABLE_MASK)
 
 static inline u32 input_page_read(const u32 *const input, const u32 offset)
 {
@@ -184,92 +178,6 @@ struct firmware_timeline_metadata {
 	size_t size;
 };
 
-static void reinit_page_fault_cnt_firmware_memory(struct kbase_device *kbdev)
-{
-	if (!kbdev->csf.page_fault_cnt_ptr)
-		return;
-
-	/* Store the GPU address of shared memory location, where the page fault counter
-	 * value will be written, inside the FW data memory.
-	 */
-	kbase_csf_update_firmware_memory(
-		kbdev, kbdev->csf.page_fault_cnt_ptr_address,
-		(u32)((kbdev->csf.firmware_trace_buffers.mcu_rw.va_reg->start_pfn << PAGE_SHIFT) +
-		      PAGE_SIZE - sizeof(u32)));
-
-	*kbdev->csf.page_fault_cnt_ptr = kbdev->csf.page_fault_cnt = 0;
-}
-
-static void init_page_fault_cnt_firmware_memory(struct kbase_device *kbdev)
-{
-	if (!kbdev->csf.page_fault_cnt_ptr_address)
-		return;
-
-	if (WARN_ON_ONCE(!kbdev->csf.firmware_trace_buffers.mcu_rw.va_reg))
-		return;
-
-	/* Save the CPU address of shared memory location where the page fault counter
-	 * value will be written.
-	 * The shared memory location comes from the last 4 bytes of the page that
-	 * is allocated to maintain the extract offset value for different trace
-	 * buffers. Only the first 4 bytes of every cacheline is used for the extract offset
-	 * value.
-	 */
-	kbdev->csf.page_fault_cnt_ptr =
-		(u32 *)((u8 *)kbdev->csf.firmware_trace_buffers.mcu_rw.cpu_addr + PAGE_SIZE -
-			sizeof(u32));
-	reinit_page_fault_cnt_firmware_memory(kbdev);
-}
-
-/**
- * set_iterator_trace_enable - Set the value for 'kbase_iter_trace_enable' global variable
- *                             according to the value of GLB_FEATURES.ITER_TRACE_SUPPORTED bit,
- *                             and the corresponding device tree entry.
- * @kbdev: Kernel base device pointer
- */
-static void set_iterator_trace_enable(struct kbase_device *kbdev)
-{
-	const struct kbase_csf_global_iface *iface = &kbdev->csf.global_iface;
-	bool dev_support_iter_trace = iface->features & GLB_FEATURES_ITER_TRACE_SUPPORTED_MASK;
-	const void *dt_iter_trace_param;
-	unsigned int val;
-
-	if (!dev_support_iter_trace) {
-		kbase_iter_trace_enable = false;
-		return;
-	}
-
-
-	/* check device tree for iterator trace enable property and
-	 * fallback to "iter_trace_enable" if not found and try again
-	 */
-	dt_iter_trace_param = of_get_property(kbdev->dev->of_node, "iter-trace-enable", NULL);
-
-	if (!dt_iter_trace_param)
-		dt_iter_trace_param =
-			of_get_property(kbdev->dev->of_node, "iter_trace_enable", NULL);
-
-	val = (dt_iter_trace_param) ? be32_to_cpup(dt_iter_trace_param) : 0;
-	dev_dbg(kbdev->dev, "Iterator trace enable device-tree config value: %u", val);
-
-	kbase_iter_trace_enable = val ? true : false;
-}
-
-static void iterator_trace_reinit(struct kbase_device *kbdev)
-{
-	if (kbase_iter_trace_enable) {
-		kbase_csf_firmware_global_input_mask(&kbdev->csf.global_iface, GLB_REQ,
-						     GLB_REQ_ITER_TRACE_ENABLE_MASK,
-						     GLB_REQ_ITER_TRACE_ENABLE_MASK);
-	}
-}
-
-static void iterator_trace_init(struct kbase_device *kbdev)
-{
-	set_iterator_trace_enable(kbdev);
-	iterator_trace_reinit(kbdev);
-}
-
 /* The shared interface area, used for communicating with firmware, is managed
  * like a virtual memory zone. Reserve the virtual space from that zone
  * corresponding to shared interface entry parsed from the firmware image.
@@ -300,6 +208,17 @@ static int setup_shared_iface_static_region(struct kbase_device *kbdev)
 	return ret;
 }
 
+static int wait_mcu_status_value(struct kbase_device *kbdev, u32 val)
+{
+	u32 max_loops = CSF_MAX_FW_STOP_LOOPS;
+
+	/* wait for the MCU_STATUS register to reach the given status value */
+	while (--max_loops && (kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(MCU_STATUS)) != val))
+		;
+
+	return (max_loops == 0) ? -1 : 0;
+}
+
 
 void kbase_csf_firmware_disable_mcu(struct kbase_device *kbdev)
 {
@@ -308,27 +227,27 @@ void kbase_csf_firmware_disable_mcu(struct kbase_device *kbdev)
 	kbase_reg_write32(kbdev, GPU_CONTROL_ENUM(MCU_CONTROL), MCU_CONTROL_REQ_DISABLE);
 }
 
-void kbase_csf_firmware_disable_mcu_wait(struct kbase_device *kbdev)
+static void wait_for_firmware_stop(struct kbase_device *kbdev)
 {
-	u32 val;
-	const u32 timeout_us =
-		kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_STOP_TIMEOUT) * USEC_PER_MSEC;
-	const int err = kbase_reg_poll32_timeout(kbdev, GPU_CONTROL_ENUM(MCU_STATUS), val,
-						 val == MCU_CONTROL_REQ_DISABLE, 0, timeout_us,
-						 false);
-
-	if (err)
-		dev_err(kbdev->dev, "Firmware failed to stop, error no: %d", err);
+	if (wait_mcu_status_value(kbdev, MCU_CONTROL_REQ_DISABLE) < 0) {
+		/* This error shall go away once MIDJM-2371 is closed */
+		dev_err(kbdev->dev, "Firmware failed to stop");
+	}
 
 	KBASE_TLSTREAM_TL_KBASE_CSFFW_FW_OFF(kbdev, kbase_backend_get_cycle_cnt(kbdev));
 }
 
-void kbase_csf_stop_firmware_and_wait(struct kbase_device *kbdev)
+void kbase_csf_firmware_disable_mcu_wait(struct kbase_device *kbdev)
+{
+	wait_for_firmware_stop(kbdev);
+}
+
+static void stop_csf_firmware(struct kbase_device *kbdev)
 {
 	/* Stop the MCU firmware */
 	kbase_csf_firmware_disable_mcu(kbdev);
 
-	kbase_csf_firmware_disable_mcu_wait(kbdev);
+	wait_for_firmware_stop(kbdev);
 }
 
 static void wait_for_firmware_boot(struct kbase_device *kbdev)
@@ -347,6 +266,7 @@ static void wait_for_firmware_boot(struct kbase_device *kbdev)
 	 */
 	remaining = wait_event_timeout(kbdev->csf.event_wait, kbdev->csf.interrupt_received == true,
 				       wait_timeout);
+
 	if (!remaining)
 		dev_err(kbdev->dev, "Timed out waiting for fw boot completion");
 
@@ -382,15 +302,22 @@ static void boot_csf_firmware(struct kbase_device *kbdev)
  */
 static int wait_ready(struct kbase_device *kbdev)
 {
-	u32 val;
-	const u32 timeout_us =
-		kbase_get_timeout_ms(kbdev, MMU_AS_INACTIVE_WAIT_TIMEOUT) * USEC_PER_MSEC;
-	const int err = kbase_reg_poll32_timeout(kbdev, MMU_AS_OFFSET(MCU_AS_NR, STATUS), val,
-						 !(val & AS_STATUS_AS_ACTIVE_EXT_MASK), 0,
-						 timeout_us, false);
+	const ktime_t wait_loop_start = ktime_get_raw();
+	const u32 mmu_as_inactive_wait_time_ms = kbdev->mmu_or_gpu_cache_op_wait_time_ms;
+	s64 diff;
 
-	if (!err)
-		return 0;
+	do {
+		unsigned int i;
+
+		for (i = 0; i < 1000; i++) {
+			/* Wait for the MMU status to indicate there is no active command */
+			if (!(kbase_reg_read32(kbdev, MMU_AS_OFFSET(MCU_AS_NR, STATUS)) &
+			      AS_STATUS_AS_ACTIVE_EXT_MASK))
+				return 0;
+		}
+
+		diff = ktime_to_ms(ktime_sub(ktime_get_raw(), wait_loop_start));
+	} while (diff < mmu_as_inactive_wait_time_ms);
 
 	dev_err(kbdev->dev,
 		"AS_ACTIVE bit stuck for MCU AS. Might be caused by unstable GPU clk/pwr or faulty system");
@@ -570,8 +497,6 @@ static int reload_fw_image(struct kbase_device *kbdev)
 	kbdev->csf.firmware_full_reload_needed = false;
 
 	kbase_csf_firmware_reload_trace_buffers_data(kbdev);
-	reinit_page_fault_cnt_firmware_memory(kbdev);
-	iterator_trace_reinit(kbdev);
 out:
 	return ret;
 }
@@ -595,11 +520,11 @@ static int reload_fw_image(struct kbase_device *kbdev)
  *        parsed FW interface entry using large page(s) from protected memory.
  *        If no appropriate entry is found it is set to NULL.
  * @num_pages: Number of pages requested.
- * @num_pages_aligned: This is an output parameter used to carry the number of small pages
+ * @num_pages_aligned: This is an output parameter used to carry the number of 4KB pages
  *                     within the 2MB pages aligned allocation.
  * @is_small_page: This is an output flag used to select between the small and large page
  *                 to be used for the FW entry allocation.
- * @force_small_page: Use small pages to allocate memory needed for FW loading
+ * @force_small_page: Use 4kB pages to allocate memory needed for FW loading
  *
  * Go through all the already initialized interfaces and find if a previously
  * allocated large page can be used to store contents of new FW interface entry.
@@ -634,16 +559,16 @@ static inline bool entry_find_large_page_to_reuse(struct kbase_device *kbdev,
 	 * then use 2MB page(s) for it.
 	 */
 	if (!(virtual_start & (SZ_2M - 1))) {
-		*num_pages_aligned = round_up(*num_pages_aligned, NUM_PAGES_IN_2MB_LARGE_PAGE);
+		*num_pages_aligned = round_up(*num_pages_aligned, NUM_4K_PAGES_IN_2MB_PAGE);
 		*is_small_page = false;
 		goto out;
 	}
 
 	/* If the section doesn't lie within the same 2MB aligned boundary,
-	 * then use small pages as it would be complicated to use a 2MB page
+	 * then use 4KB pages as it would be complicated to use a 2MB page
 	 * for such section.
 	 */
-	if ((virtual_start & ~(SZ_2M - 1UL)) != (virtual_end & ~(SZ_2M - 1UL)))
+	if ((virtual_start & ~(SZ_2M - 1)) != (virtual_end & ~(SZ_2M - 1)))
 		goto out;
 
 	/* Find the nearest 2MB aligned section which comes before the current
@@ -674,7 +599,7 @@ static inline bool entry_find_large_page_to_reuse(struct kbase_device *kbdev,
 			*phys = &target_interface->phys[page_index];
 
 		if (target_interface->pma)
-			*pma = &target_interface->pma[page_index / NUM_PAGES_IN_2MB_LARGE_PAGE];
+			*pma = &target_interface->pma[page_index / NUM_4K_PAGES_IN_2MB_PAGE];
 
 		*is_small_page = false;
 		reuse_large_page = true;
@@ -1130,14 +1055,6 @@ static int load_firmware_entry(struct kbase_device *kbdev, const struct kbase_cs
 		}
 		kbase_csf_firmware_log_parse_logging_call_list_entry(kbdev, entry);
 		return 0;
-	case CSF_FIRMWARE_ENTRY_TYPE_PAGE_FAULT_CNT:
-		/* Entry about the location of page fault counter */
-		if (size < sizeof(*entry)) {
-			dev_err(kbdev->dev, "Page fault counter entry too short (size=%u)", size);
-			return -EINVAL;
-		}
-		kbdev->csf.page_fault_cnt_ptr_address = *entry;
-		return 0;
 	case CSF_FIRMWARE_ENTRY_TYPE_CORE_DUMP:
 		/* Core Dump section */
 		if (size < CORE_DUMP_ENTRY_START_ADDR_OFFSET + sizeof(*entry)) {
@@ -1603,7 +1520,7 @@ static void handle_internal_firmware_fatal(struct kbase_device *const kbdev)
 		 * active address space and retain its refcount.
 		 */
 		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-		kctx = kbase_ctx_sched_as_to_ctx_nolock(kbdev, (size_t)as);
+		kctx = kbase_ctx_sched_as_to_ctx_nolock(kbdev, as);
 
 		if (kctx) {
 			kbase_ctx_sched_retain_ctx_refcount(kctx);
@@ -1647,6 +1564,7 @@ static bool global_request_complete(struct kbase_device *const kbdev, u32 const
 	unsigned long flags;
 
 	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+
 	if ((kbase_csf_firmware_global_output(global_iface, GLB_ACK) & req_mask) ==
 	    (kbase_csf_firmware_global_input_read(global_iface, GLB_REQ) & req_mask))
 		complete = true;
@@ -1679,8 +1597,7 @@ static int wait_for_global_request_with_timeout(struct kbase_device *const kbdev
 
 static int wait_for_global_request(struct kbase_device *const kbdev, u32 const req_mask)
 {
-	return wait_for_global_request_with_timeout(
-		kbdev, req_mask, kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT));
+	return wait_for_global_request_with_timeout(kbdev, req_mask, kbdev->csf.fw_timeout_ms);
 }
 
 static void set_global_request(const struct kbase_csf_global_iface *const global_iface,
@@ -1718,7 +1635,7 @@ static void set_shader_poweroff_timer(struct kbase_device *const kbdev,
 	kbase_csf_firmware_global_input(global_iface, GLB_PWROFF_TIMER, pwroff_reg);
 
 	kbase_csf_firmware_global_input_mask(global_iface, GLB_PWROFF_TIMER_CONFIG,
-					     kbdev->csf.mcu_core_pwroff_dur_count_no_modifier,
+					     kbdev->csf.mcu_core_pwroff_dur_count_modifier,
 					     GLB_PWROFF_TIMER_CONFIG_NO_MODIFIER_MASK);
 
 	set_global_request(global_iface, GLB_REQ_CFG_PWROFF_TIMER_MASK);
@@ -1738,27 +1655,9 @@ static void set_timeout_global(const struct kbase_csf_global_iface *const global
 	set_global_request(global_iface, GLB_REQ_CFG_PROGRESS_TIMER_MASK);
 }
 
-static inline void set_gpu_idle_timer_glb_req(struct kbase_device *const kbdev, bool set)
-{
-	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
-
-	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
-
-	if (set) {
-		kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, GLB_REQ_REQ_IDLE_ENABLE,
-						     GLB_REQ_IDLE_ENABLE_MASK);
-	} else {
-		kbase_csf_firmware_global_input_mask(
-			global_iface, GLB_REQ, GLB_REQ_REQ_IDLE_DISABLE, GLB_REQ_IDLE_DISABLE_MASK);
-	}
-
-	atomic_set(&kbdev->csf.scheduler.gpu_idle_timer_enabled, set);
-}
-
 static void enable_gpu_idle_timer(struct kbase_device *const kbdev)
 {
 	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
-	bool const fw_soi_allowed = kbase_pm_fw_sleep_on_idle_allowed(kbdev);
 
 	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
 
@@ -1766,114 +1665,15 @@ static void enable_gpu_idle_timer(struct kbase_device *const kbdev)
 					kbdev->csf.gpu_idle_dur_count);
 
 	kbase_csf_firmware_global_input_mask(global_iface, GLB_IDLE_TIMER_CONFIG,
-					     kbdev->csf.gpu_idle_dur_count_no_modifier
-						     << GLB_IDLE_TIMER_CONFIG_NO_MODIFIER_SHIFT,
+					     kbdev->csf.gpu_idle_dur_count_modifier,
 					     GLB_IDLE_TIMER_CONFIG_NO_MODIFIER_MASK);
-	kbase_csf_firmware_global_input_mask(global_iface, GLB_IDLE_TIMER_CONFIG,
-					     fw_soi_allowed
-						     << GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_SHIFT,
-					     GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_MASK);
 
-	set_gpu_idle_timer_glb_req(kbdev, true);
-	atomic_set(&kbdev->csf.scheduler.fw_soi_enabled, fw_soi_allowed);
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, GLB_REQ_REQ_IDLE_ENABLE,
+					     GLB_REQ_IDLE_ENABLE_MASK);
 	dev_dbg(kbdev->dev, "Enabling GPU idle timer with count-value: 0x%.8x",
 		kbdev->csf.gpu_idle_dur_count);
 }
 
-/**
- * convert_dur_to_suspend_count() - Convert CSG suspend timeout from ms to cycle count
- * @kbdev:        Instance of a GPU platform device that implements a CSF interface
- * @dur_ms:       Timeout value in ms
- * @no_modifier:  Indicate whether bit-shift is applied, 0 when applied, 1 otherwise
- *
- * Convert CSG suspend timeout from ms to cycle count, then generate a register value
- * combining cycle count and timer source
- *
- * Return:  Register value which will be stored into register GLB_EVICTION_TIMER.
- */
-static u32 convert_dur_to_suspend_count(struct kbase_device *kbdev, const u64 dur_ms,
-					u32 *no_modifier)
-{
-	/* Get the cntfreq_el0 value, which drives the SYSTEM_TIMESTAMP */
-	u64 freq = kbase_arch_timer_get_cntfrq(kbdev);
-	u64 dur_val = dur_ms;
-	u32 cnt_val_u32, reg_val_u32;
-	const bool src_system_timestamp = freq > 0;
-	const u8 SUSPEND_VAL_UNIT_SHIFT = 10;
-
-	if (!src_system_timestamp) {
-		/* Get the cycle_counter source alternative */
-		spin_lock(&kbdev->pm.clk_rtm.lock);
-		if (kbdev->pm.clk_rtm.clks[0])
-			freq = kbdev->pm.clk_rtm.clks[0]->clock_val;
-		else
-			dev_err(kbdev->dev, "No GPU clock, unexpected intregration issue!");
-		spin_unlock(&kbdev->pm.clk_rtm.lock);
-
-		dev_info(kbdev->dev,
-			 "No timestamp frequency, use cycle counter for csg suspend timeout!");
-	}
-
-	/* Formula for dur_val = (dur/1e3) * freq_HZ) */
-	dur_val = dur_val * freq;
-	dur_val = div_u64(dur_val, MSEC_PER_SEC);
-	if (dur_val < S32_MAX) {
-		*no_modifier = 1;
-	} else {
-		dur_val = dur_val >> SUSPEND_VAL_UNIT_SHIFT;
-		*no_modifier = 0;
-	}
-
-	/* Interface limits the value field to S32_MAX */
-	cnt_val_u32 = (dur_val > S32_MAX) ? S32_MAX : (u32)dur_val;
-
-	reg_val_u32 = GLB_EVICTION_TIMER_TIMEOUT_SET(0, cnt_val_u32);
-	/* add the source flag */
-	reg_val_u32 = GLB_EVICTION_TIMER_TIMER_SOURCE_SET(
-		reg_val_u32,
-		(src_system_timestamp ? GLB_EVICTION_TIMER_TIMER_SOURCE_SYSTEM_TIMESTAMP :
-					      GLB_EVICTION_TIMER_TIMER_SOURCE_GPU_COUNTER));
-
-	return reg_val_u32;
-}
-
-/**
- * set_csg_suspend_timeout() - Update CSG suspend timeout setting on FW side
- *
- * @kbdev:        Instance of a GPU platform device that implements a CSF interface
- */
-static void set_csg_suspend_timeout(struct kbase_device *const kbdev)
-{
-	u32 dur_ms, dur_val;
-	u32 no_modifier = 0;
-	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
-
-	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
-
-	dur_ms = kbdev->csf.csg_suspend_timeout_ms;
-	if (unlikely(dur_ms < CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MIN +
-				      CSG_SUSPEND_TIMEOUT_HOST_ADDED_MS ||
-		     dur_ms > CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MAX +
-				      CSG_SUSPEND_TIMEOUT_HOST_ADDED_MS)) {
-		dev_err(kbdev->dev, "Unexpected CSG suspend timeout: %ums, default to: %ums",
-			dur_ms, CSG_SUSPEND_TIMEOUT_MS);
-		kbdev->csf.csg_suspend_timeout_ms = CSG_SUSPEND_TIMEOUT_MS;
-		dur_ms = CSG_SUSPEND_TIMEOUT_MS;
-	}
-	dur_ms = dur_ms - CSG_SUSPEND_TIMEOUT_HOST_ADDED_MS;
-
-	dur_val = convert_dur_to_suspend_count(kbdev, dur_ms, &no_modifier);
-
-	kbase_csf_firmware_global_input(global_iface, GLB_EVICTION_TIMER, dur_val);
-
-	kbase_csf_firmware_global_input_mask(global_iface, GLB_EVICTION_TIMER_CONFIG, no_modifier,
-					     GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_MASK);
-
-	set_global_request(global_iface, GLB_REQ_CFG_EVICTION_TIMER_MASK);
-
-	dev_dbg(kbdev->dev, "Updating CSG suspend timeout with count-value: 0x%.8x", dur_val);
-}
-
 static bool global_debug_request_complete(struct kbase_device *const kbdev, u32 const req_mask)
 {
 	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
@@ -1962,8 +1762,7 @@ static void global_init(struct kbase_device *const kbdev, u64 core_mask)
 		GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_MASK | GLB_ACK_IRQ_MASK_PROTM_ENTER_MASK |
 		GLB_ACK_IRQ_MASK_PROTM_EXIT_MASK | GLB_ACK_IRQ_MASK_FIRMWARE_CONFIG_UPDATE_MASK |
 		GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_MASK | GLB_ACK_IRQ_MASK_IDLE_EVENT_MASK |
-		GLB_REQ_DEBUG_CSF_REQ_MASK | GLB_ACK_IRQ_MASK_IDLE_ENABLE_MASK |
-		GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_MASK | GLB_ACK_IRQ_MASK_ITER_TRACE_ENABLE_MASK;
+		GLB_REQ_DEBUG_CSF_REQ_MASK | GLB_ACK_IRQ_MASK_IDLE_ENABLE_MASK;
 
 	const struct kbase_csf_global_iface *const global_iface = &kbdev->csf.global_iface;
 	unsigned long flags;
@@ -1978,10 +1777,11 @@ static void global_init(struct kbase_device *const kbdev, u64 core_mask)
 
 	set_timeout_global(global_iface, kbase_csf_timeout_get(kbdev));
 
-	/* The csg suspend timeout is always enabled so customer has the flexibility to update it
-	 * at any time.
+	/* The GPU idle timer is always enabled for simplicity. Checks will be
+	 * done before scheduling the GPU idle worker to see if it is
+	 * appropriate for the current power policy.
 	 */
-	set_csg_suspend_timeout(kbdev);
+	enable_gpu_idle_timer(kbdev);
 
 	/* Unmask the interrupts */
 	kbase_csf_firmware_global_input(global_iface, GLB_ACK_IRQ_MASK, ack_irq_mask);
@@ -2101,7 +1901,6 @@ static void kbase_csf_firmware_reload_worker(struct work_struct *work)
 {
 	struct kbase_device *kbdev =
 		container_of(work, struct kbase_device, csf.firmware_reload_work);
-	unsigned long flags;
 	int err;
 
 	dev_info(kbdev->dev, "reloading firmware");
@@ -2120,9 +1919,7 @@ static void kbase_csf_firmware_reload_worker(struct work_struct *work)
 		return;
 
 	/* Reboot the firmware */
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	kbase_csf_firmware_enable_mcu(kbdev);
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 }
 
 void kbase_csf_firmware_trigger_reload(struct kbase_device *kbdev)
@@ -2159,17 +1956,16 @@ void kbase_csf_firmware_reload_completed(struct kbase_device *kbdev)
 
 	KBASE_KTRACE_ADD(kbdev, CSF_FIRMWARE_REBOOT, NULL, 0u);
 
-
 	/* Tell MCU state machine to transit to next state */
 	kbdev->csf.firmware_reloaded = true;
 	kbase_pm_update_state(kbdev);
 }
 
-static u32 convert_dur_to_idle_count(struct kbase_device *kbdev, const u64 dur_ns, u32 *no_modifier)
+static u32 convert_dur_to_idle_count(struct kbase_device *kbdev, const u32 dur_ns, u32 *modifier)
 {
 #define HYSTERESIS_VAL_UNIT_SHIFT (10)
 	/* Get the cntfreq_el0 value, which drives the SYSTEM_TIMESTAMP */
-	u64 freq = kbase_arch_timer_get_cntfrq(kbdev);
+	u64 freq = arch_timer_get_cntfrq();
 	u64 dur_val = dur_ns;
 	u32 cnt_val_u32, reg_val_u32;
 	bool src_system_timestamp = freq > 0;
@@ -2192,10 +1988,10 @@ static u32 convert_dur_to_idle_count(struct kbase_device *kbdev, const u64 dur_n
 	dur_val = dur_val * freq;
 	dur_val = div_u64(dur_val, NSEC_PER_SEC);
 	if (dur_val < S32_MAX) {
-		*no_modifier = 1;
+		*modifier = 1;
 	} else {
 		dur_val = dur_val >> HYSTERESIS_VAL_UNIT_SHIFT;
-		*no_modifier = 0;
+		*modifier = 0;
 	}
 
 	/* Interface limits the value field to S32_MAX */
@@ -2210,24 +2006,24 @@ static u32 convert_dur_to_idle_count(struct kbase_device *kbdev, const u64 dur_n
 	return reg_val_u32;
 }
 
-u64 kbase_csf_firmware_get_gpu_idle_hysteresis_time(struct kbase_device *kbdev)
+u32 kbase_csf_firmware_get_gpu_idle_hysteresis_time(struct kbase_device *kbdev)
 {
 	unsigned long flags;
-	u64 dur_ns;
+	u32 dur;
 
 	kbase_csf_scheduler_spin_lock(kbdev, &flags);
-	dur_ns = kbdev->csf.gpu_idle_hysteresis_ns;
+	dur = kbdev->csf.gpu_idle_hysteresis_ns;
 	kbase_csf_scheduler_spin_unlock(kbdev, flags);
 
-	return dur_ns;
+	return dur;
 }
 
-u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev, u64 dur_ns)
+u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev, u32 dur_ns)
 {
 	unsigned long flags;
-	u32 no_modifier = 0;
+	u32 modifier = 0;
 
-	const u32 hysteresis_val = convert_dur_to_idle_count(kbdev, dur_ns, &no_modifier);
+	const u32 hysteresis_val = convert_dur_to_idle_count(kbdev, dur_ns, &modifier);
 
 	/* The 'fw_load_lock' is taken to synchronize against the deferred
 	 * loading of FW, where the idle timer will be enabled.
@@ -2237,7 +2033,7 @@ u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev,
 		kbase_csf_scheduler_spin_lock(kbdev, &flags);
 		kbdev->csf.gpu_idle_hysteresis_ns = dur_ns;
 		kbdev->csf.gpu_idle_dur_count = hysteresis_val;
-		kbdev->csf.gpu_idle_dur_count_no_modifier = no_modifier;
+		kbdev->csf.gpu_idle_dur_count_modifier = modifier;
 		kbase_csf_scheduler_spin_unlock(kbdev, flags);
 		mutex_unlock(&kbdev->fw_load_lock);
 		goto end;
@@ -2260,44 +2056,29 @@ u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev,
 		return kbdev->csf.gpu_idle_dur_count;
 	}
 
-	/* The scheduler lock is also taken and is held till the update is not
+	/* The 'reg_lock' is also taken and is held till the update is not
 	 * complete, to ensure the update of idle timer value by multiple Users
 	 * gets serialized.
 	 */
-	kbase_csf_scheduler_lock(kbdev);
-	while (atomic_read(&kbdev->csf.scheduler.pending_gpu_idle_work) > 0) {
-		kbase_csf_scheduler_unlock(kbdev);
-		kbase_csf_scheduler_wait_for_kthread_pending_work(
-			kbdev, &kbdev->csf.scheduler.pending_gpu_idle_work);
-		kbase_csf_scheduler_lock(kbdev);
-	}
+	mutex_lock(&kbdev->csf.reg_lock);
+	/* The firmware only reads the new idle timer value when the timer is
+	 * disabled.
+	 */
 	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	kbase_csf_firmware_disable_gpu_idle_timer(kbdev);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+	/* Ensure that the request has taken effect */
+	wait_for_global_request(kbdev, GLB_REQ_IDLE_DISABLE_MASK);
 
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
 	kbdev->csf.gpu_idle_hysteresis_ns = dur_ns;
 	kbdev->csf.gpu_idle_dur_count = hysteresis_val;
-	kbdev->csf.gpu_idle_dur_count_no_modifier = no_modifier;
-
-	if (atomic_read(&kbdev->csf.scheduler.gpu_idle_timer_enabled)) {
-		/* Timer is already enabled. Disable the timer as FW only reads
-		 * the new idle timer value when timer is re-enabled.
-		 */
-		kbase_csf_firmware_disable_gpu_idle_timer(kbdev);
-		kbase_csf_scheduler_spin_unlock(kbdev, flags);
-		/* Ensure that the request has taken effect */
-		if (wait_for_global_request(kbdev, GLB_REQ_IDLE_DISABLE_MASK))
-			dev_err(kbdev->dev,
-				"Failed to disable GLB_IDLE timer when setting a new idle hysteresis timeout");
-		kbase_csf_scheduler_spin_lock(kbdev, &flags);
-		kbase_csf_firmware_enable_gpu_idle_timer(kbdev);
-		kbase_csf_scheduler_spin_unlock(kbdev, flags);
-		if (wait_for_global_request(kbdev, GLB_REQ_IDLE_ENABLE_MASK))
-			dev_err(kbdev->dev,
-				"Failed to re-enable GLB_IDLE timer when setting a new idle hysteresis timeout");
-	} else {
-		kbase_csf_scheduler_spin_unlock(kbdev, flags);
-	}
+	kbdev->csf.gpu_idle_dur_count_modifier = modifier;
+	kbase_csf_firmware_enable_gpu_idle_timer(kbdev);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+	wait_for_global_request(kbdev, GLB_REQ_IDLE_ENABLE_MASK);
+	mutex_unlock(&kbdev->csf.reg_lock);
 
-	kbase_csf_scheduler_unlock(kbdev);
 	kbase_csf_scheduler_pm_idle(kbdev);
 	kbase_reset_gpu_allow(kbdev);
 end:
@@ -2305,13 +2086,12 @@ u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev,
 
 	return hysteresis_val;
 }
-KBASE_EXPORT_TEST_API(kbase_csf_firmware_set_gpu_idle_hysteresis_time);
 
-static u32 convert_dur_to_core_pwroff_count(struct kbase_device *kbdev, const u64 dur_ns,
-					    u32 *no_modifier)
+static u32 convert_dur_to_core_pwroff_count(struct kbase_device *kbdev, const u32 dur_ns,
+					    u32 *modifier)
 {
 	/* Get the cntfreq_el0 value, which drives the SYSTEM_TIMESTAMP */
-	u64 freq = kbase_arch_timer_get_cntfrq(kbdev);
+	u64 freq = arch_timer_get_cntfrq();
 	u64 dur_val = dur_ns;
 	u32 cnt_val_u32, reg_val_u32;
 	bool src_system_timestamp = freq > 0;
@@ -2337,10 +2117,10 @@ static u32 convert_dur_to_core_pwroff_count(struct kbase_device *kbdev, const u6
 	dur_val = dur_val * freq;
 	dur_val = div_u64(dur_val, NSEC_PER_SEC);
 	if (dur_val < S32_MAX) {
-		*no_modifier = 1;
+		*modifier = 1;
 	} else {
 		dur_val = dur_val >> HYSTERESIS_VAL_UNIT_SHIFT;
-		*no_modifier = 0;
+		*modifier = 0;
 	}
 
 	if (dur_val == 0 && !always_on) {
@@ -2363,29 +2143,29 @@ static u32 convert_dur_to_core_pwroff_count(struct kbase_device *kbdev, const u6
 	return reg_val_u32;
 }
 
-u64 kbase_csf_firmware_get_mcu_core_pwroff_time(struct kbase_device *kbdev)
+u32 kbase_csf_firmware_get_mcu_core_pwroff_time(struct kbase_device *kbdev)
 {
-	u64 pwroff_ns;
+	u32 pwroff;
 	unsigned long flags;
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	pwroff_ns = kbdev->csf.mcu_core_pwroff_dur_ns;
+	pwroff = kbdev->csf.mcu_core_pwroff_dur_ns;
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
-	return pwroff_ns;
+	return pwroff;
 }
 
-u32 kbase_csf_firmware_set_mcu_core_pwroff_time(struct kbase_device *kbdev, u64 dur_ns)
+u32 kbase_csf_firmware_set_mcu_core_pwroff_time(struct kbase_device *kbdev, u32 dur_ns)
 {
 	unsigned long flags;
-	u32 no_modifier = 0;
+	u32 modifier = 0;
 
-	const u32 pwroff = convert_dur_to_core_pwroff_count(kbdev, dur_ns, &no_modifier);
+	const u32 pwroff = convert_dur_to_core_pwroff_count(kbdev, dur_ns, &modifier);
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	kbdev->csf.mcu_core_pwroff_dur_ns = dur_ns;
 	kbdev->csf.mcu_core_pwroff_dur_count = pwroff;
-	kbdev->csf.mcu_core_pwroff_dur_count_no_modifier = no_modifier;
+	kbdev->csf.mcu_core_pwroff_dur_count_modifier = modifier;
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 	dev_dbg(kbdev->dev, "MCU shader Core Poweroff input update: 0x%.8x", pwroff);
@@ -2398,10 +2178,71 @@ u32 kbase_csf_firmware_reset_mcu_core_pwroff_time(struct kbase_device *kbdev)
 	return kbase_csf_firmware_set_mcu_core_pwroff_time(kbdev, DEFAULT_GLB_PWROFF_TIMEOUT_NS);
 }
 
+/**
+ * kbase_device_csf_iterator_trace_init - Send request to enable iterator
+ *                                        trace port.
+ * @kbdev: Kernel base device pointer
+ *
+ * Return: 0 on success (or if enable request is not sent), or error
+ *         code -EINVAL on failure of GPU to acknowledge enable request.
+ */
+static int kbase_device_csf_iterator_trace_init(struct kbase_device *kbdev)
+{
+	/* Enable the iterator trace port if supported by the GPU.
+	 * It requires the GPU to have a nonzero "iter-trace-enable"
+	 * property in the device tree, and the FW must advertise
+	 * this feature in GLB_FEATURES.
+	 */
+	if (kbdev->pm.backend.gpu_powered) {
+		/* check device tree for iterator trace enable property
+		 * and fallback to "iter_trace_enable" if it is not found
+		 */
+		const void *iter_trace_param =
+			of_get_property(kbdev->dev->of_node, "iter-trace-enable", NULL);
+
+		const struct kbase_csf_global_iface *iface = &kbdev->csf.global_iface;
+
+		if (!iter_trace_param)
+			iter_trace_param =
+				of_get_property(kbdev->dev->of_node, "iter_trace_enable", NULL);
+
+		if (iter_trace_param) {
+			u32 iter_trace_value = be32_to_cpup(iter_trace_param);
+
+			if ((iface->features & GLB_FEATURES_ITER_TRACE_SUPPORTED_MASK) &&
+			    iter_trace_value) {
+				long ack_timeout;
+
+				ack_timeout = kbase_csf_timeout_in_jiffies(
+					kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT));
+
+				/* write enable request to global input */
+				kbase_csf_firmware_global_input_mask(
+					iface, GLB_REQ, GLB_REQ_ITER_TRACE_ENABLE_MASK,
+					GLB_REQ_ITER_TRACE_ENABLE_MASK);
+				/* Ring global doorbell */
+				kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+
+				ack_timeout = wait_event_timeout(
+					kbdev->csf.event_wait,
+					!((kbase_csf_firmware_global_input_read(iface, GLB_REQ) ^
+					   kbase_csf_firmware_global_output(iface, GLB_ACK)) &
+					  GLB_REQ_ITER_TRACE_ENABLE_MASK),
+					ack_timeout);
+
+				return ack_timeout ? 0 : -EINVAL;
+			}
+		}
+	}
+	return 0;
+}
+
 int kbase_csf_firmware_early_init(struct kbase_device *kbdev)
 {
 	init_waitqueue_head(&kbdev->csf.event_wait);
 
+	kbdev->csf.fw_timeout_ms = kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT);
+
 	kbase_csf_firmware_reset_mcu_core_pwroff_time(kbdev);
 	INIT_LIST_HEAD(&kbdev->csf.firmware_interfaces);
 	INIT_LIST_HEAD(&kbdev->csf.firmware_config);
@@ -2411,9 +2252,10 @@ int kbase_csf_firmware_early_init(struct kbase_device *kbdev)
 	INIT_WORK(&kbdev->csf.firmware_reload_work, kbase_csf_firmware_reload_worker);
 	INIT_WORK(&kbdev->csf.fw_error_work, firmware_error_worker);
 
-	init_rwsem(&kbdev->csf.mmu_sync_sem);
+	kbdev->csf.glb_init_request_pending = true;
+
 	mutex_init(&kbdev->csf.reg_lock);
-	kbase_csf_pending_gpuq_kick_queues_init(kbdev);
+	kbase_csf_pending_gpuq_kicks_init(kbdev);
 
 	kbdev->csf.fw = (struct kbase_csf_mcu_fw){ .data = NULL };
 
@@ -2422,13 +2264,13 @@ int kbase_csf_firmware_early_init(struct kbase_device *kbdev)
 
 void kbase_csf_firmware_early_term(struct kbase_device *kbdev)
 {
-	kbase_csf_pending_gpuq_kick_queues_term(kbdev);
+	kbase_csf_pending_gpuq_kicks_term(kbdev);
 	mutex_destroy(&kbdev->csf.reg_lock);
 }
 
 int kbase_csf_firmware_late_init(struct kbase_device *kbdev)
 {
-	u32 no_modifier = 0;
+	u32 modifier = 0;
 
 	kbdev->csf.gpu_idle_hysteresis_ns = FIRMWARE_IDLE_HYSTERESIS_TIME_NS;
 
@@ -2438,36 +2280,15 @@ int kbase_csf_firmware_late_init(struct kbase_device *kbdev)
 #endif
 	WARN_ON(!kbdev->csf.gpu_idle_hysteresis_ns);
 	kbdev->csf.gpu_idle_dur_count =
-		convert_dur_to_idle_count(kbdev, kbdev->csf.gpu_idle_hysteresis_ns, &no_modifier);
-	kbdev->csf.gpu_idle_dur_count_no_modifier = no_modifier;
-
-	kbdev->csf.csg_suspend_timeout_ms = CSG_SUSPEND_TIMEOUT_MS;
+		convert_dur_to_idle_count(kbdev, kbdev->csf.gpu_idle_hysteresis_ns, &modifier);
+	kbdev->csf.gpu_idle_dur_count_modifier = modifier;
 
 	return 0;
 }
 
-#ifdef CONFIG_MALI_CSF_INCLUDE_FW
-asm (
-"	.pushsection .rodata, \"a\"		\n"
-"	.ascii \"CSFFW_ST\"			\n"
-"	.global mali_csffw			\n"
-"mali_csffw:					\n"
-"	.incbin \"drivers/gpu/arm/bifrost/mali_csffw.bin\"	\n"
-"	.global mali_csffw_end			\n"
-"mali_csffw_end:				\n"
-"	.ascii \"CSFFW_ED\"			\n"
-"	.popsection				\n"
-);
-
-extern char mali_csffw;
-extern char mali_csffw_end;
-#endif
-
 int kbase_csf_firmware_load_init(struct kbase_device *kbdev)
 {
-#ifndef CONFIG_MALI_CSF_INCLUDE_FW
 	const struct firmware *firmware = NULL;
-#endif
 	struct kbase_csf_mcu_fw *const mcu_fw = &kbdev->csf.fw;
 	const u32 magic = FIRMWARE_HEADER_MAGIC;
 	u8 version_major, version_minor;
@@ -2475,9 +2296,7 @@ int kbase_csf_firmware_load_init(struct kbase_device *kbdev)
 	u32 entry_end_offset;
 	u32 entry_offset;
 	int ret;
-#ifndef CONFIG_MALI_CSF_INCLUDE_FW
 	const char *fw_name = default_fw_name;
-#endif
 
 	lockdep_assert_held(&kbdev->fw_load_lock);
 
@@ -2500,14 +2319,6 @@ int kbase_csf_firmware_load_init(struct kbase_device *kbdev)
 		goto err_out;
 	}
 
-#ifdef CONFIG_MALI_CSF_INCLUDE_FW
-	mcu_fw->size = &mali_csffw_end - &mali_csffw;
-
-	dev_info(kbdev->dev, "use 'driver built-in firmware' directly\n");
-	mcu_fw->data = (u8 *)(&mali_csffw);
-	dev_dbg(kbdev->dev, "Firmware image (%zu-bytes) retained in csf.fw\n",
-			mcu_fw->size);
-#else
 #if IS_ENABLED(CONFIG_OF)
 	/* If we can't read CSF firmware name from DTB,
 	 * fw_name is not modified and remains the default.
@@ -2535,7 +2346,6 @@ int kbase_csf_firmware_load_init(struct kbase_device *kbdev)
 
 #endif /* IS_ENABLED(CONFIG_OF) */
 
-	dev_info(kbdev->dev, "to load firmware image '%s'\n", fw_name);
 	if (request_firmware(&firmware, fw_name, kbdev->dev) != 0) {
 		dev_err(kbdev->dev, "Failed to load firmware image '%s'\n", fw_name);
 		ret = -ENOENT;
@@ -2554,7 +2364,6 @@ int kbase_csf_firmware_load_init(struct kbase_device *kbdev)
 
 		release_firmware(firmware);
 	}
-#endif /* CONFIG_MALI_CSF_INCLUDE_FW */
 
 	/* If error in loading or saving the image, branches to error out */
 	if (ret)
@@ -2631,8 +2440,6 @@ int kbase_csf_firmware_load_init(struct kbase_device *kbdev)
 		goto err_out;
 	}
 
-	init_page_fault_cnt_firmware_memory(kbdev);
-
 	ret = kbase_csf_firmware_cfg_fw_wa_init(kbdev);
 	if (ret != 0) {
 		dev_err(kbdev->dev, "Failed to initialize firmware workarounds");
@@ -2653,8 +2460,6 @@ int kbase_csf_firmware_load_init(struct kbase_device *kbdev)
 	if (ret != 0)
 		goto err_out;
 
-	iterator_trace_init(kbdev);
-
 	ret = kbase_csf_doorbell_mapping_init(kbdev);
 	if (ret != 0)
 		goto err_out;
@@ -2685,6 +2490,10 @@ int kbase_csf_firmware_load_init(struct kbase_device *kbdev)
 	if (ret != 0)
 		goto err_out;
 
+	ret = kbase_device_csf_iterator_trace_init(kbdev);
+	if (ret != 0)
+		goto err_out;
+
 	if (kbdev->csf.fw_core_dump.available)
 		kbase_csf_firmware_core_dump_init(kbdev);
 
@@ -2734,7 +2543,7 @@ void kbase_csf_firmware_unload_term(struct kbase_device *kbdev)
 	kbdev->csf.firmware_inited = false;
 	if (WARN_ON(kbdev->pm.backend.mcu_state != KBASE_MCU_OFF)) {
 		kbdev->pm.backend.mcu_state = KBASE_MCU_OFF;
-		kbase_csf_stop_firmware_and_wait(kbdev);
+		stop_csf_firmware(kbdev);
 	}
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
@@ -2781,8 +2590,6 @@ void kbase_csf_firmware_unload_term(struct kbase_device *kbdev)
 		kfree(metadata);
 	}
 
-	if (IS_ENABLED(CONFIG_MALI_CSF_INCLUDE_FW))
-		kbdev->csf.fw.data = NULL;
 	if (kbdev->csf.fw.data) {
 		/* Free the copy of the firmware image */
 		vfree(kbdev->csf.fw.data);
@@ -2887,10 +2694,8 @@ int kbase_csf_firmware_mcu_register_read(struct kbase_device *const kbdev, u32 c
 int kbase_csf_firmware_mcu_register_poll(struct kbase_device *const kbdev, u32 const reg_addr,
 					 u32 const val_mask, u32 const reg_val)
 {
-	unsigned long remaining =
-		kbase_csf_timeout_in_jiffies(kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT)) +
-		jiffies;
-	u32 read_val = 0;
+	unsigned long remaining = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms) + jiffies;
+	u32 read_val;
 
 	dev_dbg(kbdev->dev, "p: reg %08x val %08x mask %08x", reg_addr, reg_val, val_mask);
 
@@ -2937,10 +2742,12 @@ void kbase_csf_firmware_enable_gpu_idle_timer(struct kbase_device *kbdev)
 
 void kbase_csf_firmware_disable_gpu_idle_timer(struct kbase_device *kbdev)
 {
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+
 	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
 
-	set_gpu_idle_timer_glb_req(kbdev, false);
-	atomic_set(&kbdev->csf.scheduler.fw_soi_enabled, false);
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, GLB_REQ_REQ_IDLE_DISABLE,
+					     GLB_REQ_IDLE_DISABLE_MASK);
 	dev_dbg(kbdev->dev, "Sending request to disable gpu idle timer");
 
 	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
@@ -2964,7 +2771,6 @@ int kbase_csf_firmware_ping_wait(struct kbase_device *const kbdev, unsigned int
 	return wait_for_global_request_with_timeout(kbdev, GLB_REQ_PING_MASK, wait_timeout_ms);
 }
 
-
 int kbase_csf_firmware_set_timeout(struct kbase_device *const kbdev, u64 const timeout)
 {
 	const struct kbase_csf_global_iface *const global_iface = &kbdev->csf.global_iface;
@@ -3003,6 +2809,8 @@ int kbase_csf_wait_protected_mode_enter(struct kbase_device *kbdev)
 {
 	int err;
 
+	lockdep_assert_held(&kbdev->mmu_hw_mutex);
+
 	err = wait_for_global_request(kbdev, GLB_REQ_PROTM_ENTER_MASK);
 
 	if (!err) {
@@ -3068,7 +2876,6 @@ void kbase_csf_firmware_enable_mcu(struct kbase_device *kbdev)
 {
 	struct kbase_csf_global_iface *iface = &kbdev->csf.global_iface;
 
-	lockdep_assert_held(&kbdev->hwaccess_lock);
 		/* Clear the HALT bit before triggering the boot of MCU firmware */
 		kbase_csf_firmware_global_input_mask(iface, GLB_REQ, 0, GLB_REQ_HALT_MASK);
 
@@ -3092,23 +2899,11 @@ void kbase_csf_firmware_trigger_mcu_sleep(struct kbase_device *kbdev)
 
 bool kbase_csf_firmware_is_mcu_in_sleep(struct kbase_device *kbdev)
 {
-	bool db_notif_disabled;
-
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
 
-	db_notif_disabled = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(MCU_CONTROL)) &
-			    MCU_CNTRL_DOORBELL_DISABLE_MASK;
-
-	if (!db_notif_disabled || !kbase_csf_firmware_mcu_halted(kbdev))
-		return false;
-
-	if (global_request_complete(kbdev, GLB_REQ_SLEEP_MASK))
-		return true;
-
-	kbase_pm_enable_mcu_db_notification(kbdev);
-	dev_dbg(kbdev->dev, "Enabled DB notification");
-	return false;
+	return (global_request_complete(kbdev, GLB_REQ_SLEEP_MASK) &&
+		kbase_csf_firmware_mcu_halted(kbdev));
 }
 #endif
 
@@ -3127,12 +2922,6 @@ int kbase_csf_trigger_firmware_config_update(struct kbase_device *kbdev)
 	unsigned long flags;
 	int err = 0;
 
-	err = kbase_reset_gpu_prevent_and_wait(kbdev);
-	if (err) {
-		dev_warn(kbdev->dev, "Unsuccessful GPU reset detected when updating FW config");
-		return err;
-	}
-
 	/* Ensure GPU is powered-up until we complete config update.*/
 	kbase_csf_scheduler_pm_active(kbdev);
 	err = kbase_csf_scheduler_killable_wait_mcu_active(kbdev);
@@ -3155,7 +2944,6 @@ int kbase_csf_trigger_firmware_config_update(struct kbase_device *kbdev)
 
 exit:
 	kbase_csf_scheduler_pm_idle(kbdev);
-	kbase_reset_gpu_allow(kbdev);
 	return err;
 }
 
@@ -3360,9 +3148,6 @@ void kbase_csf_firmware_mcu_shared_mapping_term(struct kbase_device *kbdev,
 	}
 
 	if (csf_mapping->phys) {
-		/* This is on module unload path, so the pages can be left uncleared before
-		 * returning them back to kbdev memory pool.
-		 */
 		kbase_mem_pool_free_pages(&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
 					  csf_mapping->num_pages, csf_mapping->phys, false, false);
 	}
@@ -3370,127 +3155,3 @@ void kbase_csf_firmware_mcu_shared_mapping_term(struct kbase_device *kbdev,
 	vunmap(csf_mapping->cpu_addr);
 	kfree(csf_mapping->phys);
 }
-
-#ifdef KBASE_PM_RUNTIME
-
-void kbase_csf_firmware_soi_update(struct kbase_device *kbdev)
-{
-	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
-	unsigned long flags;
-
-	/* There are 3 possibilities:
-	 * - Sleep-on-Idle allowed
-	 * - Sleep-on-Idle not allowed, GLB_IDLE timer disabled
-	 * - Sleep-on-Idle not allowed, GLB_IDLE timer enabled
-	 */
-	if (kbase_pm_fw_sleep_on_idle_allowed(kbdev)) {
-		if (likely(atomic_read(&kbdev->csf.scheduler.fw_soi_enabled)))
-			return;
-	} else {
-		if (test_bit(KBASE_GPU_NON_IDLE_OFF_SLOT_GROUPS_AVAILABLE,
-			     &kbdev->pm.backend.gpu_sleep_allowed)) {
-			if (likely(!atomic_read(&kbdev->csf.scheduler.gpu_idle_timer_enabled)))
-				return;
-		} else if (likely(atomic_read(&kbdev->csf.scheduler.gpu_idle_timer_enabled))) {
-			return;
-		}
-	}
-
-	if (kbase_reset_gpu_try_prevent(kbdev))
-		return;
-
-	kbase_csf_scheduler_lock(kbdev);
-
-	if (atomic_read(&scheduler->pending_gpu_idle_work) > 0)
-		goto out_unlock_scheduler_lock;
-
-	if ((scheduler->state == SCHED_SUSPENDED) || (scheduler->state == SCHED_SLEEPING))
-		goto out_unlock_scheduler_lock;
-
-	if (kbdev->pm.backend.mcu_state != KBASE_MCU_ON)
-		goto out_unlock_scheduler_lock;
-
-	/* Ensure that an existing DISABLE request is completed before
-	 * proceeding. They are made without waiting for them to complete such
-	 * as when enabling the MCU.
-	 */
-	if (wait_for_global_request(kbdev, GLB_REQ_IDLE_DISABLE_MASK)) {
-		dev_err(kbdev->dev,
-			"Existing GLB_IDLE timer config change failed to complete in time (gpu_sleep_allowed:%lx)",
-			kbdev->pm.backend.gpu_sleep_allowed);
-		goto out_unlock_scheduler_lock;
-	}
-
-	/* Disable the GLB IDLE timer if it's currently enabled */
-	if (atomic_read(&kbdev->csf.scheduler.gpu_idle_timer_enabled)) {
-		kbase_csf_scheduler_spin_lock(kbdev, &flags);
-		kbase_csf_firmware_disable_gpu_idle_timer(kbdev);
-		kbase_csf_scheduler_spin_unlock(kbdev, flags);
-		if (wait_for_global_request(kbdev, GLB_REQ_IDLE_DISABLE_MASK)) {
-			dev_err(kbdev->dev,
-				"Failed to disable GLB_IDLE timer following FW Sleep-on-Idle config change (gpu_sleep_allowed:%lx)",
-				kbdev->pm.backend.gpu_sleep_allowed);
-			goto out_unlock_scheduler_lock;
-		}
-	}
-
-	/* The GLB IDLE timer and, consequently, FW Sleep-on-Idle could remain
-	 * disabled in certain cases. Otherwise, we shall re-enable GLB IDLE
-	 * timer with the new FW Sleep-on-Idle configuration.
-	 */
-	if (!test_bit(KBASE_GPU_NON_IDLE_OFF_SLOT_GROUPS_AVAILABLE,
-		      &kbdev->pm.backend.gpu_sleep_allowed)) {
-		kbase_csf_scheduler_spin_lock(kbdev, &flags);
-		kbase_csf_firmware_enable_gpu_idle_timer(kbdev);
-		kbase_csf_scheduler_spin_unlock(kbdev, flags);
-		if (wait_for_global_request(kbdev, GLB_REQ_IDLE_ENABLE_MASK)) {
-			dev_err(kbdev->dev,
-				"Failed to re-enable GLB_IDLE timer following FW Sleep-on-Idle config change (gpu_sleep_allowed:%lx)",
-				kbdev->pm.backend.gpu_sleep_allowed);
-			goto out_unlock_scheduler_lock;
-		}
-	}
-
-	if (atomic_read(&scheduler->fw_soi_enabled)) {
-		dev_dbg(kbdev->dev, "FW Sleep-on-Idle was enabled");
-		KBASE_KTRACE_ADD(kbdev, FIRMWARE_SLEEP_ON_IDLE_CHANGED, NULL, true);
-	} else {
-		dev_dbg(kbdev->dev, "FW Sleep-on-Idle was disabled");
-		KBASE_KTRACE_ADD(kbdev, FIRMWARE_SLEEP_ON_IDLE_CHANGED, NULL, false);
-	}
-
-out_unlock_scheduler_lock:
-	kbase_csf_scheduler_unlock(kbdev);
-	kbase_reset_gpu_allow(kbdev);
-}
-
-int kbase_csf_firmware_soi_disable_on_scheduler_suspend(struct kbase_device *kbdev)
-{
-	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
-	unsigned long flags;
-
-	lockdep_assert_held(&scheduler->lock);
-
-	if (WARN_ON_ONCE(scheduler->state != SCHED_INACTIVE))
-		return 0;
-
-	if (!atomic_read(&kbdev->csf.scheduler.fw_soi_enabled))
-		return 0;
-
-	kbase_csf_scheduler_spin_lock(kbdev, &flags);
-	if (atomic_read(&scheduler->fw_soi_enabled)) {
-		kbase_csf_firmware_disable_gpu_idle_timer(kbdev);
-		kbase_csf_scheduler_spin_unlock(kbdev, flags);
-		if (wait_for_global_request(kbdev, GLB_REQ_IDLE_DISABLE_MASK)) {
-			dev_err(kbdev->dev, "Failed to disable Sleep-on-Idle config");
-			return -ETIMEDOUT;
-		}
-		KBASE_KTRACE_ADD(kbdev, FIRMWARE_SLEEP_ON_IDLE_CHANGED, NULL, false);
-	} else {
-		kbase_csf_scheduler_spin_unlock(kbdev, flags);
-	}
-
-	return 0;
-}
-
-#endif /* KBASE_PM_RUNTIME */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.h
index 20cb03991bbe..b6f07d001d24 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.h
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -24,7 +24,6 @@
 
 #include "device/mali_kbase_device.h"
 #include <csf/mali_kbase_csf_registers.h>
-#include <hw_access/mali_kbase_hw_access_regmap.h>
 #include <uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_regmap.h>
 
 /*
@@ -591,20 +590,13 @@ void kbase_csf_firmware_enable_mcu(struct kbase_device *kbdev);
 void kbase_csf_firmware_disable_mcu(struct kbase_device *kbdev);
 
 /**
- * kbase_csf_firmware_disable_mcu_wait - Wait for the MCU to reach disabled status.
+ * kbase_csf_firmware_disable_mcu_wait - Wait for the MCU to reach disabled
+ *                                       status.
  *
  * @kbdev: Instance of a GPU platform device that implements a CSF interface.
  */
 void kbase_csf_firmware_disable_mcu_wait(struct kbase_device *kbdev);
 
-/**
- * kbase_csf_stop_firmware_and_wait - Disable firmware and wait for the MCU to reach
- *                                    disabled status.
- *
- * @kbdev: Instance of a GPU platform device that implements a CSF interface.
- */
-void kbase_csf_stop_firmware_and_wait(struct kbase_device *kbdev);
-
 #ifdef KBASE_PM_RUNTIME
 /**
  * kbase_csf_firmware_trigger_mcu_sleep - Send the command to put MCU in sleep
@@ -625,7 +617,6 @@ void kbase_csf_firmware_trigger_mcu_sleep(struct kbase_device *kbdev);
 bool kbase_csf_firmware_is_mcu_in_sleep(struct kbase_device *kbdev);
 #endif
 
-
 /**
  * kbase_csf_firmware_trigger_reload() - Trigger the reboot of MCU firmware, for
  *                                       the cold boot case firmware image would
@@ -779,9 +770,9 @@ extern bool fw_debug;
 static inline long kbase_csf_timeout_in_jiffies(const unsigned int msecs)
 {
 #ifdef CONFIG_MALI_BIFROST_DEBUG
-	return (fw_debug ? MAX_SCHEDULE_TIMEOUT : (long)msecs_to_jiffies(msecs));
+	return (fw_debug ? MAX_SCHEDULE_TIMEOUT : msecs_to_jiffies(msecs));
 #else
-	return (long)msecs_to_jiffies(msecs);
+	return msecs_to_jiffies(msecs);
 #endif
 }
 
@@ -816,15 +807,15 @@ void kbase_csf_firmware_disable_gpu_idle_timer(struct kbase_device *kbdev);
  *
  * Return: the internally recorded hysteresis (nominal) value.
  */
-u64 kbase_csf_firmware_get_gpu_idle_hysteresis_time(struct kbase_device *kbdev);
+u32 kbase_csf_firmware_get_gpu_idle_hysteresis_time(struct kbase_device *kbdev);
 
 /**
  * kbase_csf_firmware_set_gpu_idle_hysteresis_time - Set the firmware GPU idle
  *                                               detection hysteresis duration
  *
- * @kbdev:  Instance of a GPU platform device that implements a CSF interface.
- * @dur_ns: The duration value (unit: nanoseconds) for the configuring
- *          hysteresis field for GPU idle detection
+ * @kbdev: Instance of a GPU platform device that implements a CSF interface.
+ * @dur:     The duration value (unit: milliseconds) for the configuring
+ *           hysteresis field for GPU idle detection
  *
  * The supplied value will be recorded internally without any change. But the
  * actual field value will be subject to hysteresis source frequency scaling
@@ -836,7 +827,7 @@ u64 kbase_csf_firmware_get_gpu_idle_hysteresis_time(struct kbase_device *kbdev);
  *
  * Return: the actual internally configured hysteresis field value.
  */
-u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev, u64 dur_ns);
+u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev, u32 dur);
 
 /**
  * kbase_csf_firmware_get_mcu_core_pwroff_time - Get the MCU shader Core power-off
@@ -847,14 +838,14 @@ u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev,
  * Return: the internally recorded MCU shader Core power-off (nominal) timeout value. The unit
  *         of the value is in micro-seconds.
  */
-u64 kbase_csf_firmware_get_mcu_core_pwroff_time(struct kbase_device *kbdev);
+u32 kbase_csf_firmware_get_mcu_core_pwroff_time(struct kbase_device *kbdev);
 
 /**
  * kbase_csf_firmware_set_mcu_core_pwroff_time - Set the MCU shader Core power-off
  *                                               time value
  *
  * @kbdev:   Instance of a GPU platform device that implements a CSF interface.
- * @dur_ns:  The duration value (unit: nanoseconds) for configuring MCU
+ * @dur:     The duration value (unit: micro-seconds) for configuring MCU
  *           core power-off timer, when the shader cores' power
  *           transitions are delegated to the MCU (normal operational
  *           mode)
@@ -873,7 +864,7 @@ u64 kbase_csf_firmware_get_mcu_core_pwroff_time(struct kbase_device *kbdev);
  * Return: the actual internal core power-off timer value in register defined
  *         format.
  */
-u32 kbase_csf_firmware_set_mcu_core_pwroff_time(struct kbase_device *kbdev, u64 dur_ns);
+u32 kbase_csf_firmware_set_mcu_core_pwroff_time(struct kbase_device *kbdev, u32 dur);
 
 /**
  * kbase_csf_firmware_reset_mcu_core_pwroff_time - Reset the MCU shader Core power-off
@@ -934,27 +925,4 @@ int kbase_csf_trigger_firmware_config_update(struct kbase_device *kbdev);
  */
 int kbase_csf_firmware_req_core_dump(struct kbase_device *const kbdev);
 
-#ifdef KBASE_PM_RUNTIME
-
-/**
- * kbase_csf_firmware_soi_update - Update FW Sleep-on-Idle config
- *
- * @kbdev: Device pointer
- *
- * This function reconfigures the FW Sleep-on-Idle configuration if necessary.
- */
-void kbase_csf_firmware_soi_update(struct kbase_device *kbdev);
-
-/**
- * kbase_csf_firmware_soi_disable_on_scheduler_suspend - Disable FW Sleep-on-Idle config
- *                                                       on scheduler suspension
- *
- * @kbdev: Device pointer
- *
- * Return: 0 on success, otherwise failure
- */
-int kbase_csf_firmware_soi_disable_on_scheduler_suspend(struct kbase_device *kbdev);
-
-#endif /* KBASE_PM_RUNTIME */
-
 #endif
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.c
index 030a1ebf0ac6..a8dc411e6884 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.c
@@ -145,7 +145,7 @@ static ssize_t store_fw_cfg(struct kobject *kobj, struct attribute *attr, const
 		cur_val = config->cur_val;
 		if (cur_val == val) {
 			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-			return (ssize_t)count;
+			return count;
 		}
 
 		/* If configuration update cannot be performed with
@@ -209,7 +209,7 @@ static ssize_t store_fw_cfg(struct kobject *kobj, struct attribute *attr, const
 		return -EINVAL;
 	}
 
-	return (ssize_t)count;
+	return count;
 }
 
 static const struct sysfs_ops fw_cfg_ops = {
@@ -367,23 +367,23 @@ int kbase_csf_firmware_cfg_fw_wa_init(struct kbase_device *kbdev)
 	 */
 	entry_count = of_property_count_u32_elems(kbdev->dev->of_node, "quirks-ext");
 
-	if (entry_count < 0)
+	if (entry_count == -EINVAL)
 		entry_count = of_property_count_u32_elems(kbdev->dev->of_node, "quirks_ext");
 
-	if (entry_count < 0)
+	if (entry_count == -EINVAL || entry_count == -ENODATA)
 		return 0;
 
-	entry_bytes = (size_t)entry_count * sizeof(u32);
+	entry_bytes = entry_count * sizeof(u32);
 	kbdev->csf.quirks_ext = kzalloc(entry_bytes, GFP_KERNEL);
 	if (!kbdev->csf.quirks_ext)
 		return -ENOMEM;
 
 	ret = of_property_read_u32_array(kbdev->dev->of_node, "quirks-ext", kbdev->csf.quirks_ext,
-					 (size_t)entry_count);
+					 entry_count);
 
 	if (ret == -EINVAL)
 		ret = of_property_read_u32_array(kbdev->dev->of_node, "quirks_ext",
-						 kbdev->csf.quirks_ext, (size_t)entry_count);
+						 kbdev->csf.quirks_ext, entry_count);
 
 	if (ret == -EINVAL || ret == -ENODATA) {
 		/* This is unexpected since the property is already accessed for counting the number
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_log.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_log.c
index 1af173c0e0b9..b57121649966 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_log.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_log.c
@@ -144,7 +144,7 @@ static ssize_t kbasep_csf_firmware_log_debugfs_read(struct file *file, char __us
 	}
 
 	*ppos += n_read;
-	ret = (int)n_read;
+	ret = n_read;
 
 out:
 	atomic_set(&fw_log->busy, 0);
@@ -178,9 +178,8 @@ static int kbase_csf_firmware_log_mode_write(void *data, u64 val)
 		break;
 	case KBASE_CSF_FIRMWARE_LOG_MODE_AUTO_PRINT:
 	case KBASE_CSF_FIRMWARE_LOG_MODE_AUTO_DISCARD:
-		schedule_delayed_work(
-			&fw_log->poll_work,
-			msecs_to_jiffies((unsigned int)atomic_read(&fw_log->poll_period_ms)));
+		schedule_delayed_work(&fw_log->poll_work,
+				      msecs_to_jiffies(atomic_read(&fw_log->poll_period_ms)));
 		break;
 	default:
 		ret = -EINVAL;
@@ -199,7 +198,7 @@ static int kbase_csf_firmware_log_poll_period_read(void *data, u64 *val)
 	struct kbase_device *kbdev = (struct kbase_device *)data;
 	struct kbase_csf_firmware_log *fw_log = &kbdev->csf.fw_log;
 
-	*val = (u64)atomic_read(&fw_log->poll_period_ms);
+	*val = atomic_read(&fw_log->poll_period_ms);
 	return 0;
 }
 
@@ -264,7 +263,7 @@ static void kbase_csf_firmware_log_poll(struct work_struct *work)
 		return;
 
 	schedule_delayed_work(&fw_log->poll_work,
-			      msecs_to_jiffies((unsigned int)atomic_read(&fw_log->poll_period_ms)));
+			      msecs_to_jiffies(atomic_read(&fw_log->poll_period_ms)));
 }
 
 int kbase_csf_firmware_log_init(struct kbase_device *kbdev)
@@ -383,7 +382,7 @@ void kbase_csf_firmware_log_dump_buffer(struct kbase_device *kbdev)
 		pend = p + read_size;
 		p = buf;
 
-		while (p < pend && (pnewline = memchr(p, '\n', (size_t)(pend - p)))) {
+		while (p < pend && (pnewline = memchr(p, '\n', pend - p))) {
 			/* Null-terminate the string */
 			*pnewline = 0;
 
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_no_mali.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_no_mali.c
index a206ed3da210..346a28ee6772 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_no_mali.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_no_mali.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -533,9 +533,9 @@ EXPORT_SYMBOL(kbase_csf_ring_doorbell);
  */
 static void handle_internal_firmware_fatal(struct kbase_device *const kbdev)
 {
-	size_t as;
+	int as;
 
-	for (as = 0; as < (size_t)kbdev->nr_hw_address_spaces; as++) {
+	for (as = 0; as < kbdev->nr_hw_address_spaces; as++) {
 		unsigned long flags;
 		struct kbase_context *kctx;
 		struct kbase_fault fault;
@@ -604,8 +604,7 @@ static bool global_request_complete(struct kbase_device *const kbdev, u32 const
 
 static int wait_for_global_request(struct kbase_device *const kbdev, u32 const req_mask)
 {
-	const long wait_timeout =
-		kbase_csf_timeout_in_jiffies(kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT));
+	const long wait_timeout = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms);
 	long remaining;
 	int err = 0;
 
@@ -670,23 +669,6 @@ static void set_timeout_global(const struct kbase_csf_global_iface *const global
 	set_global_request(global_iface, GLB_REQ_CFG_PROGRESS_TIMER_MASK);
 }
 
-static inline void set_gpu_idle_timer_glb_req(struct kbase_device *const kbdev, bool set)
-{
-	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
-
-	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
-
-	if (set) {
-		kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, GLB_REQ_REQ_IDLE_ENABLE,
-						     GLB_REQ_IDLE_ENABLE_MASK);
-	} else {
-		kbase_csf_firmware_global_input_mask(
-			global_iface, GLB_REQ, GLB_REQ_REQ_IDLE_DISABLE, GLB_REQ_IDLE_DISABLE_MASK);
-	}
-
-	atomic_set(&kbdev->csf.scheduler.gpu_idle_timer_enabled, set);
-}
-
 static void enable_gpu_idle_timer(struct kbase_device *const kbdev)
 {
 	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
@@ -695,11 +677,8 @@ static void enable_gpu_idle_timer(struct kbase_device *const kbdev)
 
 	kbase_csf_firmware_global_input(global_iface, GLB_IDLE_TIMER,
 					kbdev->csf.gpu_idle_dur_count);
-	kbase_csf_firmware_global_input_mask(global_iface, GLB_IDLE_TIMER_CONFIG,
-					     kbdev->csf.gpu_idle_dur_count_no_modifier,
-					     GLB_IDLE_TIMER_CONFIG_NO_MODIFIER_MASK);
-
-	set_gpu_idle_timer_glb_req(kbdev, true);
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, GLB_REQ_REQ_IDLE_ENABLE,
+					     GLB_REQ_IDLE_ENABLE_MASK);
 	dev_dbg(kbdev->dev, "Enabling GPU idle timer with count-value: 0x%.8x",
 		kbdev->csf.gpu_idle_dur_count);
 }
@@ -788,6 +767,12 @@ static void global_init(struct kbase_device *const kbdev, u64 core_mask)
 
 	set_timeout_global(global_iface, kbase_csf_timeout_get(kbdev));
 
+	/* The GPU idle timer is always enabled for simplicity. Checks will be
+	 * done before scheduling the GPU idle worker to see if it is
+	 * appropriate for the current power policy.
+	 */
+	enable_gpu_idle_timer(kbdev);
+
 	/* Unmask the interrupts */
 	kbase_csf_firmware_global_input(global_iface, GLB_ACK_IRQ_MASK, ack_irq_mask);
 
@@ -871,11 +856,11 @@ static void kbase_csf_firmware_reload_worker(struct work_struct *work)
 		container_of(work, struct kbase_device, csf.firmware_reload_work);
 	unsigned long flags;
 
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	/* Reboot the firmware */
 	kbase_csf_firmware_enable_mcu(kbdev);
 
 	/* Tell MCU state machine to transit to next state */
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	kbdev->csf.firmware_reloaded = true;
 	kbase_pm_update_state(kbdev);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
@@ -895,7 +880,6 @@ void kbase_csf_firmware_trigger_reload(struct kbase_device *kbdev)
 		kbdev->csf.firmware_reloaded = true;
 	}
 }
-KBASE_EXPORT_TEST_API(kbase_csf_firmware_trigger_reload);
 
 void kbase_csf_firmware_reload_completed(struct kbase_device *kbdev)
 {
@@ -904,19 +888,18 @@ void kbase_csf_firmware_reload_completed(struct kbase_device *kbdev)
 	if (unlikely(!kbdev->csf.firmware_inited))
 		return;
 
-
 	/* Tell MCU state machine to transit to next state */
 	kbdev->csf.firmware_reloaded = true;
 	kbase_pm_update_state(kbdev);
 }
 
-static u32 convert_dur_to_idle_count(struct kbase_device *kbdev, const u32 dur_ns, u32 *no_modifier)
+static u32 convert_dur_to_idle_count(struct kbase_device *kbdev, const u32 dur_ms, u32 *modifier)
 {
 #define HYSTERESIS_VAL_UNIT_SHIFT (10)
 	/* Get the cntfreq_el0 value, which drives the SYSTEM_TIMESTAMP */
-	u64 freq = kbase_arch_timer_get_cntfrq(kbdev);
-	u64 dur_val = dur_ns;
-	u32 cnt_val_u32, reg_val_u32, timer_src;
+	u64 freq = arch_timer_get_cntfrq();
+	u64 dur_val = dur_ms;
+	u32 cnt_val_u32, reg_val_u32;
 	bool src_system_timestamp = freq > 0;
 
 	if (!src_system_timestamp) {
@@ -933,46 +916,45 @@ static u32 convert_dur_to_idle_count(struct kbase_device *kbdev, const u32 dur_n
 			"Can't get the timestamp frequency, use cycle counter format with firmware idle hysteresis!");
 	}
 
-	/* Formula for dur_val = (dur/1e9) * freq_HZ) */
-	dur_val = dur_val * freq;
-	dur_val = div_u64(dur_val, NSEC_PER_SEC);
-	if (dur_val < S32_MAX) {
-		*no_modifier = 1;
-	} else {
-		dur_val = dur_val >> HYSTERESIS_VAL_UNIT_SHIFT;
-		*no_modifier = 0;
-	}
+	/* Formula for dur_val = ((dur_ms/1000) * freq_HZ) >> 10) */
+	dur_val = (dur_val * freq) >> HYSTERESIS_VAL_UNIT_SHIFT;
+	dur_val = div_u64(dur_val, 1000);
+
+	*modifier = 0;
 
 	/* Interface limits the value field to S32_MAX */
 	cnt_val_u32 = (dur_val > S32_MAX) ? S32_MAX : (u32)dur_val;
 
 	reg_val_u32 = GLB_IDLE_TIMER_TIMEOUT_SET(0, cnt_val_u32);
 	/* add the source flag */
-	timer_src = src_system_timestamp ? GLB_IDLE_TIMER_TIMER_SOURCE_SYSTEM_TIMESTAMP :
-						 GLB_IDLE_TIMER_TIMER_SOURCE_GPU_COUNTER;
-	reg_val_u32 = GLB_IDLE_TIMER_TIMER_SOURCE_SET(reg_val_u32, timer_src);
+	if (src_system_timestamp)
+		reg_val_u32 = GLB_IDLE_TIMER_TIMER_SOURCE_SET(
+			reg_val_u32, GLB_IDLE_TIMER_TIMER_SOURCE_SYSTEM_TIMESTAMP);
+	else
+		reg_val_u32 = GLB_IDLE_TIMER_TIMER_SOURCE_SET(
+			reg_val_u32, GLB_IDLE_TIMER_TIMER_SOURCE_GPU_COUNTER);
 
 	return reg_val_u32;
 }
 
-u64 kbase_csf_firmware_get_gpu_idle_hysteresis_time(struct kbase_device *kbdev)
+u32 kbase_csf_firmware_get_gpu_idle_hysteresis_time(struct kbase_device *kbdev)
 {
 	unsigned long flags;
-	u64 dur_ns;
+	u32 dur;
 
 	kbase_csf_scheduler_spin_lock(kbdev, &flags);
-	dur_ns = kbdev->csf.gpu_idle_hysteresis_ns;
+	dur = kbdev->csf.gpu_idle_hysteresis_ns;
 	kbase_csf_scheduler_spin_unlock(kbdev, flags);
 
-	return dur_ns;
+	return dur;
 }
 
-u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev, u64 dur_ns)
+u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev, u32 dur)
 {
 	unsigned long flags;
-	u32 no_modifier = 0;
+	u32 modifier = 0;
 
-	const u32 hysteresis_val = convert_dur_to_idle_count(kbdev, dur_ns, &no_modifier);
+	const u32 hysteresis_val = convert_dur_to_idle_count(kbdev, dur, &modifier);
 
 	/* The 'fw_load_lock' is taken to synchronize against the deferred
 	 * loading of FW, where the idle timer will be enabled.
@@ -980,9 +962,9 @@ u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev,
 	mutex_lock(&kbdev->fw_load_lock);
 	if (unlikely(!kbdev->csf.firmware_inited)) {
 		kbase_csf_scheduler_spin_lock(kbdev, &flags);
-		kbdev->csf.gpu_idle_hysteresis_ns = dur_ns;
+		kbdev->csf.gpu_idle_hysteresis_ns = dur;
 		kbdev->csf.gpu_idle_dur_count = hysteresis_val;
-		kbdev->csf.gpu_idle_dur_count_no_modifier = no_modifier;
+		kbdev->csf.gpu_idle_dur_count_modifier = modifier;
 		kbase_csf_scheduler_spin_unlock(kbdev, flags);
 		mutex_unlock(&kbdev->fw_load_lock);
 		goto end;
@@ -1005,33 +987,29 @@ u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev,
 		return kbdev->csf.gpu_idle_dur_count;
 	}
 
-	/* The scheduler lock is also taken and is held till the update is not
+	/* The 'reg_lock' is also taken and is held till the update is not
 	 * complete, to ensure the update of idle timer value by multiple Users
 	 * gets serialized.
 	 */
-	kbase_csf_scheduler_lock(kbdev);
+	mutex_lock(&kbdev->csf.reg_lock);
+	/* The firmware only reads the new idle timer value when the timer is
+	 * disabled.
+	 */
 	kbase_csf_scheduler_spin_lock(kbdev, &flags);
-	kbdev->csf.gpu_idle_hysteresis_ns = dur_ns;
-	kbdev->csf.gpu_idle_dur_count = hysteresis_val;
-	kbdev->csf.gpu_idle_dur_count_no_modifier = no_modifier;
+	kbase_csf_firmware_disable_gpu_idle_timer(kbdev);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+	/* Ensure that the request has taken effect */
+	wait_for_global_request(kbdev, GLB_REQ_IDLE_DISABLE_MASK);
 
-	if (atomic_read(&kbdev->csf.scheduler.gpu_idle_timer_enabled)) {
-		/* Timer is already enabled. Disable the timer as FW only reads
-		 * the new idle timer value when timer is re-enabled.
-		 */
-		kbase_csf_firmware_disable_gpu_idle_timer(kbdev);
-		kbase_csf_scheduler_spin_unlock(kbdev, flags);
-		/* Ensure that the request has taken effect */
-		wait_for_global_request(kbdev, GLB_REQ_IDLE_DISABLE_MASK);
-		kbase_csf_scheduler_spin_lock(kbdev, &flags);
-		kbase_csf_firmware_enable_gpu_idle_timer(kbdev);
-		kbase_csf_scheduler_spin_unlock(kbdev, flags);
-		wait_for_global_request(kbdev, GLB_REQ_IDLE_ENABLE_MASK);
-	} else {
-		kbase_csf_scheduler_spin_unlock(kbdev, flags);
-	}
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	kbdev->csf.gpu_idle_hysteresis_ns = dur;
+	kbdev->csf.gpu_idle_dur_count = hysteresis_val;
+	kbdev->csf.gpu_idle_dur_count_modifier = modifier;
+	kbase_csf_firmware_enable_gpu_idle_timer(kbdev);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+	wait_for_global_request(kbdev, GLB_REQ_IDLE_ENABLE_MASK);
+	mutex_unlock(&kbdev->csf.reg_lock);
 
-	kbase_csf_scheduler_unlock(kbdev);
 	kbase_csf_scheduler_pm_idle(kbdev);
 	kbase_reset_gpu_allow(kbdev);
 end:
@@ -1040,12 +1018,12 @@ u32 kbase_csf_firmware_set_gpu_idle_hysteresis_time(struct kbase_device *kbdev,
 	return hysteresis_val;
 }
 
-static u32 convert_dur_to_core_pwroff_count(struct kbase_device *kbdev, const u64 dur_ns,
-					    u32 *no_modifier)
+static u32 convert_dur_to_core_pwroff_count(struct kbase_device *kbdev, const u32 dur_us,
+					    u32 *modifier)
 {
 	/* Get the cntfreq_el0 value, which drives the SYSTEM_TIMESTAMP */
-	u64 freq = kbase_arch_timer_get_cntfrq(kbdev);
-	u64 dur_val = dur_ns;
+	u64 freq = arch_timer_get_cntfrq();
+	u64 dur_val = dur_us;
 	u32 cnt_val_u32, reg_val_u32;
 	bool src_system_timestamp = freq > 0;
 
@@ -1063,57 +1041,50 @@ static u32 convert_dur_to_core_pwroff_count(struct kbase_device *kbdev, const u6
 			"Can't get the timestamp frequency, use cycle counter with MCU shader Core Poweroff timer!");
 	}
 
-	/* Formula for dur_val = (dur/1e9) * freq_HZ) */
-	dur_val = dur_val * freq;
-	dur_val = div_u64(dur_val, NSEC_PER_SEC);
-	if (dur_val < S32_MAX) {
-		*no_modifier = 1;
-	} else {
-		dur_val = dur_val >> HYSTERESIS_VAL_UNIT_SHIFT;
-		*no_modifier = 0;
-	}
+	/* Formula for dur_val = ((dur_us/1e6) * freq_HZ) >> 10) */
+	dur_val = (dur_val * freq) >> HYSTERESIS_VAL_UNIT_SHIFT;
+	dur_val = div_u64(dur_val, 1000000);
+
+	*modifier = 0;
 
 	/* Interface limits the value field to S32_MAX */
-	if (dur_val > S32_MAX) {
-		/* Upper Bound - as interface limits the field to S32_MAX */
-		cnt_val_u32 = S32_MAX;
-	} else {
-		cnt_val_u32 = (u32)dur_val;
-	}
+	cnt_val_u32 = (dur_val > S32_MAX) ? S32_MAX : (u32)dur_val;
 
 	reg_val_u32 = GLB_PWROFF_TIMER_TIMEOUT_SET(0, cnt_val_u32);
 	/* add the source flag */
-	reg_val_u32 = GLB_PWROFF_TIMER_TIMER_SOURCE_SET(
-		reg_val_u32,
-		(src_system_timestamp ? GLB_PWROFF_TIMER_TIMER_SOURCE_SYSTEM_TIMESTAMP :
-					      GLB_PWROFF_TIMER_TIMER_SOURCE_GPU_COUNTER));
+	if (src_system_timestamp)
+		reg_val_u32 = GLB_PWROFF_TIMER_TIMER_SOURCE_SET(
+			reg_val_u32, GLB_PWROFF_TIMER_TIMER_SOURCE_SYSTEM_TIMESTAMP);
+	else
+		reg_val_u32 = GLB_PWROFF_TIMER_TIMER_SOURCE_SET(
+			reg_val_u32, GLB_PWROFF_TIMER_TIMER_SOURCE_GPU_COUNTER);
 
 	return reg_val_u32;
 }
 
-u64 kbase_csf_firmware_get_mcu_core_pwroff_time(struct kbase_device *kbdev)
+u32 kbase_csf_firmware_get_mcu_core_pwroff_time(struct kbase_device *kbdev)
 {
-	u64 pwroff_ns;
+	u32 pwroff;
 	unsigned long flags;
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	pwroff_ns = kbdev->csf.mcu_core_pwroff_dur_ns;
+	pwroff = kbdev->csf.mcu_core_pwroff_dur_ns;
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
-	return pwroff_ns;
+	return pwroff;
 }
 
-u32 kbase_csf_firmware_set_mcu_core_pwroff_time(struct kbase_device *kbdev, u64 dur_ns)
+u32 kbase_csf_firmware_set_mcu_core_pwroff_time(struct kbase_device *kbdev, u32 dur)
 {
 	unsigned long flags;
-	u32 no_modifier = 0;
+	u32 modifier = 0;
 
-	const u32 pwroff = convert_dur_to_core_pwroff_count(kbdev, dur_ns, &no_modifier);
+	const u32 pwroff = convert_dur_to_core_pwroff_count(kbdev, dur, &modifier);
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	kbdev->csf.mcu_core_pwroff_dur_ns = dur_ns;
+	kbdev->csf.mcu_core_pwroff_dur_ns = dur;
 	kbdev->csf.mcu_core_pwroff_dur_count = pwroff;
-	kbdev->csf.mcu_core_pwroff_dur_count_no_modifier = no_modifier;
+	kbdev->csf.mcu_core_pwroff_dur_count_modifier = modifier;
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 	dev_dbg(kbdev->dev, "MCU shader Core Poweroff input update: 0x%.8x", pwroff);
@@ -1130,6 +1101,8 @@ int kbase_csf_firmware_early_init(struct kbase_device *kbdev)
 {
 	init_waitqueue_head(&kbdev->csf.event_wait);
 
+	kbdev->csf.fw_timeout_ms = kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT);
+
 	kbase_csf_firmware_reset_mcu_core_pwroff_time(kbdev);
 	INIT_LIST_HEAD(&kbdev->csf.firmware_interfaces);
 	INIT_LIST_HEAD(&kbdev->csf.firmware_config);
@@ -1138,22 +1111,21 @@ int kbase_csf_firmware_early_init(struct kbase_device *kbdev)
 	INIT_WORK(&kbdev->csf.firmware_reload_work, kbase_csf_firmware_reload_worker);
 	INIT_WORK(&kbdev->csf.fw_error_work, firmware_error_worker);
 
-	init_rwsem(&kbdev->csf.mmu_sync_sem);
 	mutex_init(&kbdev->csf.reg_lock);
-	kbase_csf_pending_gpuq_kick_queues_init(kbdev);
+	kbase_csf_pending_gpuq_kicks_init(kbdev);
 
 	return 0;
 }
 
 void kbase_csf_firmware_early_term(struct kbase_device *kbdev)
 {
-	kbase_csf_pending_gpuq_kick_queues_term(kbdev);
+	kbase_csf_pending_gpuq_kicks_term(kbdev);
 	mutex_destroy(&kbdev->csf.reg_lock);
 }
 
 int kbase_csf_firmware_late_init(struct kbase_device *kbdev)
 {
-	u32 no_modifier = 0;
+	u32 modifier = 0;
 
 	kbdev->csf.gpu_idle_hysteresis_ns = FIRMWARE_IDLE_HYSTERESIS_TIME_NS;
 #ifdef KBASE_PM_RUNTIME
@@ -1162,8 +1134,8 @@ int kbase_csf_firmware_late_init(struct kbase_device *kbdev)
 #endif
 	WARN_ON(!kbdev->csf.gpu_idle_hysteresis_ns);
 	kbdev->csf.gpu_idle_dur_count =
-		convert_dur_to_idle_count(kbdev, kbdev->csf.gpu_idle_hysteresis_ns, &no_modifier);
-	kbdev->csf.gpu_idle_dur_count_no_modifier = no_modifier;
+		convert_dur_to_idle_count(kbdev, kbdev->csf.gpu_idle_hysteresis_ns, &modifier);
+	kbdev->csf.gpu_idle_dur_count_modifier = modifier;
 
 	return 0;
 }
@@ -1206,7 +1178,6 @@ int kbase_csf_firmware_load_init(struct kbase_device *kbdev)
 
 	/* NO_MALI: Don't load the MMU tables or boot CSF firmware */
 
-
 	ret = invent_capabilities(kbdev);
 	if (ret != 0)
 		goto error;
@@ -1300,9 +1271,13 @@ void kbase_csf_firmware_enable_gpu_idle_timer(struct kbase_device *kbdev)
 
 void kbase_csf_firmware_disable_gpu_idle_timer(struct kbase_device *kbdev)
 {
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+
 	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
 
-	set_gpu_idle_timer_glb_req(kbdev, false);
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, GLB_REQ_REQ_IDLE_DISABLE,
+					     GLB_REQ_IDLE_DISABLE_MASK);
+
 	dev_dbg(kbdev->dev, "Sending request to disable gpu idle timer");
 
 	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
@@ -1326,7 +1301,6 @@ int kbase_csf_firmware_ping_wait(struct kbase_device *const kbdev, unsigned int
 	return wait_for_global_request(kbdev, GLB_REQ_PING_MASK);
 }
 
-
 int kbase_csf_firmware_set_timeout(struct kbase_device *const kbdev, u64 const timeout)
 {
 	const struct kbase_csf_global_iface *const global_iface = &kbdev->csf.global_iface;
@@ -1389,8 +1363,6 @@ void kbase_csf_firmware_trigger_mcu_halt(struct kbase_device *kbdev)
 
 void kbase_csf_firmware_enable_mcu(struct kbase_device *kbdev)
 {
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
 	/* Trigger the boot of MCU firmware, Use the AUTO mode as
 	 * otherwise on fast reset, to exit protected mode, MCU will
 	 * not reboot by itself to enter normal mode.
@@ -1405,7 +1377,6 @@ void kbase_csf_firmware_trigger_mcu_sleep(struct kbase_device *kbdev)
 	unsigned long flags;
 
 	kbase_csf_scheduler_spin_lock(kbdev, &flags);
-	set_gpu_idle_timer_glb_req(kbdev, false);
 	set_global_request(global_iface, GLB_REQ_SLEEP_MASK);
 	dev_dbg(kbdev->dev, "Sending sleep request to MCU");
 	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
@@ -1537,12 +1508,6 @@ void kbase_csf_firmware_disable_mcu(struct kbase_device *kbdev)
 	kbase_reg_write32(kbdev, GPU_CONTROL_ENUM(MCU_CONTROL), MCU_CONTROL_REQ_DISABLE);
 }
 
-void kbase_csf_stop_firmware_and_wait(struct kbase_device *kbdev)
-{
-	/* Stop the MCU firmware, no wait required on NO_MALI instance */
-	kbase_csf_firmware_disable_mcu(kbdev);
-}
-
 void kbase_csf_firmware_disable_mcu_wait(struct kbase_device *kbdev)
 {
 	/* NO_MALI: Nothing to do here */
@@ -1665,16 +1630,3 @@ void kbase_csf_firmware_mcu_shared_mapping_term(struct kbase_device *kbdev,
 	vunmap(csf_mapping->cpu_addr);
 	kfree(csf_mapping->phys);
 }
-
-#ifdef KBASE_PM_RUNTIME
-
-void kbase_csf_firmware_soi_update(struct kbase_device *kbdev)
-{
-}
-
-int kbase_csf_firmware_soi_disable_on_scheduler_suspend(struct kbase_device *kbdev)
-{
-	return 0;
-}
-
-#endif /* KBASE_PM_RUNTIME */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io.c
deleted file mode 100644
index c65f837a9f72..000000000000
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io.c
+++ /dev/null
@@ -1,251 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
-/*
- *
- * (C) COPYRIGHT 2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-#include "mali_kbase.h"
-#include "mali_kbase_csf_fw_io.h"
-#include <mali_kbase_linux.h>
-
-#include <linux/mutex.h>
-
-static inline u32 input_page_read(const u32 *const input, const u32 offset)
-{
-	WARN_ON(offset % sizeof(u32));
-
-	return input[offset / sizeof(u32)];
-}
-
-static inline void input_page_write(u32 *const input, const u32 offset, const u32 value)
-{
-	WARN_ON(offset % sizeof(u32));
-
-	input[offset / sizeof(u32)] = value;
-}
-
-static inline void input_page_partial_write(u32 *const input, const u32 offset, u32 value, u32 mask)
-{
-	WARN_ON(offset % sizeof(u32));
-
-	input[offset / sizeof(u32)] = (input_page_read(input, offset) & ~mask) | (value & mask);
-}
-
-static inline u32 output_page_read(const u32 *const output, const u32 offset)
-{
-	WARN_ON(offset % sizeof(u32));
-
-	return output[offset / sizeof(u32)];
-}
-
-void kbase_csf_fw_io_init(struct kbase_csf_fw_io *fw_io)
-{
-	spin_lock_init(&fw_io->lock);
-	bitmap_zero(fw_io->status, KBASE_FW_IO_STATUS_NUM_BITS);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_init);
-
-void kbase_csf_fw_io_term(struct kbase_csf_fw_io *fw_io)
-{
-	/* Nothing to do. */
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_term);
-
-void kbase_csf_fw_io_global_write(struct kbase_csf_fw_io *fw_io,
-				  const struct kbase_csf_global_iface *iface, u32 offset, u32 value)
-{
-	const struct kbase_device *const kbdev = iface->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "glob input w: reg %08x val %08x\n", offset, value);
-	input_page_write(iface->input, offset, value);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_global_write);
-
-void kbase_csf_fw_io_global_write_mask(struct kbase_csf_fw_io *fw_io,
-				       const struct kbase_csf_global_iface *iface, u32 offset,
-				       u32 value, u32 mask)
-{
-	const struct kbase_device *const kbdev = iface->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "glob input w: reg %08x val %08x mask %08x\n", offset, value, mask);
-	input_page_partial_write(iface->input, offset, value, mask);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_global_write_mask);
-
-u32 kbase_csf_fw_io_global_input_read(struct kbase_csf_fw_io *fw_io,
-				      const struct kbase_csf_global_iface *iface, u32 offset)
-{
-	const struct kbase_device *const kbdev = iface->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = input_page_read(iface->input, offset);
-	dev_dbg(kbdev->dev, "glob input r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_global_input_read);
-
-u32 kbase_csf_fw_io_global_read(struct kbase_csf_fw_io *fw_io,
-				const struct kbase_csf_global_iface *iface, u32 offset)
-{
-	const struct kbase_device *const kbdev = iface->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = output_page_read(iface->output, offset);
-	dev_dbg(kbdev->dev, "glob output r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-
-void kbase_csf_fw_io_group_write(struct kbase_csf_fw_io *fw_io,
-				 const struct kbase_csf_cmd_stream_group_info *info, u32 offset,
-				 u32 value)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "csg input w: reg %08x val %08x\n", offset, value);
-	input_page_write(info->input, offset, value);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_group_write);
-
-void kbase_csf_fw_io_group_write_mask(struct kbase_csf_fw_io *fw_io,
-				      const struct kbase_csf_cmd_stream_group_info *info,
-				      u32 offset, u32 value, u32 mask)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "csg input w: reg %08x val %08x mask %08x\n", offset, value, mask);
-	input_page_partial_write(info->input, offset, value, mask);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_group_write_mask);
-
-u32 kbase_csf_fw_io_group_input_read(struct kbase_csf_fw_io *fw_io,
-				     const struct kbase_csf_cmd_stream_group_info *info, u32 offset)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = input_page_read(info->input, offset);
-	dev_dbg(kbdev->dev, "csg input r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_group_input_read);
-
-u32 kbase_csf_fw_io_group_read(struct kbase_csf_fw_io *fw_io,
-			       const struct kbase_csf_cmd_stream_group_info *info, u32 offset)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = output_page_read(info->output, offset);
-	dev_dbg(kbdev->dev, "csg output r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-
-void kbase_csf_fw_io_stream_write(struct kbase_csf_fw_io *fw_io,
-				  const struct kbase_csf_cmd_stream_info *info, u32 offset,
-				  u32 value)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "cs input w: reg %08x val %08x\n", offset, value);
-	input_page_write(info->input, offset, value);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_stream_write);
-
-void kbase_csf_fw_io_stream_write_mask(struct kbase_csf_fw_io *fw_io,
-				       const struct kbase_csf_cmd_stream_info *info, u32 offset,
-				       u32 value, u32 mask)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "cs input w: reg %08x val %08x mask %08x\n", offset, value, mask);
-	input_page_partial_write(info->input, offset, value, mask);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_stream_write_mask);
-
-u32 kbase_csf_fw_io_stream_input_read(struct kbase_csf_fw_io *fw_io,
-				      const struct kbase_csf_cmd_stream_info *info, u32 offset)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = input_page_read(info->input, offset);
-	dev_dbg(kbdev->dev, "cs input r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_stream_input_read);
-
-u32 kbase_csf_fw_io_stream_read(struct kbase_csf_fw_io *fw_io,
-				const struct kbase_csf_cmd_stream_info *info, u32 offset)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = output_page_read(info->output, offset);
-	dev_dbg(kbdev->dev, "cs output r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-
-void kbase_csf_fw_io_set_status(struct kbase_csf_fw_io *fw_io,
-				enum kbase_csf_fw_io_status_bits status_bit)
-{
-	set_bit(status_bit, fw_io->status);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_set_status);
-
-void kbase_csf_fw_io_clear_status(struct kbase_csf_fw_io *fw_io,
-				  enum kbase_csf_fw_io_status_bits status_bit)
-{
-	clear_bit(status_bit, fw_io->status);
-}
-
-bool kbase_csf_fw_io_test_status(struct kbase_csf_fw_io *fw_io,
-				 enum kbase_csf_fw_io_status_bits status_bit)
-{
-	return test_bit(status_bit, fw_io->status);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_test_status);
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io.h
deleted file mode 100644
index a8eb1ab51fbc..000000000000
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io.h
+++ /dev/null
@@ -1,362 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
-/*
- *
- * (C) COPYRIGHT 2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-#ifndef _KBASE_CSF_FW_IO_H_
-#define _KBASE_CSF_FW_IO_H_
-
-#include <linux/sched.h>
-#include <linux/types.h>
-#include <linux/wait.h>
-#include <linux/spinlock.h>
-
-/** The wait completed because the GPU was lost. */
-#define KBASE_CSF_FW_IO_WAIT_GPU_LOST 1
-
-/** The wait was aborted because of an unexpected event. */
-#define KBASE_CSF_FW_IO_WAIT_UNSUPPORTED 255
-
-/**
- * enum kbase_csf_fw_io_status_bits - Status bits for firmware I/O interface.
- *
- * @KBASE_FW_IO_STATUS_GPU_SUSPENDED: The GPU is suspended.
- * @KBASE_FW_IO_STATUS_NUM_BITS: Number of bits used to encode the state.
- */
-enum kbase_csf_fw_io_status_bits {
-	KBASE_FW_IO_STATUS_GPU_SUSPENDED = 0,
-	KBASE_FW_IO_STATUS_NUM_BITS,
-};
-
-/**
- * struct kbase_csf_fw_io - Manager of firmware input/output interface.
- *
- * @lock: Mutex to serialize access to the interface.
- * @status: Internal status of the MCU interface.
- */
-struct kbase_csf_fw_io {
-	spinlock_t lock;
-	DECLARE_BITMAP(status, KBASE_FW_IO_STATUS_NUM_BITS);
-};
-
-struct kbase_csf_global_iface;
-struct kbase_csf_cmd_stream_group_info;
-struct kbase_csf_cmd_stream_info;
-
-/**
- * kbase_csf_fw_io_init() - Initialize manager of firmware input/output interface.
- *
- * @fw_io: Firmware I/O interface to initialize.
- */
-void kbase_csf_fw_io_init(struct kbase_csf_fw_io *fw_io);
-
-/**
- * kbase_csf_fw_io_term() - Terminate manager of firmware input/output interface.
- *
- * @fw_io: Firmware I/O interface to terminate.
- */
-void kbase_csf_fw_io_term(struct kbase_csf_fw_io *fw_io);
-
-/**
- * kbase_csf_fw_io_open() - Start a transaction with the firmware input/output interface.
- *
- * @fw_io: Firmware I/O interface to open.
- *
- * Return: 0 on success, otherwise an error code reflecting the status of the
- *         interface.
- */
-static inline int kbase_csf_fw_io_open(struct kbase_csf_fw_io *fw_io)
-{
-	if (test_bit(KBASE_FW_IO_STATUS_GPU_SUSPENDED, fw_io->status))
-		return -KBASE_CSF_FW_IO_WAIT_GPU_LOST;
-
-	spin_lock(&fw_io->lock);
-
-	return 0;
-}
-
-/**
- * kbase_csf_fw_io_open_force() - Force a transaction with the firmware input/output interface.
- *
- * @fw_io: Firmware I/O interface to open.
- *
- * This function forces the start of a transaction regardless of the status
- * of the interface.
- */
-static inline void kbase_csf_fw_io_open_force(struct kbase_csf_fw_io *fw_io)
-{
-	spin_lock(&fw_io->lock);
-}
-
-/**
- * kbase_csf_fw_io_close() - End a transaction with the firmware input/output interface.
- *
- * @fw_io: Firmware I/O interface to close.
- */
-static inline void kbase_csf_fw_io_close(struct kbase_csf_fw_io *fw_io)
-{
-	spin_unlock(&fw_io->lock);
-}
-
-/**
- * kbase_csf_fw_io_assert_opened() - Assert if a transaction with the firmware input/output
- *                                   interface has started.
- *
- * @fw_io: Firmware I/O interface.
- */
-static inline void kbase_csf_fw_io_assert_opened(struct kbase_csf_fw_io *fw_io)
-{
-	lockdep_assert_held(&fw_io->lock);
-}
-
-/**
- * kbase_csf_fw_io_global_write() - Write a word in the global input page.
- *
- * @fw_io:  Firmware I/O manager.
- * @iface:  CSF interface provided by the firmware.
- * @offset: Offset of the word to write, in bytes.
- * @value:  Value to be written.
- */
-void kbase_csf_fw_io_global_write(struct kbase_csf_fw_io *fw_io,
-				  const struct kbase_csf_global_iface *iface, u32 offset,
-				  u32 value);
-
-/**
- * kbase_csf_fw_io_global_write_mask() - Write part of a word in the global input page.
- *
- * @fw_io:  Firmware I/O manager.
- * @iface:  CSF interface provided by the firmware.
- * @offset: Offset of the word to write, in bytes.
- * @value:  Value to be written.
- * @mask:   Bitmask with the bits to be modified set.
- */
-void kbase_csf_fw_io_global_write_mask(struct kbase_csf_fw_io *fw_io,
-				       const struct kbase_csf_global_iface *iface, u32 offset,
-				       u32 value, u32 mask);
-
-/**
- * kbase_csf_fw_io_global_input_read() - Read a word in the global input page.
- *
- * @fw_io:  Firmware I/O manager.
- * @iface:  CSF interface provided by the firmware.
- * @offset: Offset of the word to be read, in bytes.
- *
- * Return: Value of the word read from the global input page.
- */
-u32 kbase_csf_fw_io_global_input_read(struct kbase_csf_fw_io *fw_io,
-				      const struct kbase_csf_global_iface *iface, u32 offset);
-
-/**
- * kbase_csf_fw_io_global_read() - Read a word in the global output page.
- *
- * @fw_io:  Firmware I/O manager.
- * @iface:  CSF interface provided by the firmware.
- * @offset: Offset of the word to be read, in bytes.
- *
- * Return: Value of the word read from the global output page.
- */
-u32 kbase_csf_fw_io_global_read(struct kbase_csf_fw_io *fw_io,
-				const struct kbase_csf_global_iface *iface, u32 offset);
-
-/**
- * kbase_csf_fw_io_group_write() - Write a word in a CSG's input page.
- *
- * @fw_io:  Firmware I/O manager.
- * @info:   CSG interface provided by the firmware.
- * @offset: Offset of the word to write, in bytes.
- * @value:  Value to be written.
- */
-void kbase_csf_fw_io_group_write(struct kbase_csf_fw_io *fw_io,
-				 const struct kbase_csf_cmd_stream_group_info *info, u32 offset,
-				 u32 value);
-
-/**
- * kbase_csf_fw_io_group_write_mask() - Write part of a word in a CSG's input page.
- *
- * @fw_io:  Firmware I/O manager.
- * @info:   CSG interface provided by the firmware.
- * @offset: Offset of the word to write, in bytes.
- * @value:  Value to be written.
- * @mask:   Bitmask with the bits to be modified set.
- */
-void kbase_csf_fw_io_group_write_mask(struct kbase_csf_fw_io *fw_io,
-				      const struct kbase_csf_cmd_stream_group_info *info,
-				      u32 offset, u32 value, u32 mask);
-
-/**
- * kbase_csf_fw_io_group_input_read() - Read a word in a CSG's input page.
- *
- * @fw_io:  Firmware I/O manager.
- * @info:   CSG interface provided by the firmware.
- * @offset: Offset of the word to be read, in bytes.
- *
- * Return: Value of the word read from a CSG's input page.
- */
-u32 kbase_csf_fw_io_group_input_read(struct kbase_csf_fw_io *fw_io,
-				     const struct kbase_csf_cmd_stream_group_info *info,
-				     u32 offset);
-
-/**
- * kbase_csf_fw_io_group_read() - Read a word in a CSG's output page.
- *
- * @fw_io:  Firmware I/O manager.
- * @info:   CSG interface provided by the firmware.
- * @offset: Offset of the word to be read, in bytes.
- *
- * Return: Value of the word read from the CSG's output page.
- */
-u32 kbase_csf_fw_io_group_read(struct kbase_csf_fw_io *fw_io,
-			       const struct kbase_csf_cmd_stream_group_info *info, u32 offset);
-
-/**
- * kbase_csf_fw_io_stream_write() - Write a word in a CS's input page.
- *
- * @fw_io:  Firmware I/O manager.
- * @info:   CSI interface provided by the firmware.
- * @offset: Offset of the word to write, in bytes.
- * @value:  Value to be written.
- */
-void kbase_csf_fw_io_stream_write(struct kbase_csf_fw_io *fw_io,
-				  const struct kbase_csf_cmd_stream_info *info, u32 offset,
-				  u32 value);
-
-/**
- * kbase_csf_fw_io_stream_write_mask() - Write part of a word in a CS's input page.
- *
- * @fw_io:  Firmware I/O manager.
- * @info:   CSI interface provided by the firmware.
- * @offset: Offset of the word to write, in bytes.
- * @value:  Value to be written.
- * @mask:   Bitmask with the bits to be modified set.
- */
-void kbase_csf_fw_io_stream_write_mask(struct kbase_csf_fw_io *fw_io,
-				       const struct kbase_csf_cmd_stream_info *info, u32 offset,
-				       u32 value, u32 mask);
-
-/**
- * kbase_csf_fw_io_stream_input_read() - Read a word in a CS's input page.
- *
- * @fw_io:  Firmware I/O manager.
- * @info:   CSI interface provided by the firmware.
- * @offset: Offset of the word to be read, in bytes.
- *
- * Return: Value of the word read from a CS's input page.
- */
-u32 kbase_csf_fw_io_stream_input_read(struct kbase_csf_fw_io *fw_io,
-				      const struct kbase_csf_cmd_stream_info *info, u32 offset);
-
-/**
- * kbase_csf_fw_io_stream_read() - Read a word in a CS's output page.
- *
- * @fw_io:  Firmware I/O manager.
- * @info:   CSI interface provided by the firmware.
- * @offset: Offset of the word to be read, in bytes.
- *
- * Return: Value of the word read from the CS's output page.
- */
-u32 kbase_csf_fw_io_stream_read(struct kbase_csf_fw_io *fw_io,
-				const struct kbase_csf_cmd_stream_info *info, u32 offset);
-
-/**
- * kbase_csf_fw_io_set_status() - Set a FW I/O status bit.
- *
- * @fw_io:      Firmware I/O manager.
- * @status_bit: Status bit to set.
- */
-void kbase_csf_fw_io_set_status(struct kbase_csf_fw_io *fw_io,
-				enum kbase_csf_fw_io_status_bits status_bit);
-
-/**
- * kbase_csf_fw_io_clear_status() - Clear a FW I/O status bit.
- *
- * @fw_io:      Firmware I/O manager.
- * @status_bit: Status bit to clear.
- */
-void kbase_csf_fw_io_clear_status(struct kbase_csf_fw_io *fw_io,
-				  enum kbase_csf_fw_io_status_bits status_bit);
-
-/**
- * kbase_csf_fw_io_test_status() - Test a FW I/O status bit.
- *
- * @fw_io:      Firmware I/O manager.
- * @status_bit: Status bit to test.
- *
- * Return: Value of the tested status bit.
- */
-bool kbase_csf_fw_io_test_status(struct kbase_csf_fw_io *fw_io,
-				 enum kbase_csf_fw_io_status_bits status_bit);
-
-/**
- * kbase_csf_fw_io_wait_event_timeout() - Wait until condition gets true, timeout
- * occurs or a FW I/O status bit is set. The rest of the functionalities is equal
- * to wait_event_timeout().
- *
- * @fw_io:     Firmware I/O manager.
- * @wq_head:   The waitqueue to wait on.
- * @condition: C expression for the event to wait for
- * @timeout:   Timeout, in jiffies
- *
- * Return: Remaining jiffies (at least 1) on success,
- *         0 on timeout,
- *         negative KBASE_CSF_FW_IO_WAIT_* error codes otherwise.
- */
-#define kbase_csf_fw_io_wait_event_timeout(fw_io, wq_head, condition, timeout)                \
-	({                                                                                    \
-		int __ret;                                                                    \
-		int __wait_remaining = wait_event_timeout(                                    \
-			wq_head, condition || kbasep_csf_fw_io_check_status(fw_io), timeout); \
-		__ret = kbasep_csf_fw_io_handle_wait_result(fw_io, __wait_remaining);         \
-		__ret;                                                                        \
-	})
-
-/**
- * kbasep_csf_fw_io_check_status() - Private function to check if any FW I/O status bit is set.
- *
- * @fw_io: Firmware I/O manager.
- *
- * Return: True if any FW I/O status bit is set, false otherwise.
- */
-static inline bool kbasep_csf_fw_io_check_status(struct kbase_csf_fw_io *fw_io)
-{
-	return !bitmap_empty(fw_io->status, KBASE_FW_IO_STATUS_NUM_BITS);
-}
-
-/**
- * kbasep_csf_fw_io_handle_wait_result() - Private function to handle the wait_event_timeout()
- * result.
- *
- * @fw_io:          Firmware I/O manager
- * @wait_remaining: Remaining jiffies returned by wait_event_timeout()
- *
- * Return: Remaining jiffies (at least 1) on success,
- *         0 on timeout,
- *         negative KBASE_CSF_FW_IO_WAIT_* error codes otherwise.
- */
-static inline int kbasep_csf_fw_io_handle_wait_result(struct kbase_csf_fw_io *fw_io,
-						      int wait_remaining)
-{
-	/* Check for any FW IO status bit set */
-	if (!bitmap_empty(fw_io->status, KBASE_FW_IO_STATUS_NUM_BITS))
-		return (test_bit(KBASE_FW_IO_STATUS_GPU_SUSPENDED, fw_io->status)) ?
-				     -KBASE_CSF_FW_IO_WAIT_GPU_LOST :
-				     -KBASE_CSF_FW_IO_WAIT_UNSUPPORTED;
-
-	return wait_remaining;
-}
-#endif
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io_no_mali.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io_no_mali.c
deleted file mode 100644
index 0cffc8475654..000000000000
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_fw_io_no_mali.c
+++ /dev/null
@@ -1,294 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
-/*
- *
- * (C) COPYRIGHT 2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-#include "mali_kbase.h"
-#include "mali_kbase_csf_fw_io.h"
-#include <mali_kbase_linux.h>
-
-#include <linux/mutex.h>
-
-static inline u32 input_page_read(const u32 *const input, const u32 offset)
-{
-	WARN_ON(offset % sizeof(u32));
-
-	return input[offset / sizeof(u32)];
-}
-
-static inline void input_page_write(u32 *const input, const u32 offset, const u32 value)
-{
-	WARN_ON(offset % sizeof(u32));
-
-	input[offset / sizeof(u32)] = value;
-}
-
-static inline void input_page_partial_write(u32 *const input, const u32 offset, u32 value, u32 mask)
-{
-	WARN_ON(offset % sizeof(u32));
-
-	input[offset / sizeof(u32)] = (input_page_read(input, offset) & ~mask) | (value & mask);
-}
-
-static inline u32 output_page_read(const u32 *const output, const u32 offset)
-{
-	WARN_ON(offset % sizeof(u32));
-
-	return output[offset / sizeof(u32)];
-}
-
-static inline void output_page_write(u32 *const output, const u32 offset, const u32 value)
-{
-	WARN_ON(offset % sizeof(u32));
-
-	output[offset / sizeof(u32)] = value;
-}
-
-void kbase_csf_fw_io_init(struct kbase_csf_fw_io *fw_io)
-{
-	spin_lock_init(&fw_io->lock);
-	bitmap_zero(fw_io->status, KBASE_FW_IO_STATUS_NUM_BITS);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_init);
-
-void kbase_csf_fw_io_term(struct kbase_csf_fw_io *fw_io)
-{
-	/* Nothing to do. */
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_term);
-
-void kbase_csf_fw_io_global_write(struct kbase_csf_fw_io *fw_io,
-				  const struct kbase_csf_global_iface *iface, u32 offset, u32 value)
-{
-	const struct kbase_device *const kbdev = iface->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "glob input w: reg %08x val %08x\n", offset, value);
-	input_page_write(iface->input, offset, value);
-
-	if (offset == GLB_REQ) {
-		/* NO_MALI: Immediately acknowledge requests - except for PRFCNT_ENABLE
-		 * and PRFCNT_SAMPLE. These will be processed along with the
-		 * corresponding performance counter registers when the global doorbell
-		 * is rung in order to emulate the performance counter sampling behavior
-		 * of the real firmware.
-		 */
-		const u32 ack = output_page_read(iface->output, GLB_ACK);
-		const u32 req_mask = ~(GLB_REQ_PRFCNT_ENABLE_MASK | GLB_REQ_PRFCNT_SAMPLE_MASK);
-		const u32 toggled = (value ^ ack) & req_mask;
-
-		output_page_write(iface->output, GLB_ACK, ack ^ toggled);
-	}
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_global_write);
-
-void kbase_csf_fw_io_global_write_mask(struct kbase_csf_fw_io *fw_io,
-				       const struct kbase_csf_global_iface *iface, u32 offset,
-				       u32 value, u32 mask)
-{
-	const struct kbase_device *const kbdev = iface->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "glob input w: reg %08x val %08x mask %08x\n", offset, value, mask);
-
-	/* NO_MALI: Go through existing function to capture writes */
-	kbase_csf_fw_io_global_write(fw_io, iface, offset,
-				     (input_page_read(iface->input, offset) & ~mask) |
-					     (value & mask));
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_global_write_mask);
-
-u32 kbase_csf_fw_io_global_input_read(struct kbase_csf_fw_io *fw_io,
-				      const struct kbase_csf_global_iface *iface, u32 offset)
-{
-	const struct kbase_device *const kbdev = iface->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = input_page_read(iface->input, offset);
-	dev_dbg(kbdev->dev, "glob input r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_global_input_read);
-
-u32 kbase_csf_fw_io_global_read(struct kbase_csf_fw_io *fw_io,
-				const struct kbase_csf_global_iface *iface, u32 offset)
-{
-	const struct kbase_device *const kbdev = iface->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = output_page_read(iface->output, offset);
-	dev_dbg(kbdev->dev, "glob output r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-
-void kbase_csf_fw_io_group_write(struct kbase_csf_fw_io *fw_io,
-				 const struct kbase_csf_cmd_stream_group_info *info, u32 offset,
-				 u32 value)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "csg input w: reg %08x val %08x\n", offset, value);
-	input_page_write(info->input, offset, value);
-
-	if (offset == CSG_REQ) {
-		/* NO_MALI: Immediately acknowledge requests */
-		output_page_write(info->output, CSG_ACK, value);
-	}
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_group_write);
-
-void kbase_csf_fw_io_group_write_mask(struct kbase_csf_fw_io *fw_io,
-				      const struct kbase_csf_cmd_stream_group_info *info,
-				      u32 offset, u32 value, u32 mask)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "csg input w: reg %08x val %08x mask %08x\n", offset, value, mask);
-
-	/* NO_MALI: Go through existing function to capture writes */
-	kbase_csf_fw_io_group_write(fw_io, info, offset,
-				    (input_page_read(info->input, offset) & ~mask) |
-					    (value & mask));
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_group_write_mask);
-
-u32 kbase_csf_fw_io_group_input_read(struct kbase_csf_fw_io *fw_io,
-				     const struct kbase_csf_cmd_stream_group_info *info, u32 offset)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = input_page_read(info->input, offset);
-	dev_dbg(kbdev->dev, "csg input r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_group_input_read);
-
-u32 kbase_csf_fw_io_group_read(struct kbase_csf_fw_io *fw_io,
-			       const struct kbase_csf_cmd_stream_group_info *info, u32 offset)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = output_page_read(info->output, offset);
-	dev_dbg(kbdev->dev, "csg output r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-
-void kbase_csf_fw_io_stream_write(struct kbase_csf_fw_io *fw_io,
-				  const struct kbase_csf_cmd_stream_info *info, u32 offset,
-				  u32 value)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "cs input w: reg %08x val %08x\n", offset, value);
-	input_page_write(info->input, offset, value);
-
-	if (offset == CS_REQ) {
-		/* NO_MALI: Immediately acknowledge requests */
-		output_page_write(info->output, CS_ACK, value);
-	}
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_stream_write);
-
-void kbase_csf_fw_io_stream_write_mask(struct kbase_csf_fw_io *fw_io,
-				       const struct kbase_csf_cmd_stream_info *info, u32 offset,
-				       u32 value, u32 mask)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	dev_dbg(kbdev->dev, "cs input w: reg %08x val %08x mask %08x\n", offset, value, mask);
-
-	/* NO_MALI: Go through existing function to capture writes */
-	kbase_csf_fw_io_stream_write(fw_io, info, offset,
-				     (input_page_read(info->input, offset) & ~mask) |
-					     (value & mask));
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_stream_write_mask);
-
-u32 kbase_csf_fw_io_stream_input_read(struct kbase_csf_fw_io *fw_io,
-				      const struct kbase_csf_cmd_stream_info *info, u32 offset)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = input_page_read(info->input, offset);
-	dev_dbg(kbdev->dev, "cs input r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_stream_input_read);
-
-u32 kbase_csf_fw_io_stream_read(struct kbase_csf_fw_io *fw_io,
-				const struct kbase_csf_cmd_stream_info *info, u32 offset)
-{
-	const struct kbase_device *const kbdev = info->kbdev;
-	u32 val;
-
-	lockdep_assert_held(&fw_io->lock);
-
-	val = output_page_read(info->output, offset);
-	dev_dbg(kbdev->dev, "cs output r: reg %08x val %08x\n", offset, val);
-
-	return val;
-}
-
-void kbase_csf_fw_io_set_status(struct kbase_csf_fw_io *fw_io,
-				enum kbase_csf_fw_io_status_bits status_bit)
-{
-	set_bit(status_bit, fw_io->status);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_set_status);
-
-void kbase_csf_fw_io_clear_status(struct kbase_csf_fw_io *fw_io,
-				  enum kbase_csf_fw_io_status_bits status_bit)
-{
-	clear_bit(status_bit, fw_io->status);
-}
-
-bool kbase_csf_fw_io_test_status(struct kbase_csf_fw_io *fw_io,
-				 enum kbase_csf_fw_io_status_bits status_bit)
-{
-	return test_bit(status_bit, fw_io->status);
-}
-KBASE_EXPORT_TEST_API(kbase_csf_fw_io_test_status);
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.c
index fb181026719f..12a79b4852fb 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -180,9 +180,8 @@ void kbase_csf_heap_context_allocator_term(struct kbase_csf_heap_context_allocat
 u64 kbase_csf_heap_context_allocator_alloc(struct kbase_csf_heap_context_allocator *const ctx_alloc)
 {
 	struct kbase_context *const kctx = ctx_alloc->kctx;
-	base_mem_alloc_flags flags = BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR |
-				     BASE_MEM_PROT_CPU_WR | BASEP_MEM_NO_USER_FREE |
-				     BASE_MEM_PROT_CPU_RD;
+	u64 flags = BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR | BASE_MEM_PROT_CPU_WR |
+		    BASEP_MEM_NO_USER_FREE | BASE_MEM_PROT_CPU_RD;
 	u64 nr_pages = PFN_UP(MAX_TILER_HEAPS * ctx_alloc->heap_context_size_aligned);
 	u64 heap_gpu_va = 0;
 
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.c
index 09c92f0bed4e..9da7f4dea64b 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -39,7 +39,13 @@
 static DEFINE_SPINLOCK(kbase_csf_fence_lock);
 #endif
 
+#ifdef CONFIG_MALI_BIFROST_FENCE_DEBUG
 #define FENCE_WAIT_TIMEOUT_MS 3000
+#endif
+
+static void kcpu_queue_process(struct kbase_kcpu_command_queue *kcpu_queue, bool drain_queue);
+
+static void kcpu_queue_process_worker(struct work_struct *data);
 
 static int kbase_kcpu_map_import_prepare(struct kbase_kcpu_command_queue *kcpu_queue,
 					 struct base_kcpu_command_import_info *import_info,
@@ -371,7 +377,7 @@ static int kbase_kcpu_jit_allocate_prepare(struct kbase_kcpu_command_queue *kcpu
 		goto out;
 	}
 
-	if (copy_from_user(info, data, size_mul(sizeof(*info), count)) != 0) {
+	if (copy_from_user(info, data, sizeof(*info) * count) != 0) {
 		ret = -EINVAL;
 		goto out_free;
 	}
@@ -439,16 +445,6 @@ static void kbase_kcpu_jit_allocate_finish(struct kbase_kcpu_command_queue *queu
 	kfree(cmd->info.jit_alloc.info);
 }
 
-static void enqueue_kcpuq_work(struct kbase_kcpu_command_queue *queue)
-{
-	struct kbase_context *const kctx = queue->kctx;
-
-	if (!atomic_read(&kctx->prioritized))
-		queue_work(kctx->csf.kcpu_queues.kcpu_wq, &queue->work);
-	else
-		kbase_csf_scheduler_enqueue_kcpuq_work(queue);
-}
-
 /**
  * kbase_kcpu_jit_retry_pending_allocs() - Retry blocked JIT_ALLOC commands
  *
@@ -468,7 +464,7 @@ static void kbase_kcpu_jit_retry_pending_allocs(struct kbase_context *kctx)
 	 * kbase_csf_kcpu_queue_context.jit_lock .
 	 */
 	list_for_each_entry(blocked_queue, &kctx->csf.kcpu_queues.jit_blocked_queues, jit_blocked)
-		enqueue_kcpuq_work(blocked_queue);
+		queue_work(blocked_queue->wq, &blocked_queue->work);
 }
 
 static int kbase_kcpu_jit_free_process(struct kbase_kcpu_command_queue *queue,
@@ -514,7 +510,7 @@ static int kbase_kcpu_jit_free_process(struct kbase_kcpu_command_queue *queue,
 		}
 
 		KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_EXECUTE_JIT_FREE_END(
-			queue->kctx->kbdev, queue, (u32)item_err, pages_used);
+			queue->kctx->kbdev, queue, item_err, pages_used);
 	}
 
 	/*
@@ -563,7 +559,7 @@ static int kbase_kcpu_jit_free_prepare(struct kbase_kcpu_command_queue *kcpu_que
 		goto out_free;
 	}
 
-	if (copy_from_user(ids, data, size_mul(sizeof(*ids), count))) {
+	if (copy_from_user(ids, data, sizeof(*ids) * count)) {
 		ret = -EINVAL;
 		goto out_free;
 	}
@@ -615,7 +611,7 @@ kbase_csf_queue_group_suspend_prepare(struct kbase_kcpu_command_queue *kcpu_queu
 	u64 page_addr = addr & PAGE_MASK;
 	u64 end_addr = addr + csg_suspend_buf_size - 1;
 	u64 last_page_addr = end_addr & PAGE_MASK;
-	unsigned int nr_pages = (last_page_addr - page_addr) / PAGE_SIZE + 1;
+	int nr_pages = (last_page_addr - page_addr) / PAGE_SIZE + 1;
 	int pinned_pages = 0, ret = 0;
 	struct kbase_va_region *reg;
 
@@ -651,7 +647,7 @@ kbase_csf_queue_group_suspend_prepare(struct kbase_kcpu_command_queue *kcpu_queu
 
 	if (kbase_is_region_invalid_or_free(reg)) {
 		kbase_gpu_vm_unlock(kctx);
-		pinned_pages = get_user_pages_fast(page_addr, (int)nr_pages, 1, sus_buf->pages);
+		pinned_pages = get_user_pages_fast(page_addr, nr_pages, 1, sus_buf->pages);
 		kbase_gpu_vm_lock(kctx);
 
 		if (pinned_pages < 0) {
@@ -720,7 +716,7 @@ static enum kbase_csf_event_callback_action event_cqs_callback(void *param)
 {
 	struct kbase_kcpu_command_queue *kcpu_queue = (struct kbase_kcpu_command_queue *)param;
 
-	enqueue_kcpuq_work(kcpu_queue);
+	queue_work(kcpu_queue->wq, &kcpu_queue->work);
 
 	return KBASE_CSF_EVENT_CALLBACK_KEEP;
 }
@@ -852,8 +848,7 @@ static int kbase_kcpu_cqs_wait_prepare(struct kbase_kcpu_command_queue *queue,
 	if (!objs)
 		return -ENOMEM;
 
-	if (copy_from_user(objs, u64_to_user_ptr(cqs_wait_info->objs),
-			   size_mul(nr_objs, sizeof(*objs)))) {
+	if (copy_from_user(objs, u64_to_user_ptr(cqs_wait_info->objs), nr_objs * sizeof(*objs))) {
 		kfree(objs);
 		return -ENOMEM;
 	}
@@ -958,8 +953,7 @@ static int kbase_kcpu_cqs_set_prepare(struct kbase_kcpu_command_queue *kcpu_queu
 	if (!objs)
 		return -ENOMEM;
 
-	if (copy_from_user(objs, u64_to_user_ptr(cqs_set_info->objs),
-			   size_mul(nr_objs, sizeof(*objs)))) {
+	if (copy_from_user(objs, u64_to_user_ptr(cqs_set_info->objs), nr_objs * sizeof(*objs))) {
 		kfree(objs);
 		return -ENOMEM;
 	}
@@ -1117,7 +1111,7 @@ static int kbase_kcpu_cqs_wait_operation_prepare(
 		return -ENOMEM;
 
 	if (copy_from_user(objs, u64_to_user_ptr(cqs_wait_operation_info->objs),
-			   size_mul(nr_objs, sizeof(*objs)))) {
+			   nr_objs * sizeof(*objs))) {
 		kfree(objs);
 		return -ENOMEM;
 	}
@@ -1282,7 +1276,7 @@ static int kbase_kcpu_cqs_set_operation_prepare(
 		return -ENOMEM;
 
 	if (copy_from_user(objs, u64_to_user_ptr(cqs_set_operation_info->objs),
-			   size_mul(nr_objs, sizeof(*objs)))) {
+			   nr_objs * sizeof(*objs))) {
 		kfree(objs);
 		return -ENOMEM;
 	}
@@ -1307,7 +1301,11 @@ static int kbase_kcpu_cqs_set_operation_prepare(
 }
 
 #if IS_ENABLED(CONFIG_SYNC_FILE)
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+static void kbase_csf_fence_wait_callback(struct fence *fence, struct fence_cb *cb)
+#else
 static void kbase_csf_fence_wait_callback(struct dma_fence *fence, struct dma_fence_cb *cb)
+#endif
 {
 	struct kbase_kcpu_command_fence_info *fence_info =
 		container_of(cb, struct kbase_kcpu_command_fence_info, fence_cb);
@@ -1323,7 +1321,7 @@ static void kbase_csf_fence_wait_callback(struct dma_fence *fence, struct dma_fe
 				  fence->seqno);
 
 	/* Resume kcpu command queue processing. */
-	enqueue_kcpuq_work(kcpu_queue);
+	queue_work(kcpu_queue->wq, &kcpu_queue->work);
 }
 
 static void kbasep_kcpu_fence_wait_cancel(struct kbase_kcpu_command_queue *kcpu_queue,
@@ -1359,6 +1357,7 @@ static void kbasep_kcpu_fence_wait_cancel(struct kbase_kcpu_command_queue *kcpu_
 	fence_info->fence = NULL;
 }
 
+#ifdef CONFIG_MALI_BIFROST_FENCE_DEBUG
 /**
  * fence_timeout_callback() - Timeout callback function for fence-wait
  *
@@ -1375,7 +1374,11 @@ static void fence_timeout_callback(struct timer_list *timer)
 	struct kbase_context *const kctx = kcpu_queue->kctx;
 	struct kbase_kcpu_command *cmd = &kcpu_queue->commands[kcpu_queue->start_offset];
 	struct kbase_kcpu_command_fence_info *fence_info;
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence;
+#else
 	struct dma_fence *fence;
+#endif
 	struct kbase_sync_fence_info info;
 
 	if (cmd->type != BASE_KCPU_COMMAND_TYPE_FENCE_WAIT) {
@@ -1397,7 +1400,7 @@ static void fence_timeout_callback(struct timer_list *timer)
 	kbase_sync_fence_info_get(fence, &info);
 
 	if (info.status == 1) {
-		enqueue_kcpuq_work(kcpu_queue);
+		queue_work(kcpu_queue->wq, &kcpu_queue->work);
 	} else if (info.status == 0) {
 		dev_warn(kctx->kbdev->dev, "fence has not yet signalled in %ums",
 			 FENCE_WAIT_TIMEOUT_MS);
@@ -1426,6 +1429,7 @@ static void fence_wait_timeout_start(struct kbase_kcpu_command_queue *cmd)
 {
 	mod_timer(&cmd->fence_timeout, jiffies + msecs_to_jiffies(FENCE_WAIT_TIMEOUT_MS));
 }
+#endif
 
 /**
  * kbase_kcpu_fence_wait_process() - Process the kcpu fence wait command
@@ -1440,7 +1444,11 @@ static int kbase_kcpu_fence_wait_process(struct kbase_kcpu_command_queue *kcpu_q
 					 struct kbase_kcpu_command_fence_info *fence_info)
 {
 	int fence_status = 0;
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence;
+#else
 	struct dma_fence *fence;
+#endif
 	struct kbase_context *const kctx = kcpu_queue->kctx;
 
 	lockdep_assert_held(&kcpu_queue->lock);
@@ -1464,8 +1472,9 @@ static int kbase_kcpu_fence_wait_process(struct kbase_kcpu_command_queue *kcpu_q
 		fence_status = cb_err;
 		if (cb_err == 0) {
 			kcpu_queue->fence_wait_processed = true;
-			if (IS_ENABLED(CONFIG_MALI_BIFROST_FENCE_DEBUG))
-				fence_wait_timeout_start(kcpu_queue);
+#ifdef CONFIG_MALI_BIFROST_FENCE_DEBUG
+			fence_wait_timeout_start(kcpu_queue);
+#endif
 		} else if (cb_err == -ENOENT) {
 			fence_status = dma_fence_get_status(fence);
 			if (!fence_status) {
@@ -1505,7 +1514,11 @@ static int kbase_kcpu_fence_wait_prepare(struct kbase_kcpu_command_queue *kcpu_q
 					 struct base_kcpu_command_fence_info *fence_info,
 					 struct kbase_kcpu_command *current_command)
 {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence_in;
+#else
 	struct dma_fence *fence_in;
+#endif
 	struct base_fence fence;
 
 	lockdep_assert_held(&kcpu_queue->lock);
@@ -1600,7 +1613,11 @@ static void kcpu_force_signal_fence(struct kbase_kcpu_command_queue *kcpu_queue)
 {
 	int status;
 	int i;
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence;
+#else
 	struct dma_fence *fence;
+#endif
 	struct kbase_context *const kctx = kcpu_queue->kctx;
 #ifdef CONFIG_MALI_BIFROST_FENCE_DEBUG
 	int del;
@@ -1686,7 +1703,7 @@ static void fence_signal_timeout_cb(struct timer_list *timer)
 		if (atomic_read(&kcpu_queue->fence_signal_pending_cnt) > 1)
 			fence_signal_timeout_start(kcpu_queue);
 
-		queue_work(kctx->csf.kcpu_queues.kcpu_wq, &kcpu_queue->timeout_work);
+		queue_work(kcpu_queue->wq, &kcpu_queue->timeout_work);
 	}
 }
 
@@ -1745,7 +1762,11 @@ static int kbasep_kcpu_fence_signal_init(struct kbase_kcpu_command_queue *kcpu_q
 					 struct base_fence *fence, struct sync_file **sync_file,
 					 int *fd)
 {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence_out;
+#else
 	struct dma_fence *fence_out;
+#endif
 	struct kbase_kcpu_dma_fence *kcpu_fence;
 	int ret = 0;
 
@@ -1759,7 +1780,11 @@ static int kbasep_kcpu_fence_signal_init(struct kbase_kcpu_command_queue *kcpu_q
 	kcpu_fence->metadata = kcpu_queue->metadata;
 	WARN_ON(!kbase_refcount_inc_not_zero(&kcpu_fence->metadata->refcount));
 
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	fence_out = (struct fence *)kcpu_fence;
+#else
 	fence_out = (struct dma_fence *)kcpu_fence;
+#endif
 
 	dma_fence_init(fence_out, &kbase_fence_ops, &kbase_csf_fence_lock,
 		       kcpu_queue->fence_context, ++kcpu_queue->fence_seqno);
@@ -1834,7 +1859,7 @@ static int kbase_kcpu_fence_signal_prepare(struct kbase_kcpu_command_queue *kcpu
 	 * installed, so the install step needs to be done at the last
 	 * before returning success.
 	 */
-	fd_install((unsigned int)fd, sync_file->file);
+	fd_install(fd, sync_file->file);
 
 	if (atomic_inc_return(&kcpu_queue->fence_signal_pending_cnt) == 1)
 		fence_signal_timeout_start(kcpu_queue);
@@ -1878,7 +1903,11 @@ static void kcpu_fence_timeout_dump(struct kbase_kcpu_command_queue *queue,
 	struct kbase_kcpu_command *cmd;
 	struct kbase_kcpu_command_fence_info *fence_info;
 	struct kbase_kcpu_dma_fence *kcpu_fence;
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence;
+#else
 	struct dma_fence *fence;
+#endif
 	struct kbase_sync_fence_info info;
 	u16 i;
 
@@ -1965,7 +1994,7 @@ static void kcpu_queue_process_worker(struct work_struct *data)
 		container_of(data, struct kbase_kcpu_command_queue, work);
 
 	mutex_lock(&queue->lock);
-	kbase_csf_kcpu_queue_process(queue, false);
+	kcpu_queue_process(queue, false);
 	mutex_unlock(&queue->lock);
 }
 
@@ -1998,7 +2027,7 @@ static int delete_queue(struct kbase_context *kctx, u32 id)
 		/* Drain the remaining work for this queue first and go past
 		 * all the waits.
 		 */
-		kbase_csf_kcpu_queue_process(queue, true);
+		kcpu_queue_process(queue, true);
 
 		/* All commands should have been processed */
 		WARN_ON(queue->num_pending_cmds);
@@ -2014,20 +2043,13 @@ static int delete_queue(struct kbase_context *kctx, u32 id)
 		mutex_unlock(&queue->lock);
 
 		cancel_work_sync(&queue->timeout_work);
-
-		/*
-		 * Drain a pending request to process this queue in
-		 * kbase_csf_scheduler_kthread() if any. By this point the
-		 * queue would be empty so this would be a no-op.
-		 */
-		kbase_csf_scheduler_wait_for_kthread_pending_work(kctx->kbdev,
-								  &queue->pending_kick);
-
 		cancel_work_sync(&queue->work);
 
+		destroy_workqueue(queue->wq);
+
 		mutex_destroy(&queue->lock);
 
-		vfree(queue);
+		kfree(queue);
 	} else {
 		dev_dbg(kctx->kbdev->dev, "Attempt to delete a non-existent KCPU queue");
 		mutex_unlock(&kctx->csf.kcpu_queues.lock);
@@ -2063,7 +2085,7 @@ static void KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_ALLOC_INFO(
 #endif /* CONFIG_MALI_VECTOR_DUMP */
 		}
 		KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(
-			kbdev, queue, (u32)alloc_status, gpu_alloc_addr, mmu_flags);
+			kbdev, queue, alloc_status, gpu_alloc_addr, mmu_flags);
 	}
 }
 
@@ -2080,7 +2102,7 @@ KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_FREE_END(struct kbase_device *kbde
 	KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_EXECUTE_JIT_FREE_END(kbdev, queue);
 }
 
-void kbase_csf_kcpu_queue_process(struct kbase_kcpu_command_queue *queue, bool drain_queue)
+static void kcpu_queue_process(struct kbase_kcpu_command_queue *queue, bool drain_queue)
 {
 	struct kbase_device *kbdev = queue->kctx->kbdev;
 	bool process_next = true;
@@ -2200,10 +2222,10 @@ void kbase_csf_kcpu_queue_process(struct kbase_kcpu_command_queue *queue, bool d
 				KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_START(kbdev,
 											   queue);
 
-				kbase_gpu_vm_lock_with_pmode_sync(queue->kctx);
+				kbase_gpu_vm_lock(queue->kctx);
 				meta = kbase_sticky_resource_acquire(queue->kctx,
-								     cmd->info.import.gpu_va, NULL);
-				kbase_gpu_vm_unlock_with_pmode_sync(queue->kctx);
+								     cmd->info.import.gpu_va);
+				kbase_gpu_vm_unlock(queue->kctx);
 
 				if (meta == NULL) {
 					queue->has_error = true;
@@ -2220,10 +2242,10 @@ void kbase_csf_kcpu_queue_process(struct kbase_kcpu_command_queue *queue, bool d
 
 			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_START(kbdev, queue);
 
-			kbase_gpu_vm_lock_with_pmode_sync(queue->kctx);
+			kbase_gpu_vm_lock(queue->kctx);
 			ret = kbase_sticky_resource_release(queue->kctx, NULL,
 							    cmd->info.import.gpu_va);
-			kbase_gpu_vm_unlock_with_pmode_sync(queue->kctx);
+			kbase_gpu_vm_unlock(queue->kctx);
 
 			if (!ret) {
 				queue->has_error = true;
@@ -2241,10 +2263,10 @@ void kbase_csf_kcpu_queue_process(struct kbase_kcpu_command_queue *queue, bool d
 			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_FORCE_START(kbdev,
 											   queue);
 
-			kbase_gpu_vm_lock_with_pmode_sync(queue->kctx);
+			kbase_gpu_vm_lock(queue->kctx);
 			ret = kbase_sticky_resource_release_force(queue->kctx, NULL,
 								  cmd->info.import.gpu_va);
-			kbase_gpu_vm_unlock_with_pmode_sync(queue->kctx);
+			kbase_gpu_vm_unlock(queue->kctx);
 
 			if (!ret) {
 				queue->has_error = true;
@@ -2562,7 +2584,7 @@ int kbase_csf_kcpu_queue_enqueue(struct kbase_context *kctx,
 			}
 		}
 
-		kcpu_cmd->enqueue_ts = (u64)atomic64_inc_return(&kctx->csf.kcpu_queues.cmd_seq_num);
+		kcpu_cmd->enqueue_ts = atomic64_inc_return(&kctx->csf.kcpu_queues.cmd_seq_num);
 		switch (command.type) {
 		case BASE_KCPU_COMMAND_TYPE_FENCE_WAIT:
 #if IS_ENABLED(CONFIG_SYNC_FILE)
@@ -2643,7 +2665,7 @@ int kbase_csf_kcpu_queue_enqueue(struct kbase_context *kctx,
 		}
 
 		queue->num_pending_cmds += enq->nr_commands;
-		kbase_csf_kcpu_queue_process(queue, false);
+		kcpu_queue_process(queue, false);
 	}
 
 out:
@@ -2654,14 +2676,6 @@ int kbase_csf_kcpu_queue_enqueue(struct kbase_context *kctx,
 
 int kbase_csf_kcpu_queue_context_init(struct kbase_context *kctx)
 {
-	kctx->csf.kcpu_queues.kcpu_wq =
-		alloc_workqueue("mali_kcpu_wq_%i_%i", 0, 0, kctx->tgid, kctx->id);
-	if (kctx->csf.kcpu_queues.kcpu_wq == NULL) {
-		dev_err(kctx->kbdev->dev,
-			"Failed to initialize KCPU queue high-priority workqueue");
-		return -ENOMEM;
-	}
-
 	mutex_init(&kctx->csf.kcpu_queues.lock);
 
 	return 0;
@@ -2675,12 +2689,10 @@ void kbase_csf_kcpu_queue_context_term(struct kbase_context *kctx)
 		if (WARN_ON(!kctx->csf.kcpu_queues.array[id]))
 			clear_bit(id, kctx->csf.kcpu_queues.in_use);
 		else
-			(void)delete_queue(kctx, (u32)id);
+			(void)delete_queue(kctx, id);
 	}
 
 	mutex_destroy(&kctx->csf.kcpu_queues.lock);
-
-	destroy_workqueue(kctx->csf.kcpu_queues.kcpu_wq);
 }
 KBASE_EXPORT_TEST_API(kbase_csf_kcpu_queue_context_term);
 
@@ -2690,42 +2702,15 @@ int kbase_csf_kcpu_queue_delete(struct kbase_context *kctx,
 	return delete_queue(kctx, (u32)del->id);
 }
 
-static struct kbase_kcpu_dma_fence_meta *
-kbase_csf_kcpu_queue_metadata_new(struct kbase_context *kctx, u64 fence_context)
-{
-	int n;
-	struct kbase_kcpu_dma_fence_meta *metadata = kzalloc(sizeof(*metadata), GFP_KERNEL);
-
-	if (!metadata)
-		goto early_ret;
-
-	*metadata = (struct kbase_kcpu_dma_fence_meta){
-		.kbdev = kctx->kbdev,
-		.kctx_id = kctx->id,
-	};
-
-	/* Please update MAX_TIMELINE_NAME macro when making changes to the string. */
-	n = scnprintf(metadata->timeline_name, MAX_TIMELINE_NAME, "%u-%d_%u-%llu-kcpu",
-		      kctx->kbdev->id, kctx->tgid, kctx->id, fence_context);
-	if (WARN_ON(n >= MAX_TIMELINE_NAME)) {
-		kfree(metadata);
-		metadata = NULL;
-		goto early_ret;
-	}
-
-	kbase_refcount_set(&metadata->refcount, 1);
-
-early_ret:
-	return metadata;
-}
-KBASE_ALLOW_ERROR_INJECTION_TEST_API(kbase_csf_kcpu_queue_metadata_new, ERRNO_NULL);
-
 int kbase_csf_kcpu_queue_new(struct kbase_context *kctx, struct kbase_ioctl_kcpu_queue_new *newq)
 {
 	struct kbase_kcpu_command_queue *queue;
-	struct kbase_kcpu_dma_fence_meta *metadata;
 	int idx;
+	int n;
 	int ret = 0;
+#if IS_ENABLED(CONFIG_SYNC_FILE)
+	struct kbase_kcpu_dma_fence_meta *metadata;
+#endif
 	/* The queue id is of u8 type and we use the index of the kcpu_queues
 	 * array as an id, so the number of elements in the array can't be
 	 * more than 256.
@@ -2745,48 +2730,64 @@ int kbase_csf_kcpu_queue_new(struct kbase_context *kctx, struct kbase_ioctl_kcpu
 		goto out;
 	}
 
-	queue = vzalloc(sizeof(*queue));
+	queue = kzalloc(sizeof(*queue), GFP_KERNEL);
+
 	if (!queue) {
 		ret = -ENOMEM;
 		goto out;
 	}
 
-	*queue = (struct kbase_kcpu_command_queue)
-	{
-		.kctx = kctx, .start_offset = 0, .num_pending_cmds = 0, .enqueue_failed = false,
-		.command_started = false, .has_error = false, .id = idx,
-#if IS_ENABLED(CONFIG_SYNC_FILE)
-		.fence_context = dma_fence_context_alloc(1), .fence_seqno = 0,
-		.fence_wait_processed = false,
-#endif /* IS_ENABLED(CONFIG_SYNC_FILE) */
-	};
+	queue->wq = alloc_workqueue("mali_kbase_csf_kcpu_wq_%i", WQ_UNBOUND | WQ_HIGHPRI, 0, idx);
+	if (queue->wq == NULL) {
+		kfree(queue);
+		ret = -ENOMEM;
 
-	mutex_init(&queue->lock);
-	INIT_WORK(&queue->work, kcpu_queue_process_worker);
-	INIT_LIST_HEAD(&queue->high_prio_work);
-	atomic_set(&queue->pending_kick, 0);
-	INIT_WORK(&queue->timeout_work, kcpu_queue_timeout_worker);
-	INIT_LIST_HEAD(&queue->jit_blocked);
+		goto out;
+	}
 
-	if (IS_ENABLED(CONFIG_SYNC_FILE)) {
-		metadata = kbase_csf_kcpu_queue_metadata_new(kctx, queue->fence_context);
-		if (!metadata) {
-			vfree(queue);
-			ret = -ENOMEM;
-			goto out;
-		}
+	bitmap_set(kctx->csf.kcpu_queues.in_use, idx, 1);
+	kctx->csf.kcpu_queues.array[idx] = queue;
+	mutex_init(&queue->lock);
+	queue->kctx = kctx;
+	queue->start_offset = 0;
+	queue->num_pending_cmds = 0;
+#if IS_ENABLED(CONFIG_SYNC_FILE)
+	queue->fence_context = dma_fence_context_alloc(1);
+	queue->fence_seqno = 0;
+	queue->fence_wait_processed = false;
+
+	metadata = kzalloc(sizeof(*metadata), GFP_KERNEL);
+	if (!metadata) {
+		destroy_workqueue(queue->wq);
+		kfree(queue);
+		ret = -ENOMEM;
+		goto out;
+	}
 
-		queue->metadata = metadata;
-		atomic_inc(&kctx->kbdev->live_fence_metadata);
-		atomic_set(&queue->fence_signal_pending_cnt, 0);
-		kbase_timer_setup(&queue->fence_signal_timeout, fence_signal_timeout_cb);
+	metadata->kbdev = kctx->kbdev;
+	metadata->kctx_id = kctx->id;
+	n = snprintf(metadata->timeline_name, MAX_TIMELINE_NAME, "%d-%d_%d-%lld-kcpu",
+		     kctx->kbdev->id, kctx->tgid, kctx->id, queue->fence_context);
+	if (WARN_ON(n >= MAX_TIMELINE_NAME)) {
+		destroy_workqueue(queue->wq);
+		kfree(queue);
+		kfree(metadata);
+		ret = -EINVAL;
+		goto out;
 	}
 
-	if (IS_ENABLED(CONFIG_MALI_BIFROST_FENCE_DEBUG))
-		kbase_timer_setup(&queue->fence_timeout, fence_timeout_callback);
+	kbase_refcount_set(&metadata->refcount, 1);
+	queue->metadata = metadata;
+	atomic_inc(&kctx->kbdev->live_fence_metadata);
+#endif /* CONFIG_SYNC_FILE */
+	queue->enqueue_failed = false;
+	queue->command_started = false;
+	INIT_LIST_HEAD(&queue->jit_blocked);
+	queue->has_error = false;
+	INIT_WORK(&queue->work, kcpu_queue_process_worker);
+	INIT_WORK(&queue->timeout_work, kcpu_queue_timeout_worker);
+	queue->id = idx;
 
-	bitmap_set(kctx->csf.kcpu_queues.in_use, (unsigned int)idx, 1);
-	kctx->csf.kcpu_queues.array[idx] = queue;
 	newq->id = idx;
 
 	/* Fire the tracepoint with the mutex held to enforce correct ordering
@@ -2796,6 +2797,14 @@ int kbase_csf_kcpu_queue_new(struct kbase_context *kctx, struct kbase_ioctl_kcpu
 					      queue->num_pending_cmds);
 
 	KBASE_KTRACE_ADD_CSF_KCPU(kctx->kbdev, KCPU_QUEUE_CREATE, queue, queue->fence_context, 0);
+#ifdef CONFIG_MALI_BIFROST_FENCE_DEBUG
+	kbase_timer_setup(&queue->fence_timeout, fence_timeout_callback);
+#endif
+
+#if IS_ENABLED(CONFIG_SYNC_FILE)
+	atomic_set(&queue->fence_signal_pending_cnt, 0);
+	kbase_timer_setup(&queue->fence_signal_timeout, fence_signal_timeout_cb);
+#endif
 out:
 	mutex_unlock(&kctx->csf.kcpu_queues.lock);
 
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.h
index 291509bef5a6..9ca33773941e 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.h
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.h
@@ -25,7 +25,11 @@
 #include <mali_kbase_fence.h>
 #include <mali_kbase_sync.h>
 
-#include <linux/version_compat_defs.h>
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+#include <linux/fence.h>
+#else
+#include <linux/dma-fence.h>
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0) */
 
 /* The maximum number of KCPU commands in flight, enqueueing more commands
  * than this value shall block.
@@ -52,8 +56,13 @@ struct kbase_kcpu_command_import_info {
  * @fence_has_force_signaled:	fence has forced signaled after fence timeouted
  */
 struct kbase_kcpu_command_fence_info {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence_cb fence_cb;
+	struct fence *fence;
+#else
 	struct dma_fence_cb fence_cb;
 	struct dma_fence *fence;
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0) */
 	struct kbase_kcpu_command_queue *kcpu_queue;
 	bool fence_has_force_signaled;
 };
@@ -240,22 +249,11 @@ struct kbase_kcpu_command {
  * @kctx:			The context to which this command queue belongs.
  * @commands:			Array of commands which have been successfully
  *				enqueued to this command queue.
+ * @wq:				Dedicated workqueue for processing commands.
  * @work:			struct work_struct which contains a pointer to
  *				the function which handles processing of kcpu
  *				commands enqueued into a kcpu command queue;
- *				part of kernel API for processing workqueues.
- *				This would be used if the context is not
- *				prioritised, otherwise it would be handled by
- *				kbase_csf_scheduler_kthread().
- * @high_prio_work:		A counterpart to @work, this queue would be
- *				added to a list to be processed by
- *				kbase_csf_scheduler_kthread() if it is
- *				prioritised.
- * @pending_kick:		Indicates that kbase_csf_scheduler_kthread()
- *				should re-evaluate pending commands for this
- *				queue. This would be set to false when the work
- *				is done. This is used mainly for
- *				synchronisation with queue termination.
+ *				part of kernel API for processing workqueues
  * @timeout_work:		struct work_struct which contains a pointer to the
  *				function which handles post-timeout actions
  *				queue when a fence signal timeout occurs.
@@ -298,9 +296,8 @@ struct kbase_kcpu_command_queue {
 	struct mutex lock;
 	struct kbase_context *kctx;
 	struct kbase_kcpu_command commands[KBASEP_KCPU_QUEUE_SIZE];
+	struct workqueue_struct *wq;
 	struct work_struct work;
-	struct list_head high_prio_work;
-	atomic_t pending_kick;
 	struct work_struct timeout_work;
 	u8 start_offset;
 	u8 id;
@@ -313,7 +310,9 @@ struct kbase_kcpu_command_queue {
 	bool command_started;
 	struct list_head jit_blocked;
 	bool has_error;
+#ifdef CONFIG_MALI_BIFROST_FENCE_DEBUG
 	struct timer_list fence_timeout;
+#endif /* CONFIG_MALI_BIFROST_FENCE_DEBUG */
 #if IS_ENABLED(CONFIG_SYNC_FILE)
 	struct kbase_kcpu_dma_fence_meta *metadata;
 #endif /* CONFIG_SYNC_FILE */
@@ -346,18 +345,6 @@ int kbase_csf_kcpu_queue_new(struct kbase_context *kctx, struct kbase_ioctl_kcpu
 int kbase_csf_kcpu_queue_delete(struct kbase_context *kctx,
 				struct kbase_ioctl_kcpu_queue_delete *del);
 
-/**
- * kbase_csf_kcpu_queue_process - Proces pending KCPU queue commands
- *
- * @queue:		The queue to process pending commands for
- * @drain_queue:	Whether to skip all blocking commands in the queue.
- *			This is expected to be set to true on queue
- *			termination.
- *
- * Return: 0 if successful or a negative error code on failure.
- */
-void kbase_csf_kcpu_queue_process(struct kbase_kcpu_command_queue *queue, bool drain_queue);
-
 /**
  * kbase_csf_kcpu_queue_enqueue - Enqueue a KCPU command into a KCPU command
  *				  queue.
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_fence_debugfs.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_fence_debugfs.c
index 1433aae613b4..9cbf7bab5caa 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_fence_debugfs.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_fence_debugfs.c
@@ -60,9 +60,9 @@ static ssize_t kbase_csf_kcpu_queue_fence_signal_enabled_set(struct file *file,
 	if (ret < 0)
 		return ret;
 
-	atomic_set(&kbdev->fence_signal_timeout_enabled, (int)enabled);
+	atomic_set(&kbdev->fence_signal_timeout_enabled, enabled);
 
-	return (ssize_t)count;
+	return count;
 }
 
 static const struct file_operations kbase_csf_kcpu_queue_fence_signal_fops = {
@@ -82,7 +82,7 @@ static ssize_t kbase_csf_kcpu_queue_fence_signal_timeout_get(struct file *file,
 	unsigned int timeout_ms = kbase_get_timeout_ms(kbdev, KCPU_FENCE_SIGNAL_TIMEOUT);
 
 	size = scnprintf(buffer, sizeof(buffer), "%u\n", timeout_ms);
-	return simple_read_from_buffer(buf, count, ppos, buffer, (size_t)size);
+	return simple_read_from_buffer(buf, count, ppos, buffer, size);
 }
 
 static ssize_t kbase_csf_kcpu_queue_fence_signal_timeout_set(struct file *file,
@@ -105,7 +105,7 @@ static ssize_t kbase_csf_kcpu_queue_fence_signal_timeout_set(struct file *file,
 	 */
 	kbase_device_set_timeout_ms(kbdev, KCPU_FENCE_SIGNAL_TIMEOUT, timeout_ms);
 
-	return (ssize_t)count;
+	return count;
 }
 
 static const struct file_operations kbase_csf_kcpu_queue_fence_signal_timeout_fops = {
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_mcu_shared_reg.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_mcu_shared_reg.c
index c8cf17f08e6f..11c0ba499596 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_mcu_shared_reg.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_mcu_shared_reg.c
@@ -412,7 +412,7 @@ int kbase_csf_mcu_shared_add_queue(struct kbase_device *kbdev, struct kbase_queu
 		      "No bound csg_reg, or in wrong state"))
 		return -EIO;
 
-	vpfn = CSG_REG_USERIO_VPFN(csg_reg->reg, (u32)queue->csi_index, nr_susp_pages);
+	vpfn = CSG_REG_USERIO_VPFN(csg_reg->reg, queue->csi_index, nr_susp_pages);
 	err = userio_pages_replace_phys(kbdev, vpfn, queue->phys);
 	if (likely(!err)) {
 		/* Mark the queue has been successfully mapped */
@@ -452,7 +452,7 @@ void kbase_csf_mcu_shared_drop_stopped_queue(struct kbase_device *kbdev, struct
 
 	csg_reg = get_group_bound_csg_reg(group);
 
-	vpfn = CSG_REG_USERIO_VPFN(csg_reg->reg, (u32)queue->csi_index, nr_susp_pages);
+	vpfn = CSG_REG_USERIO_VPFN(csg_reg->reg, queue->csi_index, nr_susp_pages);
 
 	WARN_ONCE(userio_pages_replace_phys(kbdev, vpfn, NULL),
 		  "Unexpected restoring to dummy map update error");
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.c
index c646e785e89a..e78144ac4bf3 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.c
@@ -79,7 +79,7 @@ kbase_csf_protected_memory_alloc(struct kbase_device *const kbdev, struct tagged
 	unsigned int num_pages_order;
 
 	if (is_small_page)
-		order = KBASE_MEM_POOL_SMALL_PAGE_TABLE_ORDER;
+		order = KBASE_MEM_POOL_4KB_PAGE_TABLE_ORDER;
 
 	num_pages_order = (1u << order);
 
@@ -135,7 +135,7 @@ void kbase_csf_protected_memory_free(struct kbase_device *const kbdev,
 	unsigned int num_pages_order = (1u << KBASE_MEM_POOL_2MB_PAGE_TABLE_ORDER);
 
 	if (is_small_page)
-		num_pages_order = (1u << KBASE_MEM_POOL_SMALL_PAGE_TABLE_ORDER);
+		num_pages_order = (1u << KBASE_MEM_POOL_4KB_PAGE_TABLE_ORDER);
 
 	if (WARN_ON(!pma_dev) || WARN_ON(!pma))
 		return;
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_registers.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_registers.h
index 7e96a9d01fc7..c4e6e4ac0df7 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_registers.h
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_registers.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -172,11 +172,6 @@
 #define CSG_STATUS_EP_CURRENT 0x0010 /* () Endpoint allocation status register */
 #define CSG_STATUS_EP_REQ 0x0014 /* () Endpoint request status register */
 #define CSG_RESOURCE_DEP 0x001C /* () Current resource dependencies */
-/* TODO: GPUCORE-xxxx: Remove after spec alignment, use 0x1C as CSG_RESOURCE_DEP is deprecated*/
-/* CSG_OUTPUT_BLOCK register offsets */
-#ifndef CSG_PROGRESS_TIMER_STATE
-#define CSG_PROGRESS_TIMER_STATE 0x001C /* () Current resource status */
-#endif
 
 /* GLB_CONTROL_BLOCK register offsets */
 #define GLB_VERSION 0x0000 /* () Global interface version */
@@ -255,7 +250,7 @@
 
 #define GLB_ACK 0x0000 /* () Global acknowledge */
 #define GLB_DB_ACK 0x0008 /* () Global doorbell acknowledge */
-#define GLB_FATAL_STATUS 0x0010 /* () Global fatal error status */
+#define GLB_HALT_STATUS 0x0010 /* () Global halt status */
 #define GLB_PRFCNT_STATUS 0x0014 /* () Performance counter status */
 #define GLB_PRFCNT_INSERT 0x0018 /* () Performance counter buffer insert index */
 #define GLB_DEBUG_FWUTF_RESULT GLB_DEBUG_ARG_OUT0 /* () Firmware debug test result */
@@ -276,21 +271,21 @@
 /* CS_KERNEL_INPUT_BLOCK register set definitions */
 /* GLB_VERSION register */
 #define GLB_VERSION_PATCH_SHIFT (0)
-#define GLB_VERSION_PATCH_MASK ((0xFFFFU) << GLB_VERSION_PATCH_SHIFT)
+#define GLB_VERSION_PATCH_MASK ((0xFFFF) << GLB_VERSION_PATCH_SHIFT)
 #define GLB_VERSION_PATCH_GET(reg_val) \
 	(((reg_val)&GLB_VERSION_PATCH_MASK) >> GLB_VERSION_PATCH_SHIFT)
 #define GLB_VERSION_PATCH_SET(reg_val, value)    \
 	(((reg_val) & ~GLB_VERSION_PATCH_MASK) | \
 	 (((value) << GLB_VERSION_PATCH_SHIFT) & GLB_VERSION_PATCH_MASK))
 #define GLB_VERSION_MINOR_SHIFT (16)
-#define GLB_VERSION_MINOR_MASK ((0xFFU) << GLB_VERSION_MINOR_SHIFT)
+#define GLB_VERSION_MINOR_MASK ((0xFF) << GLB_VERSION_MINOR_SHIFT)
 #define GLB_VERSION_MINOR_GET(reg_val) \
 	(((reg_val)&GLB_VERSION_MINOR_MASK) >> GLB_VERSION_MINOR_SHIFT)
 #define GLB_VERSION_MINOR_SET(reg_val, value)    \
 	(((reg_val) & ~GLB_VERSION_MINOR_MASK) | \
 	 (((value) << GLB_VERSION_MINOR_SHIFT) & GLB_VERSION_MINOR_MASK))
 #define GLB_VERSION_MAJOR_SHIFT (24)
-#define GLB_VERSION_MAJOR_MASK ((0xFFU) << GLB_VERSION_MAJOR_SHIFT)
+#define GLB_VERSION_MAJOR_MASK ((0xFF) << GLB_VERSION_MAJOR_SHIFT)
 #define GLB_VERSION_MAJOR_GET(reg_val) \
 	(((reg_val)&GLB_VERSION_MAJOR_MASK) >> GLB_VERSION_MAJOR_SHIFT)
 #define GLB_VERSION_MAJOR_SET(reg_val, value)    \
@@ -316,60 +311,60 @@
 	 (((value) << CS_REQ_EXTRACT_EVENT_SHIFT) & CS_REQ_EXTRACT_EVENT_MASK))
 
 #define CS_REQ_IDLE_SYNC_WAIT_SHIFT 8
-#define CS_REQ_IDLE_SYNC_WAIT_MASK (0x1U << CS_REQ_IDLE_SYNC_WAIT_SHIFT)
+#define CS_REQ_IDLE_SYNC_WAIT_MASK (0x1 << CS_REQ_IDLE_SYNC_WAIT_SHIFT)
 #define CS_REQ_IDLE_SYNC_WAIT_GET(reg_val) \
 	(((reg_val)&CS_REQ_IDLE_SYNC_WAIT_MASK) >> CS_REQ_IDLE_SYNC_WAIT_SHIFT)
 #define CS_REQ_IDLE_SYNC_WAIT_SET(reg_val, value)    \
 	(((reg_val) & ~CS_REQ_IDLE_SYNC_WAIT_MASK) | \
 	 (((value) << CS_REQ_IDLE_SYNC_WAIT_SHIFT) & CS_REQ_IDLE_SYNC_WAIT_MASK))
 #define CS_REQ_IDLE_PROTM_PEND_SHIFT 9
-#define CS_REQ_IDLE_PROTM_PEND_MASK (0x1U << CS_REQ_IDLE_PROTM_PEND_SHIFT)
+#define CS_REQ_IDLE_PROTM_PEND_MASK (0x1 << CS_REQ_IDLE_PROTM_PEND_SHIFT)
 #define CS_REQ_IDLE_PROTM_PEND_GET(reg_val) \
 	(((reg_val)&CS_REQ_IDLE_PROTM_PEND_MASK) >> CS_REQ_IDLE_PROTM_PEND_SHIFT)
 #define CS_REQ_IDLE_PROTM_PEND_SET(reg_val, value)    \
 	(((reg_val) & ~CS_REQ_IDLE_PROTM_PEND_MASK) | \
 	 (((value) << CS_REQ_IDLE_PROTM_PEND_SHIFT) & CS_REQ_IDLE_PROTM_PEND_MASK))
 #define CS_REQ_IDLE_EMPTY_SHIFT 10
-#define CS_REQ_IDLE_EMPTY_MASK (0x1U << CS_REQ_IDLE_EMPTY_SHIFT)
+#define CS_REQ_IDLE_EMPTY_MASK (0x1 << CS_REQ_IDLE_EMPTY_SHIFT)
 #define CS_REQ_IDLE_EMPTY_GET(reg_val) \
 	(((reg_val)&CS_REQ_IDLE_EMPTY_MASK) >> CS_REQ_IDLE_EMPTY_SHIFT)
 #define CS_REQ_IDLE_EMPTY_SET(reg_val, value)    \
 	(((reg_val) & ~CS_REQ_IDLE_EMPTY_MASK) | \
 	 (((value) << CS_REQ_IDLE_EMPTY_SHIFT) & CS_REQ_IDLE_EMPTY_MASK))
 #define CS_REQ_IDLE_RESOURCE_REQ_SHIFT 11
-#define CS_REQ_IDLE_RESOURCE_REQ_MASK (0x1U << CS_REQ_IDLE_RESOURCE_REQ_SHIFT)
+#define CS_REQ_IDLE_RESOURCE_REQ_MASK (0x1 << CS_REQ_IDLE_RESOURCE_REQ_SHIFT)
 #define CS_REQ_IDLE_RESOURCE_REQ_GET(reg_val) \
 	(((reg_val)&CS_REQ_IDLE_RESOURCE_REQ_MASK) >> CS_REQ_IDLE_RESOURCE_REQ_SHIFT)
 #define CS_REQ_IDLE_RESOURCE_REQ_SET(reg_val, value)    \
 	(((reg_val) & ~CS_REQ_IDLE_RESOURCE_REQ_MASK) | \
 	 (((value) << CS_REQ_IDLE_RESOURCE_REQ_SHIFT) & CS_REQ_IDLE_RESOURCE_REQ_MASK))
 #define CS_REQ_IDLE_SHARED_SB_DEC_SHIFT 12
-#define CS_REQ_IDLE_SHARED_SB_DEC_MASK (0x1U << CS_REQ_IDLE_SHARED_SB_DEC_SHIFT)
+#define CS_REQ_IDLE_SHARED_SB_DEC_MASK (0x1 << CS_REQ_IDLE_SHARED_SB_DEC_SHIFT)
 #define CS_REQ_IDLE_SHARED_SB_DEC_GET(reg_val) \
 	(((reg_val)&CS_REQ_IDLE_SHARED_SB_DEC_MASK) >> CS_REQ_IDLE_SHARED_SB_DEC_SHIFT)
 #define CS_REQ_IDLE_SHARED_SB_DEC_REQ_SET(reg_val, value) \
 	(((reg_val) & ~CS_REQ_IDLE_SHARED_SB_DEC_MASK) |  \
 	 (((value) << CS_REQ_IDLE_SHARED_SB_DEC_SHIFT) & CS_REQ_IDLE_SHARED_SB_DEC_MASK))
 #define CS_REQ_TILER_OOM_SHIFT 26
-#define CS_REQ_TILER_OOM_MASK (0x1U << CS_REQ_TILER_OOM_SHIFT)
+#define CS_REQ_TILER_OOM_MASK (0x1 << CS_REQ_TILER_OOM_SHIFT)
 #define CS_REQ_TILER_OOM_GET(reg_val) (((reg_val)&CS_REQ_TILER_OOM_MASK) >> CS_REQ_TILER_OOM_SHIFT)
 #define CS_REQ_TILER_OOM_SET(reg_val, value)    \
 	(((reg_val) & ~CS_REQ_TILER_OOM_MASK) | \
 	 (((value) << CS_REQ_TILER_OOM_SHIFT) & CS_REQ_TILER_OOM_MASK))
 #define CS_REQ_PROTM_PEND_SHIFT 27
-#define CS_REQ_PROTM_PEND_MASK (0x1U << CS_REQ_PROTM_PEND_SHIFT)
+#define CS_REQ_PROTM_PEND_MASK (0x1 << CS_REQ_PROTM_PEND_SHIFT)
 #define CS_REQ_PROTM_PEND_GET(reg_val) \
 	(((reg_val)&CS_REQ_PROTM_PEND_MASK) >> CS_REQ_PROTM_PEND_SHIFT)
 #define CS_REQ_PROTM_PEND_SET(reg_val, value)    \
 	(((reg_val) & ~CS_REQ_PROTM_PEND_MASK) | \
 	 (((value) << CS_REQ_PROTM_PEND_SHIFT) & CS_REQ_PROTM_PEND_MASK))
 #define CS_REQ_FATAL_SHIFT 30
-#define CS_REQ_FATAL_MASK (0x1U << CS_REQ_FATAL_SHIFT)
+#define CS_REQ_FATAL_MASK (0x1 << CS_REQ_FATAL_SHIFT)
 #define CS_REQ_FATAL_GET(reg_val) (((reg_val)&CS_REQ_FATAL_MASK) >> CS_REQ_FATAL_SHIFT)
 #define CS_REQ_FATAL_SET(reg_val, value) \
 	(((reg_val) & ~CS_REQ_FATAL_MASK) | (((value) << CS_REQ_FATAL_SHIFT) & CS_REQ_FATAL_MASK))
 #define CS_REQ_FAULT_SHIFT 31
-#define CS_REQ_FAULT_MASK (0x1U << CS_REQ_FAULT_SHIFT)
+#define CS_REQ_FAULT_MASK (0x1 << CS_REQ_FAULT_SHIFT)
 #define CS_REQ_FAULT_GET(reg_val) (((reg_val)&CS_REQ_FAULT_MASK) >> CS_REQ_FAULT_SHIFT)
 #define CS_REQ_FAULT_SET(reg_val, value) \
 	(((reg_val) & ~CS_REQ_FAULT_MASK) | (((value) << CS_REQ_FAULT_SHIFT) & CS_REQ_FAULT_MASK))
@@ -564,32 +559,32 @@
 #define CS_ACK_STATE_START 0x1
 /* End of CS_ACK_STATE values */
 #define CS_ACK_EXTRACT_EVENT_SHIFT 4
-#define CS_ACK_EXTRACT_EVENT_MASK (0x1U << CS_ACK_EXTRACT_EVENT_SHIFT)
+#define CS_ACK_EXTRACT_EVENT_MASK (0x1 << CS_ACK_EXTRACT_EVENT_SHIFT)
 #define CS_ACK_EXTRACT_EVENT_GET(reg_val) \
 	(((reg_val)&CS_ACK_EXTRACT_EVENT_MASK) >> CS_ACK_EXTRACT_EVENT_SHIFT)
 #define CS_ACK_EXTRACT_EVENT_SET(reg_val, value)    \
 	(((reg_val) & ~CS_ACK_EXTRACT_EVENT_MASK) | \
 	 (((value) << CS_ACK_EXTRACT_EVENT_SHIFT) & CS_ACK_EXTRACT_EVENT_MASK))
 #define CS_ACK_TILER_OOM_SHIFT 26
-#define CS_ACK_TILER_OOM_MASK (0x1U << CS_ACK_TILER_OOM_SHIFT)
+#define CS_ACK_TILER_OOM_MASK (0x1 << CS_ACK_TILER_OOM_SHIFT)
 #define CS_ACK_TILER_OOM_GET(reg_val) (((reg_val)&CS_ACK_TILER_OOM_MASK) >> CS_ACK_TILER_OOM_SHIFT)
 #define CS_ACK_TILER_OOM_SET(reg_val, value)    \
 	(((reg_val) & ~CS_ACK_TILER_OOM_MASK) | \
 	 (((value) << CS_ACK_TILER_OOM_SHIFT) & CS_ACK_TILER_OOM_MASK))
 #define CS_ACK_PROTM_PEND_SHIFT 27
-#define CS_ACK_PROTM_PEND_MASK (0x1U << CS_ACK_PROTM_PEND_SHIFT)
+#define CS_ACK_PROTM_PEND_MASK (0x1 << CS_ACK_PROTM_PEND_SHIFT)
 #define CS_ACK_PROTM_PEND_GET(reg_val) \
 	(((reg_val)&CS_ACK_PROTM_PEND_MASK) >> CS_ACK_PROTM_PEND_SHIFT)
 #define CS_ACK_PROTM_PEND_SET(reg_val, value)    \
 	(((reg_val) & ~CS_ACK_PROTM_PEND_MASK) | \
 	 (((value) << CS_ACK_PROTM_PEND_SHIFT) & CS_ACK_PROTM_PEND_MASK))
 #define CS_ACK_FATAL_SHIFT 30
-#define CS_ACK_FATAL_MASK (0x1U << CS_ACK_FATAL_SHIFT)
+#define CS_ACK_FATAL_MASK (0x1 << CS_ACK_FATAL_SHIFT)
 #define CS_ACK_FATAL_GET(reg_val) (((reg_val)&CS_ACK_FATAL_MASK) >> CS_ACK_FATAL_SHIFT)
 #define CS_ACK_FATAL_SET(reg_val, value) \
 	(((reg_val) & ~CS_ACK_FATAL_MASK) | (((value) << CS_ACK_FATAL_SHIFT) & CS_ACK_FATAL_MASK))
 #define CS_ACK_FAULT_SHIFT 31
-#define CS_ACK_FAULT_MASK (0x1U << CS_ACK_FAULT_SHIFT)
+#define CS_ACK_FAULT_MASK (0x1 << CS_ACK_FAULT_SHIFT)
 #define CS_ACK_FAULT_GET(reg_val) (((reg_val)&CS_ACK_FAULT_MASK) >> CS_ACK_FAULT_SHIFT)
 #define CS_ACK_FAULT_SET(reg_val, value) \
 	(((reg_val) & ~CS_ACK_FAULT_MASK) | (((value) << CS_ACK_FAULT_SHIFT) & CS_ACK_FAULT_MASK))
@@ -606,21 +601,21 @@
 
 /* CS_STATUS_WAIT register */
 #define CS_STATUS_WAIT_SB_MASK_SHIFT 0
-#define CS_STATUS_WAIT_SB_MASK_MASK (0xFFFFU << CS_STATUS_WAIT_SB_MASK_SHIFT)
+#define CS_STATUS_WAIT_SB_MASK_MASK (0xFFFF << CS_STATUS_WAIT_SB_MASK_SHIFT)
 #define CS_STATUS_WAIT_SB_MASK_GET(reg_val) \
 	(((reg_val)&CS_STATUS_WAIT_SB_MASK_MASK) >> CS_STATUS_WAIT_SB_MASK_SHIFT)
 #define CS_STATUS_WAIT_SB_MASK_SET(reg_val, value)    \
 	(((reg_val) & ~CS_STATUS_WAIT_SB_MASK_MASK) | \
 	 (((value) << CS_STATUS_WAIT_SB_MASK_SHIFT) & CS_STATUS_WAIT_SB_MASK_MASK))
 #define CS_STATUS_WAIT_SB_SOURCE_SHIFT 16
-#define CS_STATUS_WAIT_SB_SOURCE_MASK (0xFU << CS_STATUS_WAIT_SB_SOURCE_SHIFT)
+#define CS_STATUS_WAIT_SB_SOURCE_MASK (0xF << CS_STATUS_WAIT_SB_SOURCE_SHIFT)
 #define CS_STATUS_WAIT_SB_SOURCE_GET(reg_val) \
 	(((reg_val)&CS_STATUS_WAIT_SB_SOURCE_MASK) >> CS_STATUS_WAIT_SB_SOURCE_SHIFT)
 #define CS_STATUS_WAIT_SB_SOURCE_SET(reg_val, value)    \
 	(((reg_val) & ~CS_STATUS_WAIT_SB_SOURCE_MASK) | \
 	 (((value) << CS_STATUS_WAIT_SB_SOURCE_SHIFT) & CS_STATUS_WAIT_SB_SOURCE_MASK))
 #define CS_STATUS_WAIT_SYNC_WAIT_CONDITION_SHIFT 24
-#define CS_STATUS_WAIT_SYNC_WAIT_CONDITION_MASK (0xFU << CS_STATUS_WAIT_SYNC_WAIT_CONDITION_SHIFT)
+#define CS_STATUS_WAIT_SYNC_WAIT_CONDITION_MASK (0xF << CS_STATUS_WAIT_SYNC_WAIT_CONDITION_SHIFT)
 #define CS_STATUS_WAIT_SYNC_WAIT_CONDITION_GET(reg_val)         \
 	(((reg_val)&CS_STATUS_WAIT_SYNC_WAIT_CONDITION_MASK) >> \
 	 CS_STATUS_WAIT_SYNC_WAIT_CONDITION_SHIFT)
@@ -635,28 +630,28 @@
 /* End of CS_STATUS_WAIT_SYNC_WAIT_CONDITION values */
 /* PROGRESS_WAIT is only for before v14.x.4 */
 #define CS_STATUS_WAIT_PROGRESS_WAIT_SHIFT 28
-#define CS_STATUS_WAIT_PROGRESS_WAIT_MASK (0x1U << CS_STATUS_WAIT_PROGRESS_WAIT_SHIFT)
+#define CS_STATUS_WAIT_PROGRESS_WAIT_MASK (0x1 << CS_STATUS_WAIT_PROGRESS_WAIT_SHIFT)
 #define CS_STATUS_WAIT_PROGRESS_WAIT_GET(reg_val) \
 	(((reg_val)&CS_STATUS_WAIT_PROGRESS_WAIT_MASK) >> CS_STATUS_WAIT_PROGRESS_WAIT_SHIFT)
 #define CS_STATUS_WAIT_PROGRESS_WAIT_SET(reg_val, value)    \
 	(((reg_val) & ~CS_STATUS_WAIT_PROGRESS_WAIT_MASK) | \
 	 (((value) << CS_STATUS_WAIT_PROGRESS_WAIT_SHIFT) & CS_STATUS_WAIT_PROGRESS_WAIT_MASK))
 #define CS_STATUS_WAIT_PROTM_PEND_SHIFT 29
-#define CS_STATUS_WAIT_PROTM_PEND_MASK (0x1U << CS_STATUS_WAIT_PROTM_PEND_SHIFT)
+#define CS_STATUS_WAIT_PROTM_PEND_MASK (0x1 << CS_STATUS_WAIT_PROTM_PEND_SHIFT)
 #define CS_STATUS_WAIT_PROTM_PEND_GET(reg_val) \
 	(((reg_val)&CS_STATUS_WAIT_PROTM_PEND_MASK) >> CS_STATUS_WAIT_PROTM_PEND_SHIFT)
 #define CS_STATUS_WAIT_PROTM_PEND_SET(reg_val, value)    \
 	(((reg_val) & ~CS_STATUS_WAIT_PROTM_PEND_MASK) | \
 	 (((value) << CS_STATUS_WAIT_PROTM_PEND_SHIFT) & CS_STATUS_WAIT_PROTM_PEND_MASK))
 #define CS_STATUS_WAIT_SYNC_WAIT_SIZE_SHIFT 30
-#define CS_STATUS_WAIT_SYNC_WAIT_SIZE_MASK (0x1U << CS_STATUS_WAIT_SYNC_WAIT_SIZE_SHIFT)
+#define CS_STATUS_WAIT_SYNC_WAIT_SIZE_MASK (0x1 << CS_STATUS_WAIT_SYNC_WAIT_SIZE_SHIFT)
 #define CS_STATUS_WAIT_SYNC_WAIT_SIZE_GET(reg_val) \
 	(((reg_val)&CS_STATUS_WAIT_SYNC_WAIT_SIZE_MASK) >> CS_STATUS_WAIT_SYNC_WAIT_SIZE_SHIFT)
 #define CS_STATUS_WAIT_SYNC_WAIT_SIZE_SET(reg_val, value)    \
 	(((reg_val) & ~CS_STATUS_WAIT_SYNC_WAIT_SIZE_MASK) | \
 	 (((value) << CS_STATUS_WAIT_SYNC_WAIT_SIZE_SHIFT) & CS_STATUS_WAIT_SYNC_WAIT_SIZE_MASK))
 #define CS_STATUS_WAIT_SYNC_WAIT_SHIFT 31
-#define CS_STATUS_WAIT_SYNC_WAIT_MASK (0x1U << CS_STATUS_WAIT_SYNC_WAIT_SHIFT)
+#define CS_STATUS_WAIT_SYNC_WAIT_MASK (0x1 << CS_STATUS_WAIT_SYNC_WAIT_SHIFT)
 #define CS_STATUS_WAIT_SYNC_WAIT_GET(reg_val) \
 	(((reg_val)&CS_STATUS_WAIT_SYNC_WAIT_MASK) >> CS_STATUS_WAIT_SYNC_WAIT_SHIFT)
 #define CS_STATUS_WAIT_SYNC_WAIT_SET(reg_val, value)    \
@@ -759,7 +754,7 @@
 
 /* CS_FAULT register */
 #define CS_FAULT_EXCEPTION_TYPE_SHIFT 0
-#define CS_FAULT_EXCEPTION_TYPE_MASK (0xFFU << CS_FAULT_EXCEPTION_TYPE_SHIFT)
+#define CS_FAULT_EXCEPTION_TYPE_MASK (0xFF << CS_FAULT_EXCEPTION_TYPE_SHIFT)
 #define CS_FAULT_EXCEPTION_TYPE_GET(reg_val) \
 	(((reg_val)&CS_FAULT_EXCEPTION_TYPE_MASK) >> CS_FAULT_EXCEPTION_TYPE_SHIFT)
 #define CS_FAULT_EXCEPTION_TYPE_SET(reg_val, value)    \
@@ -801,7 +796,7 @@
 #define CS_FAULT_EXCEPTION_TYPE_MEMORY_ATTRIBUTE_FAULT_3 0xEB
 /* End of CS_FAULT_EXCEPTION_TYPE values */
 #define CS_FAULT_EXCEPTION_DATA_SHIFT 8
-#define CS_FAULT_EXCEPTION_DATA_MASK (0xFFFFFFU << CS_FAULT_EXCEPTION_DATA_SHIFT)
+#define CS_FAULT_EXCEPTION_DATA_MASK (0xFFFFFF << CS_FAULT_EXCEPTION_DATA_SHIFT)
 #define CS_FAULT_EXCEPTION_DATA_GET(reg_val) \
 	(((reg_val)&CS_FAULT_EXCEPTION_DATA_MASK) >> CS_FAULT_EXCEPTION_DATA_SHIFT)
 #define CS_FAULT_EXCEPTION_DATA_SET(reg_val, value)    \
@@ -826,7 +821,7 @@
 #define CS_FATAL_EXCEPTION_TYPE_FIRMWARE_INTERNAL_ERROR 0x68
 /* End of CS_FATAL_EXCEPTION_TYPE values */
 #define CS_FATAL_EXCEPTION_DATA_SHIFT 8
-#define CS_FATAL_EXCEPTION_DATA_MASK (0xFFFFFFU << CS_FATAL_EXCEPTION_DATA_SHIFT)
+#define CS_FATAL_EXCEPTION_DATA_MASK (0xFFFFFF << CS_FATAL_EXCEPTION_DATA_SHIFT)
 #define CS_FATAL_EXCEPTION_DATA_GET(reg_val) \
 	(((reg_val)&CS_FATAL_EXCEPTION_DATA_MASK) >> CS_FATAL_EXCEPTION_DATA_SHIFT)
 #define CS_FATAL_EXCEPTION_DATA_SET(reg_val, value)    \
@@ -946,32 +941,32 @@
 #define CSG_REQ_STATE_RESUME 0x3
 /* End of CSG_REQ_STATE values */
 #define CSG_REQ_EP_CFG_SHIFT 4
-#define CSG_REQ_EP_CFG_MASK (0x1U << CSG_REQ_EP_CFG_SHIFT)
+#define CSG_REQ_EP_CFG_MASK (0x1 << CSG_REQ_EP_CFG_SHIFT)
 #define CSG_REQ_EP_CFG_GET(reg_val) (((reg_val)&CSG_REQ_EP_CFG_MASK) >> CSG_REQ_EP_CFG_SHIFT)
 #define CSG_REQ_EP_CFG_SET(reg_val, value)    \
 	(((reg_val) & ~CSG_REQ_EP_CFG_MASK) | \
 	 (((value) << CSG_REQ_EP_CFG_SHIFT) & CSG_REQ_EP_CFG_MASK))
 #define CSG_REQ_STATUS_UPDATE_SHIFT 5
-#define CSG_REQ_STATUS_UPDATE_MASK (0x1U << CSG_REQ_STATUS_UPDATE_SHIFT)
+#define CSG_REQ_STATUS_UPDATE_MASK (0x1 << CSG_REQ_STATUS_UPDATE_SHIFT)
 #define CSG_REQ_STATUS_UPDATE_GET(reg_val) \
 	(((reg_val)&CSG_REQ_STATUS_UPDATE_MASK) >> CSG_REQ_STATUS_UPDATE_SHIFT)
 #define CSG_REQ_STATUS_UPDATE_SET(reg_val, value)    \
 	(((reg_val) & ~CSG_REQ_STATUS_UPDATE_MASK) | \
 	 (((value) << CSG_REQ_STATUS_UPDATE_SHIFT) & CSG_REQ_STATUS_UPDATE_MASK))
 #define CSG_REQ_SYNC_UPDATE_SHIFT 28
-#define CSG_REQ_SYNC_UPDATE_MASK (0x1U << CSG_REQ_SYNC_UPDATE_SHIFT)
+#define CSG_REQ_SYNC_UPDATE_MASK (0x1 << CSG_REQ_SYNC_UPDATE_SHIFT)
 #define CSG_REQ_SYNC_UPDATE_GET(reg_val) \
 	(((reg_val)&CSG_REQ_SYNC_UPDATE_MASK) >> CSG_REQ_SYNC_UPDATE_SHIFT)
 #define CSG_REQ_SYNC_UPDATE_SET(reg_val, value)    \
 	(((reg_val) & ~CSG_REQ_SYNC_UPDATE_MASK) | \
 	 (((value) << CSG_REQ_SYNC_UPDATE_SHIFT) & CSG_REQ_SYNC_UPDATE_MASK))
 #define CSG_REQ_IDLE_SHIFT 29
-#define CSG_REQ_IDLE_MASK (0x1U << CSG_REQ_IDLE_SHIFT)
+#define CSG_REQ_IDLE_MASK (0x1 << CSG_REQ_IDLE_SHIFT)
 #define CSG_REQ_IDLE_GET(reg_val) (((reg_val)&CSG_REQ_IDLE_MASK) >> CSG_REQ_IDLE_SHIFT)
 #define CSG_REQ_IDLE_SET(reg_val, value) \
 	(((reg_val) & ~CSG_REQ_IDLE_MASK) | (((value) << CSG_REQ_IDLE_SHIFT) & CSG_REQ_IDLE_MASK))
 #define CSG_REQ_PROGRESS_TIMER_EVENT_SHIFT 31
-#define CSG_REQ_PROGRESS_TIMER_EVENT_MASK (0x1U << CSG_REQ_PROGRESS_TIMER_EVENT_SHIFT)
+#define CSG_REQ_PROGRESS_TIMER_EVENT_MASK (0x1 << CSG_REQ_PROGRESS_TIMER_EVENT_SHIFT)
 #define CSG_REQ_PROGRESS_TIMER_EVENT_GET(reg_val) \
 	(((reg_val)&CSG_REQ_PROGRESS_TIMER_EVENT_MASK) >> CSG_REQ_PROGRESS_TIMER_EVENT_SHIFT)
 #define CSG_REQ_PROGRESS_TIMER_EVENT_SET(reg_val, value)    \
@@ -1138,38 +1133,38 @@
 #define CSG_ACK_STATE_RESUME 0x3
 /* End of CSG_ACK_STATE values */
 #define CSG_ACK_EP_CFG_SHIFT 4
-#define CSG_ACK_EP_CFG_MASK (0x1U << CSG_ACK_EP_CFG_SHIFT)
+#define CSG_ACK_EP_CFG_MASK (0x1 << CSG_ACK_EP_CFG_SHIFT)
 #define CSG_ACK_EP_CFG_GET(reg_val) (((reg_val)&CSG_ACK_EP_CFG_MASK) >> CSG_ACK_EP_CFG_SHIFT)
 #define CSG_ACK_EP_CFG_SET(reg_val, value)    \
 	(((reg_val) & ~CSG_ACK_EP_CFG_MASK) | \
 	 (((value) << CSG_ACK_EP_CFG_SHIFT) & CSG_ACK_EP_CFG_MASK))
 #define CSG_ACK_STATUS_UPDATE_SHIFT 5
-#define CSG_ACK_STATUS_UPDATE_MASK (0x1U << CSG_ACK_STATUS_UPDATE_SHIFT)
+#define CSG_ACK_STATUS_UPDATE_MASK (0x1 << CSG_ACK_STATUS_UPDATE_SHIFT)
 #define CSG_ACK_STATUS_UPDATE_GET(reg_val) \
 	(((reg_val)&CSG_ACK_STATUS_UPDATE_MASK) >> CSG_ACK_STATUS_UPDATE_SHIFT)
 #define CSG_ACK_STATUS_UPDATE_SET(reg_val, value)    \
 	(((reg_val) & ~CSG_ACK_STATUS_UPDATE_MASK) | \
 	 (((value) << CSG_ACK_STATUS_UPDATE_SHIFT) & CSG_ACK_STATUS_UPDATE_MASK))
 #define CSG_ACK_SYNC_UPDATE_SHIFT 28
-#define CSG_ACK_SYNC_UPDATE_MASK (0x1U << CSG_ACK_SYNC_UPDATE_SHIFT)
+#define CSG_ACK_SYNC_UPDATE_MASK (0x1 << CSG_ACK_SYNC_UPDATE_SHIFT)
 #define CSG_ACK_SYNC_UPDATE_GET(reg_val) \
 	(((reg_val)&CSG_ACK_SYNC_UPDATE_MASK) >> CSG_ACK_SYNC_UPDATE_SHIFT)
 #define CSG_ACK_SYNC_UPDATE_SET(reg_val, value)    \
 	(((reg_val) & ~CSG_ACK_SYNC_UPDATE_MASK) | \
 	 (((value) << CSG_ACK_SYNC_UPDATE_SHIFT) & CSG_ACK_SYNC_UPDATE_MASK))
 #define CSG_ACK_IDLE_SHIFT 29
-#define CSG_ACK_IDLE_MASK (0x1U << CSG_ACK_IDLE_SHIFT)
+#define CSG_ACK_IDLE_MASK (0x1 << CSG_ACK_IDLE_SHIFT)
 #define CSG_ACK_IDLE_GET(reg_val) (((reg_val)&CSG_ACK_IDLE_MASK) >> CSG_ACK_IDLE_SHIFT)
 #define CSG_ACK_IDLE_SET(reg_val, value) \
 	(((reg_val) & ~CSG_ACK_IDLE_MASK) | (((value) << CSG_ACK_IDLE_SHIFT) & CSG_ACK_IDLE_MASK))
 #define CSG_ACK_DOORBELL_SHIFT 30
-#define CSG_ACK_DOORBELL_MASK (0x1U << CSG_ACK_DOORBELL_SHIFT)
+#define CSG_ACK_DOORBELL_MASK (0x1 << CSG_ACK_DOORBELL_SHIFT)
 #define CSG_ACK_DOORBELL_GET(reg_val) (((reg_val)&CSG_ACK_DOORBELL_MASK) >> CSG_ACK_DOORBELL_SHIFT)
 #define CSG_ACK_DOORBELL_SET(reg_val, value)    \
 	(((reg_val) & ~CSG_ACK_DOORBELL_MASK) | \
 	 (((value) << CSG_ACK_DOORBELL_SHIFT) & CSG_ACK_DOORBELL_MASK))
 #define CSG_ACK_PROGRESS_TIMER_EVENT_SHIFT 31
-#define CSG_ACK_PROGRESS_TIMER_EVENT_MASK (0x1U << CSG_ACK_PROGRESS_TIMER_EVENT_SHIFT)
+#define CSG_ACK_PROGRESS_TIMER_EVENT_MASK (0x1 << CSG_ACK_PROGRESS_TIMER_EVENT_SHIFT)
 #define CSG_ACK_PROGRESS_TIMER_EVENT_GET(reg_val) \
 	(((reg_val)&CSG_ACK_PROGRESS_TIMER_EVENT_MASK) >> CSG_ACK_PROGRESS_TIMER_EVENT_SHIFT)
 #define CSG_ACK_PROGRESS_TIMER_EVENT_SET(reg_val, value)    \
@@ -1248,21 +1243,6 @@
 	  CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_MASK))
 
 
-/* CSG_PROGRESS_TIMER_STATE register */
-#ifndef CSG_PROGRESS_TIMER_STATE_GET
-#define CSG_PROGRESS_TIMER_STATE_SHIFT 0
-#define CSG_PROGRESS_TIMER_STATE_MASK ((u32)0xFFFFFFFF << CSG_PROGRESS_TIMER_STATE_SHIFT)
-#define CSG_PROGRESS_TIMER_STATE_GET(reg_val) \
-	(((reg_val)&CSG_PROGRESS_TIMER_STATE_MASK) >> CSG_PROGRESS_TIMER_STATE_SHIFT)
-#define CSG_PROGRESS_TIMER_STATE_SET(reg_val, value)    \
-	(((reg_val) & ~CSG_PROGRESS_TIMER_STATE_MASK) | \
-	 (((value) << CSG_PROGRESS_TIMER_STATE_SHIFT) & CSG_PROGRESS_TIMER_STATE_MASK))
-/* CSG_PROGRESS_TIMER_STATE values */
-#define CSG_PROGRESS_TIMER_STATE_COMPUTE 0x0
-#define CSG_PROGRESS_TIMER_STATE_FRAGMENT 0x1
-#define CSG_PROGRESS_TIMER_STATE_TILER 0x2
-#define CSG_PROGRESS_TIMER_STATE_NEURAL 0x3
-#endif
 /* End of CSG_OUTPUT_BLOCK register set definitions */
 
 /* STREAM_CONTROL_BLOCK register set definitions */
@@ -1400,13 +1380,6 @@
 #define GLB_REQ_SLEEP_SET(reg_val, value)    \
 	(((reg_val) & ~GLB_REQ_SLEEP_MASK) | \
 	 (((value) << GLB_REQ_SLEEP_SHIFT) & GLB_REQ_SLEEP_MASK))
-#define GLB_REQ_CFG_EVICTION_TIMER_SHIFT 16
-#define GLB_REQ_CFG_EVICTION_TIMER_MASK (0x1 << GLB_REQ_CFG_EVICTION_TIMER_SHIFT)
-#define GLB_REQ_CFG_EVICTION_TIMER_GET(reg_val) \
-	(((reg_val)&GLB_REQ_CFG_EVICTION_TIMER_MASK) >> GLB_REQ_CFG_EVICTION_TIMER_SHIFT)
-#define GLB_REQ_CFG_EVICTION_TIMER_SET(reg_val, value)    \
-	(((reg_val) & ~GLB_REQ_CFG_EVICTION_TIMER_MASK) | \
-	 (((value) << GLB_REQ_CFG_EVICTION_TIMER_SHIFT) & GLB_REQ_CFG_EVICTION_TIMER_MASK))
 #define GLB_REQ_INACTIVE_COMPUTE_SHIFT 20
 #define GLB_REQ_INACTIVE_COMPUTE_MASK (0x1 << GLB_REQ_INACTIVE_COMPUTE_SHIFT)
 #define GLB_REQ_INACTIVE_COMPUTE_GET(reg_val) \
@@ -1449,12 +1422,6 @@
 #define GLB_REQ_PRFCNT_OVERFLOW_SET(reg_val, value)    \
 	(((reg_val) & ~GLB_REQ_PRFCNT_OVERFLOW_MASK) | \
 	 (((value) << GLB_REQ_PRFCNT_OVERFLOW_SHIFT) & GLB_REQ_PRFCNT_OVERFLOW_MASK))
-#define GLB_ACK_FATAL_SHIFT GPU_U(27)
-#define GLB_ACK_FATAL_MASK (GPU_U(0x1) << GLB_ACK_FATAL_SHIFT)
-#define GLB_ACK_FATAL_GET(reg_val) (((reg_val)&GLB_ACK_FATAL_MASK) >> GLB_ACK_FATAL_SHIFT)
-#define GLB_ACK_FATAL_SET(reg_val, value)     \
-	(~(~(reg_val) | GLB_ACK_FATAL_MASK) | \
-	 (((value) << GLB_ACK_FATAL_SHIFT) & GLB_ACK_FATAL_MASK))
 #define GLB_REQ_DEBUG_CSF_REQ_SHIFT 30
 #define GLB_REQ_DEBUG_CSF_REQ_MASK (0x1 << GLB_REQ_DEBUG_CSF_REQ_SHIFT)
 #define GLB_REQ_DEBUG_CSF_REQ_GET(reg_val) \
@@ -1551,17 +1518,6 @@
 	(((reg_val) & ~GLB_ACK_IRQ_MASK_FIRMWARE_CONFIG_UPDATE_MASK) | \
 	 (((value) << GLB_ACK_IRQ_MASK_FIRMWARE_CONFIG_UPDATE_SHIFT) & \
 	  GLB_ACK_IRQ_MASK_FIRMWARE_CONFIG_UPDATE_MASK))
-#define GLB_ACK_IRQ_MASK_ITER_TRACE_ENABLE_SHIFT 11
-#define GLB_ACK_IRQ_MASK_ITER_TRACE_ENABLE_MASK (0x1 << GLB_ACK_IRQ_MASK_ITER_TRACE_ENABLE_SHIFT)
-#define GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_SHIFT 16
-#define GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_MASK (0x1 << GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_SHIFT)
-#define GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_GET(reg_val)         \
-	(((reg_val)&GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_MASK) >> \
-	 GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_SHIFT)
-#define GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_SET(reg_val, value)    \
-	(((reg_val) & ~GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_MASK) | \
-	 (((value) << GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_SHIFT) & \
-	  GLB_ACK_IRQ_MASK_CFG_EVICTION_TIMER_MASK))
 #define GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_SHIFT 20
 #define GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_MASK (0x1 << GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_SHIFT)
 #define GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_GET(reg_val)         \
@@ -1655,8 +1611,8 @@
 	(((reg_val) & ~GLB_PWROFF_TIMER_TIMER_SOURCE_MASK) | \
 	 (((value) << GLB_PWROFF_TIMER_TIMER_SOURCE_SHIFT) & GLB_PWROFF_TIMER_TIMER_SOURCE_MASK))
 /* GLB_PWROFF_TIMER_TIMER_SOURCE values */
-#define GLB_PWROFF_TIMER_TIMER_SOURCE_SYSTEM_TIMESTAMP 0x0U
-#define GLB_PWROFF_TIMER_TIMER_SOURCE_GPU_COUNTER 0x1U
+#define GLB_PWROFF_TIMER_TIMER_SOURCE_SYSTEM_TIMESTAMP 0x0
+#define GLB_PWROFF_TIMER_TIMER_SOURCE_GPU_COUNTER 0x1
 /* End of GLB_PWROFF_TIMER_TIMER_SOURCE values */
 
 /* GLB_PWROFF_TIMER_CONFIG register */
@@ -1673,45 +1629,6 @@
 	  GLB_PWROFF_TIMER_CONFIG_NO_MODIFIER_MASK))
 #endif /* End of GLB_PWROFF_TIMER_CONFIG values */
 
-/* GLB_EVICTION_TIMER register */
-#ifndef GLB_EVICTION_TIMER
-#define GLB_EVICTION_TIMER 0x0090
-#define GLB_EVICTION_TIMER_TIMEOUT_SHIFT (0)
-#define GLB_EVICTION_TIMER_TIMEOUT_MASK ((0x7FFFFFFF) << GLB_EVICTION_TIMER_TIMEOUT_SHIFT)
-#define GLB_EVICTION_TIMER_TIMEOUT_GET(reg_val) \
-	(((reg_val)&GLB_EVICTION_TIMER_TIMEOUT_MASK) >> GLB_EVICTION_TIMER_TIMEOUT_SHIFT)
-#define GLB_EVICTION_TIMER_TIMEOUT_SET(reg_val, value)    \
-	(((reg_val) & ~GLB_EVICTION_TIMER_TIMEOUT_MASK) | \
-	 (((value) << GLB_EVICTION_TIMER_TIMEOUT_SHIFT) & GLB_EVICTION_TIMER_TIMEOUT_MASK))
-#define GLB_EVICTION_TIMER_TIMER_SOURCE_SHIFT (31)
-#define GLB_EVICTION_TIMER_TIMER_SOURCE_MASK ((0x1) << GLB_EVICTION_TIMER_TIMER_SOURCE_SHIFT)
-#define GLB_EVICTION_TIMER_TIMER_SOURCE_GET(reg_val) \
-	(((reg_val)&GLB_EVICTION_TIMER_TIMER_SOURCE_MASK) >> GLB_EVICTION_TIMER_TIMER_SOURCE_SHIFT)
-#define GLB_EVICTION_TIMER_TIMER_SOURCE_SET(reg_val, value)    \
-	(((reg_val) & ~GLB_EVICTION_TIMER_TIMER_SOURCE_MASK) | \
-	 (((value) << GLB_EVICTION_TIMER_TIMER_SOURCE_SHIFT) & \
-	  GLB_EVICTION_TIMER_TIMER_SOURCE_MASK))
-/* GLB_EVICTION_TIMER_TIMER_SOURCE values */
-#define GLB_EVICTION_TIMER_TIMER_SOURCE_SYSTEM_TIMESTAMP 0x0U
-#define GLB_EVICTION_TIMER_TIMER_SOURCE_GPU_COUNTER 0x1U
-/* End of GLB_EVICTION_TIMER_TIMER_SOURCE values */
-#endif /* End of GLB_EVICTION_TIMER */
-
-/* GLB_EVICTION_TIMER_CONFIG register */
-#ifndef GLB_EVICTION_TIMER_CONFIG
-#define GLB_EVICTION_TIMER_CONFIG 0x0094 /* () Configuration fields for GLB_EVICTION_TIMER */
-#define GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_SHIFT 0
-#define GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_MASK \
-	(0x1 << GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_SHIFT)
-#define GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_GET(reg_val)         \
-	(((reg_val)&GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_MASK) >> \
-	 GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_SHIFT)
-#define GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_SET(reg_val, value)    \
-	(((reg_val) & ~GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_MASK) | \
-	 (((value) << GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_SHIFT) & \
-	  GLB_EVICTION_TIMER_CONFIG_NO_MODIFIER_MASK))
-#endif /* End of GLB_EVICTION_TIMER_CONFIG values */
-
 /* GLB_ALLOC_EN register */
 #define GLB_ALLOC_EN_MASK_SHIFT 0
 #define GLB_ALLOC_EN_MASK_MASK (GPU_ULL(0xFFFFFFFFFFFFFFFF) << GLB_ALLOC_EN_MASK_SHIFT)
@@ -1778,8 +1695,8 @@
 	(((reg_val) & ~GLB_IDLE_TIMER_TIMER_SOURCE_MASK) | \
 	 (((value) << GLB_IDLE_TIMER_TIMER_SOURCE_SHIFT) & GLB_IDLE_TIMER_TIMER_SOURCE_MASK))
 /* GLB_IDLE_TIMER_TIMER_SOURCE values */
-#define GLB_IDLE_TIMER_TIMER_SOURCE_SYSTEM_TIMESTAMP 0x0U
-#define GLB_IDLE_TIMER_TIMER_SOURCE_GPU_COUNTER 0x1U
+#define GLB_IDLE_TIMER_TIMER_SOURCE_SYSTEM_TIMESTAMP 0x0
+#define GLB_IDLE_TIMER_TIMER_SOURCE_GPU_COUNTER 0x1
 /* End of GLB_IDLE_TIMER_TIMER_SOURCE values */
 
 /* GLB_IDLE_TIMER_CONFIG values */
@@ -1794,15 +1711,6 @@
 	(((reg_val) & ~GLB_IDLE_TIMER_CONFIG_NO_MODIFIER_MASK) | \
 	 (((value) << GLB_IDLE_TIMER_CONFIG_NO_MODIFIER_SHIFT) & \
 	  GLB_IDLE_TIMER_CONFIG_NO_MODIFIER_MASK))
-#define GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_SHIFT 9
-#define GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_MASK (0x1 << GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_SHIFT)
-#define GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_GET(reg_val)         \
-	(((reg_val)&GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_MASK) >> \
-	 GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_SHIFT)
-#define GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_SET(reg_val, value)    \
-	(((reg_val) & ~GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_MASK) | \
-	 (((value) << GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_SHIFT) & \
-	  GLB_IDLE_TIMER_CONFIG_SLEEP_ON_IDLE_MASK))
 #endif /* End of GLB_IDLE_TIMER_CONFIG values */
 
 /* GLB_INSTR_FEATURES register */
@@ -1914,20 +1822,6 @@
 	(((reg_val) & ~GLB_DEBUG_REQ_RUN_MODE_MASK) | \
 	 (((value) << GLB_DEBUG_REQ_RUN_MODE_SHIFT) & GLB_DEBUG_REQ_RUN_MODE_MASK))
 
-/* GLB_FATAL_STATUS register */
-#define GLB_FATAL_STATUS_VALUE_SHIFT GPU_U(0)
-#define GLB_FATAL_STATUS_VALUE_MASK (GPU_U(0xFFFFFFFF) << GLB_FATAL_STATUS_VALUE_SHIFT)
-#define GLB_FATAL_STATUS_VALUE_GET(reg_val) \
-	(((reg_val)&GLB_FATAL_STATUS_VALUE_MASK) >> GLB_FATAL_STATUS_VALUE_SHIFT)
-
-enum glb_fatal_status {
-	GLB_FATAL_STATUS_VALUE_OK,
-	GLB_FATAL_STATUS_VALUE_ASSERT,
-	GLB_FATAL_STATUS_VALUE_UNEXPECTED_EXCEPTION,
-	GLB_FATAL_STATUS_VALUE_HANG,
-	GLB_FATAL_STATUS_VALUE_COUNT
-};
-
 /* GLB_DEBUG_ACK register */
 #define GLB_DEBUG_ACK_DEBUG_RUN_SHIFT GPU_U(23)
 #define GLB_DEBUG_ACK_DEBUG_RUN_MASK (GPU_U(0x1) << GLB_DEBUG_ACK_DEBUG_RUN_SHIFT)
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_reset_gpu.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_reset_gpu.c
index ffd27318cba3..c18ed5b9f6cc 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_reset_gpu.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_reset_gpu.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -224,11 +224,8 @@ static void kbase_csf_reset_end_hw_access(struct kbase_device *kbdev, int err_du
 
 static void kbase_csf_debug_dump_registers(struct kbase_device *kbdev)
 {
-	unsigned long flags;
-
 	kbase_io_history_dump(kbdev);
 
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	dev_err(kbdev->dev, "Register state:");
 	dev_err(kbdev->dev, "  GPU_IRQ_RAWSTAT=0x%08x  GPU_STATUS=0x%08x MCU_STATUS=0x%08x",
 		kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_RAWSTAT)),
@@ -243,18 +240,13 @@ static void kbase_csf_debug_dump_registers(struct kbase_device *kbdev)
 		kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_MASK)),
 		kbase_reg_read32(kbdev, JOB_CONTROL_ENUM(JOB_IRQ_MASK)),
 		kbase_reg_read32(kbdev, MMU_CONTROL_ENUM(IRQ_MASK)));
-	if (kbdev->gpu_props.gpu_id.arch_id < GPU_ID_ARCH_MAKE(14, 10, 0)) {
-		dev_err(kbdev->dev, "  PWR_OVERRIDE0=0x%08x  PWR_OVERRIDE1=0x%08x",
-			kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(PWR_OVERRIDE0)),
-			kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(PWR_OVERRIDE1)));
-		dev_err(kbdev->dev,
-			"  SHADER_CONFIG=0x%08x  L2_MMU_CONFIG=0x%08x  TILER_CONFIG=0x%08x",
-			kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(SHADER_CONFIG)),
-			kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(L2_MMU_CONFIG)),
-			kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(TILER_CONFIG)));
-	}
-
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	dev_err(kbdev->dev, "  PWR_OVERRIDE0=0x%08x  PWR_OVERRIDE1=0x%08x",
+		kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(PWR_OVERRIDE0)),
+		kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(PWR_OVERRIDE1)));
+	dev_err(kbdev->dev, "  SHADER_CONFIG=0x%08x  L2_MMU_CONFIG=0x%08x  TILER_CONFIG=0x%08x",
+		kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(SHADER_CONFIG)),
+		kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(L2_MMU_CONFIG)),
+		kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(TILER_CONFIG)));
 }
 
 /**
@@ -400,7 +392,6 @@ static int kbase_csf_reset_gpu_now(struct kbase_device *kbdev, bool firmware_ini
 	 */
 	if (likely(firmware_inited))
 		kbase_csf_scheduler_reset(kbdev);
-
 	cancel_work_sync(&kbdev->csf.firmware_reload_work);
 
 	dev_dbg(kbdev->dev, "Disable GPU hardware counters.\n");
@@ -408,7 +399,6 @@ static int kbase_csf_reset_gpu_now(struct kbase_device *kbdev, bool firmware_ini
 	kbase_hwcnt_context_disable(kbdev->hwcnt_gpu_ctx);
 
 	ret = kbase_csf_reset_gpu_once(kbdev, firmware_inited, silent);
-
 	if (ret == SOFT_RESET_FAILED) {
 		dev_err(kbdev->dev, "Soft-reset failed");
 		goto err;
@@ -496,11 +486,6 @@ static void kbase_csf_reset_gpu_worker(struct work_struct *data)
 
 bool kbase_prepare_to_reset_gpu(struct kbase_device *kbdev, unsigned int flags)
 {
-	if (kbase_pm_is_gpu_lost(kbdev)) {
-		/* GPU access has been removed, reset will be done by Arbiter instead */
-		return false;
-	}
-
 	if (flags & RESET_FLAGS_HWC_UNRECOVERABLE_ERROR)
 		kbase_hwcnt_backend_csf_on_unrecoverable_error(&kbdev->hwcnt_gpu_iface);
 
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.c
index cd6abd62f6c5..a477ee666838 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -36,13 +36,11 @@
 #include "mali_kbase_csf_tiler_heap_reclaim.h"
 #include "mali_kbase_csf_mcu_shared_reg.h"
 #include <linux/version_compat_defs.h>
-#include <hwcnt/mali_kbase_hwcnt_context.h>
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
 #include <mali_kbase_gpu_metrics.h>
 #include <csf/mali_kbase_csf_trace_buffer.h>
 #endif /* CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD */
 
-
 /* Value to indicate that a queue group is not groups_to_schedule list */
 #define KBASEP_GROUP_PREPARED_SEQ_NUM_INVALID (U32_MAX)
 
@@ -86,19 +84,13 @@ scheduler_get_protm_enter_async_group(struct kbase_device *const kbdev,
 				      struct kbase_queue_group *const group);
 static struct kbase_queue_group *get_tock_top_group(struct kbase_csf_scheduler *const scheduler);
 static void scheduler_enable_tick_timer_nolock(struct kbase_device *kbdev);
-static int suspend_active_queue_groups(struct kbase_device *kbdev, unsigned long *slot_mask,
-				       bool reset);
+static int suspend_active_queue_groups(struct kbase_device *kbdev, unsigned long *slot_mask);
 static int suspend_active_groups_on_powerdown(struct kbase_device *kbdev, bool system_suspend);
 static void schedule_in_cycle(struct kbase_queue_group *group, bool force);
 static bool queue_group_scheduled_locked(struct kbase_queue_group *group);
 
 #define kctx_as_enabled(kctx) (!kbase_ctx_flag(kctx, KCTX_AS_DISABLED_ON_FAULT))
 
-bool is_gpu_level_suspend_supported(struct kbase_device *const kbdev)
-{
-	return false;
-}
-
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
 /**
  * gpu_metrics_ctx_init() - Take a reference on GPU metrics context if it exists,
@@ -127,7 +119,7 @@ static inline int gpu_metrics_ctx_init(struct kbase_context *kctx)
 	put_cred(cred);
 
 	/* Return early if this is not a Userspace created context */
-	if (unlikely(!kctx->filp))
+	if (unlikely(!kctx->kfile))
 		return 0;
 
 	/* Serialize against the other threads trying to create/destroy Kbase contexts. */
@@ -164,7 +156,7 @@ static inline int gpu_metrics_ctx_init(struct kbase_context *kctx)
 static inline void gpu_metrics_ctx_term(struct kbase_context *kctx)
 {
 	/* Return early if this is not a Userspace created context */
-	if (unlikely(!kctx->filp))
+	if (unlikely(!kctx->kfile))
 		return;
 
 	/* Serialize against the other threads trying to create/destroy Kbase contexts. */
@@ -466,14 +458,11 @@ static void wait_for_dump_complete_on_group_deschedule(struct kbase_queue_group
  *
  * This function notifies the Userspace client waiting for the faults and wait
  * for the Client to complete the dumping.
- * The function is mainly called from Scheduling tick/tock when a request sent by
- * the Scheduler to FW times out. It can be called outside the tick/tock when timeout
- * happens in the following 3 cases :-
- * - Entry to protected mode is initiated from protm event work item.
- * - Forced exit from protected mode is triggered when GPU queue of an on-slot group is kicked.
- * - CSG termination request is sent when Userspace tries to delete the queue group.
- * In the latter 3 cases there is no wait done as scheduler lock would be released
- * immediately. In the tick/tock case the function waits and releases the scheduler
+ * The function is called only from Scheduling tick/tock when a request sent by
+ * the Scheduler to FW times out or from the protm event work item of the group
+ * when the protected mode entry request times out.
+ * In the latter case there is no wait done as scheduler lock would be released
+ * immediately. In the former case the function waits and releases the scheduler
  * lock before the wait. It has been ensured that the Scheduler view of the groups
  * won't change meanwhile, so no group can enter/exit the Scheduler, become
  * runnable or go off slot.
@@ -489,9 +478,10 @@ static void schedule_actions_trigger_df(struct kbase_device *kbdev, struct kbase
 	if (!kbase_debug_csf_fault_notify(kbdev, kctx, error))
 		return;
 
-	/* Return early if the function was called outside the tick/tock */
-	if (unlikely(scheduler->state != SCHED_BUSY))
+	if (unlikely(scheduler->state != SCHED_BUSY)) {
+		WARN_ON(error != DF_PROTECTED_MODE_ENTRY_FAILURE);
 		return;
+	}
 
 	mutex_unlock(&scheduler->lock);
 	kbase_debug_csf_fault_wait_completion(kbdev);
@@ -538,7 +528,7 @@ static int wait_for_scheduler_to_exit_sleep(struct kbase_device *kbdev)
 	/* Usually Scheduler would remain in sleeping state until the
 	 * auto-suspend timer expires and all active CSGs are suspended.
 	 */
-	sleep_exit_wait_time = (u32)autosuspend_delay + kbdev->reset_timeout_ms;
+	sleep_exit_wait_time = autosuspend_delay + kbdev->reset_timeout_ms;
 
 	remaining = kbase_csf_timeout_in_jiffies(sleep_exit_wait_time);
 
@@ -690,7 +680,7 @@ static void unassign_user_doorbell_from_queue(struct kbase_device *kbdev, struct
 		queue->doorbell_nr = KBASEP_USER_DB_NR_INVALID;
 		/* After this the dummy page would be mapped in */
 		unmap_mapping_range(kbdev->csf.db_filp->f_inode->i_mapping,
-				    (loff_t)(queue->db_file_offset << PAGE_SHIFT), PAGE_SIZE, 1);
+				    queue->db_file_offset << PAGE_SHIFT, PAGE_SIZE, 1);
 	}
 
 	mutex_unlock(&kbdev->csf.reg_lock);
@@ -724,7 +714,7 @@ static void assign_user_doorbell_to_queue(struct kbase_device *kbdev,
 
 		/* After this the real Hw doorbell page would be mapped in */
 		unmap_mapping_range(kbdev->csf.db_filp->f_inode->i_mapping,
-				    (loff_t)(queue->db_file_offset << PAGE_SHIFT), PAGE_SIZE, 1);
+				    queue->db_file_offset << PAGE_SHIFT, PAGE_SIZE, 1);
 	}
 
 	mutex_unlock(&kbdev->csf.reg_lock);
@@ -798,41 +788,47 @@ static void update_on_slot_queues_offsets(struct kbase_device *kbdev)
 static void enqueue_gpu_idle_work(struct kbase_csf_scheduler *const scheduler)
 {
 	atomic_set(&scheduler->gpu_no_longer_idle, false);
-	atomic_inc(&scheduler->pending_gpu_idle_work);
-	complete(&scheduler->kthread_signal);
+	queue_work(scheduler->idle_wq, &scheduler->gpu_idle_work);
 }
 
 void kbase_csf_scheduler_process_gpu_idle_event(struct kbase_device *kbdev)
 {
 	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	int non_idle_offslot_grps;
 	bool can_suspend_on_idle;
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 	lockdep_assert_held(&scheduler->interrupt_lock);
 
-	can_suspend_on_idle = kbase_pm_idle_groups_sched_suspendable(kbdev) &&
-			      !kbase_pm_is_mcu_inactive(kbdev, kbdev->pm.backend.mcu_state);
+	non_idle_offslot_grps = atomic_read(&scheduler->non_idle_offslot_grps);
+	can_suspend_on_idle = kbase_pm_idle_groups_sched_suspendable(kbdev);
 	KBASE_KTRACE_ADD(kbdev, SCHEDULER_GPU_IDLE_EVENT_CAN_SUSPEND, NULL,
-			 (((u64)can_suspend_on_idle) << 32));
+			 ((u64)(u32)non_idle_offslot_grps) | (((u64)can_suspend_on_idle) << 32));
 
-	if (can_suspend_on_idle) {
-		/* fast_gpu_idle_handling is protected by the
-		 * interrupt_lock, which would prevent this from being
-		 * updated whilst gpu_idle_worker() is executing.
-		 */
-		scheduler->fast_gpu_idle_handling = (kbdev->csf.gpu_idle_hysteresis_ns == 0) ||
-						    !kbase_csf_scheduler_all_csgs_idle(kbdev);
+	if (!non_idle_offslot_grps) {
+		if (can_suspend_on_idle) {
+			/* fast_gpu_idle_handling is protected by the
+			 * interrupt_lock, which would prevent this from being
+			 * updated whilst gpu_idle_worker() is executing.
+			 */
+			scheduler->fast_gpu_idle_handling =
+				(kbdev->csf.gpu_idle_hysteresis_ns == 0) ||
+				!kbase_csf_scheduler_all_csgs_idle(kbdev);
 
-		/* The GPU idle worker relies on update_on_slot_queues_offsets() to have
-		 * finished. It's queued before to reduce the time it takes till execution
-		 * but it'll eventually be blocked by the scheduler->interrupt_lock.
-		 */
-		enqueue_gpu_idle_work(scheduler);
-	}
+			/* The GPU idle worker relies on update_on_slot_queues_offsets() to have
+			 * finished. It's queued before to reduce the time it takes till execution
+			 * but it'll eventually be blocked by the scheduler->interrupt_lock.
+			 */
+			enqueue_gpu_idle_work(scheduler);
 
-	/* The extract offsets are unused in fast GPU idle handling */
-	if (!scheduler->fast_gpu_idle_handling)
-		update_on_slot_queues_offsets(kbdev);
+			/* The extract offsets are unused in fast GPU idle handling */
+			if (!scheduler->fast_gpu_idle_handling)
+				update_on_slot_queues_offsets(kbdev);
+		}
+	} else {
+		/* Invoke the scheduling tick to get the non-idle suspended groups loaded soon */
+		kbase_csf_scheduler_invoke_tick(kbdev);
+	}
 }
 
 u32 kbase_csf_scheduler_get_nr_active_csgs_locked(struct kbase_device *kbdev)
@@ -841,8 +837,8 @@ u32 kbase_csf_scheduler_get_nr_active_csgs_locked(struct kbase_device *kbdev)
 
 	lockdep_assert_held(&kbdev->csf.scheduler.interrupt_lock);
 
-	nr_active_csgs = (u32)bitmap_weight(kbdev->csf.scheduler.csg_inuse_bitmap,
-					    kbdev->csf.global_iface.group_num);
+	nr_active_csgs = bitmap_weight(kbdev->csf.scheduler.csg_inuse_bitmap,
+				       kbdev->csf.global_iface.group_num);
 
 	return nr_active_csgs;
 }
@@ -942,15 +938,13 @@ static void update_idle_protm_group_state_to_runnable(struct kbase_queue_group *
 static bool scheduler_protm_wait_quit(struct kbase_device *kbdev)
 {
 	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
-	const unsigned int fw_timeout_ms = kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT);
-	long wt = kbase_csf_timeout_in_jiffies(fw_timeout_ms);
+	long wt = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms);
 	long remaining;
 	bool success = true;
 
 	lockdep_assert_held(&scheduler->lock);
 
-	KBASE_KTRACE_ADD(kbdev, SCHEDULER_PROTM_WAIT_QUIT_START, NULL,
-			 jiffies_to_msecs((unsigned long)wt));
+	KBASE_KTRACE_ADD(kbdev, SCHEDULER_PROTM_WAIT_QUIT_START, NULL, jiffies_to_msecs(wt));
 
 	remaining = wait_event_timeout(kbdev->csf.event_wait,
 				       !kbase_csf_scheduler_protected_mode_in_use(kbdev), wt);
@@ -960,13 +954,12 @@ static bool scheduler_protm_wait_quit(struct kbase_device *kbdev)
 		struct kbase_context *kctx = group ? group->kctx : NULL;
 
 		dev_warn(kbdev->dev, "[%llu] Timeout (%d ms), protm_quit wait skipped",
-			 kbase_backend_get_cycle_cnt(kbdev), fw_timeout_ms);
+			 kbase_backend_get_cycle_cnt(kbdev), kbdev->csf.fw_timeout_ms);
 		schedule_actions_trigger_df(kbdev, kctx, DF_PROTECTED_MODE_EXIT_TIMEOUT);
 		success = false;
 	}
 
-	KBASE_KTRACE_ADD(kbdev, SCHEDULER_PROTM_WAIT_QUIT_END, NULL,
-			 jiffies_to_msecs((unsigned long)remaining));
+	KBASE_KTRACE_ADD(kbdev, SCHEDULER_PROTM_WAIT_QUIT_END, NULL, jiffies_to_msecs(remaining));
 
 	return success;
 }
@@ -1019,8 +1012,6 @@ static void scheduler_force_protm_exit(struct kbase_device *kbdev)
  *
  * @kbdev: Pointer to the device
  * @suspend_handler: Handler code for how to handle a suspend that might occur.
- * @active_after_sleep: Flag to indicate that Scheduler is being activated from
- *                      the sleeping state.
  *
  * This function is usually called when Scheduler needs to be activated.
  * The PM reference count is acquired for the Scheduler and the power on
@@ -1029,8 +1020,7 @@ static void scheduler_force_protm_exit(struct kbase_device *kbdev)
  * Return: 0 if successful or a negative error code on failure.
  */
 static int scheduler_pm_active_handle_suspend(struct kbase_device *kbdev,
-					      enum kbase_pm_suspend_handler suspend_handler,
-					      bool active_after_sleep)
+					      enum kbase_pm_suspend_handler suspend_handler)
 {
 	unsigned long flags;
 	u32 prev_count;
@@ -1038,35 +1028,24 @@ static int scheduler_pm_active_handle_suspend(struct kbase_device *kbdev,
 
 	lockdep_assert_held(&kbdev->csf.scheduler.lock);
 
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	prev_count = kbdev->csf.scheduler.pm_active_count;
 	if (!WARN_ON(prev_count == U32_MAX))
 		kbdev->csf.scheduler.pm_active_count++;
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 	/* On 0 => 1, make a pm_ctx_active request */
 	if (!prev_count) {
-		kbase_pm_lock(kbdev);
-		kbdev->pm.backend.mcu_poweron_required = true;
-		ret = kbase_pm_context_active_handle_suspend_locked(kbdev, suspend_handler);
-		if (ret) {
+		ret = kbase_pm_context_active_handle_suspend(kbdev, suspend_handler);
+		/* Invoke the PM state machines again as the change in MCU
+		 * desired status, due to the update of scheduler.pm_active_count,
+		 * may be missed by the thread that called pm_wait_for_desired_state()
+		 */
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		if (ret)
 			kbdev->csf.scheduler.pm_active_count--;
-			kbdev->pm.backend.mcu_poweron_required = false;
-		} else {
-			spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-			if (active_after_sleep) {
-				kbdev->pm.backend.runtime_suspend_abort_reason = ABORT_REASON_NONE;
-				kbdev->pm.backend.gpu_sleep_mode_active = false;
-			}
-			/* Check if the GPU is already active */
-			if (kbdev->pm.active_count > 1) {
-				/* GPU is already active, so need to invoke the PM state machines
-				 * explicitly to turn on the MCU.
-				 */
-				kbdev->pm.backend.mcu_desired = true;
-				kbase_pm_update_state(kbdev);
-			}
-			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-		}
-		kbase_pm_unlock(kbdev);
+		kbase_pm_update_state(kbdev);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 	}
 
 	return ret;
@@ -1078,6 +1057,8 @@ static int scheduler_pm_active_handle_suspend(struct kbase_device *kbdev,
  *                                     Scheduler
  *
  * @kbdev: Pointer to the device
+ * @flags: Pointer to the flags variable containing the interrupt state
+ *         when hwaccess lock was acquired.
  *
  * This function is called when Scheduler needs to be activated from the
  * sleeping state.
@@ -1085,15 +1066,40 @@ static int scheduler_pm_active_handle_suspend(struct kbase_device *kbdev,
  * MCU is initiated. It resets the flag that indicates to the MCU state
  * machine that MCU needs to be put in sleep state.
  *
+ * Note: This function shall be called with hwaccess lock held and it may
+ * release that lock and reacquire it.
+ *
  * Return: zero when the PM reference was taken and non-zero when the
  * system is being suspending/suspended.
  */
-static int scheduler_pm_active_after_sleep(struct kbase_device *kbdev)
+static int scheduler_pm_active_after_sleep(struct kbase_device *kbdev, unsigned long *flags)
 {
+	u32 prev_count;
+	int ret = 0;
+
 	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	prev_count = kbdev->csf.scheduler.pm_active_count;
+	if (!WARN_ON(prev_count == U32_MAX))
+		kbdev->csf.scheduler.pm_active_count++;
+
+	/* On 0 => 1, make a pm_ctx_active request */
+	if (!prev_count) {
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, *flags);
+
+		ret = kbase_pm_context_active_handle_suspend(
+			kbdev, KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE);
+
+		spin_lock_irqsave(&kbdev->hwaccess_lock, *flags);
+		if (ret)
+			kbdev->csf.scheduler.pm_active_count--;
+		else
+			kbdev->pm.backend.gpu_sleep_mode_active = false;
+		kbase_pm_update_state(kbdev);
+	}
 
-	return scheduler_pm_active_handle_suspend(kbdev, KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE,
-						  true);
+	return ret;
 }
 #endif
 
@@ -1113,32 +1119,28 @@ static void scheduler_pm_idle(struct kbase_device *kbdev)
 
 	lockdep_assert_held(&kbdev->csf.scheduler.lock);
 
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	prev_count = kbdev->csf.scheduler.pm_active_count;
 	if (!WARN_ON(prev_count == 0))
 		kbdev->csf.scheduler.pm_active_count--;
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 	if (prev_count == 1) {
-		kbase_pm_lock(kbdev);
-		kbdev->pm.backend.mcu_poweron_required = false;
-		kbase_pm_context_idle_locked(kbdev);
-		/* Check if GPU is still active */
-		if (kbdev->pm.active_count) {
-			/* GPU is still active, so need to invoke the PM state machines
-			 * explicitly to turn off the MCU.
-			 */
-			spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-			kbdev->pm.backend.mcu_desired = false;
-			kbase_pm_update_state(kbdev);
-			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-		}
-		kbase_pm_unlock(kbdev);
+		kbase_pm_context_idle(kbdev);
+		/* Invoke the PM state machines again as the change in MCU
+		 * desired status, due to the update of scheduler.pm_active_count,
+		 * may be missed by the thread that called pm_wait_for_desired_state()
+		 */
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		kbase_pm_update_state(kbdev);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 	}
 }
 
 #ifdef KBASE_PM_RUNTIME
 /**
  * scheduler_pm_idle_before_sleep() - Release the PM reference count and
- *                                    trigger the transition to sleep state.
+ *                                    trigger the tranistion to sleep state.
  *
  * @kbdev: Pointer to the device
  *
@@ -1149,15 +1151,28 @@ static void scheduler_pm_idle(struct kbase_device *kbdev)
 static void scheduler_pm_idle_before_sleep(struct kbase_device *kbdev)
 {
 	unsigned long flags;
+	u32 prev_count;
 
 	lockdep_assert_held(&kbdev->csf.scheduler.lock);
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	prev_count = kbdev->csf.scheduler.pm_active_count;
+	if (!WARN_ON(prev_count == 0))
+		kbdev->csf.scheduler.pm_active_count--;
 	kbdev->pm.backend.gpu_sleep_mode_active = true;
 	kbdev->pm.backend.exit_gpu_sleep_mode = false;
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
-	scheduler_pm_idle(kbdev);
+	if (prev_count == 1) {
+		kbase_pm_context_idle(kbdev);
+		/* Invoke the PM state machines again as the change in MCU
+		 * desired status, due to the update of scheduler.pm_active_count,
+		 * may be missed by the thread that called pm_wait_for_desired_state()
+		 */
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		kbase_pm_update_state(kbdev);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	}
 }
 #endif
 
@@ -1173,8 +1188,8 @@ static void scheduler_wakeup(struct kbase_device *kbdev, bool kick)
 
 	if (scheduler->state == SCHED_SUSPENDED) {
 		dev_dbg(kbdev->dev, "Re-activating the Scheduler after suspend");
-		ret = scheduler_pm_active_handle_suspend(
-			kbdev, KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE, false);
+		ret = scheduler_pm_active_handle_suspend(kbdev,
+							 KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE);
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
 		hrtimer_start(&scheduler->gpu_metrics_timer,
 			      HR_TIMER_DELAY_NSEC(kbase_gpu_metrics_get_tp_emit_interval()),
@@ -1182,8 +1197,13 @@ static void scheduler_wakeup(struct kbase_device *kbdev, bool kick)
 #endif
 	} else {
 #ifdef KBASE_PM_RUNTIME
+		unsigned long flags;
+
 		dev_dbg(kbdev->dev, "Re-activating the Scheduler out of sleep");
-		ret = scheduler_pm_active_after_sleep(kbdev);
+
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		ret = scheduler_pm_active_after_sleep(kbdev, &flags);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 #endif
 	}
 
@@ -1191,7 +1211,7 @@ static void scheduler_wakeup(struct kbase_device *kbdev, bool kick)
 		/* GPUCORE-29850 would add the handling for the case where
 		 * Scheduler could not be activated due to system suspend.
 		 */
-		dev_dbg(kbdev->dev, "Couldn't wakeup Scheduler due to system suspend");
+		dev_info(kbdev->dev, "Couldn't wakeup Scheduler due to system suspend");
 		return;
 	}
 
@@ -1202,20 +1222,13 @@ static void scheduler_wakeup(struct kbase_device *kbdev, bool kick)
 		scheduler_enable_tick_timer_nolock(kbdev);
 }
 
-static int scheduler_suspend(struct kbase_device *kbdev)
+static void scheduler_suspend(struct kbase_device *kbdev)
 {
 	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
 
 	lockdep_assert_held(&scheduler->lock);
 
 	if (!WARN_ON(scheduler->state == SCHED_SUSPENDED)) {
-#if KBASE_PM_RUNTIME
-		int ret;
-
-		ret = kbase_csf_firmware_soi_disable_on_scheduler_suspend(kbdev);
-		if (ret)
-			return ret;
-#endif /* KBASE_PM_RUNTIME */
 		dev_dbg(kbdev->dev, "Suspending the Scheduler");
 		scheduler_pm_idle(kbdev);
 		scheduler->state = SCHED_SUSPENDED;
@@ -1224,8 +1237,6 @@ static int scheduler_suspend(struct kbase_device *kbdev)
 #endif
 		KBASE_KTRACE_ADD(kbdev, SCHED_SUSPENDED, NULL, scheduler->state);
 	}
-
-	return 0;
 }
 
 /**
@@ -1268,9 +1279,8 @@ static void update_idle_suspended_group_state(struct kbase_queue_group *group)
 			unsigned int n_slots = group->kctx->kbdev->csf.global_iface.group_num;
 
 			spin_lock_irqsave(&scheduler->interrupt_lock, flags);
-			n_idle = (unsigned int)bitmap_weight(scheduler->csg_slots_idle_mask,
-							     n_slots);
-			n_used = (unsigned int)bitmap_weight(scheduler->csg_inuse_bitmap, n_slots);
+			n_idle = bitmap_weight(scheduler->csg_slots_idle_mask, n_slots);
+			n_used = bitmap_weight(scheduler->csg_inuse_bitmap, n_slots);
 			spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
 
 			if (n_idle || n_used < scheduler->num_csg_slots_for_tick ||
@@ -1282,7 +1292,7 @@ static void update_idle_suspended_group_state(struct kbase_queue_group *group)
 
 	new_val = atomic_inc_return(&scheduler->non_idle_offslot_grps);
 	KBASE_KTRACE_ADD_CSF_GRP(group->kctx->kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_INC, group,
-				 (u64)new_val);
+				 new_val);
 }
 
 int kbase_csf_scheduler_group_get_slot_locked(struct kbase_queue_group *group)
@@ -1353,7 +1363,7 @@ bool kbase_csf_scheduler_group_events_enabled(struct kbase_device *kbdev,
 }
 
 struct kbase_queue_group *kbase_csf_scheduler_get_group_on_slot(struct kbase_device *kbdev,
-								u32 slot)
+								int slot)
 {
 	lockdep_assert_held(&kbdev->csf.scheduler.interrupt_lock);
 
@@ -1368,8 +1378,7 @@ static int halt_stream_sync(struct kbase_queue *queue)
 	struct kbase_csf_cmd_stream_group_info *ginfo;
 	struct kbase_csf_cmd_stream_info *stream;
 	int csi_index = queue->csi_index;
-	const unsigned int fw_timeout_ms = kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT);
-	long remaining = kbase_csf_timeout_in_jiffies(fw_timeout_ms);
+	long remaining = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms);
 	unsigned long flags;
 
 	if (WARN_ON(!group) || WARN_ON(!kbasep_csf_scheduler_group_is_on_slot_locked(group)))
@@ -1390,8 +1399,8 @@ static int halt_stream_sync(struct kbase_queue *queue)
 			dev_warn(
 				kbdev->dev,
 				"[%llu] Timeout (%d ms) waiting for queue to start on csi %d bound to group %d on slot %d",
-				kbase_backend_get_cycle_cnt(kbdev), fw_timeout_ms, csi_index,
-				group->handle, group->csg_nr);
+				kbase_backend_get_cycle_cnt(kbdev), kbdev->csf.fw_timeout_ms,
+				csi_index, group->handle, group->csg_nr);
 			if (kbase_prepare_to_reset_gpu(kbdev, RESET_FLAGS_NONE))
 				kbase_reset_gpu(kbdev);
 
@@ -1399,7 +1408,7 @@ static int halt_stream_sync(struct kbase_queue *queue)
 			return -ETIMEDOUT;
 		}
 
-		remaining = kbase_csf_timeout_in_jiffies(fw_timeout_ms);
+		remaining = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms);
 	}
 
 	spin_lock_irqsave(&kbdev->csf.scheduler.interrupt_lock, flags);
@@ -1421,8 +1430,8 @@ static int halt_stream_sync(struct kbase_queue *queue)
 		dev_warn(
 			kbdev->dev,
 			"[%llu] Timeout (%d ms) waiting for queue to stop on csi %d bound to group %d on slot %d",
-			kbase_backend_get_cycle_cnt(kbdev), fw_timeout_ms, queue->csi_index,
-			group->handle, group->csg_nr);
+			kbase_backend_get_cycle_cnt(kbdev), kbdev->csf.fw_timeout_ms,
+			queue->csi_index, group->handle, group->csg_nr);
 
 		/* TODO GPUCORE-25328: The CSG can't be terminated, the GPU
 		 * will be reset as a work-around.
@@ -1477,8 +1486,7 @@ static int sched_halt_stream(struct kbase_queue *queue)
 	long remaining;
 	int slot;
 	int err = 0;
-	const u32 group_schedule_timeout = kbdev->csf.csg_suspend_timeout_ms;
-	const u32 fw_timeout_ms = kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT);
+	const u32 group_schedule_timeout = kbase_get_timeout_ms(kbdev, CSF_CSG_SUSPEND_TIMEOUT);
 
 	if (WARN_ON(!group))
 		return -EINVAL;
@@ -1577,14 +1585,15 @@ static int sched_halt_stream(struct kbase_queue *queue)
 					kbdev->csf.event_wait,
 					(CS_ACK_STATE_GET(kbase_csf_firmware_cs_output(
 						 stream, CS_ACK)) == CS_ACK_STATE_STOP),
-					kbase_csf_timeout_in_jiffies(fw_timeout_ms));
+					kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms));
 
 				if (!remaining) {
 					dev_warn(
 						kbdev->dev,
 						"[%llu] Timeout (%d ms) waiting for queue stop ack on csi %d bound to group %d on slot %d",
-						kbase_backend_get_cycle_cnt(kbdev), fw_timeout_ms,
-						queue->csi_index, group->handle, group->csg_nr);
+						kbase_backend_get_cycle_cnt(kbdev),
+						kbdev->csf.fw_timeout_ms, queue->csi_index,
+						group->handle, group->csg_nr);
 
 
 					err = -ETIMEDOUT;
@@ -1719,7 +1728,7 @@ static void program_cs_trace_cfg(struct kbase_csf_cmd_stream_info *stream,
 	 * queue's register_ex call.
 	 */
 	if (kbase_csf_scheduler_queue_has_trace(queue)) {
-		u32 cs_cfg = CS_INSTR_CONFIG_JASID_SET(queue->trace_cfg, (u32)queue->kctx->as_nr);
+		u32 cs_cfg = CS_INSTR_CONFIG_JASID_SET(queue->trace_cfg, queue->kctx->as_nr);
 
 		kbase_csf_firmware_cs_input(stream, CS_INSTR_CONFIG, cs_cfg);
 		kbase_csf_firmware_cs_input(stream, CS_INSTR_BUFFER_SIZE, queue->trace_buffer_size);
@@ -1792,7 +1801,7 @@ static void program_cs(struct kbase_device *kbdev, struct kbase_queue *queue,
 	kbase_csf_firmware_cs_input(stream, CS_USER_OUTPUT_HI, user_output >> 32);
 
 	kbase_csf_firmware_cs_input(stream, CS_CONFIG,
-				    ((u32)queue->doorbell_nr << 8) | (queue->priority & 0xF));
+				    (queue->doorbell_nr << 8) | (queue->priority & 0xF));
 
 	/* Program the queue's cs_trace configuration */
 	program_cs_trace_cfg(stream, queue);
@@ -1957,6 +1966,7 @@ static enum kbase_csf_csg_slot_state update_csg_slot_status(struct kbase_device
 		if ((state == CSG_ACK_STATE_START) || (state == CSG_ACK_STATE_RESUME)) {
 			slot_state = CSG_SLOT_RUNNING;
 			atomic_set(&csg_slot->state, slot_state);
+			csg_slot->trigger_jiffies = jiffies;
 			KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_RUNNING, csg_slot->resident_group,
 						 state);
 			dev_dbg(kbdev->dev, "Group %u running on slot %d\n",
@@ -1967,6 +1977,7 @@ static enum kbase_csf_csg_slot_state update_csg_slot_status(struct kbase_device
 		if ((state == CSG_ACK_STATE_SUSPEND) || (state == CSG_ACK_STATE_TERMINATE)) {
 			slot_state = CSG_SLOT_STOPPED;
 			atomic_set(&csg_slot->state, slot_state);
+			csg_slot->trigger_jiffies = jiffies;
 			KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_STOPPED, csg_slot->resident_group,
 						 state);
 			dev_dbg(kbdev->dev, "Group %u stopped on slot %d\n",
@@ -2028,7 +2039,6 @@ static void halt_csg_slot(struct kbase_queue_group *group, bool suspend)
 	struct kbase_device *kbdev = group->kctx->kbdev;
 	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
 	struct kbase_csf_csg_slot *csg_slot = kbdev->csf.scheduler.csg_slots;
-	const unsigned int fw_timeout_ms = kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT);
 	s8 slot;
 
 	lockdep_assert_held(&kbdev->csf.scheduler.lock);
@@ -2040,14 +2050,15 @@ static void halt_csg_slot(struct kbase_queue_group *group, bool suspend)
 
 	/* When in transition, wait for it to complete */
 	if (atomic_read(&csg_slot[slot].state) == CSG_SLOT_READY2RUN) {
-		long remaining = kbase_csf_timeout_in_jiffies(fw_timeout_ms);
+		long remaining = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms);
 
 		dev_dbg(kbdev->dev, "slot %d wait for up-running\n", slot);
 		remaining = wait_event_timeout(kbdev->csf.event_wait, csg_slot_running(kbdev, slot),
 					       remaining);
 		if (!remaining)
 			dev_warn(kbdev->dev, "[%llu] slot %d timeout (%d ms) on up-running\n",
-				 kbase_backend_get_cycle_cnt(kbdev), slot, fw_timeout_ms);
+				 kbase_backend_get_cycle_cnt(kbdev), slot,
+				 kbdev->csf.fw_timeout_ms);
 	}
 
 	if (csg_slot_running(kbdev, slot)) {
@@ -2065,9 +2076,10 @@ static void halt_csg_slot(struct kbase_queue_group *group, bool suspend)
 		kbase_csf_ring_csg_doorbell(kbdev, slot);
 		spin_unlock_irqrestore(&kbdev->csf.scheduler.interrupt_lock, flags);
 		atomic_set(&csg_slot[slot].state, CSG_SLOT_DOWN2STOP);
+		csg_slot[slot].trigger_jiffies = jiffies;
 		KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_STOP_REQ, group, halt_cmd);
 
-		KBASE_TLSTREAM_TL_KBASE_DEVICE_HALTING_CSG(kbdev, kbdev->id, (u32)slot, suspend);
+		KBASE_TLSTREAM_TL_KBASE_DEVICE_HALTING_CSG(kbdev, kbdev->id, slot, suspend);
 	}
 }
 
@@ -2542,8 +2554,7 @@ static void update_offslot_non_idle_cnt(struct kbase_queue_group *group)
 
 	if (group->prepared_seq_num < scheduler->non_idle_scanout_grps) {
 		int new_val = atomic_dec_return(&scheduler->non_idle_offslot_grps);
-		KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_DEC, group,
-					 (u64)new_val);
+		KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_DEC, group, new_val);
 	}
 }
 
@@ -2558,8 +2569,7 @@ static void update_offslot_non_idle_cnt_for_onslot_grp(struct kbase_queue_group
 
 	if (group->prepared_seq_num < scheduler->non_idle_scanout_grps) {
 		int new_val = atomic_dec_return(&scheduler->non_idle_offslot_grps);
-		KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_DEC, group,
-					 (u64)new_val);
+		KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_DEC, group, new_val);
 	}
 }
 
@@ -2577,13 +2587,13 @@ static void update_offslot_non_idle_cnt_on_grp_suspend(struct kbase_queue_group
 			if (group->run_state == KBASE_CSF_GROUP_SUSPENDED) {
 				int new_val = atomic_inc_return(&scheduler->non_idle_offslot_grps);
 				KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_INC,
-							 group, (u64)new_val);
+							 group, new_val);
 			}
 		} else {
 			if (group->run_state != KBASE_CSF_GROUP_SUSPENDED) {
 				int new_val = atomic_dec_return(&scheduler->non_idle_offslot_grps);
 				KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_DEC,
-							 group, (u64)new_val);
+							 group, new_val);
 			}
 		}
 	} else {
@@ -2591,7 +2601,7 @@ static void update_offslot_non_idle_cnt_on_grp_suspend(struct kbase_queue_group
 		if (group->run_state == KBASE_CSF_GROUP_SUSPENDED) {
 			int new_val = atomic_inc_return(&scheduler->non_idle_offslot_grps);
 			KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_INC, group,
-						 (u64)new_val);
+						 new_val);
 		}
 	}
 }
@@ -2783,12 +2793,13 @@ static bool cleanup_csg_slot(struct kbase_queue_group *group)
 	spin_unlock_bh(&kbdev->csf.scheduler.gpu_metrics_lock);
 #endif
 
+	csg_slot->trigger_jiffies = jiffies;
 	atomic_set(&csg_slot->state, CSG_SLOT_READY);
 
-	KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_CLEANED, group, (u64)slot);
+	KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_CLEANED, group, slot);
 	dev_dbg(kbdev->dev, "Cleanup done for group %d on slot %d\n", group->handle, slot);
 
-	KBASE_TLSTREAM_TL_KBASE_DEVICE_DEPROGRAM_CSG(kbdev, kbdev->id, (u32)slot);
+	KBASE_TLSTREAM_TL_KBASE_DEVICE_DEPROGRAM_CSG(kbdev, kbdev->id, slot);
 
 	/* Notify the group is off-slot and the csg_reg might be available for
 	 * resue with other groups in a 'lazy unbinding' style.
@@ -2954,7 +2965,7 @@ static void program_csg_slot(struct kbase_queue_group *group, s8 slot, u8 prio)
 	kbase_csf_firmware_csg_input(ginfo, CSG_EP_REQ_LO, ep_cfg & U32_MAX);
 
 	/* Program the address space number assigned to the context */
-	kbase_csf_firmware_csg_input(ginfo, CSG_CONFIG, (u32)kctx->as_nr);
+	kbase_csf_firmware_csg_input(ginfo, CSG_CONFIG, kctx->as_nr);
 
 	kbase_csf_firmware_csg_input(ginfo, CSG_SUSPEND_BUF_LO, normal_suspend_buf & U32_MAX);
 	kbase_csf_firmware_csg_input(ginfo, CSG_SUSPEND_BUF_HI, normal_suspend_buf >> 32);
@@ -2996,12 +3007,12 @@ static void program_csg_slot(struct kbase_queue_group *group, s8 slot, u8 prio)
 
 	/* Update status before rings the door-bell, marking ready => run */
 	atomic_set(&csg_slot->state, CSG_SLOT_READY2RUN);
+	csg_slot->trigger_jiffies = jiffies;
 	csg_slot->priority = prio;
 
 	/* Trace the programming of the CSG on the slot */
 	KBASE_TLSTREAM_TL_KBASE_DEVICE_PROGRAM_CSG(kbdev, kbdev->id, group->kctx->id, group->handle,
-						   (u32)slot,
-						   (state == CSG_REQ_STATE_RESUME) ? 1 : 0);
+						   slot, (state == CSG_REQ_STATE_RESUME) ? 1 : 0);
 
 	dev_dbg(kbdev->dev, "Starting group %d of context %d_%d on slot %d with priority %u\n",
 		group->handle, kctx->tgid, kctx->id, slot, prio);
@@ -3052,7 +3063,7 @@ static void sched_evict_group(struct kbase_queue_group *group, bool fault,
 		     group->run_state == KBASE_CSF_GROUP_RUNNABLE)) {
 			int new_val = atomic_dec_return(&scheduler->non_idle_offslot_grps);
 			KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_DEC, group,
-						 (u64)new_val);
+						 new_val);
 		}
 
 		for (i = 0; i < MAX_SUPPORTED_STREAMS_PER_GROUP; i++) {
@@ -3098,9 +3109,7 @@ static void sched_evict_group(struct kbase_queue_group *group, bool fault,
 static int term_group_sync(struct kbase_queue_group *group)
 {
 	struct kbase_device *kbdev = group->kctx->kbdev;
-	const unsigned int group_term_timeout_ms =
-		kbase_get_timeout_ms(kbdev, CSF_CSG_TERM_TIMEOUT);
-	long remaining = kbase_csf_timeout_in_jiffies(group_term_timeout_ms);
+	long remaining = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms);
 	int err = 0;
 
 	term_csg_slot(group);
@@ -3116,11 +3125,11 @@ static int term_group_sync(struct kbase_queue_group *group)
 		dev_warn(
 			kbdev->dev,
 			"[%llu] term request timeout (%d ms) for group %d of context %d_%d on slot %d",
-			kbase_backend_get_cycle_cnt(kbdev), group_term_timeout_ms, group->handle,
+			kbase_backend_get_cycle_cnt(kbdev), kbdev->csf.fw_timeout_ms, group->handle,
 			group->kctx->tgid, group->kctx->id, group->csg_nr);
 		if (kbase_csf_firmware_ping_wait(kbdev, FW_PING_AFTER_ERROR_TIMEOUT_MS))
 			error_type = DF_PING_REQUEST_TIMEOUT;
-		schedule_actions_trigger_df(kbdev, group->kctx, error_type);
+		kbase_debug_csf_fault_notify(kbdev, group->kctx, error_type);
 		if (kbase_prepare_to_reset_gpu(kbdev, RESET_FLAGS_NONE))
 			kbase_reset_gpu(kbdev);
 
@@ -3267,8 +3276,7 @@ static int scheduler_group_schedule(struct kbase_queue_group *group)
 		insert_group_to_runnable(&kbdev->csf.scheduler, group, KBASE_CSF_GROUP_RUNNABLE);
 		/* A new group into the scheduler */
 		new_val = atomic_inc_return(&kbdev->csf.scheduler.non_idle_offslot_grps);
-		KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_INC, group,
-					 (u64)new_val);
+		KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_INC, group, new_val);
 	}
 
 	/* Since a group has become active now, check if GPU needs to be
@@ -3294,8 +3302,7 @@ static inline void set_max_csg_slots(struct kbase_device *kbdev)
 {
 	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
 	unsigned int total_csg_slots = kbdev->csf.global_iface.group_num;
-	unsigned int max_address_space_slots =
-		(unsigned int)kbdev->nr_hw_address_spaces - NUM_RESERVED_AS_SLOTS;
+	unsigned int max_address_space_slots = kbdev->nr_hw_address_spaces - NUM_RESERVED_AS_SLOTS;
 
 	WARN_ON(scheduler->num_active_address_spaces > total_csg_slots);
 
@@ -3318,8 +3325,7 @@ static inline void count_active_address_space(struct kbase_device *kbdev,
 {
 	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
 	unsigned int total_csg_slots = kbdev->csf.global_iface.group_num;
-	unsigned int max_address_space_slots =
-		(unsigned int)kbdev->nr_hw_address_spaces - NUM_RESERVED_AS_SLOTS;
+	unsigned int max_address_space_slots = kbdev->nr_hw_address_spaces - NUM_RESERVED_AS_SLOTS;
 
 	if (scheduler->ngrp_to_schedule <= total_csg_slots) {
 		if (kctx->csf.sched.ngrp_to_schedule == 1)
@@ -3352,9 +3358,9 @@ static inline void count_active_address_space(struct kbase_device *kbdev,
  * priority of a slot belonging to a higher priority idle group will always be
  * greater than the priority of a slot belonging to a lower priority non-idle
  * group, reflecting the original position of a group in the scan order (i.e
- * static priority) 'sched_act_seq_num', which is set during the prepare phase
- * of a tick/tock before the group is moved to 'idle_groups_to_schedule' list if
- * it is idle.
+ * static priority) 'scan_seq_num', which is set during the prepare phase of a
+ * tick/tock before the group is moved to 'idle_groups_to_schedule' list if it
+ * is idle.
  * The priority range [MAX_CSG_SLOT_PRIORITY, 0] is partitioned with the first
  * 'slots_for_tick' groups in the original scan order are assigned a priority in
  * the subrange [MAX_CSG_SLOT_PRIORITY, MAX_CSG_SLOT_PRIORITY - slots_for_tick),
@@ -3379,8 +3385,8 @@ static u8 get_slot_priority(struct kbase_queue_group *group)
 		slot_prio = (u8)(MAX_CSG_SLOT_PRIORITY - used_slots);
 	} else {
 		/* There will be a mix of idle and non-idle groups. */
-		if (group->sched_act_seq_num < slots_for_tick)
-			slot_prio = (u8)(MAX_CSG_SLOT_PRIORITY - group->sched_act_seq_num);
+		if (group->scan_seq_num < slots_for_tick)
+			slot_prio = (u8)(MAX_CSG_SLOT_PRIORITY - group->scan_seq_num);
 		else if (MAX_CSG_SLOT_PRIORITY > (slots_for_tick + used_slots))
 			slot_prio = (u8)(MAX_CSG_SLOT_PRIORITY - (slots_for_tick + used_slots));
 		else
@@ -3561,7 +3567,8 @@ static void program_suspending_csg_slots(struct kbase_device *kbdev)
 
 	while (!bitmap_empty(slot_mask, MAX_SUPPORTED_CSGS)) {
 		DECLARE_BITMAP(changed, MAX_SUPPORTED_CSGS);
-		long remaining = kbase_csf_timeout_in_jiffies(kbdev->csf.csg_suspend_timeout_ms);
+		long remaining = kbase_csf_timeout_in_jiffies(
+			kbase_get_timeout_ms(kbdev, CSF_CSG_SUSPEND_TIMEOUT));
 
 		bitmap_copy(changed, slot_mask, MAX_SUPPORTED_CSGS);
 
@@ -3637,7 +3644,7 @@ static void program_suspending_csg_slots(struct kbase_device *kbdev)
 					"[%llu] Group %d of context %d_%d on slot %u failed to suspend (timeout %d ms)",
 					kbase_backend_get_cycle_cnt(kbdev), group->handle,
 					group->kctx->tgid, group->kctx->id, i,
-					kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT));
+					kbdev->csf.fw_timeout_ms);
 				if (kbase_csf_firmware_ping_wait(kbdev,
 								 FW_PING_AFTER_ERROR_TIMEOUT_MS))
 					error_type = DF_PING_REQUEST_TIMEOUT;
@@ -3706,8 +3713,7 @@ static void wait_csg_slots_start(struct kbase_device *kbdev)
 {
 	u32 num_groups = kbdev->csf.global_iface.group_num;
 	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
-	long remaining =
-		kbase_csf_timeout_in_jiffies(kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT));
+	long remaining = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms);
 	DECLARE_BITMAP(slot_mask, MAX_SUPPORTED_CSGS) = { 0 };
 	u32 i;
 
@@ -3747,9 +3753,8 @@ static void wait_csg_slots_start(struct kbase_device *kbdev)
 
 			dev_err(kbdev->dev,
 				"[%llu] Timeout (%d ms) waiting for CSG slots to start, slots: 0x%*pb\n",
-				kbase_backend_get_cycle_cnt(kbdev),
-				kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT), num_groups,
-				slot_mask);
+				kbase_backend_get_cycle_cnt(kbdev), kbdev->csf.fw_timeout_ms,
+				num_groups, slot_mask);
 			if (kbase_csf_firmware_ping_wait(kbdev, FW_PING_AFTER_ERROR_TIMEOUT_MS))
 				error_type = DF_PING_REQUEST_TIMEOUT;
 			schedule_actions_trigger_df(kbdev, group->kctx, error_type);
@@ -3879,8 +3884,7 @@ static int wait_csg_slots_handshake_ack(struct kbase_device *kbdev, u32 field_ma
 static void wait_csg_slots_finish_prio_update(struct kbase_device *kbdev)
 {
 	unsigned long *slot_mask = kbdev->csf.scheduler.csg_slots_prio_update;
-	const unsigned int fw_timeout_ms = kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT);
-	long wait_time = kbase_csf_timeout_in_jiffies(fw_timeout_ms);
+	long wait_time = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms);
 	int ret = wait_csg_slots_handshake_ack(kbdev, CSG_REQ_EP_CFG_MASK, slot_mask, wait_time);
 
 	lockdep_assert_held(&kbdev->csf.scheduler.lock);
@@ -3894,7 +3898,7 @@ static void wait_csg_slots_finish_prio_update(struct kbase_device *kbdev)
 		dev_warn(
 			kbdev->dev,
 			"[%llu] Timeout (%d ms) on CSG_REQ:EP_CFG, skipping the update wait: slot mask=0x%lx",
-			kbase_backend_get_cycle_cnt(kbdev), fw_timeout_ms, slot_mask[0]);
+			kbase_backend_get_cycle_cnt(kbdev), kbdev->csf.fw_timeout_ms, slot_mask[0]);
 		if (kbase_csf_firmware_ping_wait(kbdev, FW_PING_AFTER_ERROR_TIMEOUT_MS))
 			error_type = DF_PING_REQUEST_TIMEOUT;
 		schedule_actions_trigger_df(kbdev, group->kctx, error_type);
@@ -4105,7 +4109,7 @@ static void scheduler_group_check_protm_enter(struct kbase_device *const kbdev,
 	 * entry to protected mode happens with a memory region being locked and
 	 * the same region is then accessed by the GPU in protected mode.
 	 */
-	down_write(&kbdev->csf.mmu_sync_sem);
+	mutex_lock(&kbdev->mmu_hw_mutex);
 	spin_lock_irqsave(&scheduler->interrupt_lock, flags);
 
 	/* Check if the previous transition to enter & exit the protected
@@ -4171,7 +4175,7 @@ static void scheduler_group_check_protm_enter(struct kbase_device *const kbdev,
 				spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
 
 				err = kbase_csf_wait_protected_mode_enter(kbdev);
-				up_write(&kbdev->csf.mmu_sync_sem);
+				mutex_unlock(&kbdev->mmu_hw_mutex);
 
 				if (err)
 					schedule_actions_trigger_df(
@@ -4186,7 +4190,7 @@ static void scheduler_group_check_protm_enter(struct kbase_device *const kbdev,
 	}
 
 	spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
-	up_write(&kbdev->csf.mmu_sync_sem);
+	mutex_unlock(&kbdev->mmu_hw_mutex);
 }
 
 /**
@@ -4293,34 +4297,6 @@ static void scheduler_apply(struct kbase_device *kbdev)
 	program_suspending_csg_slots(kbdev);
 }
 
-/**
- * scheduler_scan_add_group_to_sched_list - Add a schedulable group to one of the scheduler's
- *                                          schedulable lists following exactly the scan sequence.
- *
- * @kbdev:      Pointer to the GPU device.
- * @sched_list: the scheduling list for appending the given group
- * @group:      the group that is to be added forllowing the scan out procedure.
- *
- * This function is called when a group is required to be added to a scheduler maintained list
- * for a tick/tock scheduling operation.
- */
-static void scheduler_scan_add_group_to_sched_list(struct kbase_device *kbdev,
-						   struct list_head *sched_list,
-						   struct kbase_queue_group *group)
-{
-	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
-
-	lockdep_assert_held(&scheduler->lock);
-	lockdep_assert_held(&scheduler->interrupt_lock);
-
-	list_add_tail(&group->link_to_schedule, sched_list);
-	/* Set the act_seq_num when a group is added to a schedulable list.
-	 * Otherwise it's a don't care case, as the group would not be involved
-	 * with onslot running.
-	 */
-	group->sched_act_seq_num = scheduler->csg_scan_sched_count++;
-}
-
 static void scheduler_ctx_scan_groups(struct kbase_device *kbdev, struct kbase_context *kctx,
 				      int priority, struct list_head *privileged_groups,
 				      struct list_head *active_groups)
@@ -4362,17 +4338,17 @@ static void scheduler_ctx_scan_groups(struct kbase_device *kbdev, struct kbase_c
 			update_idle_protm_group_state_to_runnable(group);
 		else if (queue_group_idle_locked(group)) {
 			if (can_schedule_idle_group(group))
-				scheduler_scan_add_group_to_sched_list(
-					kbdev, &scheduler->idle_groups_to_schedule, group);
+				list_add_tail(&group->link_to_schedule,
+					      &scheduler->idle_groups_to_schedule);
 			continue;
 		}
 
 		if (protm_req && (group->priority == KBASE_QUEUE_GROUP_PRIORITY_REALTIME)) {
-			scheduler_scan_add_group_to_sched_list(kbdev, privileged_groups, group);
+			list_add_tail(&group->link_to_schedule, privileged_groups);
 			continue;
 		}
 
-		scheduler_scan_add_group_to_sched_list(kbdev, active_groups, group);
+		list_add_tail(&group->link_to_schedule, active_groups);
 	}
 }
 
@@ -4583,9 +4559,7 @@ static void scheduler_update_idle_slots_status(struct kbase_device *kbdev,
 
 	/* The groups are aggregated into a single kernel doorbell request */
 	if (!bitmap_empty(csg_bitmap, num_groups)) {
-		const unsigned int fw_timeout_ms =
-			kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT);
-		long wt = kbase_csf_timeout_in_jiffies(fw_timeout_ms);
+		long wt = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms);
 		u32 db_slots = (u32)csg_bitmap[0];
 
 		kbase_csf_ring_csg_slots_doorbell(kbdev, db_slots);
@@ -4600,7 +4574,8 @@ static void scheduler_update_idle_slots_status(struct kbase_device *kbdev,
 			dev_warn(
 				kbdev->dev,
 				"[%llu] Timeout (%d ms) on CSG_REQ:STATUS_UPDATE, treat groups as not idle: slot mask=0x%lx",
-				kbase_backend_get_cycle_cnt(kbdev), fw_timeout_ms, csg_bitmap[0]);
+				kbase_backend_get_cycle_cnt(kbdev), kbdev->csf.fw_timeout_ms,
+				csg_bitmap[0]);
 			schedule_actions_trigger_df(kbdev, group->kctx,
 						    DF_CSG_STATUS_UPDATE_TIMEOUT);
 
@@ -4765,27 +4740,24 @@ static int suspend_active_groups_on_powerdown(struct kbase_device *kbdev, bool s
 	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
 	DECLARE_BITMAP(slot_mask, MAX_SUPPORTED_CSGS) = { 0 };
 
-	if (unlikely(suspend_active_queue_groups(kbdev, slot_mask, false))) {
-		if (!is_gpu_level_suspend_supported(kbdev)) {
-			const int csg_nr = ffs(slot_mask[0]) - 1;
-			struct kbase_queue_group *group;
-			enum dumpfault_error_type error_type = DF_CSG_SUSPEND_TIMEOUT;
+	int ret = suspend_active_queue_groups(kbdev, slot_mask);
 
-			group = scheduler->csg_slots[csg_nr].resident_group;
+	if (unlikely(ret)) {
+		const int csg_nr = ffs(slot_mask[0]) - 1;
+		struct kbase_queue_group *group = scheduler->csg_slots[csg_nr].resident_group;
+		enum dumpfault_error_type error_type = DF_CSG_SUSPEND_TIMEOUT;
 
-			/* The suspend of CSGs failed,
-			 * trigger the GPU reset to be in a deterministic state.
-			 */
-			dev_warn(
-				kbdev->dev,
-				"[%llu] Timeout (%d ms) waiting for CSG slots to suspend on power down, slot_mask: 0x%*pb\n",
-				kbase_backend_get_cycle_cnt(kbdev),
-				kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT),
-				kbdev->csf.global_iface.group_num, slot_mask);
-			if (kbase_csf_firmware_ping_wait(kbdev, FW_PING_AFTER_ERROR_TIMEOUT_MS))
-				error_type = DF_PING_REQUEST_TIMEOUT;
-			schedule_actions_trigger_df(kbdev, group->kctx, error_type);
-		}
+		/* The suspend of CSGs failed,
+		 * trigger the GPU reset to be in a deterministic state.
+		 */
+		dev_warn(
+			kbdev->dev,
+			"[%llu] Timeout (%d ms) waiting for CSG slots to suspend on power down, slot_mask: 0x%*pb\n",
+			kbase_backend_get_cycle_cnt(kbdev), kbdev->csf.fw_timeout_ms,
+			kbdev->csf.global_iface.group_num, slot_mask);
+		if (kbase_csf_firmware_ping_wait(kbdev, FW_PING_AFTER_ERROR_TIMEOUT_MS))
+			error_type = DF_PING_REQUEST_TIMEOUT;
+		schedule_actions_trigger_df(kbdev, group->kctx, error_type);
 
 		if (kbase_prepare_to_reset_gpu(kbdev, RESET_FLAGS_NONE))
 			kbase_reset_gpu(kbdev);
@@ -4793,8 +4765,6 @@ static int suspend_active_groups_on_powerdown(struct kbase_device *kbdev, bool s
 		return -1;
 	}
 
-	kbdev->csf.mcu_halted = false;
-
 	/* Check if the groups became active whilst the suspend was ongoing,
 	 * but only for the case where the system suspend is not in progress
 	 */
@@ -4812,7 +4782,7 @@ static int suspend_active_groups_on_powerdown(struct kbase_device *kbdev, bool s
  * Returns false if any of the queues inside any of the groups that have been
  * assigned a physical CSG slot have work to execute, or have executed work
  * since having received a GPU idle notification. This function is used to
- * handle a race condition between firmware reporting GPU idle and userspace
+ * handle a rance condition between firmware reporting GPU idle and userspace
  * submitting more work by directly ringing a doorbell.
  *
  * Return: false if any queue inside any resident group has work to be processed
@@ -4873,29 +4843,42 @@ static bool scheduler_idle_suspendable(struct kbase_device *kbdev)
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	spin_lock(&scheduler->interrupt_lock);
 
+	if (scheduler->fast_gpu_idle_handling) {
+		scheduler->fast_gpu_idle_handling = false;
+
+		if (scheduler->total_runnable_grps) {
+			suspend = !atomic_read(&scheduler->non_idle_offslot_grps) &&
+				  kbase_pm_idle_groups_sched_suspendable(kbdev);
+		} else
+			suspend = kbase_pm_no_runnables_sched_suspendable(kbdev);
+
+		if (suspend && unlikely(atomic_read(&scheduler->gpu_no_longer_idle)))
+			suspend = false;
+
+		spin_unlock(&scheduler->interrupt_lock);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+		return suspend;
+	}
+
 	if (scheduler->total_runnable_grps) {
 		/* Check both on-slots and off-slots groups idle status */
-		suspend = (scheduler->fast_gpu_idle_handling ||
-			   kbase_csf_scheduler_all_csgs_idle(kbdev)) &&
+		suspend = kbase_csf_scheduler_all_csgs_idle(kbdev) &&
 			  !atomic_read(&scheduler->non_idle_offslot_grps) &&
 			  kbase_pm_idle_groups_sched_suspendable(kbdev);
 	} else
 		suspend = kbase_pm_no_runnables_sched_suspendable(kbdev);
 
-	if (suspend && unlikely(atomic_read(&scheduler->gpu_no_longer_idle)))
-		suspend = false;
-
 	/* Confirm that all groups are actually idle before proceeding with
 	 * suspension as groups might potentially become active again without
 	 * informing the scheduler in case userspace rings a doorbell directly.
 	 */
-	if (suspend && !scheduler->fast_gpu_idle_handling &&
-	    unlikely(!all_on_slot_groups_remained_idle(kbdev))) {
+	if (suspend && (unlikely(atomic_read(&scheduler->gpu_no_longer_idle)) ||
+			unlikely(!all_on_slot_groups_remained_idle(kbdev)))) {
 		dev_dbg(kbdev->dev, "GPU suspension skipped due to active CSGs");
 		suspend = false;
 	}
 
-	scheduler->fast_gpu_idle_handling = false;
 	spin_unlock(&scheduler->interrupt_lock);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
@@ -4954,23 +4937,19 @@ static bool scheduler_suspend_on_idle(struct kbase_device *kbdev)
 	}
 
 	dev_dbg(kbdev->dev, "Scheduler to be suspended on GPU becoming idle");
-	ret = scheduler_suspend(kbdev);
-	if (!ret) {
-		cancel_tick_work(scheduler);
-		return true;
-	}
-
-	return false;
+	scheduler_suspend(kbdev);
+	cancel_tick_work(scheduler);
+	return true;
 }
 
-static void gpu_idle_worker(struct kbase_device *kbdev)
+static void gpu_idle_worker(struct work_struct *work)
 {
+	struct kbase_device *kbdev =
+		container_of(work, struct kbase_device, csf.scheduler.gpu_idle_work);
 	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
 	bool scheduler_is_idle_suspendable = false;
 	bool all_groups_suspended = false;
 
-	WARN_ON_ONCE(atomic_read(&scheduler->pending_gpu_idle_work) == 0);
-
 	KBASE_KTRACE_ADD(kbdev, SCHEDULER_GPU_IDLE_WORKER_START, NULL, 0u);
 
 #define __ENCODE_KTRACE_INFO(reset, idle, all_suspend) \
@@ -4980,7 +4959,7 @@ static void gpu_idle_worker(struct kbase_device *kbdev)
 		dev_warn(kbdev->dev, "Quit idle for failing to prevent gpu reset.\n");
 		KBASE_KTRACE_ADD(kbdev, SCHEDULER_GPU_IDLE_WORKER_END, NULL,
 				 __ENCODE_KTRACE_INFO(true, false, false));
-		goto exit;
+		return;
 	}
 	kbase_debug_csf_fault_wait_completion(kbdev);
 	mutex_lock(&scheduler->lock);
@@ -4989,7 +4968,7 @@ static void gpu_idle_worker(struct kbase_device *kbdev)
 	if (unlikely(scheduler->state == SCHED_BUSY)) {
 		mutex_unlock(&scheduler->lock);
 		kbase_reset_gpu_allow(kbdev);
-		goto exit;
+		return;
 	}
 #endif
 
@@ -5014,9 +4993,6 @@ static void gpu_idle_worker(struct kbase_device *kbdev)
 			 __ENCODE_KTRACE_INFO(false, scheduler_is_idle_suspendable,
 					      all_groups_suspended));
 #undef __ENCODE_KTRACE_INFO
-
-exit:
-	atomic_dec(&scheduler->pending_gpu_idle_work);
 }
 
 static int scheduler_prepare(struct kbase_device *kbdev)
@@ -5045,7 +5021,6 @@ static int scheduler_prepare(struct kbase_device *kbdev)
 	WARN_ON(!list_empty(&scheduler->idle_groups_to_schedule));
 	scheduler->num_active_address_spaces = 0;
 	scheduler->num_csg_slots_for_tick = 0;
-	scheduler->csg_scan_sched_count = 0;
 	bitmap_zero(scheduler->csg_slots_prio_update, MAX_SUPPORTED_CSGS);
 	INIT_LIST_HEAD(&privileged_groups);
 	INIT_LIST_HEAD(&active_groups);
@@ -5056,19 +5031,9 @@ static int scheduler_prepare(struct kbase_device *kbdev)
 	for (i = 0; i < KBASE_QUEUE_GROUP_PRIORITY_COUNT; ++i) {
 		struct kbase_context *kctx;
 
-		/* Scan the per-priority list of groups twice, firstly for the
-		 * prioritised contexts, then the normal ones.
-		 */
-		list_for_each_entry(kctx, &scheduler->runnable_kctxs, csf.link) {
-			if (atomic_read(&kctx->prioritized))
-				scheduler_ctx_scan_groups(kbdev, kctx, i, &privileged_groups,
-							  &active_groups);
-		}
-		list_for_each_entry(kctx, &scheduler->runnable_kctxs, csf.link) {
-			if (!atomic_read(&kctx->prioritized))
-				scheduler_ctx_scan_groups(kbdev, kctx, i, &privileged_groups,
-							  &active_groups);
-		}
+		list_for_each_entry(kctx, &scheduler->runnable_kctxs, csf.link)
+			scheduler_ctx_scan_groups(kbdev, kctx, i, &privileged_groups,
+						  &active_groups);
 	}
 	spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
 
@@ -5086,7 +5051,7 @@ static int scheduler_prepare(struct kbase_device *kbdev)
 	 * of the tick. It will be subject to up/downs during the scheduler
 	 * active phase.
 	 */
-	atomic_set(&scheduler->non_idle_offslot_grps, (int)scheduler->non_idle_scanout_grps);
+	atomic_set(&scheduler->non_idle_offslot_grps, scheduler->non_idle_scanout_grps);
 	KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_INC, NULL,
 				 scheduler->non_idle_scanout_grps);
 
@@ -5204,7 +5169,8 @@ static int wait_csg_slots_suspend(struct kbase_device *kbdev, unsigned long *slo
 	bitmap_copy(slot_mask_local, slot_mask, MAX_SUPPORTED_CSGS);
 
 	while (!bitmap_empty(slot_mask_local, MAX_SUPPORTED_CSGS)) {
-		long remaining = kbase_csf_timeout_in_jiffies(kbdev->csf.csg_suspend_timeout_ms);
+		long remaining = kbase_csf_timeout_in_jiffies(
+			kbase_get_timeout_ms(kbdev, CSF_CSG_SUSPEND_TIMEOUT));
 		DECLARE_BITMAP(changed, MAX_SUPPORTED_CSGS);
 
 		bitmap_copy(changed, slot_mask_local, MAX_SUPPORTED_CSGS);
@@ -5330,7 +5296,7 @@ static void evict_lru_or_blocked_csg(struct kbase_device *kbdev)
 	}
 
 	if (lru_idle_group != NULL) {
-		unsigned long slot_mask = 1UL << lru_idle_group->csg_nr;
+		unsigned long slot_mask = 1 << lru_idle_group->csg_nr;
 
 		dev_dbg(kbdev->dev, "Suspending LRU idle group %d of context %d_%d on slot %d",
 			lru_idle_group->handle, lru_idle_group->kctx->tgid,
@@ -5344,8 +5310,7 @@ static void evict_lru_or_blocked_csg(struct kbase_device *kbdev)
 				"[%llu] LRU idle group %d of context %d_%d failed to suspend on slot %d (timeout %d ms)",
 				kbase_backend_get_cycle_cnt(kbdev), lru_idle_group->handle,
 				lru_idle_group->kctx->tgid, lru_idle_group->kctx->id,
-				lru_idle_group->csg_nr,
-				kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT));
+				lru_idle_group->csg_nr, kbdev->csf.fw_timeout_ms);
 			if (kbase_csf_firmware_ping_wait(kbdev, FW_PING_AFTER_ERROR_TIMEOUT_MS))
 				error_type = DF_PING_REQUEST_TIMEOUT;
 			schedule_actions_trigger_df(kbdev, lru_idle_group->kctx, error_type);
@@ -5358,6 +5323,7 @@ static void schedule_actions(struct kbase_device *kbdev, bool is_tick)
 	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
 	unsigned long flags;
 	struct kbase_queue_group *protm_grp;
+	int ret;
 	bool skip_scheduling_actions;
 	bool skip_idle_slots_update;
 	bool new_protm_top_grp = false;
@@ -5366,7 +5332,8 @@ static void schedule_actions(struct kbase_device *kbdev, bool is_tick)
 	kbase_reset_gpu_assert_prevented(kbdev);
 	lockdep_assert_held(&scheduler->lock);
 
-	if (kbase_csf_scheduler_wait_mcu_active(kbdev)) {
+	ret = kbase_csf_scheduler_wait_mcu_active(kbdev);
+	if (ret) {
 		dev_err(kbdev->dev, "Wait for MCU power on failed on scheduling tick/tock");
 		return;
 	}
@@ -5394,9 +5361,8 @@ static void schedule_actions(struct kbase_device *kbdev, bool is_tick)
 		 * steps and thus extending the previous tick's arrangement,
 		 * in particular, no alterations to on-slot CSGs.
 		 */
-		if (keep_lru_on_slots(kbdev)) {
+		if (keep_lru_on_slots(kbdev))
 			return;
-		}
 	}
 
 	if (is_tick)
@@ -5489,15 +5455,6 @@ static void schedule_actions(struct kbase_device *kbdev, bool is_tick)
 	}
 
 	evict_lru_or_blocked_csg(kbdev);
-
-#ifdef KBASE_PM_RUNTIME
-	if (atomic_read(&scheduler->non_idle_offslot_grps))
-		set_bit(KBASE_GPU_NON_IDLE_OFF_SLOT_GROUPS_AVAILABLE,
-			&kbdev->pm.backend.gpu_sleep_allowed);
-	else
-		clear_bit(KBASE_GPU_NON_IDLE_OFF_SLOT_GROUPS_AVAILABLE,
-			  &kbdev->pm.backend.gpu_sleep_allowed);
-#endif /* KBASE_PM_RUNTIME */
 }
 
 /**
@@ -5531,10 +5488,9 @@ static bool can_skip_scheduling(struct kbase_device *kbdev)
 
 		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 		if (kbdev->pm.backend.exit_gpu_sleep_mode) {
-			int ret;
+			int ret = scheduler_pm_active_after_sleep(kbdev, &flags);
 
 			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-			ret = scheduler_pm_active_after_sleep(kbdev);
 			if (!ret) {
 				scheduler->state = SCHED_INACTIVE;
 				KBASE_KTRACE_ADD(kbdev, SCHED_INACTIVE, NULL, scheduler->state);
@@ -5650,28 +5606,25 @@ static void schedule_on_tick(struct kbase_device *kbdev)
 	kbase_reset_gpu_allow(kbdev);
 }
 
-
-static int suspend_active_queue_groups(struct kbase_device *kbdev, unsigned long *slot_mask,
-				       bool reset)
+static int suspend_active_queue_groups(struct kbase_device *kbdev, unsigned long *slot_mask)
 {
 	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
 	u32 num_groups = kbdev->csf.global_iface.group_num;
-	struct kbase_queue_group *group;
 	u32 slot_num;
 	int ret;
 
 	lockdep_assert_held(&scheduler->lock);
 
-		for (slot_num = 0; slot_num < num_groups; slot_num++) {
-			group = scheduler->csg_slots[slot_num].resident_group;
+	for (slot_num = 0; slot_num < num_groups; slot_num++) {
+		struct kbase_queue_group *group = scheduler->csg_slots[slot_num].resident_group;
 
-			if (group) {
-				suspend_queue_group(group);
-				set_bit(slot_num, slot_mask);
-			}
+		if (group) {
+			suspend_queue_group(group);
+			set_bit(slot_num, slot_mask);
 		}
+	}
 
-		ret = wait_csg_slots_suspend(kbdev, slot_mask);
+	ret = wait_csg_slots_suspend(kbdev, slot_mask);
 	return ret;
 }
 
@@ -5684,7 +5637,7 @@ static int suspend_active_queue_groups_on_reset(struct kbase_device *kbdev)
 
 	mutex_lock(&scheduler->lock);
 
-	ret = suspend_active_queue_groups(kbdev, slot_mask, true);
+	ret = suspend_active_queue_groups(kbdev, slot_mask);
 
 	if (ret) {
 		dev_warn(
@@ -5705,8 +5658,7 @@ static int suspend_active_queue_groups_on_reset(struct kbase_device *kbdev)
 	 * case.
 	 */
 	kbase_gpu_start_cache_clean(kbdev, GPU_COMMAND_CACHE_CLN_INV_L2_LSC);
-	ret2 = kbase_gpu_wait_cache_clean_timeout(
-		kbdev, kbase_get_timeout_ms(kbdev, MMU_AS_INACTIVE_WAIT_TIMEOUT));
+	ret2 = kbase_gpu_wait_cache_clean_timeout(kbdev, kbdev->mmu_or_gpu_cache_op_wait_time_ms);
 	if (ret2) {
 		dev_err(kbdev->dev, "[%llu] Timeout waiting for CACHE_CLN_INV_L2_LSC",
 			kbase_backend_get_cycle_cnt(kbdev));
@@ -5803,8 +5755,7 @@ static bool scheduler_handle_reset_in_protected_mode(struct kbase_device *kbdev)
 		 * anyways.
 		 */
 		new_val = atomic_inc_return(&scheduler->non_idle_offslot_grps);
-		KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_INC, group,
-					 (u64)new_val);
+		KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_NONIDLE_OFFSLOT_GRP_INC, group, new_val);
 	}
 
 unlock:
@@ -5821,12 +5772,9 @@ static void scheduler_inner_reset(struct kbase_device *kbdev)
 	WARN_ON(kbase_csf_scheduler_get_nr_active_csgs(kbdev));
 
 	/* Cancel any potential queued delayed work(s) */
+	cancel_work_sync(&kbdev->csf.scheduler.gpu_idle_work);
 	cancel_tick_work(scheduler);
 	cancel_tock_work(scheduler);
-	/* gpu_idle_worker() might already be running at this point, which
-	 * could decrement the pending_gpu_idle_worker counter to below 0.
-	 * It'd be safer to let it run if one has already been scheduled.
-	 */
 	cancel_delayed_work_sync(&scheduler->ping_work);
 
 	mutex_lock(&scheduler->lock);
@@ -5844,35 +5792,22 @@ static void scheduler_inner_reset(struct kbase_device *kbdev)
 	scheduler->top_kctx = NULL;
 	scheduler->top_grp = NULL;
 
-	atomic_set(&scheduler->gpu_idle_timer_enabled, false);
-	atomic_set(&scheduler->fw_soi_enabled, false);
-
 	KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_TOP_GRP, scheduler->top_grp,
 				 scheduler->num_active_address_spaces |
 					 (((u64)scheduler->total_runnable_grps) << 32));
 
-#ifdef KBASE_PM_RUNTIME
-	if (scheduler->state == SCHED_SLEEPING) {
-#if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
-		hrtimer_cancel(&scheduler->gpu_metrics_timer);
-#endif
-		scheduler->state = SCHED_SUSPENDED;
-		KBASE_KTRACE_ADD(kbdev, SCHED_SUSPENDED, NULL, scheduler->state);
-	}
-#endif
 	mutex_unlock(&scheduler->lock);
 }
 
 void kbase_csf_scheduler_reset(struct kbase_device *kbdev)
 {
 	struct kbase_context *kctx;
+
 	WARN_ON(!kbase_reset_gpu_is_active(kbdev));
 
 	KBASE_KTRACE_ADD(kbdev, SCHEDULER_RESET_START, NULL, 0u);
 
-	if (kbase_reset_gpu_is_active(kbdev))
-		kbase_debug_csf_fault_wait_completion(kbdev);
-
+	kbase_debug_csf_fault_wait_completion(kbdev);
 
 	if (scheduler_handle_reset_in_protected_mode(kbdev) &&
 	    !suspend_active_queue_groups_on_reset(kbdev)) {
@@ -5963,13 +5898,9 @@ static void firmware_aliveness_monitor(struct work_struct *work)
 		goto exit;
 	}
 
-	if (kbase_csf_scheduler_wait_mcu_active(kbdev)) {
-		dev_err(kbdev->dev, "Wait for MCU power on failed at fw aliveness monitor");
-		goto exit;
-	}
+	kbase_csf_scheduler_wait_mcu_active(kbdev);
 
-	err = kbase_csf_firmware_ping_wait(kbdev,
-					   kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT));
+	err = kbase_csf_firmware_ping_wait(kbdev, kbdev->csf.fw_timeout_ms);
 
 	if (err) {
 		/* It is acceptable to enqueue a reset whilst we've prevented
@@ -6460,8 +6391,8 @@ static void check_sync_update_in_sleep_mode(struct kbase_device *kbdev)
  * check_group_sync_update_worker() - Check the sync wait condition for all the
  *                                    blocked queue groups
  *
- * @kctx: The context to evaluate the wait condition for all the queue groups
- *        in idle_wait_groups list.
+ * @work:    Pointer to the context-specific work item for evaluating the wait
+ *           condition for all the queue groups in idle_wait_groups list.
  *
  * This function checks the gpu queues of all the groups present in both
  * idle_wait_groups list of a context and all on slot idle groups (if GPU
@@ -6471,14 +6402,24 @@ static void check_sync_update_in_sleep_mode(struct kbase_device *kbdev)
  * runnable groups so that Scheduler can consider scheduling the group
  * in next tick or exit protected mode.
  */
-static void check_group_sync_update_worker(struct kbase_context *kctx)
+static void check_group_sync_update_worker(struct work_struct *work)
 {
+	struct kbase_context *const kctx =
+		container_of(work, struct kbase_context, csf.sched.sync_update_work);
 	struct kbase_device *const kbdev = kctx->kbdev;
 	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
 	bool sync_updated = false;
 
 	mutex_lock(&scheduler->lock);
 
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+	if (unlikely(scheduler->state == SCHED_BUSY)) {
+		queue_work(kctx->csf.sched.sync_update_wq, &kctx->csf.sched.sync_update_work);
+		mutex_unlock(&scheduler->lock);
+		return;
+	}
+#endif
+
 	KBASE_KTRACE_ADD(kbdev, SCHEDULER_GROUP_SYNC_UPDATE_WORKER_START, kctx, 0u);
 	if (kctx->csf.sched.num_idle_wait_grps != 0) {
 		struct kbase_queue_group *group, *temp;
@@ -6519,7 +6460,7 @@ static enum kbase_csf_event_callback_action check_group_sync_update_cb(void *par
 
 	KBASE_KTRACE_ADD(kctx->kbdev, SCHEDULER_GROUP_SYNC_UPDATE_EVENT, kctx, 0u);
 
-	kbase_csf_scheduler_enqueue_sync_update_work(kctx);
+	queue_work(kctx->csf.sched.sync_update_wq, &kctx->csf.sched.sync_update_work);
 
 	return KBASE_CSF_EVENT_CALLBACK_KEEP;
 }
@@ -6530,8 +6471,6 @@ int kbase_csf_scheduler_context_init(struct kbase_context *kctx)
 	int err;
 	struct kbase_device *kbdev = kctx->kbdev;
 
-	WARN_ON_ONCE(!kbdev->csf.scheduler.kthread_running);
-
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
 	err = gpu_metrics_ctx_init(kctx);
 	if (err)
@@ -6544,7 +6483,15 @@ int kbase_csf_scheduler_context_init(struct kbase_context *kctx)
 
 	INIT_LIST_HEAD(&kctx->csf.sched.idle_wait_groups);
 
-	INIT_LIST_HEAD(&kctx->csf.sched.sync_update_work);
+	kctx->csf.sched.sync_update_wq =
+		alloc_ordered_workqueue("mali_kbase_csf_sync_update_wq", WQ_HIGHPRI);
+	if (!kctx->csf.sched.sync_update_wq) {
+		dev_err(kbdev->dev, "Failed to initialize scheduler context workqueue");
+		err = -ENOMEM;
+		goto alloc_wq_failed;
+	}
+
+	INIT_WORK(&kctx->csf.sched.sync_update_work, check_group_sync_update_worker);
 
 	kbase_csf_tiler_heap_reclaim_ctx_init(kctx);
 
@@ -6558,6 +6505,8 @@ int kbase_csf_scheduler_context_init(struct kbase_context *kctx)
 	return err;
 
 event_wait_add_failed:
+	destroy_workqueue(kctx->csf.sched.sync_update_wq);
+alloc_wq_failed:
 	kbase_ctx_sched_remove_ctx(kctx);
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
 	gpu_metrics_ctx_term(kctx);
@@ -6568,10 +6517,8 @@ int kbase_csf_scheduler_context_init(struct kbase_context *kctx)
 void kbase_csf_scheduler_context_term(struct kbase_context *kctx)
 {
 	kbase_csf_event_wait_remove(kctx, check_group_sync_update_cb, kctx);
-
-	/* Drain a pending SYNC_UPDATE work if any */
-	kbase_csf_scheduler_wait_for_kthread_pending_work(kctx->kbdev,
-							  &kctx->csf.pending_sync_update);
+	cancel_work_sync(&kctx->csf.sched.sync_update_work);
+	destroy_workqueue(kctx->csf.sched.sync_update_wq);
 
 	kbase_ctx_sched_remove_ctx(kctx);
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
@@ -6579,157 +6526,53 @@ void kbase_csf_scheduler_context_term(struct kbase_context *kctx)
 #endif /* CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD */
 }
 
-static void handle_pending_sync_update_works(struct kbase_csf_scheduler *scheduler)
-{
-	struct kbase_context *sync_update_ctx;
-
-	if (atomic_cmpxchg(&scheduler->pending_sync_update_works, true, false) == false)
-		return;
-
-	do {
-		unsigned long flags;
-
-		spin_lock_irqsave(&scheduler->sync_update_work_ctxs_lock, flags);
-		sync_update_ctx = NULL;
-		if (!list_empty(&scheduler->sync_update_work_ctxs)) {
-			sync_update_ctx = list_first_entry(&scheduler->sync_update_work_ctxs,
-							   struct kbase_context,
-							   csf.sched.sync_update_work);
-			list_del_init(&sync_update_ctx->csf.sched.sync_update_work);
-		}
-		spin_unlock_irqrestore(&scheduler->sync_update_work_ctxs_lock, flags);
-
-		if (sync_update_ctx != NULL) {
-			WARN_ON_ONCE(atomic_read(&sync_update_ctx->csf.pending_sync_update) == 0);
-			check_group_sync_update_worker(sync_update_ctx);
-			atomic_dec(&sync_update_ctx->csf.pending_sync_update);
-		}
-	} while (sync_update_ctx != NULL);
-}
-
-static void handle_pending_protm_requests(struct kbase_csf_scheduler *scheduler)
-{
-	struct kbase_queue_group *protm_grp;
-
-	if (atomic_cmpxchg(&scheduler->pending_protm_event_works, true, false) == false)
-		return;
-
-	do {
-		unsigned long flags;
-
-		spin_lock_irqsave(&scheduler->protm_event_work_grps_lock, flags);
-		protm_grp = NULL;
-		if (!list_empty(&scheduler->protm_event_work_grps)) {
-			protm_grp = list_first_entry(&scheduler->protm_event_work_grps,
-						     struct kbase_queue_group, protm_event_work);
-			list_del_init(&protm_grp->protm_event_work);
-		}
-		spin_unlock_irqrestore(&scheduler->protm_event_work_grps_lock, flags);
-
-		if (protm_grp != NULL) {
-			WARN_ON_ONCE(atomic_read(&protm_grp->pending_protm_event_work) == 0);
-			kbase_csf_process_protm_event_request(protm_grp);
-			atomic_dec(&protm_grp->pending_protm_event_work);
-		}
-	} while (protm_grp != NULL);
-}
-
-static void handle_pending_kcpuq_commands(struct kbase_csf_scheduler *scheduler)
-{
-	struct kbase_kcpu_command_queue *kcpuq;
-
-	if (atomic_cmpxchg(&scheduler->pending_kcpuq_works, true, false) == false)
-		return;
-
-	do {
-		unsigned long flags;
-
-		spin_lock_irqsave(&scheduler->kcpuq_work_queues_lock, flags);
-		kcpuq = NULL;
-		if (!list_empty(&scheduler->kcpuq_work_queues)) {
-			kcpuq = list_first_entry(&scheduler->kcpuq_work_queues,
-						 struct kbase_kcpu_command_queue, high_prio_work);
-			list_del_init(&kcpuq->high_prio_work);
-		}
-		spin_unlock_irqrestore(&scheduler->kcpuq_work_queues_lock, flags);
-
-		if (kcpuq != NULL) {
-			WARN_ON_ONCE(atomic_read(&kcpuq->pending_kick) == 0);
-
-			mutex_lock(&kcpuq->lock);
-			kbase_csf_kcpu_queue_process(kcpuq, false);
-			mutex_unlock(&kcpuq->lock);
-
-			atomic_dec(&kcpuq->pending_kick);
-		}
-	} while (kcpuq != NULL);
-}
-
-static void handle_pending_queue_kicks(struct kbase_device *kbdev)
-{
-	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
-	struct kbase_queue *queue;
-
-	if (atomic_cmpxchg(&kbdev->csf.pending_gpuq_kicks, true, false) == false)
-		return;
-
-	do {
-		u8 prio;
-
-		spin_lock(&kbdev->csf.pending_gpuq_kick_queues_lock);
-		queue = NULL;
-		for (prio = 0; prio != KBASE_QUEUE_GROUP_PRIORITY_COUNT; ++prio) {
-			if (!list_empty(&kbdev->csf.pending_gpuq_kick_queues[prio])) {
-				queue = list_first_entry(&kbdev->csf.pending_gpuq_kick_queues[prio],
-							 struct kbase_queue, pending_kick_link);
-				list_del_init(&queue->pending_kick_link);
-				break;
-			}
-		}
-		spin_unlock(&kbdev->csf.pending_gpuq_kick_queues_lock);
-
-		if (queue != NULL) {
-			WARN_ONCE(
-				prio != queue->group_priority,
-				"Queue %pK has priority %u but instead its kick was handled at priority %u",
-				(void *)queue, queue->group_priority, prio);
-			WARN_ON_ONCE(atomic_read(&queue->pending_kick) == 0);
-
-			kbase_csf_process_queue_kick(queue);
-
-			/* Perform a scheduling tock for high-priority queue groups if
-			 * required.
-			 */
-			BUILD_BUG_ON(KBASE_QUEUE_GROUP_PRIORITY_REALTIME != 0);
-			BUILD_BUG_ON(KBASE_QUEUE_GROUP_PRIORITY_HIGH != 1);
-			if ((prio <= KBASE_QUEUE_GROUP_PRIORITY_HIGH) &&
-			    atomic_read(&scheduler->pending_tock_work))
-				schedule_on_tock(kbdev);
-		}
-	} while (queue != NULL);
-}
-
 static int kbase_csf_scheduler_kthread(void *data)
 {
 	struct kbase_device *const kbdev = data;
 	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
 
 	while (scheduler->kthread_running) {
+		struct kbase_queue *queue;
+
 		if (wait_for_completion_interruptible(&scheduler->kthread_signal) != 0)
 			continue;
 		reinit_completion(&scheduler->kthread_signal);
 
-		/*
-		 * The order in which these requests are handled is based on
-		 * how they would influence each other's decisions. As a
-		 * result, the tick & tock requests must be handled after all
-		 * other requests, but before the GPU IDLE work.
-		 */
+		/* Iterate through queues with pending kicks */
+		do {
+			u8 prio;
+
+			spin_lock(&kbdev->csf.pending_gpuq_kicks_lock);
+			queue = NULL;
+			for (prio = 0; prio != KBASE_QUEUE_GROUP_PRIORITY_COUNT; ++prio) {
+				if (!list_empty(&kbdev->csf.pending_gpuq_kicks[prio])) {
+					queue = list_first_entry(
+						&kbdev->csf.pending_gpuq_kicks[prio],
+						struct kbase_queue, pending_kick_link);
+					list_del_init(&queue->pending_kick_link);
+					break;
+				}
+			}
+			spin_unlock(&kbdev->csf.pending_gpuq_kicks_lock);
+
+			if (queue != NULL) {
+				WARN_ONCE(
+					prio != queue->group_priority,
+					"Queue %pK has priority %hhu but instead its kick was handled at priority %hhu",
+					(void *)queue, queue->group_priority, prio);
+
+				kbase_csf_process_queue_kick(queue);
 
-		handle_pending_sync_update_works(scheduler);
-		handle_pending_protm_requests(scheduler);
-		handle_pending_kcpuq_commands(scheduler);
-		handle_pending_queue_kicks(kbdev);
+				/* Perform a scheduling tock for high-priority queue groups if
+				 * required.
+				 */
+				BUILD_BUG_ON(KBASE_QUEUE_GROUP_PRIORITY_REALTIME != 0);
+				BUILD_BUG_ON(KBASE_QUEUE_GROUP_PRIORITY_HIGH != 1);
+				if ((prio <= KBASE_QUEUE_GROUP_PRIORITY_HIGH) &&
+				    atomic_read(&scheduler->pending_tock_work))
+					schedule_on_tock(kbdev);
+			}
+		} while (queue != NULL);
 
 		/* Check if we need to perform a scheduling tick/tock. A tick
 		 * event shall override a tock event but not vice-versa.
@@ -6741,15 +6584,6 @@ static int kbase_csf_scheduler_kthread(void *data)
 			schedule_on_tock(kbdev);
 		}
 
-		/* Drain pending GPU idle works */
-		while (atomic_read(&scheduler->pending_gpu_idle_work) > 0)
-			gpu_idle_worker(kbdev);
-
-		/* Update GLB_IDLE timer/FW Sleep-on-Idle config (which might
-		 * have been disabled during FW boot et. al.).
-		 */
-		kbase_csf_firmware_soi_update(kbdev);
-
 		dev_dbg(kbdev->dev, "Waking up for event after a scheduling iteration.");
 		wake_up_all(&kbdev->csf.event_wait);
 	}
@@ -6779,7 +6613,7 @@ int kbase_csf_scheduler_init(struct kbase_device *kbdev)
 	scheduler->kthread_running = true;
 	scheduler->gpuq_kthread =
 		kthread_run(&kbase_csf_scheduler_kthread, kbdev, "mali-gpuq-kthread");
-	if (IS_ERR_OR_NULL(scheduler->gpuq_kthread)) {
+	if (!scheduler->gpuq_kthread) {
 		kfree(scheduler->csg_slots);
 		scheduler->csg_slots = NULL;
 
@@ -6810,9 +6644,6 @@ int kbase_csf_scheduler_init(struct kbase_device *kbdev)
 	scheduler->gpu_metrics_timer.function = gpu_metrics_timer_callback;
 #endif /* CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD */
 
-	atomic_set(&scheduler->gpu_idle_timer_enabled, false);
-	atomic_set(&scheduler->fw_soi_enabled, false);
-
 	return kbase_csf_mcu_shared_regs_data_init(kbdev);
 }
 
@@ -6822,6 +6653,12 @@ int kbase_csf_scheduler_early_init(struct kbase_device *kbdev)
 
 	atomic_set(&scheduler->timer_enabled, true);
 
+	scheduler->idle_wq = alloc_ordered_workqueue("csf_scheduler_gpu_idle_wq", WQ_HIGHPRI);
+	if (!scheduler->idle_wq) {
+		dev_err(kbdev->dev, "Failed to allocate GPU idle scheduler workqueue\n");
+		return -ENOMEM;
+	}
+
 	INIT_DEFERRABLE_WORK(&scheduler->ping_work, firmware_aliveness_monitor);
 
 	mutex_init(&scheduler->lock);
@@ -6839,30 +6676,20 @@ int kbase_csf_scheduler_early_init(struct kbase_device *kbdev)
 	KBASE_KTRACE_ADD(kbdev, SCHED_SUSPENDED, NULL, scheduler->state);
 	scheduler->csg_scheduling_period_ms = CSF_SCHEDULER_TIME_TICK_MS;
 	scheduler_doorbell_init(kbdev);
+	INIT_WORK(&scheduler->gpu_idle_work, gpu_idle_worker);
 	hrtimer_init(&scheduler->tick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	scheduler->tick_timer.function = tick_timer_callback;
 
-	atomic_set(&scheduler->pending_sync_update_works, false);
-	spin_lock_init(&scheduler->sync_update_work_ctxs_lock);
-	INIT_LIST_HEAD(&scheduler->sync_update_work_ctxs);
-	atomic_set(&scheduler->pending_protm_event_works, false);
-	spin_lock_init(&scheduler->protm_event_work_grps_lock);
-	INIT_LIST_HEAD(&scheduler->protm_event_work_grps);
-	atomic_set(&scheduler->pending_kcpuq_works, false);
-	spin_lock_init(&scheduler->kcpuq_work_queues_lock);
-	INIT_LIST_HEAD(&scheduler->kcpuq_work_queues);
-	atomic_set(&scheduler->pending_tick_work, false);
-	atomic_set(&scheduler->pending_tock_work, false);
-	atomic_set(&scheduler->pending_gpu_idle_work, 0);
+	kbase_csf_tiler_heap_reclaim_mgr_init(kbdev);
 
-	return kbase_csf_tiler_heap_reclaim_mgr_init(kbdev);
+	return 0;
 }
 
 void kbase_csf_scheduler_term(struct kbase_device *kbdev)
 {
 	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
 
-	if (!IS_ERR_OR_NULL(scheduler->gpuq_kthread)) {
+	if (scheduler->gpuq_kthread) {
 		scheduler->kthread_running = false;
 		complete(&scheduler->kthread_signal);
 		kthread_stop(scheduler->gpuq_kthread);
@@ -6876,6 +6703,7 @@ void kbase_csf_scheduler_term(struct kbase_device *kbdev)
 		 * to be active at the time of Driver unload.
 		 */
 		WARN_ON(kbase_csf_scheduler_get_nr_active_csgs(kbdev));
+		flush_work(&kbdev->csf.scheduler.gpu_idle_work);
 		mutex_lock(&kbdev->csf.scheduler.lock);
 
 		if (kbdev->csf.scheduler.state != SCHED_SUSPENDED) {
@@ -6902,6 +6730,9 @@ void kbase_csf_scheduler_term(struct kbase_device *kbdev)
 
 void kbase_csf_scheduler_early_term(struct kbase_device *kbdev)
 {
+	if (kbdev->csf.scheduler.idle_wq)
+		destroy_workqueue(kbdev->csf.scheduler.idle_wq);
+
 	kbase_csf_tiler_heap_reclaim_mgr_term(kbdev);
 	mutex_destroy(&kbdev->csf.scheduler.lock);
 }
@@ -7014,10 +6845,9 @@ int kbase_csf_scheduler_pm_suspend_no_lock(struct kbase_device *kbdev)
 			dev_warn(kbdev->dev, "failed to suspend active groups");
 			goto exit;
 		} else {
-			dev_dbg(kbdev->dev, "Scheduler PM suspend");
-			result = scheduler_suspend(kbdev);
-			if (!result)
-				cancel_tick_work(scheduler);
+			dev_info(kbdev->dev, "Scheduler PM suspend");
+			scheduler_suspend(kbdev);
+			cancel_tick_work(scheduler);
 		}
 	}
 
@@ -7057,7 +6887,7 @@ void kbase_csf_scheduler_pm_resume_no_lock(struct kbase_device *kbdev)
 
 	lockdep_assert_held(&scheduler->lock);
 	if ((scheduler->total_runnable_grps > 0) && (scheduler->state == SCHED_SUSPENDED)) {
-		dev_dbg(kbdev->dev, "Scheduler PM resume");
+		dev_info(kbdev->dev, "Scheduler PM resume");
 		scheduler_wakeup(kbdev, true);
 	}
 }
@@ -7078,7 +6908,7 @@ void kbase_csf_scheduler_pm_active(struct kbase_device *kbdev)
 	 * the CSGs before powering down the GPU.
 	 */
 	mutex_lock(&kbdev->csf.scheduler.lock);
-	scheduler_pm_active_handle_suspend(kbdev, KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE, false);
+	scheduler_pm_active_handle_suspend(kbdev, KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE);
 	mutex_unlock(&kbdev->csf.scheduler.lock);
 }
 KBASE_EXPORT_TEST_API(kbase_csf_scheduler_pm_active);
@@ -7102,7 +6932,9 @@ static int scheduler_wait_mcu_active(struct kbase_device *kbdev, bool killable_w
 
 	kbase_pm_lock(kbdev);
 	WARN_ON(!kbdev->pm.active_count);
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	WARN_ON(!scheduler->pm_active_count);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 	kbase_pm_unlock(kbdev);
 
 	if (killable_wait)
@@ -7163,7 +6995,6 @@ int kbase_csf_scheduler_handle_runtime_suspend(struct kbase_device *kbdev)
 
 		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 		kbdev->pm.backend.exit_gpu_sleep_mode = true;
-		kbdev->pm.backend.runtime_suspend_abort_reason = ABORT_REASON_NON_IDLE_CGS;
 		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 		kbase_csf_scheduler_invoke_tick(kbdev);
@@ -7183,65 +7014,6 @@ int kbase_csf_scheduler_handle_runtime_suspend(struct kbase_device *kbdev)
 	return 0;
 }
 
-void kbase_csf_scheduler_enqueue_sync_update_work(struct kbase_context *kctx)
-{
-	struct kbase_csf_scheduler *const scheduler = &kctx->kbdev->csf.scheduler;
-	unsigned long flags;
-
-	spin_lock_irqsave(&scheduler->sync_update_work_ctxs_lock, flags);
-	if (list_empty(&kctx->csf.sched.sync_update_work)) {
-		list_add_tail(&kctx->csf.sched.sync_update_work, &scheduler->sync_update_work_ctxs);
-		atomic_inc(&kctx->csf.pending_sync_update);
-		if (atomic_cmpxchg(&scheduler->pending_sync_update_works, false, true) == false)
-			complete(&scheduler->kthread_signal);
-	}
-	spin_unlock_irqrestore(&scheduler->sync_update_work_ctxs_lock, flags);
-}
-
-void kbase_csf_scheduler_enqueue_protm_event_work(struct kbase_queue_group *group)
-{
-	struct kbase_context *const kctx = group->kctx;
-	struct kbase_csf_scheduler *const scheduler = &kctx->kbdev->csf.scheduler;
-	unsigned long flags;
-
-	spin_lock_irqsave(&scheduler->protm_event_work_grps_lock, flags);
-	if (list_empty(&group->protm_event_work)) {
-		list_add_tail(&group->protm_event_work, &scheduler->protm_event_work_grps);
-		atomic_inc(&group->pending_protm_event_work);
-		if (atomic_cmpxchg(&scheduler->pending_protm_event_works, false, true) == false)
-			complete(&scheduler->kthread_signal);
-	}
-	spin_unlock_irqrestore(&scheduler->protm_event_work_grps_lock, flags);
-}
-
-void kbase_csf_scheduler_enqueue_kcpuq_work(struct kbase_kcpu_command_queue *queue)
-{
-	struct kbase_csf_scheduler *const scheduler = &queue->kctx->kbdev->csf.scheduler;
-	unsigned long flags;
-
-	spin_lock_irqsave(&scheduler->kcpuq_work_queues_lock, flags);
-	if (list_empty(&queue->high_prio_work)) {
-		list_add_tail(&queue->high_prio_work, &scheduler->kcpuq_work_queues);
-		atomic_inc(&queue->pending_kick);
-		if (atomic_cmpxchg(&scheduler->pending_kcpuq_works, false, true) == false)
-			complete(&scheduler->kthread_signal);
-	}
-	spin_unlock_irqrestore(&scheduler->kcpuq_work_queues_lock, flags);
-}
-
-void kbase_csf_scheduler_wait_for_kthread_pending_work(struct kbase_device *kbdev,
-						       atomic_t *pending)
-{
-	/*
-	 * Signal kbase_csf_scheduler_kthread() to allow for the
-	 * eventual completion of the current iteration. Once the work is
-	 * done, the event_wait wait queue shall be signalled.
-	 */
-
-	complete(&kbdev->csf.scheduler.kthread_signal);
-	wait_event(kbdev->csf.event_wait, atomic_read(pending) == 0);
-}
-
 void kbase_csf_scheduler_reval_idleness_post_sleep(struct kbase_device *kbdev)
 {
 	u32 csg_nr;
@@ -7288,4 +7060,3 @@ void kbase_csf_scheduler_force_wakeup(struct kbase_device *kbdev)
 	scheduler_wakeup(kbdev, true);
 	mutex_unlock(&scheduler->lock);
 }
-KBASE_EXPORT_TEST_API(kbase_csf_scheduler_force_wakeup);
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.h
index 915945bb495e..abd62342d38f 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.h
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -122,7 +122,7 @@ bool kbase_csf_scheduler_group_events_enabled(struct kbase_device *kbdev,
  * Note: Caller must hold the interrupt_lock.
  */
 struct kbase_queue_group *kbase_csf_scheduler_get_group_on_slot(struct kbase_device *kbdev,
-								u32 slot);
+								int slot);
 
 /**
  * kbase_csf_scheduler_group_deschedule() - Deschedule a GPU command queue
@@ -235,8 +235,7 @@ void kbase_csf_scheduler_early_term(struct kbase_device *kbdev);
  * No explicit re-initialization is done for CSG & CS interface I/O pages;
  * instead, that happens implicitly on firmware reload.
  *
- * Should be called either after initiating the GPU reset or when MCU reset is
- * expected to follow such as GPU_LOST case.
+ * Should be called only after initiating the GPU reset.
  */
 void kbase_csf_scheduler_reset(struct kbase_device *kbdev);
 
@@ -488,48 +487,6 @@ static inline bool kbase_csf_scheduler_all_csgs_idle(struct kbase_device *kbdev)
 			    kbdev->csf.global_iface.group_num);
 }
 
-/**
- * kbase_csf_scheduler_enqueue_sync_update_work() - Add a context to the list
- *                                                  of contexts to handle
- *                                                  SYNC_UPDATE events.
- *
- * @kctx: The context to handle SYNC_UPDATE event
- *
- * This function wakes up kbase_csf_scheduler_kthread() to handle pending
- * SYNC_UPDATE events for all contexts.
- */
-void kbase_csf_scheduler_enqueue_sync_update_work(struct kbase_context *kctx);
-
-/**
- * kbase_csf_scheduler_enqueue_protm_event_work() - Add a group to the list
- *                                                  of groups to handle
- *                                                  PROTM requests.
- *
- * @group: The group to handle protected mode request
- *
- * This function wakes up kbase_csf_scheduler_kthread() to handle pending
- * protected mode requests for all groups.
- */
-void kbase_csf_scheduler_enqueue_protm_event_work(struct kbase_queue_group *group);
-
-/**
- * kbase_csf_scheduler_enqueue_kcpuq_work() - Wake up kbase_csf_scheduler_kthread() to process
- *                                            pending commands for a KCPU queue.
- *
- * @queue: The queue to process pending commands for
- */
-void kbase_csf_scheduler_enqueue_kcpuq_work(struct kbase_kcpu_command_queue *queue);
-
-/**
- * kbase_csf_scheduler_wait_for_kthread_pending_work - Wait until a pending work has completed in
- *                                                     kbase_csf_scheduler_kthread().
- *
- * @kbdev: Instance of a GPU platform device that implements a CSF interface
- * @pending: The work to wait for
- */
-void kbase_csf_scheduler_wait_for_kthread_pending_work(struct kbase_device *kbdev,
-						       atomic_t *pending);
-
 /**
  * kbase_csf_scheduler_invoke_tick() - Invoke the scheduling tick
  *
@@ -693,6 +650,4 @@ void kbase_csf_scheduler_force_wakeup(struct kbase_device *kbdev);
 void kbase_csf_scheduler_force_sleep(struct kbase_device *kbdev);
 #endif
 
-bool is_gpu_level_suspend_supported(struct kbase_device *const kbdev);
-
 #endif /* _KBASE_CSF_SCHEDULER_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_sync.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_sync.c
index 27b792500bdf..b95e77ce4eaa 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_sync.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_sync.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -99,7 +99,11 @@ static void kbasep_csf_sync_print_kcpu_fence_wait_or_signal(char *buffer, int *l
 							    struct kbase_kcpu_command *cmd,
 							    const char *cmd_name)
 {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence = NULL;
+#else
 	struct dma_fence *fence = NULL;
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0) */
 	struct kbase_kcpu_command_fence_info *fence_info;
 	struct kbase_sync_fence_info info;
 	const char *timeline_name = NULL;
@@ -117,13 +121,13 @@ static void kbasep_csf_sync_print_kcpu_fence_wait_or_signal(char *buffer, int *l
 	timeline_name = fence->ops->get_timeline_name(fence);
 	is_signaled = info.status > 0;
 
-	*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-			     "cmd:%s obj:0x%pK live_value:0x%.8x | ", cmd_name, fence, is_signaled);
+	*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+			    "cmd:%s obj:0x%pK live_value:0x%.8x | ", cmd_name, fence, is_signaled);
 
 	/* Note: fence->seqno was u32 until 5.1 kernel, then u64 */
-	*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-			     "timeline_name:%s timeline_context:0x%.16llx fence_seqno:0x%.16llx",
-			     timeline_name, fence->context, (u64)fence->seqno);
+	*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+			    "timeline_name:%s timeline_context:0x%.16llx fence_seqno:0x%.16llx",
+			    timeline_name, fence->context, (u64)fence->seqno);
 
 	kbase_fence_put(fence);
 }
@@ -149,19 +153,19 @@ static void kbasep_csf_sync_print_kcpu_cqs_wait(struct kbase_context *kctx, char
 		int ret = kbasep_csf_sync_get_cqs_live_u32(kctx, cqs_obj->addr, &live_val);
 		bool live_val_valid = (ret >= 0);
 
-		*length += scnprintf(
-			buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-			"cmd:CQS_WAIT_OPERATION obj:0x%.16llx live_value:", cqs_obj->addr);
+		*length +=
+			snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+				 "cmd:CQS_WAIT_OPERATION obj:0x%.16llx live_value:", cqs_obj->addr);
 
 		if (live_val_valid)
-			*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-					     "0x%.16llx", (u64)live_val);
+			*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+					    "0x%.16llx", (u64)live_val);
 		else
-			*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-					     CQS_UNREADABLE_LIVE_VALUE);
+			*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+					    CQS_UNREADABLE_LIVE_VALUE);
 
-		*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-				     " | op:gt arg_value:0x%.8x", cqs_obj->val);
+		*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+				    " | op:gt arg_value:0x%.8x", cqs_obj->val);
 	}
 }
 
@@ -187,18 +191,18 @@ static void kbasep_csf_sync_print_kcpu_cqs_set(struct kbase_context *kctx, char
 		bool live_val_valid = (ret >= 0);
 
 		*length +=
-			scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-				  "cmd:CQS_SET_OPERATION obj:0x%.16llx live_value:", cqs_obj->addr);
+			snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+				 "cmd:CQS_SET_OPERATION obj:0x%.16llx live_value:", cqs_obj->addr);
 
 		if (live_val_valid)
-			*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-					     "0x%.16llx", (u64)live_val);
+			*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+					    "0x%.16llx", (u64)live_val);
 		else
-			*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-					     CQS_UNREADABLE_LIVE_VALUE);
+			*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+					    CQS_UNREADABLE_LIVE_VALUE);
 
-		*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-				     " | op:add arg_value:0x%.8x", 1);
+		*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+				    " | op:add arg_value:0x%.8x", 1);
 	}
 }
 
@@ -277,19 +281,19 @@ static void kbasep_csf_sync_print_kcpu_cqs_wait_op(struct kbase_context *kctx, c
 
 		bool live_val_valid = (ret >= 0);
 
-		*length += scnprintf(
-			buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-			"cmd:CQS_WAIT_OPERATION obj:0x%.16llx live_value:", wait_op->addr);
+		*length +=
+			snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+				 "cmd:CQS_WAIT_OPERATION obj:0x%.16llx live_value:", wait_op->addr);
 
 		if (live_val_valid)
-			*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-					     "0x%.16llx", live_val);
+			*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+					    "0x%.16llx", live_val);
 		else
-			*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-					     CQS_UNREADABLE_LIVE_VALUE);
+			*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+					    CQS_UNREADABLE_LIVE_VALUE);
 
-		*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-				     " | op:%s arg_value:0x%.16llx", op_name, wait_op->val);
+		*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+				    " | op:%s arg_value:0x%.16llx", op_name, wait_op->val);
 	}
 }
 
@@ -319,18 +323,18 @@ static void kbasep_csf_sync_print_kcpu_cqs_set_op(struct kbase_context *kctx, ch
 		bool live_val_valid = (ret >= 0);
 
 		*length +=
-			scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-				  "cmd:CQS_SET_OPERATION obj:0x%.16llx live_value:", set_op->addr);
+			snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+				 "cmd:CQS_SET_OPERATION obj:0x%.16llx live_value:", set_op->addr);
 
 		if (live_val_valid)
-			*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-					     "0x%.16llx", live_val);
+			*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+					    "0x%.16llx", live_val);
 		else
-			*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-					     CQS_UNREADABLE_LIVE_VALUE);
+			*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+					    CQS_UNREADABLE_LIVE_VALUE);
 
-		*length += scnprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
-				     " | op:%s arg_value:0x%.16llx", op_name, set_op->val);
+		*length += snprintf(buffer + *length, CSF_SYNC_DUMP_SIZE - *length,
+				    " | op:%s arg_value:0x%.16llx", op_name, set_op->val);
 	}
 }
 
@@ -360,8 +364,8 @@ static void kbasep_csf_sync_kcpu_print_queue(struct kbase_context *kctx,
 		int length = 0;
 
 		started_or_pending = ((i == 0) && queue->command_started) ? 'S' : 'P';
-		length += scnprintf(buffer, CSF_SYNC_DUMP_SIZE, "queue:KCPU-%d-%d exec:%c ",
-				    kctx->id, queue->id, started_or_pending);
+		length += snprintf(buffer, CSF_SYNC_DUMP_SIZE, "queue:KCPU-%d-%d exec:%c ",
+				   kctx->id, queue->id, started_or_pending);
 
 		cmd = &queue->commands[(u8)(queue->start_offset + i)];
 		switch (cmd->type) {
@@ -388,12 +392,12 @@ static void kbasep_csf_sync_kcpu_print_queue(struct kbase_context *kctx,
 			kbasep_csf_sync_print_kcpu_cqs_set_op(kctx, buffer, &length, cmd);
 			break;
 		default:
-			length += scnprintf(buffer + length, CSF_SYNC_DUMP_SIZE - length,
-					    ", U, Unknown blocking command");
+			length += snprintf(buffer + length, CSF_SYNC_DUMP_SIZE - length,
+					   ", U, Unknown blocking command");
 			break;
 		}
 
-		length += scnprintf(buffer + length, CSF_SYNC_DUMP_SIZE - length, "\n");
+		length += snprintf(buffer + length, CSF_SYNC_DUMP_SIZE - length, "\n");
 		kbasep_print(kbpr, buffer);
 	}
 
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.c
index 5a5a4c315396..f898535004c5 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -218,20 +218,20 @@ static void remove_unlinked_chunk(struct kbase_context *kctx,
 	if (WARN_ON(!list_empty(&chunk->link)))
 		return;
 
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 	kbase_vunmap(kctx, &chunk->map);
 	/* KBASE_REG_DONT_NEED regions will be confused with ephemeral regions (inc freed JIT
 	 * regions), and so we must clear that flag too before freeing.
 	 * For "no user free count", we check that the count is 1 as it is a shrinkable region;
 	 * no other code part within kbase can take a reference to it.
 	 */
-	WARN_ON(atomic64_read(&chunk->region->no_user_free_count) > 1);
+	WARN_ON(atomic_read(&chunk->region->no_user_free_count) > 1);
 	kbase_va_region_no_user_free_dec(chunk->region);
 #if !defined(CONFIG_MALI_VECTOR_DUMP)
 	chunk->region->flags &= ~KBASE_REG_DONT_NEED;
 #endif
 	kbase_mem_free_region(kctx, chunk->region);
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 
 	kfree(chunk);
 }
@@ -260,9 +260,8 @@ static struct kbase_csf_tiler_heap_chunk *alloc_new_chunk(struct kbase_context *
 							  u64 chunk_size)
 {
 	u64 nr_pages = PFN_UP(chunk_size);
-	base_mem_alloc_flags flags = BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR |
-				     BASE_MEM_PROT_CPU_WR | BASEP_MEM_NO_USER_FREE |
-				     BASE_MEM_COHERENT_LOCAL | BASE_MEM_PROT_CPU_RD;
+	u64 flags = BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR | BASE_MEM_PROT_CPU_WR |
+		    BASEP_MEM_NO_USER_FREE | BASE_MEM_COHERENT_LOCAL | BASE_MEM_PROT_CPU_RD;
 	struct kbase_csf_tiler_heap_chunk *chunk = NULL;
 	/* The chunk kernel mapping needs to be large enough to:
 	 * - initially zero the CHUNK_HDR_SIZE area
@@ -309,7 +308,7 @@ static struct kbase_csf_tiler_heap_chunk *alloc_new_chunk(struct kbase_context *
 	 * It should be fine and not a security risk if we let the region leak till
 	 * region tracker termination in such a case.
 	 */
-	if (unlikely(atomic64_read(&chunk->region->no_user_free_count) > 1)) {
+	if (unlikely(atomic_read(&chunk->region->no_user_free_count) > 1)) {
 		dev_err(kctx->kbdev->dev, "Chunk region has no_user_free_count > 1!\n");
 		goto unroll_region;
 	}
@@ -351,14 +350,13 @@ static struct kbase_csf_tiler_heap_chunk *alloc_new_chunk(struct kbase_context *
 	}
 
 	remove_external_chunk_mappings(kctx, chunk);
+	kbase_gpu_vm_unlock(kctx);
 
 	/* If page migration is enabled, we don't want to migrate tiler heap pages.
 	 * This does not change if the constituent pages are already marked as isolated.
 	 */
 	if (kbase_is_page_migration_enabled())
-		kbase_set_phy_alloc_page_status(kctx, chunk->region->gpu_alloc, NOT_MOVABLE);
-
-	kbase_gpu_vm_unlock(kctx);
+		kbase_set_phy_alloc_page_status(chunk->region->gpu_alloc, NOT_MOVABLE);
 
 	return chunk;
 
@@ -642,7 +640,7 @@ static bool kbasep_is_buffer_descriptor_region_suitable(struct kbase_context *co
 
 	if (!(reg->flags & KBASE_REG_CPU_RD) || kbase_is_region_shrinkable(reg) ||
 	    (reg->flags & KBASE_REG_PF_GROW)) {
-		dev_err(kctx->kbdev->dev, "Region has invalid flags: 0x%llX!\n", reg->flags);
+		dev_err(kctx->kbdev->dev, "Region has invalid flags: 0x%lX!\n", reg->flags);
 		return false;
 	}
 
@@ -739,7 +737,7 @@ int kbase_csf_tiler_heap_init(struct kbase_context *const kctx, u32 const chunk_
 					  KBASE_VMAP_FLAG_PERMANENT_MAP_ACCOUNTING);
 
 		if (kbase_is_page_migration_enabled())
-			kbase_set_phy_alloc_page_status(kctx, buf_desc_reg->gpu_alloc, NOT_MOVABLE);
+			kbase_set_phy_alloc_page_status(buf_desc_reg->gpu_alloc, NOT_MOVABLE);
 
 		kbase_gpu_vm_unlock(kctx);
 
@@ -1060,7 +1058,6 @@ static bool delete_chunk_physical_pages(struct kbase_csf_tiler_heap *heap, u64 c
 	struct kbase_csf_tiler_heap_chunk *chunk = NULL;
 
 	lockdep_assert_held(&heap->kctx->csf.tiler_heaps.lock);
-	lockdep_assert_held(&kctx->kbdev->csf.scheduler.lock);
 
 	chunk = find_chunk(heap, chunk_gpu_va);
 	if (unlikely(!chunk)) {
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_reclaim.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_reclaim.c
index df4feb77f0cd..788cfb2ad601 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_reclaim.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_reclaim.c
@@ -29,7 +29,7 @@
 #define HEAP_SHRINKER_SEEKS (DEFAULT_SEEKS + 2)
 
 /* Tiler heap shrinker batch value */
-#define HEAP_SHRINKER_BATCH (512 / (GPU_PAGES_PER_CPU_PAGE))
+#define HEAP_SHRINKER_BATCH (512)
 
 /* Tiler heap reclaim scan (free) method size for limiting a scan run length */
 #define HEAP_RECLAIM_SCAN_BATCH_SIZE (HEAP_SHRINKER_BATCH << 7)
@@ -69,8 +69,8 @@ static void detach_ctx_from_heap_reclaim_mgr(struct kbase_context *kctx)
 
 		list_del_init(&info->mgr_link);
 		if (remaining)
-			WARN_ON(atomic_sub_return((int)remaining,
-						  &scheduler->reclaim_mgr.unused_pages) < 0);
+			WARN_ON(atomic_sub_return(remaining, &scheduler->reclaim_mgr.unused_pages) <
+				0);
 
 		dev_dbg(kctx->kbdev->dev,
 			"Reclaim_mgr_detach: ctx_%d_%d, est_pages=0%u, freed_pages=%u", kctx->tgid,
@@ -96,7 +96,7 @@ static void attach_ctx_to_heap_reclaim_mgr(struct kbase_context *kctx)
 
 	list_add_tail(&info->mgr_link, &scheduler->reclaim_mgr.ctx_lists[prio]);
 	/* Accumulate the estimated pages to the manager total field */
-	atomic_add((int)info->nr_est_unused_pages, &scheduler->reclaim_mgr.unused_pages);
+	atomic_add(info->nr_est_unused_pages, &scheduler->reclaim_mgr.unused_pages);
 
 	dev_dbg(kctx->kbdev->dev, "Reclaim_mgr_attach: ctx_%d_%d, est_count_pages=%u", kctx->tgid,
 		kctx->id, info->nr_est_unused_pages);
@@ -201,8 +201,8 @@ static unsigned long reclaim_unused_heap_pages(struct kbase_device *kbdev)
 		 * headers of the individual chunks and buffer descriptors.
 		 */
 		kbase_gpu_start_cache_clean(kbdev, GPU_COMMAND_CACHE_CLN_INV_L2);
-		if (kbase_gpu_wait_cache_clean_timeout(
-			    kbdev, kbase_get_timeout_ms(kbdev, MMU_AS_INACTIVE_WAIT_TIMEOUT)))
+		if (kbase_gpu_wait_cache_clean_timeout(kbdev,
+						       kbdev->mmu_or_gpu_cache_op_wait_time_ms))
 			dev_warn(
 				kbdev->dev,
 				"[%llu] Timeout waiting for CACHE_CLN_INV_L2 to complete before Tiler heap reclaim",
@@ -241,7 +241,7 @@ static unsigned long reclaim_unused_heap_pages(struct kbase_device *kbdev)
 				u32 rm_cnt = MIN(info->nr_est_unused_pages - info->nr_freed_pages,
 						 freed_pages);
 
-				WARN_ON(atomic_sub_return((int)rm_cnt, &mgr->unused_pages) < 0);
+				WARN_ON(atomic_sub_return(rm_cnt, &mgr->unused_pages) < 0);
 
 				/* tracking the freed pages, before a potential detach call */
 				info->nr_freed_pages += freed_pages;
@@ -278,7 +278,7 @@ static unsigned long kbase_csf_tiler_heap_reclaim_count_free_pages(struct kbase_
 								   struct shrink_control *sc)
 {
 	struct kbase_csf_sched_heap_reclaim_mgr *mgr = &kbdev->csf.scheduler.reclaim_mgr;
-	unsigned long page_cnt = (unsigned long)atomic_read(&mgr->unused_pages);
+	unsigned long page_cnt = atomic_read(&mgr->unused_pages);
 
 	CSTD_UNUSED(sc);
 
@@ -300,14 +300,14 @@ static unsigned long kbase_csf_tiler_heap_reclaim_scan_free_pages(struct kbase_d
 
 		/* Wait for roughly 2-ms */
 		wait_event_timeout(kbdev->csf.event_wait, (scheduler->state != SCHED_BUSY),
-				   (long)msecs_to_jiffies(2));
+				   msecs_to_jiffies(2));
 		if (!mutex_trylock(&kbdev->csf.scheduler.lock)) {
 			dev_dbg(kbdev->dev, "Tiler heap reclaim scan see device busy (freed: 0)");
 			return 0;
 		}
 	}
 
-	avail = (unsigned long)atomic_read(&mgr->unused_pages);
+	avail = atomic_read(&mgr->unused_pages);
 	if (avail)
 		freed = reclaim_unused_heap_pages(kbdev);
 
@@ -331,8 +331,8 @@ static unsigned long kbase_csf_tiler_heap_reclaim_scan_free_pages(struct kbase_d
 static unsigned long kbase_csf_tiler_heap_reclaim_count_objects(struct shrinker *s,
 								struct shrink_control *sc)
 {
-	struct kbase_device *kbdev = KBASE_GET_KBASE_DATA_FROM_SHRINKER(
-		s, struct kbase_device, csf.scheduler.reclaim_mgr.heap_reclaim);
+	struct kbase_device *kbdev =
+		container_of(s, struct kbase_device, csf.scheduler.reclaim_mgr.heap_reclaim);
 
 	return kbase_csf_tiler_heap_reclaim_count_free_pages(kbdev, sc);
 }
@@ -340,8 +340,8 @@ static unsigned long kbase_csf_tiler_heap_reclaim_count_objects(struct shrinker
 static unsigned long kbase_csf_tiler_heap_reclaim_scan_objects(struct shrinker *s,
 							       struct shrink_control *sc)
 {
-	struct kbase_device *kbdev = KBASE_GET_KBASE_DATA_FROM_SHRINKER(
-		s, struct kbase_device, csf.scheduler.reclaim_mgr.heap_reclaim);
+	struct kbase_device *kbdev =
+		container_of(s, struct kbase_device, csf.scheduler.reclaim_mgr.heap_reclaim);
 
 	return kbase_csf_tiler_heap_reclaim_scan_free_pages(kbdev, sc);
 }
@@ -352,17 +352,11 @@ void kbase_csf_tiler_heap_reclaim_ctx_init(struct kbase_context *kctx)
 	INIT_LIST_HEAD(&kctx->csf.sched.heap_info.mgr_link);
 }
 
-int kbase_csf_tiler_heap_reclaim_mgr_init(struct kbase_device *kbdev)
+void kbase_csf_tiler_heap_reclaim_mgr_init(struct kbase_device *kbdev)
 {
 	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	struct shrinker *reclaim = &scheduler->reclaim_mgr.heap_reclaim;
 	u8 prio;
-	struct shrinker *reclaim;
-
-	reclaim =
-		KBASE_INIT_RECLAIM(&(scheduler->reclaim_mgr), heap_reclaim, "mali-csf-tiler-heap");
-	if (!reclaim)
-		return -ENOMEM;
-	KBASE_SET_RECLAIM(&(scheduler->reclaim_mgr), heap_reclaim, reclaim);
 
 	for (prio = KBASE_QUEUE_GROUP_PRIORITY_REALTIME; prio < KBASE_QUEUE_GROUP_PRIORITY_COUNT;
 	     prio++)
@@ -372,11 +366,6 @@ int kbase_csf_tiler_heap_reclaim_mgr_init(struct kbase_device *kbdev)
 	reclaim->scan_objects = kbase_csf_tiler_heap_reclaim_scan_objects;
 	reclaim->seeks = HEAP_SHRINKER_SEEKS;
 	reclaim->batch = HEAP_SHRINKER_BATCH;
-
-	if (!IS_ENABLED(CONFIG_MALI_VECTOR_DUMP))
-		KBASE_REGISTER_SHRINKER(reclaim, "mali-csf-tiler-heap", kbdev);
-
-	return 0;
 }
 
 void kbase_csf_tiler_heap_reclaim_mgr_term(struct kbase_device *kbdev)
@@ -384,9 +373,6 @@ void kbase_csf_tiler_heap_reclaim_mgr_term(struct kbase_device *kbdev)
 	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
 	u8 prio;
 
-	if (!IS_ENABLED(CONFIG_MALI_VECTOR_DUMP))
-		KBASE_UNREGISTER_SHRINKER(scheduler->reclaim_mgr.heap_reclaim);
-
 	for (prio = KBASE_QUEUE_GROUP_PRIORITY_REALTIME; prio < KBASE_QUEUE_GROUP_PRIORITY_COUNT;
 	     prio++)
 		WARN_ON(!list_empty(&scheduler->reclaim_mgr.ctx_lists[prio]));
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_reclaim.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_reclaim.h
index d41b7baabd02..7880de04c84f 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_reclaim.h
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_reclaim.h
@@ -66,10 +66,8 @@ void kbase_csf_tiler_heap_reclaim_ctx_init(struct kbase_context *kctx);
  * @kbdev: Pointer to the device.
  *
  * This function must be called only when a kbase device is initialized.
- *
- * Return: 0 if issuing reclaim_mgr init was successful, otherwise an error code.
  */
-int kbase_csf_tiler_heap_reclaim_mgr_init(struct kbase_device *kbdev);
+void kbase_csf_tiler_heap_reclaim_mgr_init(struct kbase_device *kbdev);
 
 /**
  * kbase_csf_tiler_heap_reclaim_mgr_term - Termination call for the tiler heap reclaim manger.
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.c
index 7845ad1397d6..3b2e5ae6ae79 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.c
@@ -51,7 +51,7 @@ static int set_timeout(struct kbase_device *const kbdev, u64 const timeout)
 
 	dev_dbg(kbdev->dev, "New progress timeout: %llu cycles\n", timeout);
 
-	atomic64_set(&kbdev->csf.progress_timeout, (s64)timeout);
+	atomic64_set(&kbdev->csf.progress_timeout, timeout);
 	kbase_device_set_timeout(kbdev, CSF_SCHED_PROTM_PROGRESS_TIMEOUT, timeout, 1);
 
 	return 0;
@@ -112,7 +112,7 @@ static ssize_t progress_timeout_store(struct device *const dev, struct device_at
 	if (err)
 		return err;
 
-	return (ssize_t)count;
+	return count;
 }
 
 /**
@@ -179,5 +179,5 @@ void kbase_csf_timeout_term(struct kbase_device *const kbdev)
 
 u64 kbase_csf_timeout_get(struct kbase_device *const kbdev)
 {
-	return (u64)atomic64_read(&kbdev->csf.progress_timeout);
+	return atomic64_read(&kbdev->csf.progress_timeout);
 }
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.c
index eb5c8a40b8c9..54054661f7a9 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.c
@@ -151,22 +151,13 @@ static bool tl_reader_overflow_check(struct kbase_csf_tl_reader *self, u16 event
  *
  * Reset the reader to the default state, i.e. set all the
  * mutable fields to zero.
- *
- * NOTE: this function expects the irq spinlock to be held.
  */
 static void tl_reader_reset(struct kbase_csf_tl_reader *self)
 {
-	lockdep_assert_held(&self->read_lock);
-
 	self->got_first_event = false;
 	self->is_active = false;
 	self->expected_event_id = 0;
 	self->tl_header.btc = 0;
-
-	/* There might be data left in the trace buffer from the previous
-	 * tracing session. We don't want it to leak into this session.
-	 */
-	kbase_csf_firmware_trace_buffer_discard_all(self->trace_buffer);
 }
 
 int kbase_csf_tl_reader_flush_buffer(struct kbase_csf_tl_reader *self)
@@ -333,16 +324,21 @@ static int tl_reader_update_enable_bit(struct kbase_csf_tl_reader *self, bool va
 
 void kbase_csf_tl_reader_init(struct kbase_csf_tl_reader *self, struct kbase_tlstream *stream)
 {
-	*self = (struct kbase_csf_tl_reader){
-		.timer_interval = KBASE_CSF_TL_READ_INTERVAL_DEFAULT,
-		.stream = stream,
-		.kbdev = NULL, /* This will be initialized by tl_reader_init_late() */
-		.is_active = false,
-	};
+	self->timer_interval = KBASE_CSF_TL_READ_INTERVAL_DEFAULT;
 
 	kbase_timer_setup(&self->read_timer, kbasep_csf_tl_reader_read_callback);
 
+	self->stream = stream;
+
+	/* This will be initialized by tl_reader_init_late() */
+	self->kbdev = NULL;
+	self->trace_buffer = NULL;
+	self->tl_header.data = NULL;
+	self->tl_header.size = 0;
+
 	spin_lock_init(&self->read_lock);
+
+	tl_reader_reset(self);
 }
 
 void kbase_csf_tl_reader_term(struct kbase_csf_tl_reader *self)
@@ -352,19 +348,13 @@ void kbase_csf_tl_reader_term(struct kbase_csf_tl_reader *self)
 
 int kbase_csf_tl_reader_start(struct kbase_csf_tl_reader *self, struct kbase_device *kbdev)
 {
-	unsigned long flags;
 	int rcode;
 
-	spin_lock_irqsave(&self->read_lock, flags);
-
 	/* If already running, early exit. */
-	if (self->is_active) {
-		spin_unlock_irqrestore(&self->read_lock, flags);
+	if (self->is_active)
 		return 0;
-	}
 
 	if (tl_reader_init_late(self, kbdev)) {
-		spin_unlock_irqrestore(&self->read_lock, flags);
 #if IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
 		dev_warn(kbdev->dev, "CSFFW timeline is not available for MALI_BIFROST_NO_MALI builds!");
 		return 0;
@@ -376,9 +366,6 @@ int kbase_csf_tl_reader_start(struct kbase_csf_tl_reader *self, struct kbase_dev
 	tl_reader_reset(self);
 
 	self->is_active = true;
-
-	spin_unlock_irqrestore(&self->read_lock, flags);
-
 	/* Set bytes to copy to the header size. This is to trigger copying
 	 * of the header to the user space.
 	 */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.c
index 233f9147a408..8ed7c91553a6 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -123,7 +123,7 @@ static const struct firmware_trace_buffer_data trace_buffer_data[] = {
 	{ KBASE_CSFFW_BENCHMARK_BUF_NAME, { 0 }, 2 },
 	{ KBASE_CSFFW_TIMELINE_BUF_NAME, { 0 }, KBASE_CSF_TL_BUFFER_NR_PAGES },
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
-	{ KBASE_CSFFW_GPU_METRICS_BUF_NAME, { 0 }, 32 },
+	{ KBASE_CSFFW_GPU_METRICS_BUF_NAME, { 0 }, 8 },
 #endif /* CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD */
 };
 
@@ -469,15 +469,14 @@ unsigned int kbase_csf_firmware_trace_buffer_read_data(struct firmware_trace_buf
 	} else {
 		unsigned int bytes_copied_head, bytes_copied_tail;
 
-		bytes_copied_tail =
-			min_t(unsigned int, num_bytes, size_sub(buffer_size, extract_offset));
+		bytes_copied_tail = min_t(unsigned int, num_bytes, (buffer_size - extract_offset));
 		memcpy(data, &data_cpu_va[extract_offset], bytes_copied_tail);
 
 		bytes_copied_head =
 			min_t(unsigned int, (num_bytes - bytes_copied_tail), insert_offset);
 		memcpy(&data[bytes_copied_tail], data_cpu_va, bytes_copied_head);
 
-		bytes_copied = size_add(bytes_copied_head, bytes_copied_tail);
+		bytes_copied = bytes_copied_head + bytes_copied_tail;
 		extract_offset += bytes_copied;
 		if (extract_offset >= buffer_size)
 			extract_offset = bytes_copied_head;
@@ -520,14 +519,6 @@ void kbase_csf_firmware_trace_buffer_discard(struct firmware_trace_buffer *trace
 }
 EXPORT_SYMBOL(kbase_csf_firmware_trace_buffer_discard);
 
-void kbase_csf_firmware_trace_buffer_discard_all(struct firmware_trace_buffer *trace_buffer)
-{
-	if (WARN_ON(!trace_buffer))
-		return;
-
-	*(trace_buffer->cpu_va.extract_cpu_va) = *(trace_buffer->cpu_va.insert_cpu_va);
-}
-
 static void update_trace_buffer_active_mask64(struct firmware_trace_buffer *tb, u64 mask)
 {
 	unsigned int i;
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.h
index 35988eaf8f5a..90dfcb2699bc 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.h
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.h
@@ -179,15 +179,6 @@ unsigned int kbase_csf_firmware_trace_buffer_read_data(struct firmware_trace_buf
  */
 void kbase_csf_firmware_trace_buffer_discard(struct firmware_trace_buffer *trace_buffer);
 
-/**
- * kbase_csf_firmware_trace_buffer_discard_all - Discard all data from a trace buffer
- *
- * @trace_buffer: Trace buffer handle
- *
- * Discard all the data in the trace buffer to make it empty.
- */
-void kbase_csf_firmware_trace_buffer_discard_all(struct firmware_trace_buffer *trace_buffer);
-
 /**
  * kbase_csf_firmware_trace_buffer_get_active_mask64 - Get trace buffer active mask
  *
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_util.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_util.c
index 5f13672e70b8..7dc32a11bb29 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_util.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_util.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -115,7 +115,7 @@ struct kbasep_printer *kbasep_printer_buffer_init(struct kbase_device *kbdev,
 
 	if (kbpr) {
 		if (kfifo_alloc(&kbpr->fifo, KBASEP_PRINTER_BUFFER_MAX_SIZE, GFP_KERNEL)) {
-			vfree(kbpr);
+			kfree(kbpr);
 			return NULL;
 		}
 		kbpr->kbdev = kbdev;
@@ -224,7 +224,7 @@ __attribute__((format(__printf__, 2, 3))) void kbasep_print(struct kbasep_printe
 	va_list arglist;
 
 	va_start(arglist, fmt);
-	len = vscnprintf(buffer, KBASEP_PRINT_FORMAT_BUFFER_MAX_SIZE, fmt, arglist);
+	len = vsnprintf(buffer, KBASEP_PRINT_FORMAT_BUFFER_MAX_SIZE, fmt, arglist);
 	if (len <= 0) {
 		pr_err("message write to the buffer failed");
 		goto exit;
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_debug_csf_fault.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_debug_csf_fault.c
index 32c34b2e04d7..2e87c08ded0e 100644
--- a/drivers/gpu/arm/bifrost/csf/mali_kbase_debug_csf_fault.c
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_debug_csf_fault.c
@@ -88,8 +88,8 @@ bool kbase_debug_csf_fault_notify(struct kbase_device *kbdev, struct kbase_conte
 		goto unlock;
 	}
 
-	kbdev->csf.dof.kctx_tgid = kctx ? (unsigned int)kctx->tgid : 0U;
-	kbdev->csf.dof.kctx_id = kctx ? kctx->id : 0U;
+	kbdev->csf.dof.kctx_tgid = kctx ? kctx->tgid : 0;
+	kbdev->csf.dof.kctx_id = kctx ? kctx->id : 0;
 	kbdev->csf.dof.error_code = error;
 	kbase_debug_csf_fault_wakeup(kbdev);
 
@@ -142,7 +142,7 @@ static ssize_t debug_csf_fault_read(struct file *file, char __user *buffer, size
 	spin_unlock_irqrestore(&kbdev->csf.dof.lock, flags);
 
 	dev_info(kbdev->dev, "debug csf fault info read");
-	return simple_read_from_buffer(buffer, size, f_pos, buf, (size_t)count);
+	return simple_read_from_buffer(buffer, size, f_pos, buf, count);
 }
 
 static int debug_csf_fault_open(struct inode *in, struct file *file)
@@ -197,7 +197,7 @@ static ssize_t debug_csf_fault_write(struct file *file, const char __user *ubuf,
 	 */
 	wake_up(&kbdev->csf.dof.dump_wait_wq);
 
-	return (ssize_t)count;
+	return count;
 }
 
 static int debug_csf_fault_release(struct inode *in, struct file *file)
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_coresight_csf.c b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_coresight_csf.c
index da56d71f473f..46ad63692a34 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_coresight_csf.c
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_coresight_csf.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2022-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -443,7 +443,7 @@ kbase_debug_coresight_csf_config_create(void *client_data,
 	}
 
 	config = kzalloc(sizeof(struct kbase_debug_coresight_csf_config), GFP_KERNEL);
-	if (WARN_ON(!config))
+	if (WARN_ON(!client))
 		return NULL;
 
 	config->client = client;
@@ -778,8 +778,7 @@ KBASE_EXPORT_TEST_API(kbase_debug_coresight_csf_state_check);
 bool kbase_debug_coresight_csf_state_wait(struct kbase_device *kbdev,
 					  enum kbase_debug_coresight_csf_state state)
 {
-	const long wait_timeout =
-		kbase_csf_timeout_in_jiffies(kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT));
+	const long wait_timeout = kbase_csf_timeout_in_jiffies(kbdev->csf.fw_timeout_ms);
 	struct kbase_debug_coresight_csf_config *config_entry, *next_config_entry;
 	unsigned long flags;
 	bool success = true;
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_csf.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_csf.h
index 04da9c8b9057..18520db15502 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_csf.h
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_csf.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -212,9 +212,6 @@ KBASE_KTRACE_CODE_MAKE_CODE(SCHEDULER_EVICT_CTX_SLOTS_START),
 	KBASE_KTRACE_CODE_MAKE_CODE(SCHED_BUSY), KBASE_KTRACE_CODE_MAKE_CODE(SCHED_INACTIVE),
 	KBASE_KTRACE_CODE_MAKE_CODE(SCHED_SUSPENDED), KBASE_KTRACE_CODE_MAKE_CODE(SCHED_SLEEPING),
 
-	/* info_val == true if FW Sleep-on-Idle is enabled, false otherwise */
-	KBASE_KTRACE_CODE_MAKE_CODE(FIRMWARE_SLEEP_ON_IDLE_CHANGED),
-
 /* info_val = mcu state */
 #define KBASEP_MCU_STATE(n) KBASE_KTRACE_CODE_MAKE_CODE(PM_MCU_##n),
 #include "backend/gpu/mali_kbase_pm_mcu_states.h"
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.c b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.c
index b14ffc69c54c..46eb6db36c82 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.c
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -27,9 +27,8 @@
 
 void kbasep_ktrace_backend_format_header(char *buffer, int sz, s32 *written)
 {
-	*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0),
-				  "group,slot,prio,csi,kcpu"),
-			0);
+	*written += MAX(
+		snprintf(buffer + *written, MAX(sz - *written, 0), "group,slot,prio,csi,kcpu"), 0);
 }
 
 void kbasep_ktrace_backend_format_msg(struct kbase_ktrace_msg *trace_msg, char *buffer, int sz,
@@ -44,40 +43,38 @@ void kbasep_ktrace_backend_format_msg(struct kbase_ktrace_msg *trace_msg, char *
 	if (be_msg->gpu.flags & KBASE_KTRACE_FLAG_CSF_GROUP) {
 		const s8 slot = be_msg->gpu.csg_nr;
 		/* group,slot, */
-		*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0),
-					  "%u,%d,", be_msg->gpu.group_handle, slot),
+		*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), "%u,%d,",
+					 be_msg->gpu.group_handle, slot),
 				0);
 
 		/* prio */
 		if (slot >= 0)
-			*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0),
-						  "%u", be_msg->gpu.slot_prio),
+			*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), "%u",
+						 be_msg->gpu.slot_prio),
 					0);
 
 		/* , */
-		*written +=
-			MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0), ","), 0);
+		*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), ","), 0);
 	} else {
 		/* No group,slot,prio fields, but ensure ending with "," */
-		*written +=
-			MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0), ",,,"), 0);
+		*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), ",,,"), 0);
 	}
 
 	/* queue parts: csi */
 	if (trace_msg->backend.gpu.flags & KBASE_KTRACE_FLAG_CSF_QUEUE)
-		*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0), "%d",
-					  be_msg->gpu.csi_index),
+		*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), "%d",
+					 be_msg->gpu.csi_index),
 				0);
 
 	/* , */
-	*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0), ","), 0);
+	*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), ","), 0);
 
 	if (be_msg->gpu.flags & KBASE_KTRACE_FLAG_CSF_KCPU) {
 		/* kcpu data */
-		*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0),
-					  "kcpu %d (0x%llx)", be_msg->kcpu.id,
-					  be_msg->kcpu.extra_info_val),
-				0);
+		*written +=
+			MAX(snprintf(buffer + *written, MAX(sz - *written, 0), "kcpu %d (0x%llx)",
+				     be_msg->kcpu.id, be_msg->kcpu.extra_info_val),
+			    0);
 	}
 
 	/* Don't end with a trailing "," - this is a 'standalone' formatted
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_jm.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_jm.h
index 9978e4939d14..c1f60dd16981 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_jm.h
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_jm.h
@@ -98,7 +98,7 @@ union kbase_ktrace_backend {
 				    * KBASE_KTRACE_FLAG_JM_ATOM
 				    */
 		u64 gpu_addr;
-		unsigned int atom_number; /* Only valid for KBASE_KTRACE_FLAG_JM_ATOM */
+		int atom_number; /* Only valid for KBASE_KTRACE_FLAG_JM_ATOM */
 		/* Pack smaller members together */
 		kbase_ktrace_code_t code;
 		kbase_ktrace_flag_t flags;
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.c b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.c
index 39306e7d45e3..8f95ca67f4d6 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.c
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -27,8 +27,8 @@
 
 void kbasep_ktrace_backend_format_header(char *buffer, int sz, s32 *written)
 {
-	*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0),
-				  "katom,gpu_addr,jobslot,refcount"),
+	*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0),
+				 "katom,gpu_addr,jobslot,refcount"),
 			0);
 }
 
@@ -37,41 +37,40 @@ void kbasep_ktrace_backend_format_msg(struct kbase_ktrace_msg *trace_msg, char *
 {
 	/* katom */
 	if (trace_msg->backend.gpu.flags & KBASE_KTRACE_FLAG_JM_ATOM)
-		*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0),
-					  "atom %u (ud: 0x%llx 0x%llx)",
-					  trace_msg->backend.gpu.atom_number,
-					  trace_msg->backend.gpu.atom_udata[0],
-					  trace_msg->backend.gpu.atom_udata[1]),
+		*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0),
+					 "atom %d (ud: 0x%llx 0x%llx)",
+					 trace_msg->backend.gpu.atom_number,
+					 trace_msg->backend.gpu.atom_udata[0],
+					 trace_msg->backend.gpu.atom_udata[1]),
 				0);
 
 	/* gpu_addr */
 	if (trace_msg->backend.gpu.flags & KBASE_KTRACE_FLAG_BACKEND)
-		*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0),
-					  ",%.8llx,", trace_msg->backend.gpu.gpu_addr),
+		*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), ",%.8llx,",
+					 trace_msg->backend.gpu.gpu_addr),
 				0);
 	else
-		*written +=
-			MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0), ",,"), 0);
+		*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), ",,"), 0);
 
 	/* jobslot */
 	if (trace_msg->backend.gpu.flags & KBASE_KTRACE_FLAG_JM_JOBSLOT)
-		*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0), "%d",
-					  trace_msg->backend.gpu.jobslot),
+		*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), "%d",
+					 trace_msg->backend.gpu.jobslot),
 				0);
 
-	*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0), ","), 0);
+	*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), ","), 0);
 
 	/* refcount */
 	if (trace_msg->backend.gpu.flags & KBASE_KTRACE_FLAG_JM_REFCOUNT)
-		*written += MAX(scnprintf(buffer + *written, (size_t)MAX(sz - *written, 0), "%d",
-					  trace_msg->backend.gpu.refcount),
+		*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0), "%d",
+					 trace_msg->backend.gpu.refcount),
 				0);
 }
 
 void kbasep_ktrace_add_jm(struct kbase_device *kbdev, enum kbase_ktrace_code code,
 			  struct kbase_context *kctx, const struct kbase_jd_atom *katom,
-			  u64 gpu_addr, kbase_ktrace_flag_t flags, int refcount,
-			  unsigned int jobslot, u64 info_val)
+			  u64 gpu_addr, kbase_ktrace_flag_t flags, int refcount, int jobslot,
+			  u64 info_val)
 {
 	unsigned long irqflags;
 	struct kbase_ktrace_msg *trace_msg;
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.h
index e9c78e1d9fc6..b91176deac26 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.h
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.h
@@ -42,8 +42,8 @@
  */
 void kbasep_ktrace_add_jm(struct kbase_device *kbdev, enum kbase_ktrace_code code,
 			  struct kbase_context *kctx, const struct kbase_jd_atom *katom,
-			  u64 gpu_addr, kbase_ktrace_flag_t flags, int refcount,
-			  unsigned int jobslot, u64 info_val);
+			  u64 gpu_addr, kbase_ktrace_flag_t flags, int refcount, int jobslot,
+			  u64 info_val);
 
 #define KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, gpu_addr, flags, refcount, jobslot, \
 				 info_val)                                                     \
@@ -175,7 +175,7 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev, enum kbase_ktrace_code cod
 	do {                                                                                      \
 		/* capture values that could come from non-pure function calls */                 \
 		u64 __gpu_addr = gpu_addr;                                                        \
-		unsigned int __jobslot = jobslot;                                                 \
+		int __jobslot = jobslot;                                                          \
 		KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, __gpu_addr,                    \
 					 KBASE_KTRACE_FLAG_JM_JOBSLOT, 0, __jobslot, 0);          \
 		KBASE_KTRACE_FTRACE_ADD_JM_SLOT(kbdev, code, kctx, katom, __gpu_addr, __jobslot); \
@@ -202,7 +202,7 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev, enum kbase_ktrace_code cod
 	do {                                                                                      \
 		/* capture values that could come from non-pure function calls */                 \
 		u64 __gpu_addr = gpu_addr;                                                        \
-		unsigned int __jobslot = jobslot;                                                 \
+		int __jobslot = jobslot;                                                          \
 		u64 __info_val = info_val;                                                        \
 		KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, __gpu_addr,                    \
 					 KBASE_KTRACE_FLAG_JM_JOBSLOT, 0, __jobslot, __info_val); \
@@ -234,7 +234,7 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev, enum kbase_ktrace_code cod
 		KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, __gpu_addr,              \
 					 KBASE_KTRACE_FLAG_JM_REFCOUNT, __refcount, 0, 0u); \
 		KBASE_KTRACE_FTRACE_ADD_JM_REFCOUNT(kbdev, code, kctx, katom, __gpu_addr,   \
-						    (unsigned int)__refcount);              \
+						    __refcount);                            \
 	} while (0)
 
 /**
@@ -265,7 +265,7 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev, enum kbase_ktrace_code cod
 					 KBASE_KTRACE_FLAG_JM_REFCOUNT, __refcount, 0,            \
 					 __info_val);                                             \
 		KBASE_KTRACE_FTRACE_ADD_JM_REFCOUNT(kbdev, code, kctx, katom, __gpu_addr,         \
-						    (unsigned int)__refcount, __info_val);        \
+						    __refcount, __info_val);                      \
 	} while (0)
 
 /**
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_csf.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_csf.h
index 7c40f472a78b..0b0de2385f85 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_csf.h
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_csf.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -69,7 +69,6 @@ DEFINE_MALI_ADD_EVENT(SCHED_BUSY);
 DEFINE_MALI_ADD_EVENT(SCHED_INACTIVE);
 DEFINE_MALI_ADD_EVENT(SCHED_SUSPENDED);
 DEFINE_MALI_ADD_EVENT(SCHED_SLEEPING);
-DEFINE_MALI_ADD_EVENT(FIRMWARE_SLEEP_ON_IDLE_CHANGED);
 #define KBASEP_MCU_STATE(n) DEFINE_MALI_ADD_EVENT(PM_MCU_##n);
 #include "backend/gpu/mali_kbase_pm_mcu_states.h"
 #undef KBASEP_MCU_STATE
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_jm.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_jm.h
index 3e5b0ec89758..fd62bae9e0a9 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_jm.h
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_jm.h
@@ -28,7 +28,7 @@
 #define _KBASE_DEBUG_LINUX_KTRACE_JM_H_
 
 DECLARE_EVENT_CLASS(mali_jm_slot_template,
-		    TP_PROTO(struct kbase_context *kctx, unsigned int jobslot, u64 info_val),
+		    TP_PROTO(struct kbase_context *kctx, int jobslot, u64 info_val),
 		    TP_ARGS(kctx, jobslot, info_val),
 		    TP_STRUCT__entry(__field(pid_t, kctx_tgid) __field(u32, kctx_id)
 					     __field(unsigned int, jobslot) __field(u64, info_val)),
@@ -38,9 +38,9 @@ DECLARE_EVENT_CLASS(mali_jm_slot_template,
 		    TP_printk("kctx=%d_%u jobslot=%u info=0x%llx", __entry->kctx_tgid,
 			      __entry->kctx_id, __entry->jobslot, __entry->info_val));
 
-#define DEFINE_MALI_JM_SLOT_EVENT(name)                                                        \
-	DEFINE_EVENT(mali_jm_slot_template, mali_##name,                                       \
-		     TP_PROTO(struct kbase_context *kctx, unsigned int jobslot, u64 info_val), \
+#define DEFINE_MALI_JM_SLOT_EVENT(name)                                               \
+	DEFINE_EVENT(mali_jm_slot_template, mali_##name,                              \
+		     TP_PROTO(struct kbase_context *kctx, int jobslot, u64 info_val), \
 		     TP_ARGS(kctx, jobslot, info_val))
 DEFINE_MALI_JM_SLOT_EVENT(JM_RETURN_ATOM_TO_JS);
 DEFINE_MALI_JM_SLOT_EVENT(JM_MARK_FOR_RETURN_TO_JS);
@@ -78,7 +78,7 @@ DEFINE_MALI_JM_SLOT_EVENT(JS_SLOT_PRIO_IS_BLOCKED);
 #undef DEFINE_MALI_JM_SLOT_EVENT
 
 DECLARE_EVENT_CLASS(mali_jm_refcount_template,
-		    TP_PROTO(struct kbase_context *kctx, unsigned int refcount, u64 info_val),
+		    TP_PROTO(struct kbase_context *kctx, int refcount, u64 info_val),
 		    TP_ARGS(kctx, refcount, info_val),
 		    TP_STRUCT__entry(__field(pid_t, kctx_tgid) __field(u32, kctx_id)
 					     __field(unsigned int, refcount)
@@ -89,9 +89,9 @@ DECLARE_EVENT_CLASS(mali_jm_refcount_template,
 		    TP_printk("kctx=%d_%u refcount=%u info=0x%llx", __entry->kctx_tgid,
 			      __entry->kctx_id, __entry->refcount, __entry->info_val));
 
-#define DEFINE_MALI_JM_REFCOUNT_EVENT(name)                                                     \
-	DEFINE_EVENT(mali_jm_refcount_template, mali_##name,                                    \
-		     TP_PROTO(struct kbase_context *kctx, unsigned int refcount, u64 info_val), \
+#define DEFINE_MALI_JM_REFCOUNT_EVENT(name)                                            \
+	DEFINE_EVENT(mali_jm_refcount_template, mali_##name,                           \
+		     TP_PROTO(struct kbase_context *kctx, int refcount, u64 info_val), \
 		     TP_ARGS(kctx, refcount, info_val))
 DEFINE_MALI_JM_REFCOUNT_EVENT(JS_ADD_JOB);
 DEFINE_MALI_JM_REFCOUNT_EVENT(JS_REMOVE_JOB);
diff --git a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.c b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.c
index 036d1f5968f6..12a722765c5e 100644
--- a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.c
+++ b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -71,15 +71,15 @@ static const char *const kbasep_ktrace_code_string[] = {
 
 static void kbasep_ktrace_format_header(char *buffer, int sz, s32 written)
 {
-	written += MAX(scnprintf(buffer + written, (size_t)MAX(sz - written, 0),
-				 "secs,thread_id,cpu,code,kctx,"),
+	written += MAX(snprintf(buffer + written, MAX(sz - written, 0),
+				"secs,thread_id,cpu,code,kctx,"),
 		       0);
 
 	kbasep_ktrace_backend_format_header(buffer, sz, &written);
 
-	written += MAX(scnprintf(buffer + written, (size_t)MAX(sz - written, 0),
-				 ",info_val,ktrace_version=%u.%u", KBASE_KTRACE_VERSION_MAJOR,
-				 KBASE_KTRACE_VERSION_MINOR),
+	written += MAX(snprintf(buffer + written, MAX(sz - written, 0),
+				",info_val,ktrace_version=%u.%u", KBASE_KTRACE_VERSION_MAJOR,
+				KBASE_KTRACE_VERSION_MINOR),
 		       0);
 
 	buffer[sz - 1] = 0;
@@ -93,21 +93,21 @@ static void kbasep_ktrace_format_msg(struct kbase_ktrace_msg *trace_msg, char *b
 	 *
 	 * secs,thread_id,cpu,code,
 	 */
-	written += MAX(scnprintf(buffer + written, (size_t)MAX(sz - written, 0),
-				 "%d.%.6d,%d,%d,%s,", (int)trace_msg->timestamp.tv_sec,
-				 (int)(trace_msg->timestamp.tv_nsec / 1000), trace_msg->thread_id,
-				 trace_msg->cpu,
-				 kbasep_ktrace_code_string[trace_msg->backend.gpu.code]),
+	written += MAX(snprintf(buffer + written, MAX(sz - written, 0), "%d.%.6d,%d,%d,%s,",
+				(int)trace_msg->timestamp.tv_sec,
+				(int)(trace_msg->timestamp.tv_nsec / 1000), trace_msg->thread_id,
+				trace_msg->cpu,
+				kbasep_ktrace_code_string[trace_msg->backend.gpu.code]),
 		       0);
 
 	/* kctx part: */
 	if (trace_msg->kctx_tgid) {
-		written += MAX(scnprintf(buffer + written, (size_t)MAX(sz - written, 0), "%d_%u",
-					 trace_msg->kctx_tgid, trace_msg->kctx_id),
+		written += MAX(snprintf(buffer + written, MAX(sz - written, 0), "%d_%u",
+					trace_msg->kctx_tgid, trace_msg->kctx_id),
 			       0);
 	}
 	/* Trailing comma */
-	written += MAX(scnprintf(buffer + written, (size_t)MAX(sz - written, 0), ","), 0);
+	written += MAX(snprintf(buffer + written, MAX(sz - written, 0), ","), 0);
 
 	/* Backend parts */
 	kbasep_ktrace_backend_format_msg(trace_msg, buffer, sz, &written);
@@ -119,8 +119,8 @@ static void kbasep_ktrace_format_msg(struct kbase_ktrace_msg *trace_msg, char *b
 	 * Note that the last column is empty, it's simply to hold the ktrace
 	 * version in the header
 	 */
-	written += MAX(scnprintf(buffer + written, (size_t)MAX(sz - written, 0), ",0x%.16llx",
-				 (unsigned long long)trace_msg->info_val),
+	written += MAX(snprintf(buffer + written, MAX(sz - written, 0), ",0x%.16llx",
+				(unsigned long long)trace_msg->info_val),
 		       0);
 	buffer[sz - 1] = 0;
 }
@@ -156,7 +156,7 @@ void kbasep_ktrace_msg_init(struct kbase_ktrace *ktrace, struct kbase_ktrace_msg
 {
 	lockdep_assert_held(&ktrace->lock);
 
-	trace_msg->thread_id = (u32)task_pid_nr(current);
+	trace_msg->thread_id = task_pid_nr(current);
 	trace_msg->cpu = task_cpu(current);
 
 	ktime_get_real_ts64(&trace_msg->timestamp);
diff --git a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_codes.h b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_codes.h
index d40eec013cb5..991f70fe8540 100644
--- a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_codes.h
+++ b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_codes.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2011-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -149,17 +149,13 @@ KBASE_KTRACE_CODE_MAKE_CODE(CORE_CTX_DESTROY),
 	KBASE_KTRACE_CODE_MAKE_CODE(SCHED_RETAIN_CTX_NOLOCK),
 	/* info_val == kctx->refcount */
 	KBASE_KTRACE_CODE_MAKE_CODE(SCHED_RELEASE_CTX),
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	/*
 	 * Arbitration events
 	 */
-	KBASE_KTRACE_CODE_MAKE_CODE(ARB_VM_STATE),
+	KBASE_KTRACE_CODE_MAKE_CODE(ARB_GPU_LOST), KBASE_KTRACE_CODE_MAKE_CODE(ARB_VM_STATE),
 	KBASE_KTRACE_CODE_MAKE_CODE(ARB_VM_EVT),
-	KBASE_KTRACE_CODE_MAKE_CODE(ARB_GPU_GRANTED),
-	KBASE_KTRACE_CODE_MAKE_CODE(ARB_GPU_LOST),
-	KBASE_KTRACE_CODE_MAKE_CODE(ARB_GPU_STARTED),
-	KBASE_KTRACE_CODE_MAKE_CODE(ARB_GPU_STOP_REQUESTED),
-	KBASE_KTRACE_CODE_MAKE_CODE(ARB_GPU_STOPPED),
-	KBASE_KTRACE_CODE_MAKE_CODE(ARB_GPU_REQUESTED),
+#endif
 
 #if MALI_USE_CSF
 #include "debug/backend/mali_kbase_debug_ktrace_codes_csf.h"
diff --git a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_linux_ktrace.h b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_linux_ktrace.h
index acc78eb5b0b2..1ebddfa3f44f 100644
--- a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_linux_ktrace.h
+++ b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_linux_ktrace.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -95,16 +95,13 @@ DEFINE_MALI_ADD_EVENT(PM_RUNTIME_RESUME_CALLBACK);
 #undef KBASEP_L2_STATE
 DEFINE_MALI_ADD_EVENT(SCHED_RETAIN_CTX_NOLOCK);
 DEFINE_MALI_ADD_EVENT(SCHED_RELEASE_CTX);
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 
+DEFINE_MALI_ADD_EVENT(ARB_GPU_LOST);
 DEFINE_MALI_ADD_EVENT(ARB_VM_STATE);
 DEFINE_MALI_ADD_EVENT(ARB_VM_EVT);
-DEFINE_MALI_ADD_EVENT(ARB_GPU_GRANTED);
-DEFINE_MALI_ADD_EVENT(ARB_GPU_LOST);
-DEFINE_MALI_ADD_EVENT(ARB_GPU_STARTED);
-DEFINE_MALI_ADD_EVENT(ARB_GPU_STOP_REQUESTED);
-DEFINE_MALI_ADD_EVENT(ARB_GPU_STOPPED);
-DEFINE_MALI_ADD_EVENT(ARB_GPU_REQUESTED);
 
+#endif
 #if MALI_USE_CSF
 #include "backend/mali_kbase_debug_linux_ktrace_csf.h"
 #else
diff --git a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_csf.c b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_csf.c
index 218022ac3186..36778923f364 100644
--- a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_csf.c
+++ b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_csf.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -87,8 +87,8 @@ static int kbase_backend_late_init(struct kbase_device *kbdev)
 
 #ifdef CONFIG_MALI_BIFROST_DEBUG
 #if IS_ENABLED(CONFIG_MALI_REAL_HW)
-	if (kbase_validate_interrupts(kbdev) != 0) {
-		dev_err(kbdev->dev, "Interrupt validation failed.\n");
+	if (kbasep_common_test_interrupt_handlers(kbdev) != 0) {
+		dev_err(kbdev->dev, "Interrupt assignment check failed.\n");
 		err = -EINVAL;
 		goto fail_interrupt_test;
 	}
@@ -132,15 +132,11 @@ static int kbase_backend_late_init(struct kbase_device *kbdev)
 
 fail_update_l2_features:
 	kbase_backend_devfreq_term(kbdev);
-
 fail_devfreq_init:
-	{
-		kbasep_pm_metrics_term(kbdev);
-	}
+	kbasep_pm_metrics_term(kbdev);
 fail_pm_metrics_init:
-	{
-		kbase_ipa_control_term(kbdev);
-	}
+	kbase_ipa_control_term(kbdev);
+
 #ifdef CONFIG_MALI_BIFROST_DEBUG
 #if IS_ENABLED(CONFIG_MALI_REAL_HW)
 fail_interrupt_test:
@@ -163,11 +159,9 @@ static int kbase_backend_late_init(struct kbase_device *kbdev)
  */
 static void kbase_backend_late_term(struct kbase_device *kbdev)
 {
-	{
-		kbase_backend_devfreq_term(kbdev);
-		kbasep_pm_metrics_term(kbdev);
-		kbase_ipa_control_term(kbdev);
-	}
+	kbase_backend_devfreq_term(kbdev);
+	kbasep_pm_metrics_term(kbdev);
+	kbase_ipa_control_term(kbdev);
 	kbase_hwaccess_pm_halt(kbdev);
 	kbase_reset_gpu_term(kbdev);
 	kbase_hwaccess_pm_term(kbdev);
@@ -284,9 +278,11 @@ static const struct kbase_device_init dev_init[] = {
 #if !IS_ENABLED(CONFIG_MALI_REAL_HW)
 	{ kbase_gpu_device_create, kbase_gpu_device_destroy, "Dummy model initialization failed" },
 #else /* !IS_ENABLED(CONFIG_MALI_REAL_HW) */
-	{ kbase_get_irqs, NULL, "IRQ search failed" },
-	{ registers_map, registers_unmap, "Register map failed" },
+	{ assign_irqs, NULL, "IRQ search failed" },
 #endif /* !IS_ENABLED(CONFIG_MALI_REAL_HW) */
+#if !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
+	{ registers_map, registers_unmap, "Register map failed" },
+#endif /* !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI) */
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
 	{ kbase_gpu_metrics_init, kbase_gpu_metrics_term, "GPU metrics initialization failed" },
 #endif /* IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD) */
diff --git a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_csf.c b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_csf.c
index 3b27b87657a5..c7f34bc78137 100644
--- a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_csf.c
+++ b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_csf.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -28,16 +28,6 @@
 #include <mali_kbase_reset_gpu.h>
 #include <mmu/mali_kbase_mmu.h>
 #include <mali_kbase_ctx_sched.h>
-#include <mmu/mali_kbase_mmu_faults_decoder.h>
-
-bool kbase_is_gpu_removed(struct kbase_device *kbdev)
-{
-	if (!kbase_has_arbiter(kbdev))
-		return false;
-
-
-	return (KBASE_REG_READ(kbdev, GPU_CONTROL_ENUM(GPU_ID)) == 0);
-}
 
 /**
  * kbase_report_gpu_fault - Report a GPU fault of the device.
@@ -88,7 +78,6 @@ static void kbase_gpu_fault_interrupt(struct kbase_device *kbdev)
 void kbase_gpu_interrupt(struct kbase_device *kbdev, u32 val)
 {
 	u32 power_changed_mask = (POWER_CHANGED_ALL | MCU_STATUS_GPU_IRQ);
-	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
 
 
 	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ, NULL, val);
@@ -96,6 +85,7 @@ void kbase_gpu_interrupt(struct kbase_device *kbdev, u32 val)
 		kbase_gpu_fault_interrupt(kbdev);
 
 	if (val & GPU_PROTECTED_FAULT) {
+		struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
 		unsigned long flags;
 
 		dev_err_ratelimited(kbdev->dev, "GPU fault in protected mode");
@@ -149,33 +139,10 @@ void kbase_gpu_interrupt(struct kbase_device *kbdev, u32 val)
 		unsigned long flags;
 
 		dev_dbg(kbdev->dev, "Doorbell mirror interrupt received");
-
-		/* Assume that the doorbell comes from userspace which
-		 * presents new works in order to invalidate a possible GPU
-		 * idle event.
-		 * If the doorbell was raised by KBase then the FW would handle
-		 * the pending doorbell then raise a 2nd GBL_IDLE IRQ which
-		 * would allow us to put the GPU to sleep.
-		 */
-		atomic_set(&scheduler->gpu_no_longer_idle, true);
-
 		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 		kbase_pm_disable_db_mirror_interrupt(kbdev);
-
-		if (likely(kbdev->pm.backend.mcu_state == KBASE_MCU_IN_SLEEP)) {
-			kbdev->pm.backend.exit_gpu_sleep_mode = true;
-			kbase_csf_scheduler_invoke_tick(kbdev);
-		} else if (likely(test_bit(KBASE_GPU_SUPPORTS_FW_SLEEP_ON_IDLE,
-					   &kbdev->pm.backend.gpu_sleep_allowed)) &&
-			   (kbdev->pm.backend.mcu_state != KBASE_MCU_ON_PEND_SLEEP)) {
-			/* The firmware is going to sleep on its own but new
-			 * doorbells were rung before we manage to handle
-			 * the GLB_IDLE IRQ in the bottom half. We shall enable
-			 * DB notification to allow the DB to be handled by FW.
-			 */
-			dev_dbg(kbdev->dev, "Re-enabling MCU immediately following DB_MIRROR IRQ");
-			kbase_pm_enable_mcu_db_notification(kbdev);
-		}
+		kbdev->pm.backend.exit_gpu_sleep_mode = true;
+		kbase_csf_scheduler_invoke_tick(kbdev);
 		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 	}
 #endif
@@ -202,14 +169,10 @@ void kbase_gpu_interrupt(struct kbase_device *kbdev, u32 val)
 		 * cores.
 		 */
 		if (kbdev->pm.backend.l2_always_on ||
-		    kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TTRX_921))
+		    kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TTRX_921))
 			kbase_pm_power_changed(kbdev);
 	}
 
-	if (val & MCU_STATUS_GPU_IRQ)
-		wake_up_all(&kbdev->csf.event_wait);
-
 	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ_DONE, NULL, val);
 }
-KBASE_EXPORT_TEST_API(kbase_gpu_interrupt);
 
diff --git a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_jm.c b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_jm.c
index 4dd9a228aa11..d530010c096e 100644
--- a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_jm.c
+++ b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_jm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -28,14 +28,6 @@
 #include <mali_kbase_reset_gpu.h>
 #include <mmu/mali_kbase_mmu.h>
 
-bool kbase_is_gpu_removed(struct kbase_device *kbdev)
-{
-	if (!kbase_has_arbiter(kbdev))
-		return false;
-
-	return (KBASE_REG_READ(kbdev, GPU_CONTROL_ENUM(GPU_ID)) == 0);
-}
-
 /**
  * kbase_report_gpu_fault - Report a GPU fault.
  * @kbdev:    Kbase device pointer
@@ -103,10 +95,9 @@ void kbase_gpu_interrupt(struct kbase_device *kbdev, u32 val)
 		 * cores.
 		 */
 		if (kbdev->pm.backend.l2_always_on ||
-		    kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TTRX_921))
+		    kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TTRX_921))
 			kbase_pm_power_changed(kbdev);
 	}
 
 	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ_DONE, NULL, val);
 }
-KBASE_EXPORT_TEST_API(kbase_gpu_interrupt);
diff --git a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_jm.c b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_jm.c
index 8cdf26e28ac6..556e388e11bd 100644
--- a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_jm.c
+++ b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_jm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -32,6 +32,10 @@
 #include <hwcnt/backend/mali_kbase_hwcnt_backend_jm_watchdog.h>
 #include <backend/gpu/mali_kbase_model_linux.h>
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+#include <arbiter/mali_kbase_arbiter_pm.h>
+#endif
+
 #include <mali_kbase.h>
 #include <backend/gpu/mali_kbase_irq_internal.h>
 #include <backend/gpu/mali_kbase_jm_internal.h>
@@ -71,8 +75,8 @@ static int kbase_backend_late_init(struct kbase_device *kbdev)
 
 #ifdef CONFIG_MALI_BIFROST_DEBUG
 #if IS_ENABLED(CONFIG_MALI_REAL_HW)
-	if (kbase_validate_interrupts(kbdev) != 0) {
-		dev_err(kbdev->dev, "Interrupt validation failed.\n");
+	if (kbasep_common_test_interrupt_handlers(kbdev) != 0) {
+		dev_err(kbdev->dev, "Interrupt assignment check failed.\n");
 		err = -EINVAL;
 		goto fail_interrupt_test;
 	}
@@ -212,15 +216,17 @@ static const struct kbase_device_init dev_init[] = {
 #if !IS_ENABLED(CONFIG_MALI_REAL_HW)
 	{ kbase_gpu_device_create, kbase_gpu_device_destroy, "Dummy model initialization failed" },
 #else /* !IS_ENABLED(CONFIG_MALI_REAL_HW) */
-	{ kbase_get_irqs, NULL, "IRQ search failed" },
-	{ registers_map, registers_unmap, "Register map failed" },
+	{ assign_irqs, NULL, "IRQ search failed" },
 #endif /* !IS_ENABLED(CONFIG_MALI_REAL_HW) */
+#if !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
+	{ registers_map, registers_unmap, "Register map failed" },
+#endif /* !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI) */
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
 	{ kbase_gpu_metrics_init, kbase_gpu_metrics_term, "GPU metrics initialization failed" },
 #endif /* IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD) */
-	{ power_control_init, power_control_term, "Power control initialization failed" },
 	{ kbase_device_io_history_init, kbase_device_io_history_term,
 	  "Register access history initialization failed" },
+	{ kbase_device_pm_init, kbase_device_pm_term, "Power management initialization failed" },
 	{ kbase_device_early_init, kbase_device_early_term, "Early device initialization failed" },
 	{ kbase_backend_time_init, NULL, "Time backend initialization failed" },
 	{ kbase_device_misc_init, kbase_device_misc_term,
diff --git a/drivers/gpu/arm/bifrost/device/mali_kbase_device.c b/drivers/gpu/arm/bifrost/device/mali_kbase_device.c
index e5bed33d1129..32cab0d6d212 100644
--- a/drivers/gpu/arm/bifrost/device/mali_kbase_device.c
+++ b/drivers/gpu/arm/bifrost/device/mali_kbase_device.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -51,7 +51,10 @@
 #include "backend/gpu/mali_kbase_irq_internal.h"
 #include "mali_kbase_regs_history_debugfs.h"
 #include "mali_kbase_pbha.h"
+
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 #include "arbiter/mali_kbase_arbiter_pm.h"
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 #if defined(CONFIG_DEBUG_FS) && !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
 
@@ -64,23 +67,7 @@
 
 static DEFINE_MUTEX(kbase_dev_list_lock);
 static LIST_HEAD(kbase_dev_list);
-static unsigned int kbase_dev_nr;
-
-static unsigned int mma_wa_id;
-
-static int set_mma_wa_id(const char *val, const struct kernel_param *kp)
-{
-	return kbase_param_set_uint_minmax(val, kp, 1, 15);
-}
-
-static const struct kernel_param_ops mma_wa_id_ops = {
-	.set = set_mma_wa_id,
-	.get = param_get_uint,
-};
-
-module_param_cb(mma_wa_id, &mma_wa_id_ops, &mma_wa_id, 0444);
-__MODULE_PARM_TYPE(mma_wa_id, "uint");
-MODULE_PARM_DESC(mma_wa_id, "PBHA ID for MMA workaround. Valid range is from 1 to 15.");
+static int kbase_dev_nr;
 
 struct kbase_device *kbase_device_alloc(void)
 {
@@ -96,10 +83,9 @@ struct kbase_device *kbase_device_alloc(void)
  */
 static int kbase_device_all_as_init(struct kbase_device *kbdev)
 {
-	unsigned int i;
-	int err = 0;
+	int i, err = 0;
 
-	for (i = 0; i < (unsigned int)kbdev->nr_hw_address_spaces; i++) {
+	for (i = 0; i < kbdev->nr_hw_address_spaces; i++) {
 		err = kbase_mmu_as_init(kbdev, i);
 		if (err)
 			break;
@@ -115,39 +101,12 @@ static int kbase_device_all_as_init(struct kbase_device *kbdev)
 
 static void kbase_device_all_as_term(struct kbase_device *kbdev)
 {
-	unsigned int i;
+	int i;
 
-	for (i = 0; i < (unsigned int)kbdev->nr_hw_address_spaces; i++)
+	for (i = 0; i < kbdev->nr_hw_address_spaces; i++)
 		kbase_mmu_as_term(kbdev, i);
 }
 
-static int pcm_prioritized_process_cb(struct notifier_block *nb, unsigned long action, void *data)
-{
-#if MALI_USE_CSF
-
-	struct kbase_device *const kbdev =
-		container_of(nb, struct kbase_device, pcm_prioritized_process_nb);
-	struct pcm_prioritized_process_notifier_data *const notifier_data = data;
-	int ret = -EINVAL;
-
-	switch (action) {
-	case ADD_PRIORITIZED_PROCESS:
-		if (kbasep_adjust_prioritized_process(kbdev, true, notifier_data->pid))
-			ret = 0;
-		break;
-	case REMOVE_PRIORITIZED_PROCESS:
-		if (kbasep_adjust_prioritized_process(kbdev, false, notifier_data->pid))
-			ret = 0;
-		break;
-	}
-
-	return ret;
-
-#endif /* MALI_USE_CSF */
-
-	return 0;
-}
-
 int kbase_device_pcm_dev_init(struct kbase_device *const kbdev)
 {
 	int err = 0;
@@ -181,18 +140,6 @@ int kbase_device_pcm_dev_init(struct kbase_device *const kbdev)
 				dev_info(kbdev->dev,
 					 "Priority control manager successfully loaded");
 				kbdev->pcm_dev = pcm_dev;
-
-				kbdev->pcm_prioritized_process_nb = (struct notifier_block){
-					.notifier_call = &pcm_prioritized_process_cb,
-				};
-				if (pcm_dev->ops.pcm_prioritized_process_notifier_register !=
-				    NULL) {
-					if (pcm_dev->ops.pcm_prioritized_process_notifier_register(
-						    pcm_dev, &kbdev->pcm_prioritized_process_nb))
-						dev_warn(
-							kbdev->dev,
-							"Failed to register for changes in prioritized processes");
-				}
 			}
 		}
 		of_node_put(prio_ctrl_node);
@@ -204,14 +151,8 @@ int kbase_device_pcm_dev_init(struct kbase_device *const kbdev)
 
 void kbase_device_pcm_dev_term(struct kbase_device *const kbdev)
 {
-	struct priority_control_manager_device *const pcm_dev = kbdev->pcm_dev;
-
-	if (pcm_dev) {
-		if (pcm_dev->ops.pcm_prioritized_process_notifier_unregister != NULL)
-			pcm_dev->ops.pcm_prioritized_process_notifier_unregister(
-				pcm_dev, &kbdev->pcm_prioritized_process_nb);
-		module_put(pcm_dev->owner);
-	}
+	if (kbdev->pcm_dev)
+		module_put(kbdev->pcm_dev->owner);
 }
 
 #define KBASE_PAGES_TO_KIB(pages) (((unsigned int)pages) << (PAGE_SHIFT - 10))
@@ -327,16 +268,12 @@ int kbase_device_misc_init(struct kbase_device *const kbdev)
 	if (err)
 		goto dma_set_mask_failed;
 
-	kbdev->nr_hw_address_spaces = (s8)kbdev->gpu_props.num_address_spaces;
+	kbdev->nr_hw_address_spaces = kbdev->gpu_props.num_address_spaces;
 
 	err = kbase_device_all_as_init(kbdev);
 	if (err)
 		goto dma_set_mask_failed;
 
-	/* Set mma_wa_id if it has been passed in as a module parameter */
-	if ((kbdev->gpu_props.gpu_id.arch_id >= GPU_ID_ARCH_MAKE(14, 8, 0)) && mma_wa_id != 0)
-		kbdev->mma_wa_id = mma_wa_id;
-
 	err = kbase_pbha_read_dtb(kbdev);
 	if (err)
 		goto term_as;
@@ -354,6 +291,8 @@ int kbase_device_misc_init(struct kbase_device *const kbdev)
 #endif /* !MALI_USE_CSF */
 
 	kbdev->mmu_mode = kbase_mmu_mode_get_aarch64();
+	kbdev->mmu_or_gpu_cache_op_wait_time_ms =
+		kbase_get_timeout_ms(kbdev, MMU_AS_INACTIVE_WAIT_TIMEOUT);
 	mutex_init(&kbdev->kctx_list_lock);
 	INIT_LIST_HEAD(&kbdev->kctx_list);
 
@@ -409,26 +348,22 @@ void kbase_device_misc_term(struct kbase_device *kbdev)
 #if !MALI_USE_CSF
 void kbase_enable_quick_reset(struct kbase_device *kbdev)
 {
-	unsigned long flags;
-
-	spin_lock_irqsave(&kbdev->quick_reset_lock, flags);
+	spin_lock(&kbdev->quick_reset_lock);
 
 	kbdev->quick_reset_enabled = true;
 	kbdev->num_of_atoms_hw_completed = 0;
 
-	spin_unlock_irqrestore(&kbdev->quick_reset_lock, flags);
+	spin_unlock(&kbdev->quick_reset_lock);
 }
 
 void kbase_disable_quick_reset(struct kbase_device *kbdev)
 {
-	unsigned long flags;
-
-	spin_lock_irqsave(&kbdev->quick_reset_lock, flags);
+	spin_lock(&kbdev->quick_reset_lock);
 
 	kbdev->quick_reset_enabled = false;
 	kbdev->num_of_atoms_hw_completed = 0;
 
-	spin_unlock_irqrestore(&kbdev->quick_reset_lock, flags);
+	spin_unlock(&kbdev->quick_reset_lock);
 }
 
 bool kbase_is_quick_reset_enabled(struct kbase_device *kbdev)
@@ -495,6 +430,7 @@ void kbase_device_kinstr_prfcnt_term(struct kbase_device *kbdev)
 	kbase_kinstr_prfcnt_term(kbdev->kinstr_prfcnt_ctx);
 }
 
+#if defined(CONFIG_DEBUG_FS) && !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
 int kbase_device_io_history_init(struct kbase_device *kbdev)
 {
 	return kbase_io_history_init(&kbdev->io_history, KBASEP_DEFAULT_REGISTER_HISTORY_SIZE);
@@ -504,6 +440,7 @@ void kbase_device_io_history_term(struct kbase_device *kbdev)
 {
 	kbase_io_history_term(&kbdev->io_history);
 }
+#endif
 
 int kbase_device_misc_register(struct kbase_device *kbdev)
 {
@@ -573,26 +510,13 @@ int kbase_device_early_init(struct kbase_device *kbdev)
 	/* Ensure we can access the GPU registers */
 	kbase_pm_register_access_enable(kbdev);
 
-	/*
-	 * If -EPERM is returned, it means the device backend is not supported, but
-	 * device initialization can continue.
-	 */
-	err = kbase_device_backend_init(kbdev);
-	if (err != 0 && err != -EPERM)
-		goto pm_runtime_term;
-
-	/*
-	 * Initialize register mapping LUTs. This would have been initialized on HW
-	 * Arbitration but not on PV or non-arbitration devices.
-	 */
-	if (!kbase_reg_is_init(kbdev)) {
-		/* Initialize GPU_ID props */
-		kbase_gpuprops_parse_gpu_id(&kbdev->gpu_props.gpu_id, kbase_reg_get_gpu_id(kbdev));
+	/* Initialize GPU_ID props */
+	kbase_gpuprops_parse_gpu_id(&kbdev->gpu_props.gpu_id, kbase_reg_get_gpu_id(kbdev));
 
-		err = kbase_regmap_init(kbdev);
-		if (err)
-			goto backend_term;
-	}
+	/* Initialize register mapping LUTs */
+	err = kbase_regmap_init(kbdev);
+	if (err)
+		goto pm_runtime_term;
 
 	/* Set the list of features available on the current HW
 	 * (identified by the GPU_ID register)
@@ -602,7 +526,7 @@ int kbase_device_early_init(struct kbase_device *kbdev)
 	/* Find out GPU properties based on the GPU feature registers. */
 	err = kbase_gpuprops_init(kbdev);
 	if (err)
-		goto backend_term;
+		goto regmap_term;
 
 	/* Get the list of workarounds for issues on the current HW
 	 * (identified by the GPU_ID register and impl_tech in THREAD_FEATURES)
@@ -614,12 +538,14 @@ int kbase_device_early_init(struct kbase_device *kbdev)
 	/* We're done accessing the GPU registers for now. */
 	kbase_pm_register_access_disable(kbdev);
 
-	if (kbase_has_arbiter(kbdev)) {
-		if (kbdev->pm.arb_vm_state)
-			err = kbase_arbiter_pm_install_interrupts(kbdev);
-	} else {
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbdev->arb.arb_if)
+		err = kbase_arbiter_pm_install_interrupts(kbdev);
+	else
 		err = kbase_install_interrupts(kbdev);
-	}
+#else
+	err = kbase_install_interrupts(kbdev);
+#endif
 	if (err)
 		goto gpuprops_term;
 
@@ -627,13 +553,9 @@ int kbase_device_early_init(struct kbase_device *kbdev)
 
 gpuprops_term:
 	kbase_gpuprops_term(kbdev);
-backend_term:
-	kbase_device_backend_term(kbdev);
+regmap_term:
 	kbase_regmap_term(kbdev);
 pm_runtime_term:
-	if (kbdev->pm.backend.gpu_powered)
-		kbase_pm_register_access_disable(kbdev);
-
 	kbase_pm_runtime_term(kbdev);
 platform_device_term:
 	kbasep_platform_device_term(kbdev);
@@ -645,13 +567,15 @@ int kbase_device_early_init(struct kbase_device *kbdev)
 
 void kbase_device_early_term(struct kbase_device *kbdev)
 {
-	if (kbase_has_arbiter(kbdev))
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbdev->arb.arb_if)
 		kbase_arbiter_pm_release_interrupts(kbdev);
 	else
 		kbase_release_interrupts(kbdev);
+#else
+	kbase_release_interrupts(kbdev);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 	kbase_gpuprops_term(kbdev);
-	kbase_device_backend_term(kbdev);
-	kbase_regmap_term(kbdev);
 	kbase_pm_runtime_term(kbdev);
 	kbasep_platform_device_term(kbdev);
 	kbase_ktrace_term(kbdev);
diff --git a/drivers/gpu/arm/bifrost/device/mali_kbase_device.h b/drivers/gpu/arm/bifrost/device/mali_kbase_device.h
index 1b15ff059194..9cca6aff4554 100644
--- a/drivers/gpu/arm/bifrost/device/mali_kbase_device.h
+++ b/drivers/gpu/arm/bifrost/device/mali_kbase_device.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -58,9 +58,6 @@ void kbase_increment_device_id(void);
  * When a device file is opened for the first time,
  * load firmware and initialize hardware counter components.
  *
- * It is safe for this function to be called multiple times without ill
- * effects. Only the first call would be effective.
- *
  * Return: 0 on success. An error code on failure.
  */
 int kbase_device_firmware_init_once(struct kbase_device *kbdev);
diff --git a/drivers/gpu/arm/bifrost/device/mali_kbase_device_hw.c b/drivers/gpu/arm/bifrost/device/mali_kbase_device_hw.c
index 91379ac6429d..3b507c4fec63 100644
--- a/drivers/gpu/arm/bifrost/device/mali_kbase_device_hw.c
+++ b/drivers/gpu/arm/bifrost/device/mali_kbase_device_hw.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -27,6 +27,14 @@
 #include <mali_kbase_reset_gpu.h>
 #include <mmu/mali_kbase_mmu.h>
 
+bool kbase_is_gpu_removed(struct kbase_device *kbdev)
+{
+	if (!IS_ENABLED(CONFIG_MALI_ARBITER_SUPPORT))
+		return false;
+
+	return (kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_ID)) == 0);
+}
+
 /**
  * busy_wait_cache_operation - Wait for a pending cache flush to complete
  *
@@ -40,7 +48,7 @@
 static int busy_wait_cache_operation(struct kbase_device *kbdev, u32 irq_bit)
 {
 	const ktime_t wait_loop_start = ktime_get_raw();
-	const u32 wait_time_ms = kbase_get_timeout_ms(kbdev, MMU_AS_INACTIVE_WAIT_TIMEOUT);
+	const u32 wait_time_ms = kbdev->mmu_or_gpu_cache_op_wait_time_ms;
 	bool completed = false;
 	s64 diff;
 	u32 irq_bits_to_check = irq_bit;
@@ -161,7 +169,7 @@ int kbase_gpu_cache_flush_and_busy_wait(struct kbase_device *kbdev, u32 flush_op
 				  irq_mask & ~CLEAN_CACHES_COMPLETED);
 
 		/* busy wait irq status to be enabled */
-		ret = busy_wait_cache_operation(kbdev, CLEAN_CACHES_COMPLETED);
+		ret = busy_wait_cache_operation(kbdev, (u32)CLEAN_CACHES_COMPLETED);
 		if (ret)
 			return ret;
 
@@ -180,7 +188,7 @@ int kbase_gpu_cache_flush_and_busy_wait(struct kbase_device *kbdev, u32 flush_op
 	kbase_reg_write32(kbdev, GPU_CONTROL_ENUM(GPU_COMMAND), flush_op);
 
 	/* 3. Busy-wait irq status to be enabled. */
-	ret = busy_wait_cache_operation(kbdev, CLEAN_CACHES_COMPLETED);
+	ret = busy_wait_cache_operation(kbdev, (u32)CLEAN_CACHES_COMPLETED);
 	if (ret)
 		return ret;
 
@@ -293,7 +301,7 @@ void kbase_gpu_wait_cache_clean(struct kbase_device *kbdev)
 
 int kbase_gpu_wait_cache_clean_timeout(struct kbase_device *kbdev, unsigned int wait_timeout_ms)
 {
-	long remaining = (long)msecs_to_jiffies(wait_timeout_ms);
+	long remaining = msecs_to_jiffies(wait_timeout_ms);
 	int result = 0;
 
 	while (remaining && get_cache_clean_flag(kbdev)) {
diff --git a/drivers/gpu/arm/bifrost/device/mali_kbase_device_internal.h b/drivers/gpu/arm/bifrost/device/mali_kbase_device_internal.h
index c900f57e1ff6..d21a8da329b9 100644
--- a/drivers/gpu/arm/bifrost/device/mali_kbase_device_internal.h
+++ b/drivers/gpu/arm/bifrost/device/mali_kbase_device_internal.h
@@ -59,8 +59,18 @@ void kbase_device_hwcnt_virtualizer_term(struct kbase_device *kbdev);
 int kbase_device_list_init(struct kbase_device *kbdev);
 void kbase_device_list_term(struct kbase_device *kbdev);
 
+#if defined(CONFIG_DEBUG_FS) && !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
 int kbase_device_io_history_init(struct kbase_device *kbdev);
 void kbase_device_io_history_term(struct kbase_device *kbdev);
+#else
+static inline int kbase_device_io_history_init(struct kbase_device *kbdev)
+{
+	return 0;
+}
+static inline void kbase_device_io_history_term(struct kbase_device *kbdev)
+{
+}
+#endif
 
 int kbase_device_misc_register(struct kbase_device *kbdev);
 void kbase_device_misc_deregister(struct kbase_device *kbdev);
diff --git a/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_csf.c b/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_csf.c
index 0ba6b2d0e3d2..60ba9beab91c 100644
--- a/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_csf.c
+++ b/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_csf.c
@@ -146,28 +146,28 @@ const char *kbase_gpu_exception_name(u32 const exception_code)
 		e = "ADDRESS_SIZE_FAULT_IN";
 		break;
 	case CS_FAULT_EXCEPTION_TYPE_ADDRESS_SIZE_FAULT_OUT_0:
-		e = "ADDRESS_SIZE_FAULT_OUT at level 0";
+		e = "ADDRESS_SIZE_FAULT_OUT_0 at level 0";
 		break;
 	case CS_FAULT_EXCEPTION_TYPE_ADDRESS_SIZE_FAULT_OUT_1:
-		e = "ADDRESS_SIZE_FAULT_OUT at level 1";
+		e = "ADDRESS_SIZE_FAULT_OUT_1 at level 1";
 		break;
 	case CS_FAULT_EXCEPTION_TYPE_ADDRESS_SIZE_FAULT_OUT_2:
-		e = "ADDRESS_SIZE_FAULT_OUT at level 2";
+		e = "ADDRESS_SIZE_FAULT_OUT_2 at level 2";
 		break;
 	case CS_FAULT_EXCEPTION_TYPE_ADDRESS_SIZE_FAULT_OUT_3:
-		e = "ADDRESS_SIZE_FAULT_OUT at level 3";
+		e = "ADDRESS_SIZE_FAULT_OUT_3 at level 3";
 		break;
 	case CS_FAULT_EXCEPTION_TYPE_MEMORY_ATTRIBUTE_FAULT_0:
-		e = "MEMORY_ATTRIBUTE_FAULT at level 0";
+		e = "MEMORY_ATTRIBUTE_FAULT_0 at level 0";
 		break;
 	case CS_FAULT_EXCEPTION_TYPE_MEMORY_ATTRIBUTE_FAULT_1:
-		e = "MEMORY_ATTRIBUTE_FAULT at level 1";
+		e = "MEMORY_ATTRIBUTE_FAULT_1 at level 1";
 		break;
 	case CS_FAULT_EXCEPTION_TYPE_MEMORY_ATTRIBUTE_FAULT_2:
-		e = "MEMORY_ATTRIBUTE_FAULT at level 2";
+		e = "MEMORY_ATTRIBUTE_FAULT_2 at level 2";
 		break;
 	case CS_FAULT_EXCEPTION_TYPE_MEMORY_ATTRIBUTE_FAULT_3:
-		e = "MEMORY_ATTRIBUTE_FAULT at level 3";
+		e = "MEMORY_ATTRIBUTE_FAULT_3 at level 3";
 		break;
 	/* Any other exception code is unknown */
 	default:
diff --git a/drivers/gpu/arm/bifrost/hw_access/backend/mali_kbase_hw_access_model_linux.c b/drivers/gpu/arm/bifrost/hw_access/backend/mali_kbase_hw_access_model_linux.c
index 9993b787ed21..ca1ccbfb3dbe 100644
--- a/drivers/gpu/arm/bifrost/hw_access/backend/mali_kbase_hw_access_model_linux.c
+++ b/drivers/gpu/arm/bifrost/hw_access/backend/mali_kbase_hw_access_model_linux.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -46,7 +46,7 @@ u32 kbase_reg_read32(struct kbase_device *kbdev, u32 reg_enum)
 	u32 val = 0;
 	u32 offset;
 
-	if (WARN_ON(!kbase_reg_is_powered_access_allowed(kbdev, reg_enum)))
+	if (WARN_ON(!kbdev->pm.backend.gpu_powered))
 		return 0;
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum,
 					      KBASE_REGMAP_PERM_READ | KBASE_REGMAP_WIDTH_32_BIT)))
@@ -68,7 +68,7 @@ u64 kbase_reg_read64(struct kbase_device *kbdev, u32 reg_enum)
 	u32 val32[2] = { 0 };
 	u32 offset;
 
-	if (WARN_ON(!kbase_reg_is_powered_access_allowed(kbdev, reg_enum)))
+	if (WARN_ON(!kbdev->pm.backend.gpu_powered))
 		return 0;
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum,
 					      KBASE_REGMAP_PERM_READ | KBASE_REGMAP_WIDTH_64_BIT)))
@@ -91,7 +91,7 @@ u64 kbase_reg_read64_coherent(struct kbase_device *kbdev, u32 reg_enum)
 	u32 hi1 = 0, hi2 = 0, lo = 0;
 	u32 offset;
 
-	if (WARN_ON(!kbase_reg_is_powered_access_allowed(kbdev, reg_enum)))
+	if (WARN_ON(!kbdev->pm.backend.gpu_powered))
 		return 0;
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum,
 					      KBASE_REGMAP_PERM_READ | KBASE_REGMAP_WIDTH_64_BIT)))
@@ -116,7 +116,7 @@ void kbase_reg_write32(struct kbase_device *kbdev, u32 reg_enum, u32 value)
 	unsigned long flags;
 	u32 offset;
 
-	if (WARN_ON(!kbase_reg_is_powered_access_allowed(kbdev, reg_enum)))
+	if (WARN_ON(!kbdev->pm.backend.gpu_powered))
 		return;
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum,
 					      KBASE_REGMAP_PERM_WRITE | KBASE_REGMAP_WIDTH_32_BIT)))
@@ -135,7 +135,7 @@ void kbase_reg_write64(struct kbase_device *kbdev, u32 reg_enum, u64 value)
 	unsigned long flags;
 	u32 offset;
 
-	if (WARN_ON(!kbase_reg_is_powered_access_allowed(kbdev, reg_enum)))
+	if (WARN_ON(!kbdev->pm.backend.gpu_powered))
 		return;
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum,
 					      KBASE_REGMAP_PERM_WRITE | KBASE_REGMAP_WIDTH_64_BIT)))
diff --git a/drivers/gpu/arm/bifrost/hw_access/backend/mali_kbase_hw_access_real_hw.c b/drivers/gpu/arm/bifrost/hw_access/backend/mali_kbase_hw_access_real_hw.c
index ecf58cb45d15..f4afbf55e312 100644
--- a/drivers/gpu/arm/bifrost/hw_access/backend/mali_kbase_hw_access_real_hw.c
+++ b/drivers/gpu/arm/bifrost/hw_access/backend/mali_kbase_hw_access_real_hw.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -24,13 +24,12 @@
 
 #include <mali_kbase.h>
 #include <hw_access/mali_kbase_hw_access.h>
-#include <linux/mali_hw_access.h>
 
 u64 kbase_reg_get_gpu_id(struct kbase_device *kbdev)
 {
 	u32 val[2] = { 0 };
 
-	val[0] = mali_readl(kbdev->reg);
+	val[0] = readl(kbdev->reg);
 
 
 	return (u64)val[0] | ((u64)val[1] << 32);
@@ -40,13 +39,13 @@ u32 kbase_reg_read32(struct kbase_device *kbdev, u32 reg_enum)
 {
 	u32 val;
 
-	if (WARN_ON(!kbase_reg_is_powered_access_allowed(kbdev, reg_enum)))
+	if (WARN_ON(!kbdev->pm.backend.gpu_powered))
 		return 0;
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum,
 					      KBASE_REGMAP_PERM_READ | KBASE_REGMAP_WIDTH_32_BIT)))
 		return 0;
 
-	val = mali_readl(kbdev->regmap.regs[reg_enum]);
+	val = readl(kbdev->regmap.regs[reg_enum]);
 
 #if IS_ENABLED(CONFIG_DEBUG_FS)
 	if (unlikely(kbdev->io_history.enabled))
@@ -64,13 +63,14 @@ u64 kbase_reg_read64(struct kbase_device *kbdev, u32 reg_enum)
 {
 	u64 val;
 
-	if (WARN_ON(!kbase_reg_is_powered_access_allowed(kbdev, reg_enum)))
+	if (WARN_ON(!kbdev->pm.backend.gpu_powered))
 		return 0;
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum,
 					      KBASE_REGMAP_PERM_READ | KBASE_REGMAP_WIDTH_64_BIT)))
 		return 0;
 
-	val = mali_readq(kbdev->regmap.regs[reg_enum]);
+	val = (u64)readl(kbdev->regmap.regs[reg_enum]) |
+	      ((u64)readl(kbdev->regmap.regs[reg_enum] + 4) << 32);
 
 #if IS_ENABLED(CONFIG_DEBUG_FS)
 	if (unlikely(kbdev->io_history.enabled)) {
@@ -90,14 +90,23 @@ KBASE_EXPORT_TEST_API(kbase_reg_read64);
 u64 kbase_reg_read64_coherent(struct kbase_device *kbdev, u32 reg_enum)
 {
 	u64 val;
+#if !IS_ENABLED(CONFIG_MALI_64BIT_HW_ACCESS)
+	u32 hi1, hi2, lo;
+#endif
 
-	if (WARN_ON(!kbase_reg_is_powered_access_allowed(kbdev, reg_enum)))
+	if (WARN_ON(!kbdev->pm.backend.gpu_powered))
 		return 0;
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum,
 					      KBASE_REGMAP_PERM_READ | KBASE_REGMAP_WIDTH_64_BIT)))
 		return 0;
 
-	val = mali_readq_coherent(kbdev->regmap.regs[reg_enum]);
+	do {
+		hi1 = readl(kbdev->regmap.regs[reg_enum] + 4);
+		lo = readl(kbdev->regmap.regs[reg_enum]);
+		hi2 = readl(kbdev->regmap.regs[reg_enum] + 4);
+	} while (hi1 != hi2);
+
+	val = lo | (((u64)hi1) << 32);
 
 #if IS_ENABLED(CONFIG_DEBUG_FS)
 	if (unlikely(kbdev->io_history.enabled)) {
@@ -116,13 +125,13 @@ KBASE_EXPORT_TEST_API(kbase_reg_read64_coherent);
 
 void kbase_reg_write32(struct kbase_device *kbdev, u32 reg_enum, u32 value)
 {
-	if (WARN_ON(!kbase_reg_is_powered_access_allowed(kbdev, reg_enum)))
+	if (WARN_ON(!kbdev->pm.backend.gpu_powered))
 		return;
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum,
 					      KBASE_REGMAP_PERM_WRITE | KBASE_REGMAP_WIDTH_32_BIT)))
 		return;
 
-	mali_writel(value, kbdev->regmap.regs[reg_enum]);
+	writel(value, kbdev->regmap.regs[reg_enum]);
 
 #if IS_ENABLED(CONFIG_DEBUG_FS)
 	if (unlikely(kbdev->io_history.enabled))
@@ -136,13 +145,14 @@ KBASE_EXPORT_TEST_API(kbase_reg_write32);
 
 void kbase_reg_write64(struct kbase_device *kbdev, u32 reg_enum, u64 value)
 {
-	if (WARN_ON(!kbase_reg_is_powered_access_allowed(kbdev, reg_enum)))
+	if (WARN_ON(!kbdev->pm.backend.gpu_powered))
 		return;
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum,
 					      KBASE_REGMAP_PERM_WRITE | KBASE_REGMAP_WIDTH_64_BIT)))
 		return;
 
-	mali_writeq(value, kbdev->regmap.regs[reg_enum]);
+	writel(value & 0xFFFFFFFF, kbdev->regmap.regs[reg_enum]);
+	writel(value >> 32, kbdev->regmap.regs[reg_enum] + 4);
 
 #if IS_ENABLED(CONFIG_DEBUG_FS)
 	if (unlikely(kbdev->io_history.enabled)) {
diff --git a/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access.c b/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access.c
index d7dd6200d497..16a27c780d3b 100644
--- a/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access.c
+++ b/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -24,56 +24,9 @@
 
 #include <mali_kbase.h>
 #include "mali_kbase_hw_access.h"
-#include "mali_kbase_hw_access_regmap.h"
 
 #include <uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h>
 
-#define KBASE_REGMAP_ACCESS_ALWAYS_POWERED (1U << 16)
-
-static u32 always_powered_regs[] = {
-
-#if !MALI_USE_CSF
-	PTM_AW_IRQ_CLEAR,
-	PTM_AW_IRQ_INJECTION,
-	PTM_AW_IRQ_MASK,
-	PTM_AW_IRQ_RAWSTAT,
-	PTM_AW_IRQ_STATUS,
-	PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE0,
-	PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE1,
-	PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE0,
-	PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE1,
-	PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE_STATUS,
-	PTM_ID,
-#endif /* !MALI_USE_CSF */
-};
-
-static void kbasep_reg_setup_always_powered_registers(struct kbase_device *kbdev)
-{
-	u32 i;
-
-
-#if !MALI_USE_CSF
-	if (kbdev->gpu_props.gpu_id.arch_id < GPU_ID_ARCH_MAKE(9, 14, 0))
-		return;
-#endif /* MALI_USE_CSF */
-
-	for (i = 0; i < ARRAY_SIZE(always_powered_regs); i++) {
-		u32 reg_enum = always_powered_regs[i];
-
-		if (!kbase_reg_is_valid(kbdev, reg_enum))
-			continue;
-
-		kbdev->regmap.flags[reg_enum] |= KBASE_REGMAP_ACCESS_ALWAYS_POWERED;
-	}
-}
-
-bool kbase_reg_is_powered_access_allowed(struct kbase_device *kbdev, u32 reg_enum)
-{
-	if (kbdev->regmap.flags[reg_enum] & KBASE_REGMAP_ACCESS_ALWAYS_POWERED)
-		return true;
-	return kbdev->pm.backend.gpu_powered;
-}
-
 bool kbase_reg_is_size64(struct kbase_device *kbdev, u32 reg_enum)
 {
 	if (WARN_ON(reg_enum >= kbdev->regmap.size))
@@ -114,11 +67,6 @@ bool kbase_reg_is_accessible(struct kbase_device *kbdev, u32 reg_enum, u32 flags
 	return true;
 }
 
-bool kbase_reg_is_init(struct kbase_device *kbdev)
-{
-	return (kbdev->regmap.regs != NULL) && (kbdev->regmap.flags != NULL);
-}
-
 int kbase_reg_get_offset(struct kbase_device *kbdev, u32 reg_enum, u32 *offset)
 {
 	if (unlikely(!kbase_reg_is_accessible(kbdev, reg_enum, 0)))
@@ -160,12 +108,12 @@ int kbase_regmap_init(struct kbase_device *kbdev)
 		return -ENOMEM;
 	}
 
-	kbasep_reg_setup_always_powered_registers(kbdev);
-
 	dev_info(kbdev->dev, "Register LUT %08x initialized for GPU arch 0x%08x\n", lut_arch_id,
 		 kbdev->gpu_props.gpu_id.arch_id);
 
-
+#if IS_ENABLED(CONFIG_MALI_64BIT_HW_ACCESS) && IS_ENABLED(CONFIG_MALI_REAL_HW)
+	dev_info(kbdev->dev, "64-bit HW access enabled\n");
+#endif
 	return 0;
 }
 
diff --git a/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access.h b/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access.h
index 654fb685fa06..c56c0b67a17f 100644
--- a/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access.h
+++ b/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -22,8 +22,6 @@
 #ifndef _MALI_KBASE_HW_ACCESS_H_
 #define _MALI_KBASE_HW_ACCESS_H_
 
-#include <linux/version_compat_defs.h>
-
 #define KBASE_REGMAP_PERM_READ (1U << 0)
 #define KBASE_REGMAP_PERM_WRITE (1U << 1)
 #define KBASE_REGMAP_WIDTH_32_BIT (1U << 2)
@@ -128,25 +126,6 @@ bool kbase_reg_is_valid(struct kbase_device *kbdev, u32 reg_enum);
  */
 bool kbase_reg_is_accessible(struct kbase_device *kbdev, u32 reg_enum, u32 flags);
 
-/**
- * kbase_reg_is_powered_access_allowed - check if registered is accessible given
- * current power state
- *
- * @kbdev:    Kbase device pointer
- * @reg_enum: Register enum
- *
- * Return: boolean if register is accessible
- */
-bool kbase_reg_is_powered_access_allowed(struct kbase_device *kbdev, u32 reg_enum);
-
-/**
- * kbase_reg_is_init - check if regmap is initialized
- *
- * @kbdev:     Kbase device pointer
- * Return:     boolean if regmap is initialized
- */
-bool kbase_reg_is_init(struct kbase_device *kbdev);
-
 /**
  * kbase_reg_get_offset - get register offset from enum
  * @kbdev:    Kbase device pointer
@@ -207,37 +186,4 @@ u32 kbase_regmap_backend_init(struct kbase_device *kbdev);
  */
 void kbase_regmap_term(struct kbase_device *kbdev);
 
-/**
- * kbase_reg_poll32_timeout - Poll a 32 bit register with timeout
- * @kbdev:             Kbase device pointer
- * @reg_enum:          Register enum
- * @val:               Variable for result of read
- * @cond:              Condition to be met
- * @delay_us:          Delay between each poll (in uS)
- * @timeout_us:        Timeout (in uS)
- * @delay_before_read: If true delay for @delay_us before read
- *
- * Return: 0 if condition is met, -ETIMEDOUT if timed out.
- */
-#define kbase_reg_poll32_timeout(kbdev, reg_enum, val, cond, delay_us, timeout_us,  \
-				 delay_before_read)                                 \
-	read_poll_timeout_atomic(kbase_reg_read32, val, cond, delay_us, timeout_us, \
-				 delay_before_read, kbdev, reg_enum)
-
-/**
- * kbase_reg_poll64_timeout - Poll a 64 bit register with timeout
- * @kbdev:             Kbase device pointer
- * @reg_enum:          Register enum
- * @val:               Variable for result of read
- * @cond:              Condition to be met
- * @delay_us:          Delay between each poll (in uS)
- * @timeout_us:        Timeout (in uS)
- * @delay_before_read: If true delay for @delay_us before read
- *
- * Return: 0 if condition is met, -ETIMEDOUT if timed out.
- */
-#define kbase_reg_poll64_timeout(kbdev, reg_enum, val, cond, delay_us, timeout_us,  \
-				 delay_before_read)                                 \
-	read_poll_timeout_atomic(kbase_reg_read64, val, cond, delay_us, timeout_us, \
-				 delay_before_read, kbdev, reg_enum)
 #endif /* _MALI_KBASE_HW_ACCESS_H_ */
diff --git a/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access_regmap.h b/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access_regmap.h
index 591391c6a8a1..ead7ac8dfb30 100644
--- a/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access_regmap.h
+++ b/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access_regmap.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -290,10 +290,10 @@
 /* THREAD_* registers */
 
 /* THREAD_FEATURES IMPLEMENTATION_TECHNOLOGY values */
-#define THREAD_FEATURES_IMPLEMENTATION_TECHNOLOGY_NOT_SPECIFIED 0U
-#define THREAD_FEATURES_IMPLEMENTATION_TECHNOLOGY_SILICON 1U
-#define THREAD_FEATURES_IMPLEMENTATION_TECHNOLOGY_FPGA 2U
-#define THREAD_FEATURES_IMPLEMENTATION_TECHNOLOGY_SOFTWARE 3U
+#define THREAD_FEATURES_IMPLEMENTATION_TECHNOLOGY_NOT_SPECIFIED 0
+#define THREAD_FEATURES_IMPLEMENTATION_TECHNOLOGY_SILICON 1
+#define THREAD_FEATURES_IMPLEMENTATION_TECHNOLOGY_FPGA 2
+#define THREAD_FEATURES_IMPLEMENTATION_TECHNOLOGY_SOFTWARE 3
 
 /* End THREAD_* registers */
 
@@ -308,16 +308,6 @@
 #define TC_CLOCK_GATE_OVERRIDE (1ul << 0)
 /* End TILER_CONFIG register */
 
-/* L2_FEATURES register */
-#define L2_FEATURES_CACHE_SIZE_SHIFT GPU_U(16)
-#define L2_FEATURES_CACHE_SIZE_MASK (GPU_U(0xFF) << L2_FEATURES_CACHE_SIZE_SHIFT)
-#define L2_FEATURES_CACHE_SIZE_GET(reg_val) \
-	(((reg_val)&L2_FEATURES_CACHE_SIZE_MASK) >> L2_FEATURES_CACHE_SIZE_SHIFT)
-#define L2_FEATURES_CACHE_SIZE_SET(reg_val, value)     \
-	(~(~(reg_val) | L2_FEATURES_CACHE_SIZE_MASK) | \
-	 (((value) << L2_FEATURES_CACHE_SIZE_SHIFT) & L2_FEATURES_CACHE_SIZE_MASK))
-/* End L2_FEATURES register */
-
 /* L2_CONFIG register */
 #define L2_CONFIG_SIZE_SHIFT 16
 #define L2_CONFIG_SIZE_MASK (0xFFul << L2_CONFIG_SIZE_SHIFT)
@@ -440,7 +430,7 @@
 
 /* SYSC_ALLOC register */
 #define SYSC_ALLOC_R_SYSC_ALLOC0_SHIFT (0)
-#define SYSC_ALLOC_R_SYSC_ALLOC0_MASK ((0xFU) << SYSC_ALLOC_R_SYSC_ALLOC0_SHIFT)
+#define SYSC_ALLOC_R_SYSC_ALLOC0_MASK ((0xF) << SYSC_ALLOC_R_SYSC_ALLOC0_SHIFT)
 #define SYSC_ALLOC_R_SYSC_ALLOC0_GET(reg_val) \
 	(((reg_val)&SYSC_ALLOC_R_SYSC_ALLOC0_MASK) >> SYSC_ALLOC_R_SYSC_ALLOC0_SHIFT)
 #define SYSC_ALLOC_R_SYSC_ALLOC0_SET(reg_val, value)    \
@@ -448,7 +438,7 @@
 	 (((value) << SYSC_ALLOC_R_SYSC_ALLOC0_SHIFT) & SYSC_ALLOC_R_SYSC_ALLOC0_MASK))
 /* End of SYSC_ALLOC_R_SYSC_ALLOC0 values */
 #define SYSC_ALLOC_W_SYSC_ALLOC0_SHIFT (4)
-#define SYSC_ALLOC_W_SYSC_ALLOC0_MASK ((0xFU) << SYSC_ALLOC_W_SYSC_ALLOC0_SHIFT)
+#define SYSC_ALLOC_W_SYSC_ALLOC0_MASK ((0xF) << SYSC_ALLOC_W_SYSC_ALLOC0_SHIFT)
 #define SYSC_ALLOC_W_SYSC_ALLOC0_GET(reg_val) \
 	(((reg_val)&SYSC_ALLOC_W_SYSC_ALLOC0_MASK) >> SYSC_ALLOC_W_SYSC_ALLOC0_SHIFT)
 #define SYSC_ALLOC_W_SYSC_ALLOC0_SET(reg_val, value)    \
@@ -456,7 +446,7 @@
 	 (((value) << SYSC_ALLOC_W_SYSC_ALLOC0_SHIFT) & SYSC_ALLOC_W_SYSC_ALLOC0_MASK))
 /* End of SYSC_ALLOC_W_SYSC_ALLOC0 values */
 #define SYSC_ALLOC_R_SYSC_ALLOC1_SHIFT (8)
-#define SYSC_ALLOC_R_SYSC_ALLOC1_MASK ((0xFU) << SYSC_ALLOC_R_SYSC_ALLOC1_SHIFT)
+#define SYSC_ALLOC_R_SYSC_ALLOC1_MASK ((0xF) << SYSC_ALLOC_R_SYSC_ALLOC1_SHIFT)
 #define SYSC_ALLOC_R_SYSC_ALLOC1_GET(reg_val) \
 	(((reg_val)&SYSC_ALLOC_R_SYSC_ALLOC1_MASK) >> SYSC_ALLOC_R_SYSC_ALLOC1_SHIFT)
 #define SYSC_ALLOC_R_SYSC_ALLOC1_SET(reg_val, value)    \
@@ -464,7 +454,7 @@
 	 (((value) << SYSC_ALLOC_R_SYSC_ALLOC1_SHIFT) & SYSC_ALLOC_R_SYSC_ALLOC1_MASK))
 /* End of SYSC_ALLOC_R_SYSC_ALLOC1 values */
 #define SYSC_ALLOC_W_SYSC_ALLOC1_SHIFT (12)
-#define SYSC_ALLOC_W_SYSC_ALLOC1_MASK ((0xFU) << SYSC_ALLOC_W_SYSC_ALLOC1_SHIFT)
+#define SYSC_ALLOC_W_SYSC_ALLOC1_MASK ((0xF) << SYSC_ALLOC_W_SYSC_ALLOC1_SHIFT)
 #define SYSC_ALLOC_W_SYSC_ALLOC1_GET(reg_val) \
 	(((reg_val)&SYSC_ALLOC_W_SYSC_ALLOC1_MASK) >> SYSC_ALLOC_W_SYSC_ALLOC1_SHIFT)
 #define SYSC_ALLOC_W_SYSC_ALLOC1_SET(reg_val, value)    \
@@ -472,7 +462,7 @@
 	 (((value) << SYSC_ALLOC_W_SYSC_ALLOC1_SHIFT) & SYSC_ALLOC_W_SYSC_ALLOC1_MASK))
 /* End of SYSC_ALLOC_W_SYSC_ALLOC1 values */
 #define SYSC_ALLOC_R_SYSC_ALLOC2_SHIFT (16)
-#define SYSC_ALLOC_R_SYSC_ALLOC2_MASK ((0xFU) << SYSC_ALLOC_R_SYSC_ALLOC2_SHIFT)
+#define SYSC_ALLOC_R_SYSC_ALLOC2_MASK ((0xF) << SYSC_ALLOC_R_SYSC_ALLOC2_SHIFT)
 #define SYSC_ALLOC_R_SYSC_ALLOC2_GET(reg_val) \
 	(((reg_val)&SYSC_ALLOC_R_SYSC_ALLOC2_MASK) >> SYSC_ALLOC_R_SYSC_ALLOC2_SHIFT)
 #define SYSC_ALLOC_R_SYSC_ALLOC2_SET(reg_val, value)    \
@@ -480,7 +470,7 @@
 	 (((value) << SYSC_ALLOC_R_SYSC_ALLOC2_SHIFT) & SYSC_ALLOC_R_SYSC_ALLOC2_MASK))
 /* End of SYSC_ALLOC_R_SYSC_ALLOC2 values */
 #define SYSC_ALLOC_W_SYSC_ALLOC2_SHIFT (20)
-#define SYSC_ALLOC_W_SYSC_ALLOC2_MASK ((0xFU) << SYSC_ALLOC_W_SYSC_ALLOC2_SHIFT)
+#define SYSC_ALLOC_W_SYSC_ALLOC2_MASK ((0xF) << SYSC_ALLOC_W_SYSC_ALLOC2_SHIFT)
 #define SYSC_ALLOC_W_SYSC_ALLOC2_GET(reg_val) \
 	(((reg_val)&SYSC_ALLOC_W_SYSC_ALLOC2_MASK) >> SYSC_ALLOC_W_SYSC_ALLOC2_SHIFT)
 #define SYSC_ALLOC_W_SYSC_ALLOC2_SET(reg_val, value)    \
@@ -488,7 +478,7 @@
 	 (((value) << SYSC_ALLOC_W_SYSC_ALLOC2_SHIFT) & SYSC_ALLOC_W_SYSC_ALLOC2_MASK))
 /* End of SYSC_ALLOC_W_SYSC_ALLOC2 values */
 #define SYSC_ALLOC_R_SYSC_ALLOC3_SHIFT (24)
-#define SYSC_ALLOC_R_SYSC_ALLOC3_MASK ((0xFU) << SYSC_ALLOC_R_SYSC_ALLOC3_SHIFT)
+#define SYSC_ALLOC_R_SYSC_ALLOC3_MASK ((0xF) << SYSC_ALLOC_R_SYSC_ALLOC3_SHIFT)
 #define SYSC_ALLOC_R_SYSC_ALLOC3_GET(reg_val) \
 	(((reg_val)&SYSC_ALLOC_R_SYSC_ALLOC3_MASK) >> SYSC_ALLOC_R_SYSC_ALLOC3_SHIFT)
 #define SYSC_ALLOC_R_SYSC_ALLOC3_SET(reg_val, value)    \
@@ -496,7 +486,7 @@
 	 (((value) << SYSC_ALLOC_R_SYSC_ALLOC3_SHIFT) & SYSC_ALLOC_R_SYSC_ALLOC3_MASK))
 /* End of SYSC_ALLOC_R_SYSC_ALLOC3 values */
 #define SYSC_ALLOC_W_SYSC_ALLOC3_SHIFT (28)
-#define SYSC_ALLOC_W_SYSC_ALLOC3_MASK ((0xFU) << SYSC_ALLOC_W_SYSC_ALLOC3_SHIFT)
+#define SYSC_ALLOC_W_SYSC_ALLOC3_MASK ((0xF) << SYSC_ALLOC_W_SYSC_ALLOC3_SHIFT)
 #define SYSC_ALLOC_W_SYSC_ALLOC3_GET(reg_val) \
 	(((reg_val)&SYSC_ALLOC_W_SYSC_ALLOC3_MASK) >> SYSC_ALLOC_W_SYSC_ALLOC3_SHIFT)
 #define SYSC_ALLOC_W_SYSC_ALLOC3_SET(reg_val, value)    \
diff --git a/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access_regmap_legacy.h b/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access_regmap_legacy.h
index 9392d44f684b..a62d1707ebb7 100644
--- a/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access_regmap_legacy.h
+++ b/drivers/gpu/arm/bifrost/hw_access/mali_kbase_hw_access_regmap_legacy.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_csf_macros.h b/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_csf_macros.h
index e0568d8f8c6d..8f6164ee23bf 100644
--- a/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_csf_macros.h
+++ b/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_csf_macros.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -69,7 +69,6 @@
 	ENUM_OFFSET(n, DOORBELL_BLOCK_ENUM(0, regname), DOORBELL_BLOCK_ENUM(1, regname))
 
 /* register value macros */
-
 /* L2_CONFIG PBHA values */
 #define L2_CONFIG_PBHA_HWU_SHIFT GPU_U(12)
 #define L2_CONFIG_PBHA_HWU_MASK (GPU_U(0xF) << L2_CONFIG_PBHA_HWU_SHIFT)
@@ -80,8 +79,7 @@
 	 (((value) << L2_CONFIG_PBHA_HWU_SHIFT) & L2_CONFIG_PBHA_HWU_MASK))
 
 #define PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_SHIFT (0)
-#define PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_MASK \
-	((0xFFU) << PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_SHIFT)
+#define PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_MASK ((0xFF) << PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_SHIFT)
 #define PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_GET(reg_val)         \
 	(((reg_val)&PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_MASK) >> \
 	 PRFCNT_FEATURES_COUNTER_BLOCK_SIZE_SHIFT)
@@ -185,7 +183,6 @@
  */
 #define AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SHARED 0x0
 
-
 /* CSF_CONFIG register */
 #define CSF_CONFIG_FORCE_COHERENCY_FEATURES_SHIFT 2
 
@@ -404,17 +401,16 @@
 	 (((value) << GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT) & GPU_FAULTSTATUS_ADDRESS_VALID_MASK))
 
 /* GPU IRQ flags */
-#define GPU_FAULT (1U << 0) /* A GPU Fault has occurred */
-#define GPU_PROTECTED_FAULT (1U << 1) /* A GPU fault has occurred in protected mode */
-#define RESET_COMPLETED (1U << 8) /* Set when a reset has completed.  */
-#define POWER_CHANGED_SINGLE \
-	(1U << 9) /* Set when a single core has finished powering up or down. */
-#define POWER_CHANGED_ALL (1U << 10) /* Set when all cores have finished powering up or down. */
-#define CLEAN_CACHES_COMPLETED (1U << 17) /* Set when a cache clean operation has completed. */
-#define DOORBELL_MIRROR (1U << 18) /* Mirrors the doorbell interrupt line to the CPU */
-#define MCU_STATUS_GPU_IRQ (1U << 19) /* MCU requires attention */
+#define GPU_FAULT (1 << 0) /* A GPU Fault has occurred */
+#define GPU_PROTECTED_FAULT (1 << 1) /* A GPU fault has occurred in protected mode */
+#define RESET_COMPLETED (1 << 8) /* Set when a reset has completed.  */
+#define POWER_CHANGED_SINGLE (1 << 9) /* Set when a single core has finished powering up or down. */
+#define POWER_CHANGED_ALL (1 << 10) /* Set when all cores have finished powering up or down. */
+#define CLEAN_CACHES_COMPLETED (1 << 17) /* Set when a cache clean operation has completed. */
+#define DOORBELL_MIRROR (1 << 18) /* Mirrors the doorbell interrupt line to the CPU */
+#define MCU_STATUS_GPU_IRQ (1 << 19) /* MCU requires attention */
 #define FLUSH_PA_RANGE_COMPLETED \
-	(1U << 20) /* Set when a physical range cache clean operation has completed. */
+	(1 << 20) /* Set when a physical range cache clean operation has completed. */
 
 
 /* GPU_FEATURES register */
diff --git a/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm.c b/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm.c
index 4f41693ff3c2..178d45501916 100644
--- a/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm.c
+++ b/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -2240,56 +2240,6 @@ static void kbase_regmap_v9_2_init(struct kbase_device *kbdev)
 	kbdev->regmap.regs[GPU_CONTROL__L2_CONFIG] = kbdev->reg + 0x48;
 }
 
-static void kbase_regmap_v9_14_init(struct kbase_device *kbdev)
-{
-	if (kbdev->regmap.regs == NULL && kbdev->regmap.flags == NULL) {
-		kbdev->regmap.size = NR_V9_14_REGS;
-		kbdev->regmap.regs =
-			kcalloc(kbdev->regmap.size, sizeof(void __iomem *), GFP_KERNEL);
-		kbdev->regmap.flags = kcalloc(kbdev->regmap.size, sizeof(u32), GFP_KERNEL);
-	}
-
-	if (WARN_ON(kbdev->regmap.regs == NULL))
-		return;
-	if (WARN_ON(kbdev->regmap.flags == NULL))
-		return;
-
-	kbase_regmap_v9_2_init(kbdev);
-
-	kbdev->regmap.flags[PTM_AW_IRQ_CLEAR] = KBASE_REGMAP_WIDTH_32_BIT | KBASE_REGMAP_PERM_READ |
-						KBASE_REGMAP_PERM_WRITE;
-	kbdev->regmap.flags[PTM_AW_IRQ_INJECTION] =
-		KBASE_REGMAP_WIDTH_32_BIT | KBASE_REGMAP_PERM_READ | KBASE_REGMAP_PERM_WRITE;
-	kbdev->regmap.flags[PTM_AW_IRQ_MASK] = KBASE_REGMAP_WIDTH_32_BIT | KBASE_REGMAP_PERM_READ |
-					       KBASE_REGMAP_PERM_WRITE;
-	kbdev->regmap.flags[PTM_AW_IRQ_RAWSTAT] = KBASE_REGMAP_WIDTH_32_BIT |
-						  KBASE_REGMAP_PERM_READ;
-	kbdev->regmap.flags[PTM_AW_IRQ_STATUS] = KBASE_REGMAP_WIDTH_32_BIT | KBASE_REGMAP_PERM_READ;
-	kbdev->regmap.flags[PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE0] = KBASE_REGMAP_WIDTH_32_BIT |
-								     KBASE_REGMAP_PERM_READ;
-	kbdev->regmap.flags[PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE1] = KBASE_REGMAP_WIDTH_32_BIT |
-								     KBASE_REGMAP_PERM_READ;
-	kbdev->regmap.flags[PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE0] =
-		KBASE_REGMAP_WIDTH_32_BIT | KBASE_REGMAP_PERM_READ | KBASE_REGMAP_PERM_WRITE;
-	kbdev->regmap.flags[PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE1] =
-		KBASE_REGMAP_WIDTH_32_BIT | KBASE_REGMAP_PERM_READ | KBASE_REGMAP_PERM_WRITE;
-	kbdev->regmap.flags[PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE_STATUS] =
-		KBASE_REGMAP_WIDTH_32_BIT | KBASE_REGMAP_PERM_READ;
-	kbdev->regmap.flags[PTM_ID] = KBASE_REGMAP_WIDTH_32_BIT | KBASE_REGMAP_PERM_READ;
-
-	kbdev->regmap.regs[PTM_AW_IRQ_CLEAR] = kbdev->reg + 0x1ffc8;
-	kbdev->regmap.regs[PTM_AW_IRQ_INJECTION] = kbdev->reg + 0x1ffd4;
-	kbdev->regmap.regs[PTM_AW_IRQ_MASK] = kbdev->reg + 0x1ffcc;
-	kbdev->regmap.regs[PTM_AW_IRQ_RAWSTAT] = kbdev->reg + 0x1ffc4;
-	kbdev->regmap.regs[PTM_AW_IRQ_STATUS] = kbdev->reg + 0x1ffd0;
-	kbdev->regmap.regs[PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE0] = kbdev->reg + 0x1ffd8;
-	kbdev->regmap.regs[PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE1] = kbdev->reg + 0x1ffdc;
-	kbdev->regmap.regs[PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE0] = kbdev->reg + 0x1ffe4;
-	kbdev->regmap.regs[PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE1] = kbdev->reg + 0x1ffe8;
-	kbdev->regmap.regs[PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE_STATUS] = kbdev->reg + 0x1ffe0;
-	kbdev->regmap.regs[PTM_ID] = kbdev->reg + 0x1ffc0;
-}
-
 u32 kbase_regmap_backend_init(struct kbase_device *kbdev)
 {
 	int i = 0;
@@ -2304,7 +2254,6 @@ u32 kbase_regmap_backend_init(struct kbase_device *kbdev)
 		{ GPU_ID_ARCH_MAKE(7, 2, 0), kbase_regmap_v7_2_init },
 		{ GPU_ID_ARCH_MAKE(9, 0, 0), kbase_regmap_v9_0_init },
 		{ GPU_ID_ARCH_MAKE(9, 2, 0), kbase_regmap_v9_2_init },
-		{ GPU_ID_ARCH_MAKE(9, 14, 0), kbase_regmap_v9_14_init },
 	};
 
 	for (i = 0; i < ARRAY_SIZE(init_array) - 1; i++) {
@@ -3018,18 +2967,6 @@ static char *enum_strings[] = {
 	[GPU_CONTROL__CORE_FEATURES] = "GPU_CONTROL__CORE_FEATURES",
 	[GPU_CONTROL__THREAD_TLS_ALLOC] = "GPU_CONTROL__THREAD_TLS_ALLOC",
 	[GPU_CONTROL__L2_CONFIG] = "GPU_CONTROL__L2_CONFIG",
-	[PTM_AW_IRQ_CLEAR] = "PTM_AW_IRQ_CLEAR",
-	[PTM_AW_IRQ_INJECTION] = "PTM_AW_IRQ_INJECTION",
-	[PTM_AW_IRQ_MASK] = "PTM_AW_IRQ_MASK",
-	[PTM_AW_IRQ_RAWSTAT] = "PTM_AW_IRQ_RAWSTAT",
-	[PTM_AW_IRQ_STATUS] = "PTM_AW_IRQ_STATUS",
-	[PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE0] = "PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE0",
-	[PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE1] = "PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE1",
-	[PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE0] = "PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE0",
-	[PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE1] = "PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE1",
-	[PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE_STATUS] =
-		"PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE_STATUS",
-	[PTM_ID] = "PTM_ID",
 };
 
 const char *kbase_reg_get_enum_string(u32 reg_enum)
diff --git a/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm_enums.h b/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm_enums.h
index 59d8745eaf4a..f5618c4794db 100644
--- a/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm_enums.h
+++ b/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm_enums.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -759,19 +759,4 @@ enum kbase_regmap_enum_v9_2 {
 	NR_V9_2_REGS,
 };
 
-enum kbase_regmap_enum_v9_14 {
-	PTM_AW_IRQ_CLEAR = NR_V9_2_REGS, /* (RW) 32-bit 0x1FFC8 */
-	PTM_AW_IRQ_INJECTION, /* (RW) 32-bit 0x1FFD4 */
-	PTM_AW_IRQ_MASK, /* (RW) 32-bit 0x1FFCC */
-	PTM_AW_IRQ_RAWSTAT, /* (RO) 32-bit 0x1FFC4 */
-	PTM_AW_IRQ_STATUS, /* (RO) 32-bit 0x1FFD0 */
-	PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE0, /* (RO) 32-bit 0x1FFD8 */
-	PTM_AW_MESSAGE__PTM_INCOMING_MESSAGE1, /* (RO) 32-bit 0x1FFDC */
-	PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE0, /* (RW) 32-bit 0x1FFE4 */
-	PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE1, /* (RW) 32-bit 0x1FFE8 */
-	PTM_AW_MESSAGE__PTM_OUTGOING_MESSAGE_STATUS, /* (RO) 32-bit 0x1FFE0 */
-	PTM_ID, /* (RO) 32-bit 0x1FFC0 */
-	NR_V9_14_REGS,
-};
-
 #endif /* _MALI_KBASE_REGMAP_JM_ENUMS_H_ */
diff --git a/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm_macros.h b/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm_macros.h
index 650ed9b31eea..1cdd215735eb 100644
--- a/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm_macros.h
+++ b/drivers/gpu/arm/bifrost/hw_access/regmap/mali_kbase_regmap_jm_macros.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -47,8 +47,6 @@
 #define MMU_AS_OFFSET(n, regname) ENUM_OFFSET(n, MMU_AS_ENUM(0, regname), MMU_AS_ENUM(1, regname))
 #define MMU_AS_BASE_OFFSET(n) MMU_AS_OFFSET(n, TRANSTAB)
 
-#define PTM_AW_MESSAGE_ENUM(regname) PTM_AW_MESSAGE__##regname
-
 /* register value macros */
 /* GPU_STATUS values */
 #define GPU_STATUS_PRFCNT_ACTIVE (1 << 2) /* Set if the performance counters are active. */
@@ -271,14 +269,13 @@
 #define GPU_COMMAND_FLUSH_CACHE_MERGE(cmd1, cmd2) ((cmd1) > (cmd2) ? (cmd1) : (cmd2))
 
 /* IRQ flags */
-#define GPU_FAULT (1U << 0) /* A GPU Fault has occurred */
-#define MULTIPLE_GPU_FAULTS (1U << 7) /* More than one GPU Fault occurred.  */
-#define RESET_COMPLETED (1U << 8) /* Set when a reset has completed.  */
-#define POWER_CHANGED_SINGLE \
-	(1U << 9) /* Set when a single core has finished powering up or down. */
-#define POWER_CHANGED_ALL (1U << 10) /* Set when all cores have finished powering up or down. */
-#define PRFCNT_SAMPLE_COMPLETED (1U << 16) /* Set when a performance count sample has completed. */
-#define CLEAN_CACHES_COMPLETED (1U << 17) /* Set when a cache clean operation has completed. */
+#define GPU_FAULT (1 << 0) /* A GPU Fault has occurred */
+#define MULTIPLE_GPU_FAULTS (1 << 7) /* More than one GPU Fault occurred.  */
+#define RESET_COMPLETED (1 << 8) /* Set when a reset has completed.  */
+#define POWER_CHANGED_SINGLE (1 << 9) /* Set when a single core has finished powering up or down. */
+#define POWER_CHANGED_ALL (1 << 10) /* Set when all cores have finished powering up or down. */
+#define PRFCNT_SAMPLE_COMPLETED (1 << 16) /* Set when a performance count sample has completed. */
+#define CLEAN_CACHES_COMPLETED (1 << 17) /* Set when a cache clean operation has completed. */
 #define FLUSH_PA_RANGE_COMPLETED \
 	(1 << 20) /* Set when a physical range cache clean operation has completed. */
 
@@ -297,11 +294,4 @@
 	(GPU_FAULT | MULTIPLE_GPU_FAULTS | RESET_COMPLETED | POWER_CHANGED_ALL | \
 	 PRFCNT_SAMPLE_COMPLETED)
 
-#define WINDOW_IRQ_MESSAGE (1U << 0)
-#define WINDOW_IRQ_INVALID_ACCESS (1U << 1)
-#define WINDOW_IRQ_GPU (1U << 2)
-#define WINDOW_IRQ_JOB (1U << 3)
-#define WINDOW_IRQ_MMU (1U << 4)
-#define WINDOW_IRQ_EVENT (1U << 5)
-
 #endif /* _MALI_KBASE_REGMAP_JM_MACROS_H_ */
diff --git a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend.h b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend.h
index a6d418b8e82c..cc3ba98ab6fe 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend.h
+++ b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -78,18 +78,6 @@ typedef int kbase_hwcnt_backend_init_fn(const struct kbase_hwcnt_backend_info *i
  */
 typedef void kbase_hwcnt_backend_term_fn(struct kbase_hwcnt_backend *backend);
 
-/**
- * typedef kbase_hwcnt_backend_acquire_fn - Enable counter collection.
- * @backend: Non-NULL pointer to backend interface.
- */
-typedef void kbase_hwcnt_backend_acquire_fn(const struct kbase_hwcnt_backend *backend);
-
-/**
- * typedef kbase_hwcnt_backend_release_fn - Disable counter collection.
- * @backend: Non-NULL pointer to backend interface.
- */
-typedef void kbase_hwcnt_backend_release_fn(const struct kbase_hwcnt_backend *backend);
-
 /**
  * typedef kbase_hwcnt_backend_timestamp_ns_fn - Get the current backend
  *                                               timestamp.
@@ -218,10 +206,6 @@ typedef int kbase_hwcnt_backend_dump_get_fn(struct kbase_hwcnt_backend *backend,
  *                      metadata.
  * @init:               Function ptr to initialise an instance of the backend.
  * @term:               Function ptr to terminate an instance of the backend.
- * @acquire:            Callback to indicate that counter collection has
- *                      been enabled.
- * @release:            Callback to indicate that counter collection has
- *                      been disabled.
  * @timestamp_ns:       Function ptr to get the current backend timestamp.
  * @dump_enable:        Function ptr to enable dumping.
  * @dump_enable_nolock: Function ptr to enable dumping while the
@@ -238,8 +222,6 @@ struct kbase_hwcnt_backend_interface {
 	kbase_hwcnt_backend_metadata_fn *metadata;
 	kbase_hwcnt_backend_init_fn *init;
 	kbase_hwcnt_backend_term_fn *term;
-	kbase_hwcnt_backend_acquire_fn *acquire;
-	kbase_hwcnt_backend_release_fn *release;
 	kbase_hwcnt_backend_timestamp_ns_fn *timestamp_ns;
 	kbase_hwcnt_backend_dump_enable_fn *dump_enable;
 	kbase_hwcnt_backend_dump_enable_nolock_fn *dump_enable_nolock;
diff --git a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf.c b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf.c
index b937c047a94a..f23a5aacdbfd 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf.c
+++ b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2021-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2021-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -21,6 +21,7 @@
 
 #include "hwcnt/backend/mali_kbase_hwcnt_backend_csf.h"
 #include "hwcnt/mali_kbase_hwcnt_gpu.h"
+#include "hwcnt/mali_kbase_hwcnt_types.h"
 
 #include <linux/log2.h>
 #include <linux/kernel.h>
@@ -30,7 +31,6 @@
 #include <linux/wait.h>
 #include <linux/workqueue.h>
 #include <linux/completion.h>
-#include <linux/version_compat_defs.h>
 
 #ifndef BASE_MAX_NR_CLOCKS_REGULATORS
 #define BASE_MAX_NR_CLOCKS_REGULATORS 4
@@ -44,9 +44,6 @@
 #define HWCNT_BACKEND_WATCHDOG_TIMER_INTERVAL_MS ((u32)1000)
 #endif /* IS_FPGA && !NO_MALI */
 
-/* Used to check for a sample in which all counters in the block are disabled */
-#define HWCNT_BLOCK_EMPTY_SAMPLE (2)
-
 /**
  * enum kbase_hwcnt_backend_csf_dump_state - HWC CSF backend dumping states.
  *
@@ -255,8 +252,7 @@ struct kbase_hwcnt_csf_physical_layout {
  * @hwc_threshold_work:         Worker for consuming available samples when
  *                              threshold interrupt raised.
  * @num_l2_slices:              Current number of L2 slices allocated to the GPU.
- * @powered_shader_core_mask:   The common mask between the debug_core_mask
- *                              and the shader_present_bitmap.
+ * @shader_present_bitmap:      Current shader-present bitmap that is allocated to the GPU.
  */
 struct kbase_hwcnt_backend_csf {
 	struct kbase_hwcnt_backend_csf_info *info;
@@ -284,7 +280,7 @@ struct kbase_hwcnt_backend_csf {
 	struct work_struct hwc_dump_work;
 	struct work_struct hwc_threshold_work;
 	size_t num_l2_slices;
-	u64 powered_shader_core_mask;
+	u64 shader_present_bitmap;
 };
 
 static bool kbasep_hwcnt_backend_csf_backend_exists(struct kbase_hwcnt_backend_csf_info *csf_info)
@@ -297,11 +293,9 @@ static bool kbasep_hwcnt_backend_csf_backend_exists(struct kbase_hwcnt_backend_c
 }
 
 void kbase_hwcnt_backend_csf_set_hw_availability(struct kbase_hwcnt_backend_interface *iface,
-						 size_t num_l2_slices, u64 shader_present,
-						 u64 power_core_mask)
+						 size_t num_l2_slices, u64 shader_present_bitmap)
 {
 	struct kbase_hwcnt_backend_csf_info *csf_info;
-	u64 norm_shader_present = power_core_mask & shader_present;
 
 	if (!iface)
 		return;
@@ -312,17 +306,16 @@ void kbase_hwcnt_backend_csf_set_hw_availability(struct kbase_hwcnt_backend_inte
 	if (!csf_info || !csf_info->backend)
 		return;
 
-
 	if (WARN_ON(csf_info->backend->enable_state != KBASE_HWCNT_BACKEND_CSF_DISABLED))
 		return;
 
 	if (WARN_ON(num_l2_slices > csf_info->backend->phys_layout.mmu_l2_cnt) ||
-	    WARN_ON((norm_shader_present & csf_info->backend->phys_layout.shader_avail_mask) !=
-		    norm_shader_present))
+	    WARN_ON((shader_present_bitmap & csf_info->backend->phys_layout.shader_avail_mask) !=
+		    shader_present_bitmap))
 		return;
 
 	csf_info->backend->num_l2_slices = num_l2_slices;
-	csf_info->backend->powered_shader_core_mask = norm_shader_present;
+	csf_info->backend->shader_present_bitmap = shader_present_bitmap;
 }
 
 /**
@@ -346,7 +339,8 @@ kbasep_hwcnt_backend_csf_cc_initial_sample(struct kbase_hwcnt_backend_csf *backe
 	backend_csf->info->csf_if->get_gpu_cycle_count(backend_csf->info->csf_if->ctx, cycle_counts,
 						       clk_enable_map);
 
-	kbase_hwcnt_metadata_for_each_clock(enable_map->metadata, clk) {
+	kbase_hwcnt_metadata_for_each_clock(enable_map->metadata, clk)
+	{
 		if (kbase_hwcnt_clk_enable_map_enabled(clk_enable_map, clk))
 			backend_csf->prev_cycle_count[clk] = cycle_counts[clk];
 	}
@@ -367,7 +361,8 @@ static void kbasep_hwcnt_backend_csf_cc_update(struct kbase_hwcnt_backend_csf *b
 	backend_csf->info->csf_if->get_gpu_cycle_count(backend_csf->info->csf_if->ctx, cycle_counts,
 						       backend_csf->clk_enable_map);
 
-	kbase_hwcnt_metadata_for_each_clock(backend_csf->info->metadata, clk) {
+	kbase_hwcnt_metadata_for_each_clock(backend_csf->info->metadata, clk)
+	{
 		if (kbase_hwcnt_clk_enable_map_enabled(backend_csf->clk_enable_map, clk)) {
 			backend_csf->cycle_count_elapsed[clk] =
 				cycle_counts[clk] - backend_csf->prev_cycle_count[clk];
@@ -389,28 +384,34 @@ static u64 kbasep_hwcnt_backend_csf_timestamp_ns(struct kbase_hwcnt_backend *bac
 
 /** kbasep_hwcnt_backend_csf_process_enable_map() - Process the enable_map to
  *                                                  guarantee headers are
- *                                                  enabled.
+ *                                                  enabled if any counter is
+ *                                                  required.
  *@phys_enable_map: HWC physical enable map to be processed.
  */
-void kbasep_hwcnt_backend_csf_process_enable_map(
-	struct kbase_hwcnt_physical_enable_map *phys_enable_map)
+static void
+kbasep_hwcnt_backend_csf_process_enable_map(struct kbase_hwcnt_physical_enable_map *phys_enable_map)
 {
 	WARN_ON(!phys_enable_map);
 
-	/* Unconditionally enable each block header and first counter,
-	 * the header is controlled by bit 0 of the enable mask.
+	/* Enable header if any counter is required from user, the header is
+	 * controlled by bit 0 of the enable mask.
 	 */
-	phys_enable_map->fe_bm |= 3;
+	phys_enable_map->fe_bm |= 1;
 
-	phys_enable_map->tiler_bm |= 3;
+	if (phys_enable_map->tiler_bm)
+		phys_enable_map->tiler_bm |= 1;
 
-	phys_enable_map->mmu_l2_bm |= 3;
+	if (phys_enable_map->mmu_l2_bm)
+		phys_enable_map->mmu_l2_bm |= 1;
 
-	phys_enable_map->shader_bm |= 3;
+	if (phys_enable_map->shader_bm)
+		phys_enable_map->shader_bm |= 1;
 
-	phys_enable_map->fw_bm |= 3;
+	if (phys_enable_map->fw_bm)
+		phys_enable_map->fw_bm |= 1;
 
-	phys_enable_map->csg_bm |= 3;
+	if (phys_enable_map->csg_bm)
+		phys_enable_map->csg_bm |= 1;
 
 }
 
@@ -428,7 +429,7 @@ static void kbasep_hwcnt_backend_csf_init_layout(
 	WARN_ON(!prfcnt_info);
 	WARN_ON(!phys_layout);
 
-	shader_core_cnt = (size_t)fls64(prfcnt_info->sc_core_mask);
+	shader_core_cnt = fls64(prfcnt_info->core_mask);
 	values_per_block = prfcnt_info->prfcnt_block_size / KBASE_HWCNT_VALUE_HW_BYTES;
 	fw_block_cnt = div_u64(prfcnt_info->prfcnt_fw_size, prfcnt_info->prfcnt_block_size);
 	hw_block_cnt = div_u64(prfcnt_info->prfcnt_hw_size, prfcnt_info->prfcnt_block_size);
@@ -449,7 +450,7 @@ static void kbasep_hwcnt_backend_csf_init_layout(
 		.fw_block_cnt = fw_block_cnt,
 		.hw_block_cnt = hw_block_cnt,
 		.block_cnt = fw_block_cnt + hw_block_cnt,
-		.shader_avail_mask = prfcnt_info->sc_core_mask,
+		.shader_avail_mask = prfcnt_info->core_mask,
 		.headers_per_block = KBASE_HWCNT_V5_HEADERS_PER_BLOCK,
 		.values_per_block = values_per_block,
 		.counters_per_block = values_per_block - KBASE_HWCNT_V5_HEADERS_PER_BLOCK,
@@ -458,20 +459,17 @@ static void kbasep_hwcnt_backend_csf_init_layout(
 }
 
 static void
-kbasep_hwcnt_backend_csf_reset_internal_buffers(struct kbase_hwcnt_backend_csf *backend_csf,
-						bool user_bufs)
+kbasep_hwcnt_backend_csf_reset_internal_buffers(struct kbase_hwcnt_backend_csf *backend_csf)
 {
 	size_t user_buf_bytes = backend_csf->info->metadata->dump_buf_bytes;
 	size_t block_state_bytes = backend_csf->phys_layout.block_cnt *
 				   KBASE_HWCNT_BLOCK_STATE_BYTES * KBASE_HWCNT_BLOCK_STATE_STRIDE;
 
+	memset(backend_csf->to_user_buf, 0, user_buf_bytes);
 	memset(backend_csf->accum_buf, 0, user_buf_bytes);
 	memset(backend_csf->old_sample_buf, 0, backend_csf->info->prfcnt_info.dump_bytes);
 	memset(backend_csf->block_states, 0, block_state_bytes);
-	if (user_bufs) {
-		memset(backend_csf->to_user_buf, 0, user_buf_bytes);
-		memset(backend_csf->to_user_block_states, 0, block_state_bytes);
-	}
+	memset(backend_csf->to_user_block_states, 0, block_state_bytes);
 }
 
 static void
@@ -524,21 +522,33 @@ static void kbasep_hwcnt_backend_csf_update_user_sample(struct kbase_hwcnt_backe
 	memset(backend_csf->block_states, 0, block_state_bytes);
 }
 
-void kbasep_hwcnt_backend_csf_update_block_state(struct kbase_hwcnt_backend_csf *backend,
-						 const u32 enable_mask, bool exiting_protm,
-						 size_t block_idx, blk_stt_t *const block_state,
-						 bool fw_in_protected_mode)
+/**
+ * kbasep_hwcnt_backend_csf_update_block_state - Update block state of a block instance with
+ *						   information from a sample.
+ * @phys_layout:                Physical memory layout information of HWC
+ *                              sample buffer.
+ * @enable_mask:                Counter enable mask for the block whose state is being updated.
+ * @enable_state:               The CSF backend internal enabled state.
+ * @exiting_protm:              Whether or not the sample is taken when the GPU is exiting
+ *                              protected mode.
+ * @block_idx:                  Index of block within the ringbuffer.
+ * @block_state:                Pointer to existing block state of the block whose state is being
+ *                              updated.
+ * @fw_in_protected_mode:       Whether or not GPU is in protected mode during sampling.
+ */
+static void kbasep_hwcnt_backend_csf_update_block_state(
+	const struct kbase_hwcnt_csf_physical_layout *phys_layout, const u32 enable_mask,
+	enum kbase_hwcnt_backend_csf_enable_state enable_state, bool exiting_protm,
+	size_t block_idx, blk_stt_t *const block_state, bool fw_in_protected_mode)
 {
-	const struct kbase_hwcnt_csf_physical_layout *phys_layout = &backend->phys_layout;
 	/* Offset of shader core blocks from the start of the HW blocks in the sample */
-	size_t shader_core_block_offset =
-		(size_t)(phys_layout->block_cnt - phys_layout->shader_cnt);
+	size_t shader_core_block_offset = phys_layout->hw_block_cnt - phys_layout->shader_cnt;
 	bool is_shader_core_block;
 
-	is_shader_core_block = (block_idx >= shader_core_block_offset);
+	is_shader_core_block = block_idx >= shader_core_block_offset;
 
 	/* Set power bits for the block state for the block, for the sample */
-	switch (backend->enable_state) {
+	switch (enable_state) {
 	/* Disabled states */
 	case KBASE_HWCNT_BACKEND_CSF_DISABLED:
 	case KBASE_HWCNT_BACKEND_CSF_TRANSITIONING_TO_ENABLED:
@@ -586,63 +596,40 @@ void kbasep_hwcnt_backend_csf_update_block_state(struct kbase_hwcnt_backend_csf
 								    KBASE_HWCNT_STATE_NORMAL);
 	else
 		kbase_hwcnt_block_state_append(block_state, KBASE_HWCNT_STATE_NORMAL);
-
-	/* powered_shader_core_mask stored in the backend is a combination of
-	 * the shader present and the debug core mask, so explicit checking of the
-	 * core mask is not required here.
-	 */
-	if (is_shader_core_block) {
-		u64 current_shader_core = 1ULL << (block_idx - shader_core_block_offset);
-
-		WARN_ON_ONCE(backend->phys_layout.shader_cnt > 64);
-
-		if (current_shader_core & backend->info->backend->powered_shader_core_mask)
-			kbase_hwcnt_block_state_append(block_state, KBASE_HWCNT_STATE_AVAILABLE);
-		else if (current_shader_core & ~backend->info->backend->powered_shader_core_mask)
-			kbase_hwcnt_block_state_append(block_state, KBASE_HWCNT_STATE_UNAVAILABLE);
-		else
-			WARN_ON_ONCE(true);
-	}
-	else
-		kbase_hwcnt_block_state_append(block_state, KBASE_HWCNT_STATE_AVAILABLE);
 }
 
-static void kbasep_hwcnt_backend_csf_accumulate_sample(struct kbase_hwcnt_backend_csf *backend,
-						       const u32 *old_sample_buf,
-						       const u32 *new_sample_buf)
+static void kbasep_hwcnt_backend_csf_accumulate_sample(
+	const struct kbase_hwcnt_csf_physical_layout *phys_layout, size_t dump_bytes,
+	u64 *accum_buf, const u32 *old_sample_buf, const u32 *new_sample_buf,
+	blk_stt_t *const block_states, bool clearing_samples,
+	enum kbase_hwcnt_backend_csf_enable_state enable_state, bool fw_in_protected_mode)
 {
-	const struct kbase_hwcnt_csf_physical_layout *phys_layout = &backend->phys_layout;
-	const size_t dump_bytes = backend->info->prfcnt_info.dump_bytes;
-	const size_t values_per_block = phys_layout->values_per_block;
-	blk_stt_t *const block_states = backend->block_states;
-	const bool fw_in_protected_mode = backend->info->fw_in_protected_mode;
-	const bool clearing_samples = backend->info->prfcnt_info.clearing_samples;
-	u64 *accum_buf = backend->accum_buf;
-
 	size_t block_idx;
 	const u32 *old_block = old_sample_buf;
 	const u32 *new_block = new_sample_buf;
 	u64 *acc_block = accum_buf;
-	/* Flag to indicate whether current sample is exiting protected mode. */
+	/* Flag to indicate whether current sample is when exiting protected mode. */
 	bool exiting_protm = false;
+	const size_t values_per_block = phys_layout->values_per_block;
 
 	/* The block pointers now point to the first HW block, which is always a CSHW/front-end
 	 * block. The counter enable mask for this block can be checked to determine whether this
 	 * sample is taken after leaving protected mode - this is the only scenario where the CSHW
-	 * block counter enable mask has only the first bit set, and no others. In this case,
-	 * the values in this sample would not be meaningful, so they don't need to be accumulated.
+	 * block counter enable mask is all-zero. In this case, the values in this sample would not
+	 * be meaningful, so they don't need to be accumulated.
 	 */
-	exiting_protm = (new_block[phys_layout->enable_mask_offset] == 1);
+	exiting_protm = !new_block[phys_layout->enable_mask_offset];
 
 	for (block_idx = 0; block_idx < phys_layout->block_cnt; block_idx++) {
 		const u32 old_enable_mask = old_block[phys_layout->enable_mask_offset];
 		const u32 new_enable_mask = new_block[phys_layout->enable_mask_offset];
 		/* Update block state with information of the current sample */
-		kbasep_hwcnt_backend_csf_update_block_state(backend, new_enable_mask, exiting_protm,
-							    block_idx, &block_states[block_idx],
+		kbasep_hwcnt_backend_csf_update_block_state(phys_layout, new_enable_mask,
+							    enable_state, exiting_protm, block_idx,
+							    &block_states[block_idx],
 							    fw_in_protected_mode);
 
-		if (!(new_enable_mask & HWCNT_BLOCK_EMPTY_SAMPLE)) {
+		if (new_enable_mask == 0) {
 			/* Hardware block was unavailable or we didn't turn on
 			 * any counters. Do nothing.
 			 */
@@ -675,7 +662,7 @@ static void kbasep_hwcnt_backend_csf_accumulate_sample(struct kbase_hwcnt_backen
 			 * saturating at their maximum value.
 			 */
 			if (!clearing_samples) {
-				if (!(old_enable_mask & HWCNT_BLOCK_EMPTY_SAMPLE)) {
+				if (old_enable_mask == 0) {
 					/* Block was previously
 					 * unavailable. Accumulate the new
 					 * counters only, as we know previous
@@ -723,6 +710,7 @@ static void kbasep_hwcnt_backend_csf_accumulate_samples(struct kbase_hwcnt_backe
 	u8 *cpu_dump_base = (u8 *)backend_csf->ring_buf_cpu_base;
 	const size_t ring_buf_cnt = backend_csf->info->ring_buf_cnt;
 	const size_t buf_dump_bytes = backend_csf->info->prfcnt_info.dump_bytes;
+	bool clearing_samples = backend_csf->info->prfcnt_info.clearing_samples;
 	u32 *old_sample_buf = backend_csf->old_sample_buf;
 	u32 *new_sample_buf = old_sample_buf;
 	const struct kbase_hwcnt_csf_physical_layout *phys_layout = &backend_csf->phys_layout;
@@ -756,8 +744,10 @@ static void kbasep_hwcnt_backend_csf_accumulate_samples(struct kbase_hwcnt_backe
 		const u32 buf_idx = raw_idx & (ring_buf_cnt - 1);
 
 		new_sample_buf = (u32 *)&cpu_dump_base[buf_idx * buf_dump_bytes];
-		kbasep_hwcnt_backend_csf_accumulate_sample(backend_csf, old_sample_buf,
-							   new_sample_buf);
+		kbasep_hwcnt_backend_csf_accumulate_sample(
+			phys_layout, buf_dump_bytes, backend_csf->accum_buf, old_sample_buf,
+			new_sample_buf, backend_csf->block_states, clearing_samples,
+			backend_csf->enable_state, backend_csf->info->fw_in_protected_mode);
 
 		old_sample_buf = new_sample_buf;
 	}
@@ -1229,6 +1219,11 @@ static void kbasep_hwcnt_backend_csf_dump_disable(struct kbase_hwcnt_backend *ba
 						 backend_csf->ring_buf, 0,
 						 backend_csf->info->ring_buf_cnt, false);
 
+	/* Reset accumulator, old_sample_buf and user_sample to all-0 to prepare
+	 * for next enable.
+	 */
+	kbasep_hwcnt_backend_csf_reset_internal_buffers(backend_csf);
+
 	/* Disabling HWCNT is an indication that blocks have been powered off. This is important to
 	 * know for L2, CSHW, and Tiler blocks, as this is currently the only way a backend can
 	 * know if they are being powered off.
@@ -1264,12 +1259,6 @@ static void kbasep_hwcnt_backend_csf_dump_disable(struct kbase_hwcnt_backend *ba
 		kbase_hwcnt_block_state_set(&backend_csf->accum_all_blk_stt,
 					    KBASE_HWCNT_STATE_UNKNOWN);
 	}
-
-	/* Reset accumulator, old_sample_buf and block_states to all-0 to prepare for next enable.
-	 * Reset user buffers if ownership is transferred to the caller (i.e. dump_buffer
-	 * is provided).
-	 */
-	kbasep_hwcnt_backend_csf_reset_internal_buffers(backend_csf, dump_buffer);
 }
 
 /* CSF backend implementation of kbase_hwcnt_backend_dump_request_fn */
@@ -1294,11 +1283,6 @@ static int kbasep_hwcnt_backend_csf_dump_request(struct kbase_hwcnt_backend *bac
 		backend_csf->dump_state = KBASE_HWCNT_BACKEND_CSF_DUMP_COMPLETED;
 		*dump_time_ns = kbasep_hwcnt_backend_csf_timestamp_ns(backend);
 		kbasep_hwcnt_backend_csf_cc_update(backend_csf);
-		/* There is a possibility that the transition to enabled state will remain
-		 * during multiple dumps, hence append the OFF state.
-		 */
-		kbase_hwcnt_block_state_append(&backend_csf->accum_all_blk_stt,
-					       KBASE_HWCNT_STATE_OFF);
 		backend_csf->user_requested = true;
 		backend_csf->info->csf_if->unlock(backend_csf->info->csf_if->ctx, flags);
 		return 0;
@@ -1460,7 +1444,8 @@ static int kbasep_hwcnt_backend_csf_dump_get(struct kbase_hwcnt_backend *backend
 		return -EINVAL;
 
 	/* Extract elapsed cycle count for each clock domain if enabled. */
-	kbase_hwcnt_metadata_for_each_clock(dst_enable_map->metadata, clk) {
+	kbase_hwcnt_metadata_for_each_clock(dst_enable_map->metadata, clk)
+	{
 		if (!kbase_hwcnt_clk_enable_map_enabled(dst_enable_map->clk_enable_map, clk))
 			continue;
 
@@ -1477,7 +1462,7 @@ static int kbasep_hwcnt_backend_csf_dump_get(struct kbase_hwcnt_backend *backend
 	ret = kbase_hwcnt_csf_dump_get(dst, backend_csf->to_user_buf,
 				       backend_csf->to_user_block_states, dst_enable_map,
 				       backend_csf->num_l2_slices,
-				       backend_csf->powered_shader_core_mask, accumulate);
+				       backend_csf->shader_present_bitmap, accumulate);
 
 	/* If no error occurred (zero ret value), then update block state for all blocks in the
 	 * accumulation with the current sample's block state.
@@ -1489,12 +1474,6 @@ static int kbasep_hwcnt_backend_csf_dump_get(struct kbase_hwcnt_backend *backend
 					    KBASE_HWCNT_STATE_UNKNOWN);
 	}
 
-	/* Clear consumed user buffers. */
-	memset(backend_csf->to_user_buf, 0, backend_csf->info->metadata->dump_buf_bytes);
-	memset(backend_csf->to_user_block_states, 0,
-	       backend_csf->phys_layout.block_cnt * KBASE_HWCNT_BLOCK_STATE_BYTES *
-		       KBASE_HWCNT_BLOCK_STATE_STRIDE);
-
 	return ret;
 }
 
@@ -1710,22 +1689,6 @@ static void kbasep_hwcnt_backend_csf_term(struct kbase_hwcnt_backend *backend)
 	kbasep_hwcnt_backend_csf_destroy(backend_csf);
 }
 
-static void kbasep_hwcnt_backend_csf_acquire(const struct kbase_hwcnt_backend *backend)
-{
-	struct kbase_hwcnt_backend_csf *backend_csf = (struct kbase_hwcnt_backend_csf *)backend;
-	struct kbase_hwcnt_backend_csf_info *csf_info = backend_csf->info;
-
-	csf_info->csf_if->acquire(csf_info->csf_if->ctx);
-}
-
-static void kbasep_hwcnt_backend_csf_release(const struct kbase_hwcnt_backend *backend)
-{
-	struct kbase_hwcnt_backend_csf *backend_csf = (struct kbase_hwcnt_backend_csf *)backend;
-	struct kbase_hwcnt_backend_csf_info *csf_info = backend_csf->info;
-
-	csf_info->csf_if->release(csf_info->csf_if->ctx);
-}
-
 /**
  * kbasep_hwcnt_backend_csf_info_destroy() - Destroy a CSF backend info.
  * @info: Pointer to info to destroy.
@@ -2140,7 +2103,7 @@ int kbase_hwcnt_backend_csf_metadata_init(struct kbase_hwcnt_backend_interface *
 	gpu_info.has_fw_counters = csf_info->prfcnt_info.prfcnt_fw_size > 0;
 	gpu_info.l2_count = csf_info->prfcnt_info.l2_count;
 	gpu_info.csg_cnt = csf_info->prfcnt_info.csg_count;
-	gpu_info.sc_core_mask = csf_info->prfcnt_info.sc_core_mask;
+	gpu_info.core_mask = csf_info->prfcnt_info.core_mask;
 	gpu_info.clk_cnt = csf_info->prfcnt_info.clk_cnt;
 	gpu_info.prfcnt_values_per_block =
 		csf_info->prfcnt_info.prfcnt_block_size / KBASE_HWCNT_VALUE_HW_BYTES;
@@ -2157,7 +2120,7 @@ void kbase_hwcnt_backend_csf_metadata_term(struct kbase_hwcnt_backend_interface
 
 	csf_info = (struct kbase_hwcnt_backend_csf_info *)iface->info;
 	if (csf_info->metadata) {
-		kbase_hwcnt_metadata_destroy(csf_info->metadata);
+		kbase_hwcnt_csf_metadata_destroy(csf_info->metadata);
 		csf_info->metadata = NULL;
 	}
 }
@@ -2184,8 +2147,6 @@ int kbase_hwcnt_backend_csf_create(struct kbase_hwcnt_backend_csf_if *csf_if, u3
 	iface->metadata = kbasep_hwcnt_backend_csf_metadata;
 	iface->init = kbasep_hwcnt_backend_csf_init;
 	iface->term = kbasep_hwcnt_backend_csf_term;
-	iface->acquire = kbasep_hwcnt_backend_csf_acquire;
-	iface->release = kbasep_hwcnt_backend_csf_release;
 	iface->timestamp_ns = kbasep_hwcnt_backend_csf_timestamp_ns;
 	iface->dump_enable = kbasep_hwcnt_backend_csf_dump_enable;
 	iface->dump_enable_nolock = kbasep_hwcnt_backend_csf_dump_enable_nolock;
diff --git a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf.h b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf.h
index 104f9c77a945..da78c1f76aae 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf.h
+++ b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2021-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2021-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -30,10 +30,6 @@
 #include "hwcnt/backend/mali_kbase_hwcnt_backend.h"
 #include "hwcnt/backend/mali_kbase_hwcnt_backend_csf_if.h"
 #include "hwcnt/mali_kbase_hwcnt_watchdog_if.h"
-#include "hwcnt/mali_kbase_hwcnt_types.h"
-
-struct kbase_hwcnt_physical_enable_map;
-struct kbase_hwcnt_backend_csf;
 
 /**
  * kbase_hwcnt_backend_csf_create() - Create a CSF hardware counter backend
@@ -125,21 +121,11 @@ void kbase_hwcnt_backend_csf_on_before_reset(struct kbase_hwcnt_backend_interfac
  *                                                 this function is called.
  * @iface: Non-NULL pointer to HWC backend interface.
  * @num_l2_slices: Current number of L2 slices allocated to the GPU.
- * @shader_present: Shader_present of the current configuration.
- * @power_core_mask: Mask containing changed shader core power state.
+ * @shader_present_bitmap: Current shader-present bitmap that is allocated to the GPU.
  */
 void kbase_hwcnt_backend_csf_set_hw_availability(struct kbase_hwcnt_backend_interface *iface,
-						 size_t num_l2_slices, u64 shader_present,
-						 u64 power_core_mask);
-
-/** kbasep_hwcnt_backend_csf_process_enable_map() - Process the enable_map to
- *                                                  guarantee headers are
- *                                                  enabled if any counter is
- *                                                  required.
- * @phys_enable_map: HWC physical enable map to be processed.
- */
-void kbasep_hwcnt_backend_csf_process_enable_map(
-	struct kbase_hwcnt_physical_enable_map *phys_enable_map);
+						 size_t num_l2_slices,
+						 uint64_t shader_present_bitmap);
 
 /**
  * kbase_hwcnt_backend_csf_on_prfcnt_sample() - CSF performance counter sample
@@ -177,21 +163,4 @@ void kbase_hwcnt_backend_csf_on_prfcnt_enable(struct kbase_hwcnt_backend_interfa
  */
 void kbase_hwcnt_backend_csf_on_prfcnt_disable(struct kbase_hwcnt_backend_interface *iface);
 
-/**
- * kbasep_hwcnt_backend_csf_update_block_state - Update block state of a block instance with
- *                              information from a sample.
- * @backend:                    CSF hardware counter backend.
- * @enable_mask:                Counter enable mask for the block whose state is being updated.
- * @exiting_protm:              Whether or not the sample is taken when the GPU is exiting
- *                              protected mode.
- * @block_idx:                  Index of block within the ringbuffer.
- * @block_state:                Pointer to existing block state of the block whose state is being
- *                              updated.
- * @fw_in_protected_mode:       Whether or not GPU is in protected mode during sampling.
- */
-void kbasep_hwcnt_backend_csf_update_block_state(struct kbase_hwcnt_backend_csf *backend,
-						 const u32 enable_mask, bool exiting_protm,
-						 size_t block_idx, blk_stt_t *const block_state,
-						 bool fw_in_protected_mode);
-
 #endif /* _KBASE_HWCNT_BACKEND_CSF_H_ */
diff --git a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf_if.h b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf_if.h
index 81f809fdc83a..65bb965bcf9c 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf_if.h
+++ b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf_if.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2021-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2021-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -68,7 +68,7 @@ struct kbase_hwcnt_backend_csf_if_enable {
  * @prfcnt_block_size: Bytes of each performance counter block.
  * @l2_count:          The MMU L2 cache count.
  * @csg_count:         The total number of CSGs in the system
- * @sc_core_mask:         Shader core mask.
+ * @core_mask:         Shader core mask.
  * @clk_cnt:           Clock domain count in the system.
  * @clearing_samples:  Indicates whether counters are cleared after each sample
  *                     is taken.
@@ -80,7 +80,7 @@ struct kbase_hwcnt_backend_csf_if_prfcnt_info {
 	size_t prfcnt_block_size;
 	size_t l2_count;
 	u32 csg_count;
-	u64 sc_core_mask;
+	u64 core_mask;
 	u8 clk_cnt;
 	bool clearing_samples;
 };
@@ -114,20 +114,6 @@ typedef void (*kbase_hwcnt_backend_csf_if_lock_fn)(struct kbase_hwcnt_backend_cs
 typedef void (*kbase_hwcnt_backend_csf_if_unlock_fn)(struct kbase_hwcnt_backend_csf_if_ctx *ctx,
 						     unsigned long flags);
 
-/**
- * typedef kbase_hwcnt_backend_csf_if_acquire_fn - Enable counter collection.
- *
- * @ctx:   Non-NULL pointer to a CSF context.
- */
-typedef void (*kbase_hwcnt_backend_csf_if_acquire_fn)(struct kbase_hwcnt_backend_csf_if_ctx *ctx);
-
-/**
- * typedef kbase_hwcnt_backend_csf_if_release_fn - Disable counter collection.
- *
- * @ctx:   Non-NULL pointer to a CSF context.
- */
-typedef void (*kbase_hwcnt_backend_csf_if_release_fn)(struct kbase_hwcnt_backend_csf_if_ctx *ctx);
-
 /**
  * typedef kbase_hwcnt_backend_csf_if_get_prfcnt_info_fn - Get performance
  *                                                         counter information.
@@ -286,10 +272,6 @@ typedef void (*kbase_hwcnt_backend_csf_if_get_gpu_cycle_count_fn)(
  * @assert_lock_held:    Function ptr to assert backend spinlock is held.
  * @lock:                Function ptr to acquire backend spinlock.
  * @unlock:              Function ptr to release backend spinlock.
- * @acquire:             Callback to indicate that counter collection has
- *                       been enabled.
- * @release:             Callback to indicate that counter collection has
- *                       been disabled.
  * @get_prfcnt_info:     Function ptr to get performance counter related
  *                       information.
  * @ring_buf_alloc:      Function ptr to allocate ring buffer for CSF HWC.
@@ -310,8 +292,6 @@ struct kbase_hwcnt_backend_csf_if {
 	kbase_hwcnt_backend_csf_if_assert_lock_held_fn assert_lock_held;
 	kbase_hwcnt_backend_csf_if_lock_fn lock;
 	kbase_hwcnt_backend_csf_if_unlock_fn unlock;
-	kbase_hwcnt_backend_csf_if_acquire_fn acquire;
-	kbase_hwcnt_backend_csf_if_release_fn release;
 	kbase_hwcnt_backend_csf_if_get_prfcnt_info_fn get_prfcnt_info;
 	kbase_hwcnt_backend_csf_if_ring_buf_alloc_fn ring_buf_alloc;
 	kbase_hwcnt_backend_csf_if_ring_buf_sync_fn ring_buf_sync;
diff --git a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf_if_fw.c b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf_if_fw.c
index 29f8a2a8838d..a44651949abf 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf_if_fw.c
+++ b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_csf_if_fw.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2021-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2021-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -131,26 +131,6 @@ static void kbasep_hwcnt_backend_csf_if_fw_unlock(struct kbase_hwcnt_backend_csf
 	kbase_csf_scheduler_spin_unlock(kbdev, flags);
 }
 
-static void kbasep_hwcnt_backend_csf_if_fw_acquire(struct kbase_hwcnt_backend_csf_if_ctx *ctx)
-{
-	struct kbase_hwcnt_backend_csf_if_fw_ctx *fw_ctx =
-		(struct kbase_hwcnt_backend_csf_if_fw_ctx *)ctx;
-
-	/* Mark performance counters collection as enabled */
-	set_bit(KBASE_GPU_PERF_COUNTERS_COLLECTION_ENABLED,
-		&fw_ctx->kbdev->pm.backend.gpu_sleep_allowed);
-}
-
-static void kbasep_hwcnt_backend_csf_if_fw_release(struct kbase_hwcnt_backend_csf_if_ctx *ctx)
-{
-	struct kbase_hwcnt_backend_csf_if_fw_ctx *fw_ctx =
-		(struct kbase_hwcnt_backend_csf_if_fw_ctx *)ctx;
-
-	/* Mark performance counters collection as disabled */
-	clear_bit(KBASE_GPU_PERF_COUNTERS_COLLECTION_ENABLED,
-		  &fw_ctx->kbdev->pm.backend.gpu_sleep_allowed);
-}
-
 /**
  * kbasep_hwcnt_backend_csf_if_fw_on_freq_change() - On freq change callback
  *
@@ -249,7 +229,7 @@ static void kbasep_hwcnt_backend_csf_if_fw_get_prfcnt_info(
 
 	*prfcnt_info = (struct kbase_hwcnt_backend_csf_if_prfcnt_info){
 		.l2_count = KBASE_DUMMY_MODEL_MAX_MEMSYS_BLOCKS,
-		.sc_core_mask = (1ull << KBASE_DUMMY_MODEL_MAX_SHADER_CORES) - 1,
+		.core_mask = (1ull << KBASE_DUMMY_MODEL_MAX_SHADER_CORES) - 1,
 		.prfcnt_hw_size =
 			KBASE_DUMMY_MODEL_MAX_NUM_HARDWARE_BLOCKS * KBASE_DUMMY_MODEL_BLOCK_SIZE,
 		.prfcnt_fw_size =
@@ -310,13 +290,12 @@ static void kbasep_hwcnt_backend_csf_if_fw_get_prfcnt_info(
 		.dump_bytes = fw_ctx->buf_bytes,
 		.prfcnt_block_size = prfcnt_block_size,
 		.l2_count = kbdev->gpu_props.num_l2_slices,
-		.sc_core_mask = kbasep_hwcnt_backend_csf_core_mask(&kbdev->gpu_props),
+		.core_mask = kbasep_hwcnt_backend_csf_core_mask(&kbdev->gpu_props),
 		.csg_count = fw_block_count > 1 ? csg_count : 0,
 		.clk_cnt = fw_ctx->clk_cnt,
 		.clearing_samples = true,
 	};
 
-
 	/* Block size must be multiple of counter size. */
 	WARN_ON((prfcnt_info->prfcnt_block_size % KBASE_HWCNT_VALUE_HW_BYTES) != 0);
 	/* Total size must be multiple of block size. */
@@ -534,15 +513,10 @@ kbasep_hwcnt_backend_csf_if_fw_ring_buf_free(struct kbase_hwcnt_backend_csf_if_c
 			fw_ring_buf->phys, fw_ring_buf->num_pages, fw_ring_buf->num_pages,
 			MCU_AS_NR));
 
-		/* Clear the dump ring_buf content to zeros */
-		memset(fw_ring_buf->cpu_dump_base, 0, fw_ring_buf->num_pages * PAGE_SIZE);
 		vunmap(fw_ring_buf->cpu_dump_base);
 
-		/* After zeroing, the ring_buf pages are dirty so need to pass the 'dirty' flag
-		 * as true when freeing the pages to the Global pool.
-		 */
 		kbase_mem_pool_free_pages(&fw_ctx->kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
-					  fw_ring_buf->num_pages, fw_ring_buf->phys, true, false);
+					  fw_ring_buf->num_pages, fw_ring_buf->phys, false, false);
 
 		kfree(fw_ring_buf->phys);
 
@@ -562,7 +536,7 @@ kbasep_hwcnt_backend_csf_if_fw_dump_enable(struct kbase_hwcnt_backend_csf_if_ctx
 		(struct kbase_hwcnt_backend_csf_if_fw_ctx *)ctx;
 	struct kbase_hwcnt_backend_csf_if_fw_ring_buf *fw_ring_buf =
 		(struct kbase_hwcnt_backend_csf_if_fw_ring_buf *)ring_buf;
-	u32 csg_mask;
+	u32 max_csg_slots;
 
 	WARN_ON(!ctx);
 	WARN_ON(!ring_buf);
@@ -571,7 +545,7 @@ kbasep_hwcnt_backend_csf_if_fw_dump_enable(struct kbase_hwcnt_backend_csf_if_ctx
 
 	kbdev = fw_ctx->kbdev;
 	global_iface = &kbdev->csf.global_iface;
-	csg_mask = (1 << kbdev->csf.global_iface.group_num) - 1;
+	max_csg_slots = kbdev->csf.global_iface.group_num;
 
 	/* Configure */
 	prfcnt_config = GLB_PRFCNT_CONFIG_SIZE_SET(0, fw_ring_buf->buf_count);
@@ -596,7 +570,7 @@ kbasep_hwcnt_backend_csf_if_fw_dump_enable(struct kbase_hwcnt_backend_csf_if_ctx
 	kbase_csf_firmware_global_input(global_iface, GLB_PRFCNT_CSG_EN, enable->csg_bm);
 
 	/* Enable all of the CSGs by default. */
-	kbase_csf_firmware_global_input(global_iface, GLB_PRFCNT_CSG_SELECT, csg_mask);
+	kbase_csf_firmware_global_input(global_iface, GLB_PRFCNT_CSG_SELECT, max_csg_slots);
 
 
 	/* Configure the HWC set and buffer size */
@@ -833,8 +807,6 @@ int kbase_hwcnt_backend_csf_if_fw_create(struct kbase_device *kbdev,
 	if_fw->assert_lock_held = kbasep_hwcnt_backend_csf_if_fw_assert_lock_held;
 	if_fw->lock = kbasep_hwcnt_backend_csf_if_fw_lock;
 	if_fw->unlock = kbasep_hwcnt_backend_csf_if_fw_unlock;
-	if_fw->acquire = kbasep_hwcnt_backend_csf_if_fw_acquire;
-	if_fw->release = kbasep_hwcnt_backend_csf_if_fw_release;
 	if_fw->get_prfcnt_info = kbasep_hwcnt_backend_csf_if_fw_get_prfcnt_info;
 	if_fw->ring_buf_alloc = kbasep_hwcnt_backend_csf_if_fw_ring_buf_alloc;
 	if_fw->ring_buf_sync = kbasep_hwcnt_backend_csf_if_fw_ring_buf_sync;
diff --git a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_jm.c b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_jm.c
index c3f2bcdbf256..5156706bdf2b 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_jm.c
+++ b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_jm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -80,40 +80,34 @@ struct kbase_hwcnt_jm_physical_layout {
 
 /**
  * struct kbase_hwcnt_backend_jm - Instance of a JM hardware counter backend.
- * @info:                Info used to create the backend.
- * @kctx:                KBase context used for GPU memory allocation and
- *                       counter dumping.
- * @gpu_dump_va:         GPU hardware counter dump buffer virtual address.
- * @cpu_dump_va:         CPU mapping of gpu_dump_va.
- * @vmap:                Dump buffer vmap.
- * @to_user_buf:         HWC sample buffer for client user, size
- *                       metadata.dump_buf_bytes.
- * @enabled:             True if dumping has been enabled, else false.
- * @accum_all_blk_stt:   Block State to accumulate on next sample, for all types
- *                       of block.
+ * @info:             Info used to create the backend.
+ * @kctx:             KBase context used for GPU memory allocation and
+ *                    counter dumping.
+ * @gpu_dump_va:      GPU hardware counter dump buffer virtual address.
+ * @cpu_dump_va:      CPU mapping of gpu_dump_va.
+ * @vmap:             Dump buffer vmap.
+ * @to_user_buf:      HWC sample buffer for client user, size
+ *                    metadata.dump_buf_bytes.
+ * @enabled:          True if dumping has been enabled, else false.
+ * @accum_all_blk_stt: Block State to accumulate on next sample, for all types
+ *                     of block.
  * @sampled_all_blk_stt: Block State to accumulate into the current sample, for
  *                       all types of block.
- * @debug_core_mask:     User-set mask of shader cores that can be used.
- * @pm_core_mask:        PM state sync-ed shaders core mask for the enabled
- *                       dumping.
- * @curr_config:         Current allocated hardware resources to correctly map the
- *                       source raw dump buffer to the destination dump buffer.
- * @max_core_mask:       Core mask of all cores allocated to the GPU (non
- *                       virtualized platforms) or resource group (virtualized
- *                       platforms).
- * @max_l2_slices:       Maximum number of L2 slices allocated to the GPU (non
- *                       virtualized platforms) or resource group (virtualized
- *                       platforms).
- * @clk_enable_map:      The enable map specifying enabled clock domains.
- * @cycle_count_elapsed: Cycle count elapsed for a given sample period.
- *                       The top clock cycle, index 0, is read directly from
- *                       hardware, but the other clock domains need to be
- *                       calculated with software estimation.
- * @prev_cycle_count:    Previous cycle count to calculate the cycle count for
- *                       sample period.
- * @rate_listener:       Clock rate listener callback state.
- * @ccswe_shader_cores:  Shader cores cycle count software estimator.
- * @phys_layout:         Physical memory layout information of HWC sample buffer.
+ * @pm_core_mask:     PM state sync-ed shaders core mask for the enabled
+ *                    dumping.
+ * @curr_config:      Current allocated hardware resources to correctly map the
+ *                    source raw dump buffer to the destination dump buffer.
+ * @clk_enable_map:   The enable map specifying enabled clock domains.
+ * @cycle_count_elapsed:
+ *                    Cycle count elapsed for a given sample period.
+ *                    The top clock cycle, index 0, is read directly from
+ *                    hardware, but the other clock domains need to be
+ *                    calculated with software estimation.
+ * @prev_cycle_count: Previous cycle count to calculate the cycle count for
+ *                    sample period.
+ * @rate_listener:    Clock rate listener callback state.
+ * @ccswe_shader_cores: Shader cores cycle count software estimator.
+ * @phys_layout:      Physical memory layout information of HWC sample buffer.
  */
 struct kbase_hwcnt_backend_jm {
 	const struct kbase_hwcnt_backend_jm_info *info;
@@ -125,11 +119,8 @@ struct kbase_hwcnt_backend_jm {
 	bool enabled;
 	blk_stt_t accum_all_blk_stt;
 	blk_stt_t sampled_all_blk_stt;
-	u64 debug_core_mask;
 	u64 pm_core_mask;
 	struct kbase_hwcnt_curr_config curr_config;
-	u64 max_core_mask;
-	size_t max_l2_slices;
 	u64 clk_enable_map;
 	u64 cycle_count_elapsed[BASE_MAX_NR_CLOCKS_REGULATORS];
 	u64 prev_cycle_count[BASE_MAX_NR_CLOCKS_REGULATORS];
@@ -165,7 +156,7 @@ static int kbasep_hwcnt_backend_jm_gpu_info_init(struct kbase_device *kbdev,
 #endif
 
 	info->l2_count = l2_count;
-	info->sc_core_mask = core_mask;
+	info->core_mask = core_mask;
 	info->prfcnt_values_per_block = KBASE_HWCNT_V5_DEFAULT_VALUES_PER_BLOCK;
 
 	/* Determine the number of available clock domains. */
@@ -186,7 +177,7 @@ static void kbasep_hwcnt_backend_jm_init_layout(const struct kbase_hwcnt_gpu_inf
 	WARN_ON(!gpu_info);
 	WARN_ON(!phys_layout);
 
-	shader_core_cnt = fls64(gpu_info->sc_core_mask);
+	shader_core_cnt = fls64(gpu_info->core_mask);
 
 	*phys_layout = (struct kbase_hwcnt_jm_physical_layout){
 		.fe_cnt = KBASE_HWCNT_V5_FE_BLOCK_COUNT,
@@ -195,7 +186,7 @@ static void kbasep_hwcnt_backend_jm_init_layout(const struct kbase_hwcnt_gpu_inf
 		.shader_cnt = shader_core_cnt,
 		.block_cnt = KBASE_HWCNT_V5_FE_BLOCK_COUNT + KBASE_HWCNT_V5_TILER_BLOCK_COUNT +
 			     gpu_info->l2_count + shader_core_cnt,
-		.shader_avail_mask = gpu_info->sc_core_mask,
+		.shader_avail_mask = gpu_info->core_mask,
 		.headers_per_block = KBASE_HWCNT_V5_HEADERS_PER_BLOCK,
 		.values_per_block = gpu_info->prfcnt_values_per_block,
 		.counters_per_block =
@@ -364,9 +355,9 @@ kbasep_hwcnt_backend_jm_dump_enable_nolock(struct kbase_hwcnt_backend *backend,
 	struct kbase_hwcnt_backend_jm *backend_jm = (struct kbase_hwcnt_backend_jm *)backend;
 	struct kbase_context *kctx;
 	struct kbase_device *kbdev;
-	struct kbase_hwcnt_physical_enable_map phys_enable_map = { 0 };
+	struct kbase_hwcnt_physical_enable_map phys_enable_map;
 	enum kbase_hwcnt_physical_set phys_counter_set;
-	struct kbase_instr_hwcnt_enable enable = { 0 };
+	struct kbase_instr_hwcnt_enable enable;
 	u64 timestamp_ns;
 
 	if (!backend_jm || !enable_map || backend_jm->enabled ||
@@ -382,19 +373,18 @@ kbasep_hwcnt_backend_jm_dump_enable_nolock(struct kbase_hwcnt_backend *backend,
 
 	kbase_hwcnt_gpu_set_to_physical(&phys_counter_set, backend_jm->info->counter_set);
 
-	enable = (struct kbase_instr_hwcnt_enable)
-	{
-		.fe_bm = phys_enable_map.fe_bm, .shader_bm = phys_enable_map.shader_bm,
-		.tiler_bm = phys_enable_map.tiler_bm, .mmu_l2_bm = phys_enable_map.mmu_l2_bm,
-		.counter_set = phys_counter_set,
+	enable.fe_bm = phys_enable_map.fe_bm;
+	enable.shader_bm = phys_enable_map.shader_bm;
+	enable.tiler_bm = phys_enable_map.tiler_bm;
+	enable.mmu_l2_bm = phys_enable_map.mmu_l2_bm;
+	enable.counter_set = phys_counter_set;
 #if IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
-		/* The dummy model needs the CPU mapping. */
-			.dump_buffer = (uintptr_t)backend_jm->cpu_dump_va,
+	/* The dummy model needs the CPU mapping. */
+	enable.dump_buffer = (uintptr_t)backend_jm->cpu_dump_va;
 #else
-		.dump_buffer = backend_jm->gpu_dump_va,
+	enable.dump_buffer = backend_jm->gpu_dump_va;
 #endif /* CONFIG_MALI_BIFROST_NO_MALI */
-		.dump_buffer_bytes = backend_jm->info->dump_bytes,
-	};
+	enable.dump_buffer_bytes = backend_jm->info->dump_bytes;
 
 	timestamp_ns = kbasep_hwcnt_backend_jm_timestamp_ns(backend);
 
@@ -407,10 +397,6 @@ kbasep_hwcnt_backend_jm_dump_enable_nolock(struct kbase_hwcnt_backend *backend,
 	if (errcode)
 		goto error;
 
-	backend_jm->debug_core_mask = kbase_pm_ca_get_debug_core_mask(kbdev);
-	backend_jm->max_l2_slices = backend_jm->info->hwcnt_gpu_info.l2_count;
-	backend_jm->max_core_mask = backend_jm->info->hwcnt_gpu_info.sc_core_mask;
-
 	backend_jm->pm_core_mask = kbase_pm_ca_get_instr_core_mask(kbdev);
 
 	backend_jm->enabled = true;
@@ -551,7 +537,8 @@ static int kbasep_hwcnt_backend_jm_dump_request(struct kbase_hwcnt_backend *back
 		*dump_time_ns = kbasep_hwcnt_backend_jm_timestamp_ns(backend);
 		ret = kbase_instr_hwcnt_request_dump(backend_jm->kctx);
 
-		kbase_hwcnt_metadata_for_each_clock(metadata, clk) {
+		kbase_hwcnt_metadata_for_each_clock(metadata, clk)
+		{
 			if (!kbase_hwcnt_clk_enable_map_enabled(backend_jm->clk_enable_map, clk))
 				continue;
 
@@ -633,7 +620,8 @@ static int kbasep_hwcnt_backend_jm_dump_get(struct kbase_hwcnt_backend *backend,
 	kbasep_hwcnt_backend_jm_dump_sample(backend_jm);
 
 	/* Extract elapsed cycle count for each clock domain if enabled. */
-	kbase_hwcnt_metadata_for_each_clock(dst_enable_map->metadata, clk) {
+	kbase_hwcnt_metadata_for_each_clock(dst_enable_map->metadata, clk)
+	{
 		if (!kbase_hwcnt_clk_enable_map_enabled(dst_enable_map->clk_enable_map, clk))
 			continue;
 
@@ -657,8 +645,7 @@ static int kbasep_hwcnt_backend_jm_dump_get(struct kbase_hwcnt_backend *backend,
 		return errcode;
 #endif /* CONFIG_MALI_BIFROST_NO_MALI */
 	errcode = kbase_hwcnt_jm_dump_get(dst, backend_jm->to_user_buf, dst_enable_map,
-					  backend_jm->pm_core_mask, backend_jm->debug_core_mask,
-					  backend_jm->max_l2_slices, &backend_jm->curr_config,
+					  backend_jm->pm_core_mask, &backend_jm->curr_config,
 					  accumulate);
 
 	if (errcode)
@@ -683,7 +670,7 @@ static int kbasep_hwcnt_backend_jm_dump_alloc(const struct kbase_hwcnt_backend_j
 					      struct kbase_context *kctx, u64 *gpu_dump_va)
 {
 	struct kbase_va_region *reg;
-	base_mem_alloc_flags flags;
+	u64 flags;
 	u64 nr_pages;
 
 	/* Calls to this function are inherently asynchronous, with respect to
@@ -851,14 +838,6 @@ static void kbasep_hwcnt_backend_jm_term(struct kbase_hwcnt_backend *backend)
 	kbasep_hwcnt_backend_jm_destroy((struct kbase_hwcnt_backend_jm *)backend);
 }
 
-static void kbasep_hwcnt_backend_jm_acquire(const struct kbase_hwcnt_backend *backend)
-{
-}
-
-static void kbasep_hwcnt_backend_jm_release(const struct kbase_hwcnt_backend *backend)
-{
-}
-
 /**
  * kbasep_hwcnt_backend_jm_info_destroy() - Destroy a JM backend info.
  * @info: Pointer to info to destroy.
@@ -870,7 +849,7 @@ static void kbasep_hwcnt_backend_jm_info_destroy(const struct kbase_hwcnt_backen
 	if (!info)
 		return;
 
-	kbase_hwcnt_metadata_destroy(info->metadata);
+	kbase_hwcnt_jm_metadata_destroy(info->metadata);
 	kfree(info);
 }
 
@@ -940,8 +919,6 @@ int kbase_hwcnt_backend_jm_create(struct kbase_device *kbdev,
 	iface->metadata = kbasep_hwcnt_backend_jm_metadata;
 	iface->init = kbasep_hwcnt_backend_jm_init;
 	iface->term = kbasep_hwcnt_backend_jm_term;
-	iface->acquire = kbasep_hwcnt_backend_jm_acquire;
-	iface->release = kbasep_hwcnt_backend_jm_release;
 	iface->timestamp_ns = kbasep_hwcnt_backend_jm_timestamp_ns;
 	iface->dump_enable = kbasep_hwcnt_backend_jm_dump_enable;
 	iface->dump_enable_nolock = kbasep_hwcnt_backend_jm_dump_enable_nolock;
diff --git a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_jm_watchdog.c b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_jm_watchdog.c
index 88917e72ac58..cf2a2e65bc25 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_jm_watchdog.c
+++ b/drivers/gpu/arm/bifrost/hwcnt/backend/mali_kbase_hwcnt_backend_jm_watchdog.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2021-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2021-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -317,14 +317,6 @@ kbasep_hwcnt_backend_jm_watchdog_term_partial(struct kbase_hwcnt_backend_jm_watc
 	kfree(wd_backend);
 }
 
-static void kbasep_hwcnt_backend_jm_watchdog_acquire(const struct kbase_hwcnt_backend *backend)
-{
-}
-
-static void kbasep_hwcnt_backend_jm_watchdog_release(const struct kbase_hwcnt_backend *backend)
-{
-}
-
 /* Job manager watchdog backend, implementation of kbase_hwcnt_backend_term_fn
  * Calling term does *not* destroy the interface
  */
@@ -815,8 +807,6 @@ int kbase_hwcnt_backend_jm_watchdog_create(struct kbase_hwcnt_backend_interface
 		.metadata = kbasep_hwcnt_backend_jm_watchdog_metadata,
 		.init = kbasep_hwcnt_backend_jm_watchdog_init,
 		.term = kbasep_hwcnt_backend_jm_watchdog_term,
-		.acquire = kbasep_hwcnt_backend_jm_watchdog_acquire,
-		.release = kbasep_hwcnt_backend_jm_watchdog_release,
 		.timestamp_ns = kbasep_hwcnt_backend_jm_watchdog_timestamp_ns,
 		.dump_enable = kbasep_hwcnt_backend_jm_watchdog_dump_enable,
 		.dump_enable_nolock = kbasep_hwcnt_backend_jm_watchdog_dump_enable_nolock,
diff --git a/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt.c b/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt.c
index 8d308f1138a7..8b1de2e1cdaf 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt.c
+++ b/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -599,9 +599,6 @@ int kbase_hwcnt_accumulator_acquire(struct kbase_hwcnt_context *hctx,
 		return errcode;
 	}
 
-	/* Inform the backend that counter collection has been enabled. */
-	hctx->iface->acquire(hctx->accum.backend);
-
 	spin_lock_irqsave(&hctx->state_lock, flags);
 
 	WARN_ON(hctx->disable_count == 0);
@@ -649,9 +646,6 @@ void kbase_hwcnt_accumulator_release(struct kbase_hwcnt_accumulator *accum)
 
 	mutex_unlock(&hctx->accum_lock);
 
-	/* Inform the backend that counter collection has been disabled. */
-	hctx->iface->release(hctx->accum.backend);
-
 	kbasep_hwcnt_accumulator_term(hctx);
 
 	mutex_lock(&hctx->accum_lock);
diff --git a/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_gpu.c b/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_gpu.c
index 7cd16a0de4ce..875643654627 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_gpu.c
+++ b/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_gpu.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -169,7 +169,7 @@ static int kbasep_hwcnt_backend_gpu_metadata_create(const struct kbase_hwcnt_gpu
 	/* Calculate number of block instances that aren't cores */
 	non_core_block_count = 2 + gpu_info->l2_count;
 	/* Calculate number of block instances that are shader cores */
-	sc_block_count = (size_t)fls64(gpu_info->sc_core_mask);
+	sc_block_count = fls64(gpu_info->core_mask);
 	/* Determine the total number of cores */
 	core_block_count = sc_block_count;
 
@@ -274,10 +274,9 @@ static int kbasep_hwcnt_backend_gpu_metadata_create(const struct kbase_hwcnt_gpu
 	desc.clk_cnt = gpu_info->clk_cnt;
 
 	/* The JM, Tiler, and L2s are always available, and are before cores */
-	kbase_hwcnt_set_avail_mask(&desc.avail_mask, 0, 0);
-	kbase_hwcnt_set_avail_mask_bits(&desc.avail_mask, 0, non_core_block_count, U64_MAX);
+	kbase_hwcnt_set_avail_mask(&desc.avail_mask, (1ull << non_core_block_count) - 1, 0);
 	kbase_hwcnt_set_avail_mask_bits(&desc.avail_mask, non_core_block_count, sc_block_count,
-					gpu_info->sc_core_mask);
+					gpu_info->core_mask);
 
 
 	return kbase_hwcnt_metadata_create(&desc, metadata);
@@ -294,7 +293,7 @@ static size_t kbasep_hwcnt_backend_jm_dump_bytes(const struct kbase_hwcnt_gpu_in
 {
 	WARN_ON(!gpu_info);
 
-	return (2 + gpu_info->l2_count + (size_t)fls64(gpu_info->sc_core_mask)) *
+	return (2 + gpu_info->l2_count + fls64(gpu_info->core_mask)) *
 	       gpu_info->prfcnt_values_per_block * KBASE_HWCNT_VALUE_HW_BYTES;
 }
 
@@ -338,6 +337,14 @@ int kbase_hwcnt_jm_metadata_create(const struct kbase_hwcnt_gpu_info *gpu_info,
 	return 0;
 }
 
+void kbase_hwcnt_jm_metadata_destroy(const struct kbase_hwcnt_metadata *metadata)
+{
+	if (!metadata)
+		return;
+
+	kbase_hwcnt_metadata_destroy(metadata);
+}
+
 int kbase_hwcnt_csf_metadata_create(const struct kbase_hwcnt_gpu_info *gpu_info,
 				    enum kbase_hwcnt_set counter_set,
 				    const struct kbase_hwcnt_metadata **out_metadata)
@@ -357,51 +364,41 @@ int kbase_hwcnt_csf_metadata_create(const struct kbase_hwcnt_gpu_info *gpu_info,
 	return 0;
 }
 
-bool kbase_hwcnt_is_block_type_shader(const enum kbase_hwcnt_gpu_v5_block_type blk_type)
+void kbase_hwcnt_csf_metadata_destroy(const struct kbase_hwcnt_metadata *metadata)
+{
+	if (!metadata)
+		return;
+
+	kbase_hwcnt_metadata_destroy(metadata);
+}
+
+static bool is_block_type_shader(const u64 blk_type)
 {
+	bool is_shader = false;
+
 	if (blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC ||
 	    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC2 ||
 	    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC3 ||
 	    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC_UNDEFINED)
-		return true;
+		is_shader = true;
 
-	return false;
+	return is_shader;
 }
 
-
-bool kbase_hwcnt_is_block_type_memsys(const enum kbase_hwcnt_gpu_v5_block_type blk_type)
+static bool is_block_type_l2_cache(const u64 blk_type)
 {
+	bool is_l2_cache = false;
+
 	if (blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_MEMSYS ||
 	    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_MEMSYS2 ||
 	    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_MEMSYS_UNDEFINED)
-		return true;
-
-	return false;
-}
-
-bool kbase_hwcnt_is_block_type_tiler(const enum kbase_hwcnt_gpu_v5_block_type blk_type)
-{
-	if (blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_TILER ||
-	    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_TILER_UNDEFINED)
-		return true;
-
-	return false;
-}
-
-bool kbase_hwcnt_is_block_type_fe(const enum kbase_hwcnt_gpu_v5_block_type blk_type)
-{
-	if (blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_FE ||
-	    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_FE2 ||
-	    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_FE3 ||
-	    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_FE_UNDEFINED)
-		return true;
+		is_l2_cache = true;
 
-	return false;
+	return is_l2_cache;
 }
 
 int kbase_hwcnt_jm_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 			    const struct kbase_hwcnt_enable_map *dst_enable_map, u64 pm_core_mask,
-			    u64 debug_core_mask, size_t max_l2_slices,
 			    const struct kbase_hwcnt_curr_config *curr_config, bool accumulate)
 {
 	const struct kbase_hwcnt_metadata *metadata;
@@ -409,7 +406,6 @@ int kbase_hwcnt_jm_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 	const u64 *dump_src = src;
 	size_t src_offset = 0;
 	u64 core_mask = pm_core_mask;
-	u64 shader_present = curr_config->shader_present;
 
 	/* Variables to deal with the current configuration */
 	size_t l2_count = 0;
@@ -419,12 +415,13 @@ int kbase_hwcnt_jm_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 
 	metadata = dst->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		const size_t hdr_cnt = kbase_hwcnt_metadata_block_headers_count(metadata, blk);
 		const size_t ctr_cnt = kbase_hwcnt_metadata_block_counters_count(metadata, blk);
 		const u64 blk_type = kbase_hwcnt_metadata_block_type(metadata, blk);
-		const bool is_shader_core = kbase_hwcnt_is_block_type_shader(blk_type);
-		const bool is_l2_cache = kbase_hwcnt_is_block_type_memsys(blk_type);
+		const bool is_shader_core = is_block_type_shader(blk_type);
+		const bool is_l2_cache = is_block_type_l2_cache(blk_type);
 		const bool is_undefined = kbase_hwcnt_is_block_type_undefined(blk_type);
 		blk_stt_t *dst_blk_stt =
 			kbase_hwcnt_dump_buffer_block_state_instance(dst, blk, blk_inst);
@@ -451,7 +448,9 @@ int kbase_hwcnt_jm_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 		else
 			hw_res_available = true;
 
-		/* Skip block if no values in the destination block are enabled. */
+		/*
+		 * Skip block if no values in the destination block are enabled.
+		 */
 		if (kbase_hwcnt_enable_map_block_enabled(dst_enable_map, blk, blk_inst)) {
 			u64 *dst_blk = kbase_hwcnt_dump_buffer_block_instance(dst, blk, blk_inst);
 			const u64 *src_blk = dump_src + src_offset;
@@ -459,52 +458,25 @@ int kbase_hwcnt_jm_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 
 			if (blk_valid) {
 				bool blk_powered;
-				blk_stt_t current_block_state = 0;
+				blk_stt_t current_block_state;
 
 				if (!is_shader_core) {
-					/* The L2 block must be available at this point, or handled
-					 * differently below.
-					 * Every partition must have a FE and a tiler, so they
-					 * must be implicitly available as part of the current
-					 * configuration.
+					/* Under the current PM system, counters will only be
+					 * enabled after all non shader core blocks are powered up.
 					 */
 					blk_powered = true;
-					current_block_state |= KBASE_HWCNT_STATE_AVAILABLE;
 				} else {
 					/* Check the PM core mask to see if the shader core is
 					 * powered up.
 					 */
 					blk_powered = core_mask & 1;
-
-					/* Set availability bits based on whether the core is
-					 * present in both the shader_present AND the core
-					 * mask in sysFS. The core masks are shifted to the
-					 * right at the end of the loop so always check the
-					 * rightmost bit.
-					 */
-					if ((shader_present & debug_core_mask) & 0x1)
-						current_block_state |= KBASE_HWCNT_STATE_AVAILABLE;
-					else {
-						/* If this branch is taken, the shader core may
-						 * be:
-						 * * in the max configuration, but not enabled
-						 * through the sysFS core mask
-						 * * in the max configuration, but not in the
-						 * current configuration
-						 * * physically not present
-						 */
-						current_block_state |=
-							KBASE_HWCNT_STATE_UNAVAILABLE;
-					}
 				}
+				current_block_state = (blk_powered) ? KBASE_HWCNT_STATE_ON :
+									    KBASE_HWCNT_STATE_OFF;
 
 				/* Note: KBASE_HWCNT_STATE_OFF for non-shader cores (L2, Tiler, JM)
-				 * is handled on this backend's dump_disable function (since
-				 * they are considered to always be powered here).
+				 * is handled on this backend's dump_disable function
 				 */
-				current_block_state |= (blk_powered) ? KBASE_HWCNT_STATE_ON :
-									     KBASE_HWCNT_STATE_OFF;
-
 				if (accumulate) {
 					/* Only update existing counter values if block was powered
 					 * and valid
@@ -528,22 +500,10 @@ int kbase_hwcnt_jm_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 					kbase_hwcnt_block_state_set(dst_blk_stt,
 								    current_block_state);
 				}
-			} else if (is_l2_cache && !is_undefined) {
-				/* Defined L2 can only reach here when the partition does not
-				 * own it. Check that the L2 count is within the resource
-				 * group or whole GPU's max L2 count, and if so,
-				 * mark it as unavailable.
-				 */
-				if (l2_count <= max_l2_slices) {
-					kbase_hwcnt_block_state_set(
-						dst_blk_stt, KBASE_HWCNT_STATE_OFF |
-								     KBASE_HWCNT_STATE_UNAVAILABLE);
-				}
-				kbase_hwcnt_dump_buffer_block_zero(dst_blk, (hdr_cnt + ctr_cnt));
 			} else {
-				/* Even though the block is undefined, the user has
-				 * enabled counter collection for it. We should not propagate
-				 * garbage data, or copy/accumulate the block states.
+				/* Even though the block might be undefined, the user has enabled
+				 * counter collection for it. We should not propagate garbage data,
+				 * or copy/accumulate the block states.
 				 */
 				if (accumulate) {
 					/* No-op to preserve existing values */
@@ -560,12 +520,8 @@ int kbase_hwcnt_jm_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 		/* Just increase the src_offset if the HW is available */
 		if (hw_res_available)
 			src_offset += (hdr_cnt + ctr_cnt);
-		if (is_shader_core) {
-			/* Shift each core mask right by 1 */
-			core_mask >>= 1;
-			debug_core_mask >>= 1;
-			shader_present >>= 1;
-		}
+		if (is_shader_core)
+			core_mask = core_mask >> 1;
 	}
 
 	return 0;
@@ -574,7 +530,7 @@ int kbase_hwcnt_jm_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 int kbase_hwcnt_csf_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 			     blk_stt_t *src_block_stt,
 			     const struct kbase_hwcnt_enable_map *dst_enable_map,
-			     size_t num_l2_slices, u64 powered_shader_core_mask, bool accumulate)
+			     size_t num_l2_slices, u64 shader_present_bitmap, bool accumulate)
 {
 	const struct kbase_hwcnt_metadata *metadata;
 	const u64 *dump_src = src;
@@ -588,7 +544,8 @@ int kbase_hwcnt_csf_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 
 	metadata = dst->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		const size_t hdr_cnt = kbase_hwcnt_metadata_block_headers_count(metadata, blk);
 		const size_t ctr_cnt = kbase_hwcnt_metadata_block_counters_count(metadata, blk);
 		const uint64_t blk_type = kbase_hwcnt_metadata_block_type(metadata, blk);
@@ -596,7 +553,9 @@ int kbase_hwcnt_csf_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 		blk_stt_t *dst_blk_stt =
 			kbase_hwcnt_dump_buffer_block_state_instance(dst, blk, blk_inst);
 
-		/* Skip block if no values in the destination block are enabled. */
+		/*
+		 * Skip block if no values in the destination block are enabled.
+		 */
 		if (kbase_hwcnt_enable_map_block_enabled(dst_enable_map, blk, blk_inst)) {
 			u64 *dst_blk = kbase_hwcnt_dump_buffer_block_instance(dst, blk, blk_inst);
 			const u64 *src_blk = dump_src + src_offset;
@@ -691,7 +650,8 @@ void kbase_hwcnt_gpu_enable_map_to_physical(struct kbase_hwcnt_physical_enable_m
 
 	metadata = src->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		const u64 blk_type = kbase_hwcnt_metadata_block_type(metadata, blk);
 		const u64 *blk_map = kbase_hwcnt_enable_map_block_instance(src, blk, blk_inst);
 		const size_t map_stride =
@@ -788,32 +748,15 @@ void kbase_hwcnt_gpu_set_to_physical(enum kbase_hwcnt_physical_set *dst, enum kb
 
 void kbase_hwcnt_gpu_enable_map_from_physical(struct kbase_hwcnt_enable_map *dst,
 					      const struct kbase_hwcnt_physical_enable_map *src)
-{
-	struct kbase_hwcnt_enable_cm cm = {};
-
-	if (WARN_ON(!src) || WARN_ON(!dst))
-		return;
-
-	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->fe_bm, &cm.fe_bm[EM_LO],
-							 &cm.fe_bm[EM_HI]);
-	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->shader_bm, &cm.shader_bm[EM_LO],
-							 &cm.shader_bm[EM_HI]);
-	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->tiler_bm, &cm.tiler_bm[EM_LO],
-							 &cm.tiler_bm[EM_HI]);
-	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->mmu_l2_bm, &cm.mmu_l2_bm[EM_LO],
-							 &cm.mmu_l2_bm[EM_HI]);
-	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->fw_bm, &cm.fw_bm[EM_LO],
-							 &cm.fw_bm[EM_HI]);
-	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->csg_bm, &cm.csg_bm[EM_LO],
-							 &cm.csg_bm[EM_HI]);
-
-	kbase_hwcnt_gpu_enable_map_from_cm(dst, &cm);
-}
-
-void kbase_hwcnt_gpu_enable_map_from_cm(struct kbase_hwcnt_enable_map *dst,
-					const struct kbase_hwcnt_enable_cm *src)
 {
 	const struct kbase_hwcnt_metadata *metadata;
+
+	u64 fe_bm[EM_COUNT] = { 0 };
+	u64 shader_bm[EM_COUNT] = { 0 };
+	u64 tiler_bm[EM_COUNT] = { 0 };
+	u64 mmu_l2_bm[EM_COUNT] = { 0 };
+	u64 fw_bm[EM_COUNT] = { 0 };
+	u64 csg_bm[EM_COUNT] = { 0 };
 	size_t blk, blk_inst;
 
 	if (WARN_ON(!src) || WARN_ON(!dst))
@@ -821,7 +764,19 @@ void kbase_hwcnt_gpu_enable_map_from_cm(struct kbase_hwcnt_enable_map *dst,
 
 	metadata = dst->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->fe_bm, &fe_bm[EM_LO], &fe_bm[EM_HI]);
+	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->shader_bm, &shader_bm[EM_LO],
+							 &shader_bm[EM_HI]);
+	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->tiler_bm, &tiler_bm[EM_LO],
+							 &tiler_bm[EM_HI]);
+	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->mmu_l2_bm, &mmu_l2_bm[EM_LO],
+							 &mmu_l2_bm[EM_HI]);
+	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->fw_bm, &fw_bm[EM_LO], &fw_bm[EM_HI]);
+	kbasep_hwcnt_backend_gpu_block_map_from_physical(src->csg_bm, &csg_bm[EM_LO],
+							 &csg_bm[EM_HI]);
+
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		const u64 blk_type = kbase_hwcnt_metadata_block_type(metadata, blk);
 		u64 *blk_map = kbase_hwcnt_enable_map_block_instance(dst, blk, blk_inst);
 		const size_t map_stride =
@@ -851,36 +806,36 @@ void kbase_hwcnt_gpu_enable_map_from_cm(struct kbase_hwcnt_enable_map *dst,
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_FE2:
 				fallthrough;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_FE3:
-				blk_map[map_idx] = src->fe_bm[map_idx];
+				blk_map[map_idx] = fe_bm[map_idx];
 				break;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_TILER:
-				blk_map[map_idx] = src->tiler_bm[map_idx];
+				blk_map[map_idx] = tiler_bm[map_idx];
 				break;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC:
 				fallthrough;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC2:
 				fallthrough;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC3:
-				blk_map[map_idx] = src->shader_bm[map_idx];
+				blk_map[map_idx] = shader_bm[map_idx];
 				break;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_MEMSYS:
 				fallthrough;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_MEMSYS2:
-				blk_map[map_idx] = src->mmu_l2_bm[map_idx];
+				blk_map[map_idx] = mmu_l2_bm[map_idx];
 				break;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_FW:
 				fallthrough;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_FW2:
 				fallthrough;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_FW3:
-				blk_map[map_idx] = src->fw_bm[map_idx];
+				blk_map[map_idx] = fw_bm[map_idx];
 				break;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_CSG:
 				fallthrough;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_CSG2:
 				fallthrough;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_CSG3:
-				blk_map[map_idx] = src->csg_bm[map_idx];
+				blk_map[map_idx] = csg_bm[map_idx];
 				break;
 			default:
 				WARN(true, "Invalid block type %llu", blk_type);
@@ -900,7 +855,8 @@ void kbase_hwcnt_gpu_patch_dump_headers(struct kbase_hwcnt_dump_buffer *buf,
 
 	metadata = buf->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		u64 *buf_blk = kbase_hwcnt_dump_buffer_block_instance(buf, blk, blk_inst);
 		const u64 *blk_map =
 			kbase_hwcnt_enable_map_block_instance(enable_map, blk, blk_inst);
diff --git a/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_gpu.h b/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_gpu.h
index 896f1389eb37..2f500fdb2237 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_gpu.h
+++ b/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_gpu.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -138,24 +138,6 @@ struct kbase_hwcnt_physical_enable_map {
 	u32 csg_bm;
 };
 
-/**
- * struct kbase_hwcnt_enable_cm - 128-bit enable counter masks.
- * @fe_bm:     Front end (JM/CSHW) counters selection bitmask.
- * @shader_bm: Shader counters selection bitmask.
- * @tiler_bm:  Tiler counters selection bitmask.
- * @mmu_l2_bm: MMU_L2 counters selection bitmask.
- * @fw_bm: CSF firmware counters selection bitmask.
- * @csg_bm: CSF CSG counters selection bitmask.
- */
-struct kbase_hwcnt_enable_cm {
-	u64 fe_bm[2];
-	u64 shader_bm[2];
-	u64 tiler_bm[2];
-	u64 mmu_l2_bm[2];
-	u64 fw_bm[2];
-	u64 csg_bm[2];
-};
-
 /*
  * Values for Hardware Counter SET_SELECT value.
  * Directly passed to HW.
@@ -169,7 +151,7 @@ enum kbase_hwcnt_physical_set {
 /**
  * struct kbase_hwcnt_gpu_info - Information about hwcnt blocks on the GPUs.
  * @l2_count:                L2 cache count.
- * @sc_core_mask:            Shader core mask. May be sparse.
+ * @core_mask:               Shader core mask. May be sparse.
  * @clk_cnt:                 Number of clock domains available.
  * @csg_cnt:                 Number of CSGs available.
  * @prfcnt_values_per_block: Total entries (header + counters) of performance
@@ -178,7 +160,7 @@ enum kbase_hwcnt_physical_set {
  */
 struct kbase_hwcnt_gpu_info {
 	size_t l2_count;
-	u64 sc_core_mask;
+	u64 core_mask;
 	u8 clk_cnt;
 	u8 csg_cnt;
 	size_t prfcnt_values_per_block;
@@ -261,6 +243,13 @@ int kbase_hwcnt_jm_metadata_create(const struct kbase_hwcnt_gpu_info *info,
 				   const struct kbase_hwcnt_metadata **out_metadata,
 				   size_t *out_dump_bytes);
 
+/**
+ * kbase_hwcnt_jm_metadata_destroy() - Destroy JM GPU hardware counter metadata.
+ *
+ * @metadata: Pointer to metadata to destroy.
+ */
+void kbase_hwcnt_jm_metadata_destroy(const struct kbase_hwcnt_metadata *metadata);
+
 /**
  * kbase_hwcnt_csf_metadata_create() - Create hardware counter metadata for the
  *                                     CSF GPUs.
@@ -275,24 +264,27 @@ int kbase_hwcnt_csf_metadata_create(const struct kbase_hwcnt_gpu_info *info,
 				    enum kbase_hwcnt_set counter_set,
 				    const struct kbase_hwcnt_metadata **out_metadata);
 
+/**
+ * kbase_hwcnt_csf_metadata_destroy() - Destroy CSF GPU hardware counter
+ *                                      metadata.
+ * @metadata: Pointer to metadata to destroy.
+ */
+void kbase_hwcnt_csf_metadata_destroy(const struct kbase_hwcnt_metadata *metadata);
+
 /**
  * kbase_hwcnt_jm_dump_get() - Copy or accumulate enabled counters from the raw
  *                             dump buffer in src into the dump buffer
  *                             abstraction in dst.
- * @dst:             Non-NULL pointer to destination dump buffer.
- * @src:             Non-NULL pointer to source raw dump buffer, of same length
- *                   as dump_buf_bytes in the metadata of destination dump
- *                   buffer.
- * @dst_enable_map:  Non-NULL pointer to enable map specifying enabled values.
- * @pm_core_mask:    PM state synchronized shaders core mask with the dump.
- * @debug_core_mask: User-set mask of cores to be used by the GPU.
- * @max_l2_slices:   Maximum number of L2 slices allocated to the GPU (non
- *                   virtualised platforms) or resource group (virtualized
- *                   platforms).
- * @curr_config:     Current allocated hardware resources to correctly map the
- *                   source raw dump buffer to the destination dump buffer.
- * @accumulate:      True if counters in source should be accumulated into
- *                   destination, rather than copied.
+ * @dst:            Non-NULL pointer to destination dump buffer.
+ * @src:            Non-NULL pointer to source raw dump buffer, of same length
+ *                  as dump_buf_bytes in the metadata of destination dump
+ *                  buffer.
+ * @dst_enable_map: Non-NULL pointer to enable map specifying enabled values.
+ * @pm_core_mask:   PM state synchronized shaders core mask with the dump.
+ * @curr_config:    Current allocated hardware resources to correctly map the
+ *                  source raw dump buffer to the destination dump buffer.
+ * @accumulate:     True if counters in source should be accumulated into
+ *                  destination, rather than copied.
  *
  * The dst and dst_enable_map MUST have been created from the same metadata as
  * returned from the call to kbase_hwcnt_jm_metadata_create as was used to get
@@ -302,23 +294,22 @@ int kbase_hwcnt_csf_metadata_create(const struct kbase_hwcnt_gpu_info *info,
  */
 int kbase_hwcnt_jm_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 			    const struct kbase_hwcnt_enable_map *dst_enable_map,
-			    const u64 pm_core_mask, u64 debug_core_mask, size_t max_l2_slices,
+			    const u64 pm_core_mask,
 			    const struct kbase_hwcnt_curr_config *curr_config, bool accumulate);
 
 /**
  * kbase_hwcnt_csf_dump_get() - Copy or accumulate enabled counters from the raw
  *                              dump buffer in src into the dump buffer
  *                              abstraction in dst.
- * @dst:                      Non-NULL pointer to destination dump buffer.
- * @src:                      Non-NULL pointer to source raw dump buffer, of same length
- *                            as dump_buf_bytes in the metadata of dst dump buffer.
- * @src_block_stt:            Non-NULL pointer to source block state buffer.
- * @dst_enable_map:           Non-NULL pointer to enable map specifying enabled values.
- * @num_l2_slices:            Current number of L2 slices allocated to the GPU.
- * @powered_shader_core_mask: The common mask between the debug_core_mask
- *                            and the shader_present_bitmap.
- * @accumulate:               True if counters in src should be accumulated into
- *                            destination, rather than copied.
+ * @dst:                   Non-NULL pointer to destination dump buffer.
+ * @src:                   Non-NULL pointer to source raw dump buffer, of same length
+ *                         as dump_buf_bytes in the metadata of dst dump buffer.
+ * @src_block_stt:         Non-NULL pointer to source block state buffer.
+ * @dst_enable_map:        Non-NULL pointer to enable map specifying enabled values.
+ * @num_l2_slices:         Current number of L2 slices allocated to the GPU.
+ * @shader_present_bitmap: Current shader-present bitmap that is allocated to the GPU.
+ * @accumulate:            True if counters in src should be accumulated into
+ *                         destination, rather than copied.
  *
  * The dst and dst_enable_map MUST have been created from the same metadata as
  * returned from the call to kbase_hwcnt_csf_metadata_create as was used to get
@@ -329,7 +320,7 @@ int kbase_hwcnt_jm_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 int kbase_hwcnt_csf_dump_get(struct kbase_hwcnt_dump_buffer *dst, u64 *src,
 			     blk_stt_t *src_block_stt,
 			     const struct kbase_hwcnt_enable_map *dst_enable_map,
-			     size_t num_l2_slices, u64 powered_shader_core_mask, bool accumulate);
+			     size_t num_l2_slices, u64 shader_present_bitmap, bool accumulate);
 
 /**
  * kbase_hwcnt_backend_gpu_block_map_to_physical() - Convert from a block
@@ -429,24 +420,4 @@ void kbase_hwcnt_gpu_enable_map_from_physical(struct kbase_hwcnt_enable_map *dst
 void kbase_hwcnt_gpu_patch_dump_headers(struct kbase_hwcnt_dump_buffer *buf,
 					const struct kbase_hwcnt_enable_map *enable_map);
 
-bool kbase_hwcnt_is_block_type_shader(const enum kbase_hwcnt_gpu_v5_block_type blk_type);
-
-bool kbase_hwcnt_is_block_type_memsys(const enum kbase_hwcnt_gpu_v5_block_type blk_type);
-
-bool kbase_hwcnt_is_block_type_tiler(const enum kbase_hwcnt_gpu_v5_block_type blk_type);
-
-bool kbase_hwcnt_is_block_type_fe(const enum kbase_hwcnt_gpu_v5_block_type blk_type);
-
-/**
- * kbase_hwcnt_gpu_enable_map_from_cm() - Builds enable map abstraction from
- *                                        counter selection bitmasks.
- * @dst: Non-NULL pointer to destination enable map abstraction.
- * @src: Non-NULL pointer to source counter selection bitmasks.
- *
- * The dst must have been created from a metadata returned from a call to
- * kbase_hwcnt_jm_metadata_create or kbase_hwcnt_csf_metadata_create.
- */
-void kbase_hwcnt_gpu_enable_map_from_cm(struct kbase_hwcnt_enable_map *dst,
-					const struct kbase_hwcnt_enable_cm *src);
-
 #endif /* _KBASE_HWCNT_GPU_H_ */
diff --git a/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_types.c b/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_types.c
index 3d2fd5e088da..e7f6743f1fb1 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_types.c
+++ b/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_types.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -125,9 +125,6 @@ int kbase_hwcnt_metadata_create(const struct kbase_hwcnt_description *desc,
 
 void kbase_hwcnt_metadata_destroy(const struct kbase_hwcnt_metadata *metadata)
 {
-	if (!metadata)
-		return;
-
 	kfree(metadata);
 }
 
@@ -211,7 +208,8 @@ void kbase_hwcnt_dump_buffer_zero(struct kbase_hwcnt_dump_buffer *dst,
 
 	metadata = dst->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		u64 *dst_blk;
 		size_t val_cnt;
 
@@ -250,7 +248,8 @@ void kbase_hwcnt_dump_buffer_zero_non_enabled(struct kbase_hwcnt_dump_buffer *ds
 
 	metadata = dst->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		u64 *dst_blk = kbase_hwcnt_dump_buffer_block_instance(dst, blk, blk_inst);
 		blk_stt_t *dst_blk_stt =
 			kbase_hwcnt_dump_buffer_block_state_instance(dst, blk, blk_inst);
@@ -292,7 +291,8 @@ void kbase_hwcnt_dump_buffer_copy(struct kbase_hwcnt_dump_buffer *dst,
 
 	metadata = dst->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		u64 *dst_blk;
 		const u64 *src_blk;
 		blk_stt_t *dst_blk_stt;
@@ -312,7 +312,8 @@ void kbase_hwcnt_dump_buffer_copy(struct kbase_hwcnt_dump_buffer *dst,
 		kbase_hwcnt_block_state_copy(dst_blk_stt, src_blk_stt);
 	}
 
-	kbase_hwcnt_metadata_for_each_clock(metadata, clk) {
+	kbase_hwcnt_metadata_for_each_clock(metadata, clk)
+	{
 		if (kbase_hwcnt_clk_enable_map_enabled(dst_enable_map->clk_enable_map, clk))
 			dst->clk_cnt_buf[clk] = src->clk_cnt_buf[clk];
 	}
@@ -333,7 +334,8 @@ void kbase_hwcnt_dump_buffer_copy_strict(struct kbase_hwcnt_dump_buffer *dst,
 
 	metadata = dst->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		u64 *dst_blk = kbase_hwcnt_dump_buffer_block_instance(dst, blk, blk_inst);
 		const u64 *src_blk = kbase_hwcnt_dump_buffer_block_instance(src, blk, blk_inst);
 		blk_stt_t *dst_blk_stt =
@@ -356,7 +358,8 @@ void kbase_hwcnt_dump_buffer_copy_strict(struct kbase_hwcnt_dump_buffer *dst,
 			kbase_hwcnt_block_state_set(dst_blk_stt, KBASE_HWCNT_STATE_UNKNOWN);
 	}
 
-	kbase_hwcnt_metadata_for_each_clock(metadata, clk) {
+	kbase_hwcnt_metadata_for_each_clock(metadata, clk)
+	{
 		bool clk_enabled =
 			kbase_hwcnt_clk_enable_map_enabled(dst_enable_map->clk_enable_map, clk);
 
@@ -379,7 +382,8 @@ void kbase_hwcnt_dump_buffer_accumulate(struct kbase_hwcnt_dump_buffer *dst,
 
 	metadata = dst->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		u64 *dst_blk;
 		const u64 *src_blk;
 		blk_stt_t *dst_blk_stt;
@@ -401,7 +405,8 @@ void kbase_hwcnt_dump_buffer_accumulate(struct kbase_hwcnt_dump_buffer *dst,
 		kbase_hwcnt_block_state_accumulate(dst_blk_stt, src_blk_stt);
 	}
 
-	kbase_hwcnt_metadata_for_each_clock(metadata, clk) {
+	kbase_hwcnt_metadata_for_each_clock(metadata, clk)
+	{
 		if (kbase_hwcnt_clk_enable_map_enabled(dst_enable_map->clk_enable_map, clk))
 			dst->clk_cnt_buf[clk] += src->clk_cnt_buf[clk];
 	}
@@ -422,7 +427,8 @@ void kbase_hwcnt_dump_buffer_accumulate_strict(struct kbase_hwcnt_dump_buffer *d
 
 	metadata = dst->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		u64 *dst_blk = kbase_hwcnt_dump_buffer_block_instance(dst, blk, blk_inst);
 		const u64 *src_blk = kbase_hwcnt_dump_buffer_block_instance(src, blk, blk_inst);
 		const u64 *blk_em =
@@ -449,7 +455,8 @@ void kbase_hwcnt_dump_buffer_accumulate_strict(struct kbase_hwcnt_dump_buffer *d
 			kbase_hwcnt_block_state_set(dst_blk_stt, KBASE_HWCNT_STATE_UNKNOWN);
 	}
 
-	kbase_hwcnt_metadata_for_each_clock(metadata, clk) {
+	kbase_hwcnt_metadata_for_each_clock(metadata, clk)
+	{
 		if (kbase_hwcnt_clk_enable_map_enabled(dst_enable_map->clk_enable_map, clk))
 			dst->clk_cnt_buf[clk] += src->clk_cnt_buf[clk];
 		else
@@ -470,12 +477,14 @@ void kbase_hwcnt_dump_buffer_block_state_update(struct kbase_hwcnt_dump_buffer *
 
 	metadata = dst->metadata;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		if (kbase_hwcnt_metadata_block_instance_avail(metadata, blk, blk_inst) &&
 		    kbase_hwcnt_enable_map_block_enabled(dst_enable_map, blk, blk_inst)) {
 			blk_stt_t *dst_blk_stt =
 				kbase_hwcnt_dump_buffer_block_state_instance(dst, blk, blk_inst);
 
+			/* Block is available and enabled, so update the block state */
 			*dst_blk_stt |= blk_stt_val;
 		}
 	}
diff --git a/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_types.h b/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_types.h
index 45f67f7c9a1b..16f68ead170e 100644
--- a/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_types.h
+++ b/drivers/gpu/arm/bifrost/hwcnt/mali_kbase_hwcnt_types.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2018-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -19,6 +19,65 @@
  *
  */
 
+/*
+ * Hardware counter types.
+ * Contains structures for describing the physical layout of hardware counter
+ * dump buffers and enable maps within a system.
+ *
+ * Also contains helper functions for manipulation of these dump buffers and
+ * enable maps.
+ *
+ * Through use of these structures and functions, hardware counters can be
+ * enabled, copied, accumulated, and generally manipulated in a generic way,
+ * regardless of the physical counter dump layout.
+ *
+ * Terminology:
+ *
+ * Hardware Counter System:
+ *   A collection of hardware counter blocks, making a full hardware counter
+ *   system.
+ * Hardware Counter Block:
+ *   A block of hardware counters (e.g. shader block, tiler block).
+ * Hardware Counter Block Instance:
+ *   An instance of a Hardware Counter Block (e.g. an MP4 GPU might have
+ *   4 shader block instances).
+ *
+ * Block Header:
+ *   A header value inside a counter block. Headers don't count anything,
+ *   so it is only valid to copy or zero them. Headers are always the first
+ *   values in the block.
+ * Block Counter:
+ *   A counter value inside a counter block. Counters can be zeroed, copied,
+ *   or accumulated. Counters are always immediately after the headers in the
+ *   block.
+ * Block Value:
+ *   A catch-all term for block headers and block counters.
+ *
+ * Enable Map:
+ *   An array of u64 bitfields, where each bit either enables exactly one
+ *   block value, or is unused (padding). Note that this is derived from
+ *   the client configuration, and is not obtained from the hardware.
+ * Dump Buffer:
+ *   An array of u64 values, where each u64 corresponds either to one block
+ *   value, or is unused (padding).
+ * Block State Buffer:
+ *   An array of blk_stt_t values, where each blk_stt_t corresponds to one block
+ *   instance and is used to track the on/off power state transitions, as well has
+ *   hardware resource availability, and whether the block was operating
+ *   in normal or protected mode.
+ * Availability Mask:
+ *   A bitfield, where each bit corresponds to whether a block instance is
+ *   physically available (e.g. an MP3 GPU may have a sparse core mask of
+ *   0b1011, meaning it only has 3 cores but for hardware counter dumps has the
+ *   same dump buffer layout as an MP4 GPU with a core mask of 0b1111. In this
+ *   case, the availability mask might be 0b1011111 (the exact layout will
+ *   depend on the specific hardware architecture), with the 3 extra early bits
+ *   corresponding to other block instances in the hardware counter system).
+ * Metadata:
+ *   Structure describing the physical layout of the enable map and dump buffers
+ *   for a specific hardware counter system.
+ */
+
 #ifndef _KBASE_HWCNT_TYPES_H_
 #define _KBASE_HWCNT_TYPES_H_
 
@@ -262,7 +321,7 @@ static inline void kbase_hwcnt_cp_avail_mask(struct kbase_hwcnt_avail_mask *dst_
  *                  placed is expected to be fully contained by the array of bitmask elements.
  * @length_in_bits: The length of the value being placed in the bitmask. Assumed to be no more
  *                  than 64 bits in length.
- * @value:          The source value to be written into the bitmask.
+ * @value:          Pointer to the source value to be written into the bitmask.
  */
 static inline void kbase_hwcnt_set_avail_mask_bits(struct kbase_hwcnt_avail_mask *avail_mask,
 						   size_t offset_in_bits, size_t length_in_bits,
@@ -870,7 +929,8 @@ kbase_hwcnt_enable_map_any_enabled(const struct kbase_hwcnt_enable_map *enable_m
 	if (enable_map->metadata->clk_cnt > 0 && (enable_map->clk_enable_map & clk_enable_map_mask))
 		return true;
 
-	kbase_hwcnt_metadata_for_each_block(enable_map->metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(enable_map->metadata, blk, blk_inst)
+	{
 		if (kbase_hwcnt_enable_map_block_enabled(enable_map, blk, blk_inst))
 			return true;
 	}
diff --git a/drivers/gpu/arm/bifrost/ipa/backend/mali_kbase_ipa_counter_common_csf.c b/drivers/gpu/arm/bifrost/ipa/backend/mali_kbase_ipa_counter_common_csf.c
index 323dd3bceaea..22ba78dee7d4 100644
--- a/drivers/gpu/arm/bifrost/ipa/backend/mali_kbase_ipa_counter_common_csf.c
+++ b/drivers/gpu/arm/bifrost/ipa/backend/mali_kbase_ipa_counter_common_csf.c
@@ -68,7 +68,7 @@ static s64 kbase_ipa_group_energy(s32 coeff, u64 counter_value)
 	/* Range: 0 < counter_value < 2^38 */
 
 	/* Range: -2^59 < ret < 2^59 (as -2^21 < coeff < 2^21) */
-	return (s64)counter_value * (s64)coeff;
+	return counter_value * (s64)coeff;
 }
 
 /**
@@ -183,7 +183,7 @@ static int calculate_coeff(struct kbase_ipa_counter_model_data *model_data,
 
 	/* Range: 0 <= coeff < 2^63 */
 	if (total_energy >= 0)
-		coeff = (u64)total_energy;
+		coeff = total_energy;
 	else
 		dev_dbg(model_data->kbdev->dev, "Energy value came negative as %lld", total_energy);
 
@@ -224,7 +224,7 @@ static int calculate_coeff(struct kbase_ipa_counter_model_data *model_data,
 	/* Scale by user-specified integer factor.
 	 * Range: 0 <= coeff_mul < 2^43
 	 */
-	coeff_mul = coeff * (u64)model_data->scaling_factor;
+	coeff_mul = coeff * model_data->scaling_factor;
 
 	/* The power models have results with units
 	 * mW/(MHz V^2), i.e. nW/(Hz V^2). With precision of 1/1000000, this
diff --git a/drivers/gpu/arm/bifrost/ipa/backend/mali_kbase_ipa_counter_common_jm.c b/drivers/gpu/arm/bifrost/ipa/backend/mali_kbase_ipa_counter_common_jm.c
index a716e15b3df3..6e2976d9bbf9 100644
--- a/drivers/gpu/arm/bifrost/ipa/backend/mali_kbase_ipa_counter_common_jm.c
+++ b/drivers/gpu/arm/bifrost/ipa/backend/mali_kbase_ipa_counter_common_jm.c
@@ -211,7 +211,7 @@ int kbase_ipa_vinstr_dynamic_coeff(struct kbase_ipa_model *model, u32 *coeffp)
 
 	/* Range: 0 <= coeff < 2^57 */
 	if (energy > 0)
-		coeff = (u64)energy;
+		coeff = energy;
 
 	/* Range: 0 <= coeff < 2^57 (because active_cycles >= 1). However, this
 	 * can be constrained further: Counter values can only be increased by
@@ -244,7 +244,7 @@ int kbase_ipa_vinstr_dynamic_coeff(struct kbase_ipa_model *model, u32 *coeffp)
 	/* Scale by user-specified integer factor.
 	 * Range: 0 <= coeff_mul < 2^57
 	 */
-	coeff_mul = coeff * (u64)model_data->scaling_factor;
+	coeff_mul = coeff * model_data->scaling_factor;
 
 	/* The power models have results with units
 	 * mW/(MHz V^2), i.e. nW/(Hz V^2). With precision of 1/1000000, this
diff --git a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.c b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.c
index d432dc8fc8e2..e3b61d5afade 100644
--- a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.c
+++ b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.c
@@ -174,7 +174,7 @@ int kbase_ipa_model_add_param_string(struct kbase_ipa_model *model, const char *
 	of_node_put(model_dt_node);
 
 	if (err && dt_required) {
-		strscpy(addr, "", size);
+		strncpy(addr, "", size - 1);
 		dev_warn(model->kbdev->dev, "Error %d, no DT entry: %s.%s = \'%s\'\n", err,
 			 model->ops->name, name, addr);
 		err = 0;
@@ -182,7 +182,7 @@ int kbase_ipa_model_add_param_string(struct kbase_ipa_model *model, const char *
 	} else if (err && !dt_required) {
 		origin = "default";
 	} else /* !err */ {
-		strscpy(addr, string_prop_value, size);
+		strncpy(addr, string_prop_value, size - 1);
 		origin = "DT";
 	}
 
diff --git a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_debugfs.c b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_debugfs.c
index 9305747ff472..97ab8b8e39c1 100644
--- a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_debugfs.c
+++ b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_debugfs.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2017-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2017-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -96,7 +96,7 @@ static ssize_t param_string_set(struct file *file, const char __user *user_buf,
 	struct kbase_ipa_model_param *param = file->private_data;
 	struct kbase_ipa_model *model = param->model;
 	char *old_str = NULL;
-	ssize_t ret = (ssize_t)count;
+	ssize_t ret = count;
 	size_t buf_size;
 	int err;
 
@@ -115,7 +115,7 @@ static ssize_t param_string_set(struct file *file, const char __user *user_buf,
 		goto end;
 	}
 
-	buf_size = min(size_sub(param->size, 1), count);
+	buf_size = min(param->size - 1, count);
 	if (copy_from_user(param->addr.str, user_buf, buf_size)) {
 		ret = -EFAULT;
 		goto end;
diff --git a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_simple.c b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_simple.c
index d8a356c3e8c2..71dbc27fc025 100644
--- a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_simple.c
+++ b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_simple.c
@@ -299,9 +299,6 @@ static int kbase_simple_power_model_recalculate(struct kbase_ipa_model *model)
 
 	if (!strnlen(model_data->tz_name, sizeof(model_data->tz_name))) {
 		model_data->gpu_tz = NULL;
-		dev_warn(model->kbdev->dev,
-			 "No thermal zone specified, will use the default temperature value of %u",
-			 FALLBACK_STATIC_TEMPERATURE);
 	} else {
 		char tz_name[THERMAL_NAME_LENGTH];
 		u32 string_len = strscpy(tz_name, model_data->tz_name, sizeof(tz_name));
diff --git a/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_defs.h b/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_defs.h
index 23e919314333..ddd58eb64b82 100644
--- a/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_defs.h
+++ b/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_defs.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -29,8 +29,6 @@
 
 #include "mali_kbase_js_defs.h"
 
-#include <linux/version_compat_defs.h>
-
 /* Dump Job slot trace on error (only active if KBASE_KTRACE_ENABLE != 0) */
 #define KBASE_KTRACE_DUMP_ON_JOB_SLOT_ERROR 1
 
@@ -72,29 +70,29 @@
 #define KBASE_DISABLE_SCHEDULING_HARD_STOPS 0
 
 /* Atom has been previously soft-stopped */
-#define KBASE_KATOM_FLAG_BEEN_SOFT_STOPPED (1U << 1)
+#define KBASE_KATOM_FLAG_BEEN_SOFT_STOPPED (1 << 1)
 /* Atom has been previously retried to execute */
-#define KBASE_KATOM_FLAGS_RERUN (1U << 2)
+#define KBASE_KATOM_FLAGS_RERUN (1 << 2)
 /* Atom submitted with JOB_CHAIN_FLAG bit set in JS_CONFIG_NEXT register, helps
  * to disambiguate short-running job chains during soft/hard stopping of jobs
  */
-#define KBASE_KATOM_FLAGS_JOBCHAIN (1U << 3)
+#define KBASE_KATOM_FLAGS_JOBCHAIN (1 << 3)
 /* Atom has been previously hard-stopped. */
-#define KBASE_KATOM_FLAG_BEEN_HARD_STOPPED (1U << 4)
+#define KBASE_KATOM_FLAG_BEEN_HARD_STOPPED (1 << 4)
 /* Atom has caused us to enter disjoint state */
-#define KBASE_KATOM_FLAG_IN_DISJOINT (1U << 5)
+#define KBASE_KATOM_FLAG_IN_DISJOINT (1 << 5)
 /* Atom blocked on cross-slot dependency */
-#define KBASE_KATOM_FLAG_X_DEP_BLOCKED (1U << 7)
+#define KBASE_KATOM_FLAG_X_DEP_BLOCKED (1 << 7)
 /* Atom has fail dependency on cross-slot dependency */
-#define KBASE_KATOM_FLAG_FAIL_BLOCKER (1U << 8)
+#define KBASE_KATOM_FLAG_FAIL_BLOCKER (1 << 8)
 /* Atom is currently in the list of atoms blocked on cross-slot dependencies */
-#define KBASE_KATOM_FLAG_JSCTX_IN_X_DEP_LIST (1U << 9)
+#define KBASE_KATOM_FLAG_JSCTX_IN_X_DEP_LIST (1 << 9)
 /* Atom requires GPU to be in protected mode */
-#define KBASE_KATOM_FLAG_PROTECTED (1U << 11)
+#define KBASE_KATOM_FLAG_PROTECTED (1 << 11)
 /* Atom has been stored in runnable_tree */
-#define KBASE_KATOM_FLAG_JSCTX_IN_TREE (1U << 12)
+#define KBASE_KATOM_FLAG_JSCTX_IN_TREE (1 << 12)
 /* Atom is waiting for L2 caches to power up in order to enter protected mode */
-#define KBASE_KATOM_FLAG_HOLDING_L2_REF_PROT (1U << 13)
+#define KBASE_KATOM_FLAG_HOLDING_L2_REF_PROT (1 << 13)
 
 /* SW related flags about types of JS_COMMAND action
  * NOTE: These must be masked off by JS_COMMAND_MASK
@@ -133,9 +131,6 @@
  * @JM_DEFAULT_JS_FREE_TIMEOUT: Maximum timeout to wait for JS_COMMAND_NEXT
  *                              to be updated on HW side so a Job Slot is
  *                              considered free.
- * @KBASE_PRFCNT_ACTIVE_TIMEOUT: Waiting time for prfcnt to be ready.
- * @KBASE_CLEAN_CACHE_TIMEOUT: Waiting time for cache flush to complete.
- * @KBASE_AS_INACTIVE_TIMEOUT: Waiting time for MCU address space to become inactive.
  * @KBASE_TIMEOUT_SELECTOR_COUNT: Number of timeout selectors.
  * @KBASE_DEFAULT_TIMEOUT: Fallthrough in case an invalid timeout is
  *                         passed.
@@ -143,9 +138,7 @@
 enum kbase_timeout_selector {
 	MMU_AS_INACTIVE_WAIT_TIMEOUT,
 	JM_DEFAULT_JS_FREE_TIMEOUT,
-	KBASE_PRFCNT_ACTIVE_TIMEOUT,
-	KBASE_CLEAN_CACHE_TIMEOUT,
-	KBASE_AS_INACTIVE_TIMEOUT,
+
 	/* Must be the last in the enum */
 	KBASE_TIMEOUT_SELECTOR_COUNT,
 	KBASE_DEFAULT_TIMEOUT = JM_DEFAULT_JS_FREE_TIMEOUT
@@ -308,11 +301,11 @@ enum kbase_atom_gpu_rb_state {
  *                      powered down and GPU shall come out of fully
  *                      coherent mode before entering protected mode.
  * @KBASE_ATOM_ENTER_PROTECTED_SET_COHERENCY: Prepare coherency change;
- *                      for KBASE_HW_ISSUE_TGOX_R1_1234 also request L2 power on
+ *                      for BASE_HW_ISSUE_TGOX_R1_1234 also request L2 power on
  *                      so that coherency register contains correct value when
  *                      GPU enters protected mode.
  * @KBASE_ATOM_ENTER_PROTECTED_FINISHED: End state; for
- *                      KBASE_HW_ISSUE_TGOX_R1_1234 check
+ *                      BASE_HW_ISSUE_TGOX_R1_1234 check
  *                      that L2 is powered up and switch GPU to protected mode.
  */
 enum kbase_atom_enter_protected_state {
@@ -500,6 +493,10 @@ enum kbase_atom_exit_protected_state {
  *                         is snapshot of the age_count counter in kbase
  *                         context.
  * @jobslot: Job slot to use when BASE_JD_REQ_JOB_SLOT is specified.
+ * @renderpass_id:Renderpass identifier used to associate an atom that has
+ *                 BASE_JD_REQ_START_RENDERPASS set in its core requirements
+ *                 with an atom that has BASE_JD_REQ_END_RENDERPASS set.
+ * @jc_fragment:          Set of GPU fragment job chains
  */
 struct kbase_jd_atom {
 	struct work_struct work;
@@ -529,7 +526,13 @@ struct kbase_jd_atom {
 		/* Use the functions/API defined in mali_kbase_fence.h to
 		 * when working with this sub struct
 		 */
+#if IS_ENABLED(CONFIG_SYNC_FILE)
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+		struct fence *fence_in;
+#else
 		struct dma_fence *fence_in;
+#endif
+#endif
 		/* This points to the dma-buf output fence for this atom. If
 		 * this is NULL then there is no fence for this atom and the
 		 * following fields related to dma_fence may have invalid data.
@@ -541,12 +544,20 @@ struct kbase_jd_atom {
 		 * regardless of the event_code of the katom (signal also on
 		 * failure).
 		 */
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+		struct fence *fence;
+#else
 		struct dma_fence *fence;
+#endif
 
 		/* This is the callback object that is registered for the fence_in.
 		 * The callback is invoked when the fence_in is signaled.
 		 */
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+		struct fence_cb fence_cb;
+#else
 		struct dma_fence_cb fence_cb;
+#endif
 		bool fence_cb_added;
 
 		unsigned int context;
@@ -560,6 +571,8 @@ struct kbase_jd_atom {
 	enum base_jd_event_code event_code;
 	base_jd_core_req core_req;
 	u8 jobslot;
+	u8 renderpass_id;
+	struct base_jd_fragment jc_fragment;
 
 	u32 ticks;
 	int sched_priority;
@@ -670,6 +683,71 @@ static inline bool kbase_jd_atom_is_earlier(const struct kbase_jd_atom *katom_a,
 
 #define KBASE_JD_DEP_QUEUE_SIZE 256
 
+/**
+ * enum kbase_jd_renderpass_state - State of a renderpass
+ * @KBASE_JD_RP_COMPLETE: Unused or completed renderpass. Can only transition to
+ *                        START.
+ * @KBASE_JD_RP_START:    Renderpass making a first attempt at tiling.
+ *                        Can transition to PEND_OOM or COMPLETE.
+ * @KBASE_JD_RP_PEND_OOM: Renderpass whose first attempt at tiling used too much
+ *                        memory and has a soft-stop pending. Can transition to
+ *                        OOM or COMPLETE.
+ * @KBASE_JD_RP_OOM:      Renderpass whose first attempt at tiling used too much
+ *                        memory and therefore switched to incremental
+ *                        rendering. The fragment job chain is forced to run.
+ *                        Can only transition to RETRY.
+ * @KBASE_JD_RP_RETRY:    Renderpass making a second or subsequent attempt at
+ *                        tiling. Can transition to RETRY_PEND_OOM or COMPLETE.
+ * @KBASE_JD_RP_RETRY_PEND_OOM: Renderpass whose second or subsequent attempt at
+ *                              tiling used too much memory again and has a
+ *                              soft-stop pending. Can transition to RETRY_OOM
+ *                              or COMPLETE.
+ * @KBASE_JD_RP_RETRY_OOM: Renderpass whose second or subsequent attempt at
+ *                         tiling used too much memory again. The fragment job
+ *                         chain is forced to run. Can only transition to RETRY.
+ *
+ * A state machine is used to control incremental rendering.
+ */
+enum kbase_jd_renderpass_state {
+	KBASE_JD_RP_COMPLETE, /* COMPLETE => START */
+	KBASE_JD_RP_START, /* START => PEND_OOM or COMPLETE */
+	KBASE_JD_RP_PEND_OOM, /* PEND_OOM => OOM or COMPLETE */
+	KBASE_JD_RP_OOM, /* OOM => RETRY */
+	KBASE_JD_RP_RETRY, /* RETRY => RETRY_PEND_OOM or COMPLETE */
+	KBASE_JD_RP_RETRY_PEND_OOM, /* RETRY_PEND_OOM => RETRY_OOM or COMPLETE */
+	KBASE_JD_RP_RETRY_OOM /* RETRY_OOM => RETRY */
+};
+
+/**
+ * struct kbase_jd_renderpass - Data for a renderpass
+ * @state:        Current state of the renderpass. If KBASE_JD_RP_COMPLETE then
+ *                all other members are invalid.
+ *                Both the job dispatcher context and hwaccess_lock must be
+ *                locked to modify this so that it can be read with either
+ *                (or both) locked.
+ * @start_katom:  Address of the atom that is the start of a renderpass.
+ *                Both the job dispatcher context and hwaccess_lock must be
+ *                locked to modify this so that it can be read with either
+ *                (or both) locked.
+ * @end_katom:    Address of the atom that is the end of a renderpass, or NULL
+ *                if that atom hasn't been added to the job scheduler yet.
+ *                The job dispatcher context and hwaccess_lock must be
+ *                locked to modify this so that it can be read with either
+ *                (or both) locked.
+ * @oom_reg_list: A list of region structures which triggered out-of-memory.
+ *                The hwaccess_lock must be locked to access this.
+ *
+ * Atoms tagged with BASE_JD_REQ_START_RENDERPASS or BASE_JD_REQ_END_RENDERPASS
+ * are associated with an object of this type, which is created and maintained
+ * by kbase to keep track of each renderpass.
+ */
+struct kbase_jd_renderpass {
+	enum kbase_jd_renderpass_state state;
+	struct kbase_jd_atom *start_katom;
+	struct kbase_jd_atom *end_katom;
+	struct list_head oom_reg_list;
+};
+
 /**
  * struct kbase_jd_context  - per context object encapsulating all the
  *                            Job dispatcher related state.
@@ -680,6 +758,9 @@ static inline bool kbase_jd_atom_is_earlier(const struct kbase_jd_atom *katom_a,
  * @atoms:                    Array of the objects representing atoms,
  *                            containing the complete state and attributes
  *                            of an atom.
+ * @renderpasses:             Array of renderpass state for incremental
+ *                            rendering, indexed by user-specified renderpass
+ *                            ID.
  * @job_nr:                   Tracks the number of atoms being processed by the
  *                            kbase. This includes atoms that are not tracked by
  *                            scheduler: 'not ready to run' & 'dependency-only'
@@ -729,6 +810,7 @@ struct kbase_jd_context {
 	struct mutex lock;
 	struct kbasep_js_kctx_info sched_info;
 	struct kbase_jd_atom atoms[BASE_JD_ATOM_COUNT];
+	struct kbase_jd_renderpass renderpasses[BASE_JD_RP_COUNT];
 	struct workqueue_struct *job_done_wq;
 
 	wait_queue_head_t zero_jobs_wait;
@@ -775,7 +857,7 @@ struct jsctx_queue {
  * @current_setup:     Stores the MMU configuration for this address space.
  */
 struct kbase_as {
-	unsigned int number;
+	int number;
 	struct workqueue_struct *pf_wq;
 	struct work_struct work_pagefault;
 	struct work_struct work_busfault;
diff --git a/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_js.h b/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_js.h
index 65b54c68d8c7..8955b0473155 100644
--- a/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_js.h
+++ b/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_js.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -567,6 +567,22 @@ bool kbase_js_complete_atom_wq(struct kbase_context *kctx, struct kbase_jd_atom
  */
 struct kbase_jd_atom *kbase_js_complete_atom(struct kbase_jd_atom *katom, ktime_t *end_timestamp);
 
+/**
+ * kbase_js_atom_blocked_on_x_dep - Decide whether to ignore a cross-slot
+ *                                  dependency
+ * @katom:	Pointer to an atom in the slot ringbuffer
+ *
+ * A cross-slot dependency is ignored if necessary to unblock incremental
+ * rendering. If the atom at the start of a renderpass used too much memory
+ * and was soft-stopped then the atom at the end of a renderpass is submitted
+ * to hardware regardless of its dependency on the start-of-renderpass atom.
+ * This can happen multiple times for the same pair of atoms.
+ *
+ * Return: true to block the atom or false to allow it to be submitted to
+ * hardware.
+ */
+bool kbase_js_atom_blocked_on_x_dep(struct kbase_jd_atom *katom);
+
 /**
  * kbase_js_sched - Submit atoms from all available contexts.
  *
@@ -793,7 +809,8 @@ static inline bool
 kbasep_js_has_atom_finished(const struct kbasep_js_atom_retained_state *katom_retained_state)
 {
 	return (bool)(katom_retained_state->event_code != BASE_JD_EVENT_STOPPED &&
-		      katom_retained_state->event_code != BASE_JD_EVENT_REMOVED_FROM_NEXT);
+		      katom_retained_state->event_code != BASE_JD_EVENT_REMOVED_FROM_NEXT &&
+		      katom_retained_state->event_code != BASE_JD_EVENT_END_RP_DONE);
 }
 
 /**
@@ -889,7 +906,7 @@ static inline void kbase_js_runpool_dec_context_count(struct kbase_device *kbdev
  */
 static inline void kbase_js_sched_all(struct kbase_device *kbdev)
 {
-	kbase_js_sched(kbdev, (1U << kbdev->gpu_props.num_job_slots) - 1U);
+	kbase_js_sched(kbdev, (1 << kbdev->gpu_props.num_job_slots) - 1);
 }
 
 extern const int kbasep_js_atom_priority_to_relative[BASE_JD_NR_PRIO_LEVELS];
diff --git a/drivers/gpu/arm/bifrost/mali_base_hwconfig_features.h b/drivers/gpu/arm/bifrost/mali_base_hwconfig_features.h
new file mode 100644
index 000000000000..1f32fc9dd553
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_base_hwconfig_features.h
@@ -0,0 +1,161 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+/* AUTOMATICALLY GENERATED FILE. If you want to amend the issues/features,
+ * please update base/tools/hwconfig_generator/hwc_{issues,features}.py
+ * For more information see base/tools/docs/hwconfig_generator.md
+ */
+
+#ifndef _BASE_HWCONFIG_FEATURES_H_
+#define _BASE_HWCONFIG_FEATURES_H_
+
+enum base_hw_feature {
+	BASE_HW_FEATURE_FLUSH_REDUCTION,
+	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_TLS_HASHING,
+	BASE_HW_FEATURE_THREAD_GROUP_SPLIT,
+	BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_IDVS_GROUP_SIZE,
+	BASE_HW_FEATURE_L2_CONFIG,
+	BASE_HW_FEATURE_L2_SLICE_HASH,
+	BASE_HW_FEATURE_GPU_SLEEP,
+	BASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER,
+	BASE_HW_FEATURE_CORE_FEATURES,
+	BASE_HW_FEATURE_PBHA_HWU,
+	BASE_HW_FEATURE_LARGE_PAGE_ALLOC,
+	BASE_HW_FEATURE_THREAD_TLS_ALLOC,
+	BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_generic[] = {
+	BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tMIx[] = {
+	BASE_HW_FEATURE_THREAD_GROUP_SPLIT, BASE_HW_FEATURE_FLUSH_REDUCTION, BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tHEx[] = {
+	BASE_HW_FEATURE_THREAD_GROUP_SPLIT, BASE_HW_FEATURE_FLUSH_REDUCTION,
+	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE, BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tSIx[] = {
+	BASE_HW_FEATURE_THREAD_GROUP_SPLIT, BASE_HW_FEATURE_FLUSH_REDUCTION,
+	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE, BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tDVx[] = {
+	BASE_HW_FEATURE_THREAD_GROUP_SPLIT, BASE_HW_FEATURE_FLUSH_REDUCTION,
+	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE, BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tNOx[] = {
+	BASE_HW_FEATURE_THREAD_GROUP_SPLIT,   BASE_HW_FEATURE_FLUSH_REDUCTION,
+	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE, BASE_HW_FEATURE_TLS_HASHING,
+	BASE_HW_FEATURE_IDVS_GROUP_SIZE,      BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tGOx[] = {
+	BASE_HW_FEATURE_THREAD_GROUP_SPLIT,   BASE_HW_FEATURE_FLUSH_REDUCTION,
+	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE, BASE_HW_FEATURE_TLS_HASHING,
+	BASE_HW_FEATURE_IDVS_GROUP_SIZE,      BASE_HW_FEATURE_CORE_FEATURES,
+	BASE_HW_FEATURE_THREAD_TLS_ALLOC,     BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tTRx[] = {
+	BASE_HW_FEATURE_FLUSH_REDUCTION,	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_IDVS_GROUP_SIZE,	BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER, BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tNAx[] = {
+	BASE_HW_FEATURE_FLUSH_REDUCTION,	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_IDVS_GROUP_SIZE,	BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER, BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tBEx[] = {
+	BASE_HW_FEATURE_FLUSH_REDUCTION,
+	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_IDVS_GROUP_SIZE,
+	BASE_HW_FEATURE_L2_CONFIG,
+	BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER,
+	BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tBAx[] = {
+	BASE_HW_FEATURE_FLUSH_REDUCTION,
+	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_IDVS_GROUP_SIZE,
+	BASE_HW_FEATURE_L2_CONFIG,
+	BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER,
+	BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tODx[] = {
+	BASE_HW_FEATURE_FLUSH_REDUCTION, BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_L2_CONFIG, BASE_HW_FEATURE_CLEAN_ONLY_SAFE, BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tGRx[] = {
+	BASE_HW_FEATURE_FLUSH_REDUCTION, BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_L2_CONFIG,	 BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_CORE_FEATURES,	 BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tVAx[] = {
+	BASE_HW_FEATURE_FLUSH_REDUCTION, BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_L2_CONFIG,	 BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_CORE_FEATURES,	 BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tTUx[] = {
+	BASE_HW_FEATURE_FLUSH_REDUCTION, BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_L2_CONFIG,	 BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_L2_SLICE_HASH,	 BASE_HW_FEATURE_GPU_SLEEP,
+	BASE_HW_FEATURE_CORE_FEATURES,	 BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tTIx[] = {
+	BASE_HW_FEATURE_FLUSH_REDUCTION,
+	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_L2_CONFIG,
+	BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_L2_SLICE_HASH,
+	BASE_HW_FEATURE_GPU_SLEEP,
+	BASE_HW_FEATURE_CORE_FEATURES,
+	BASE_HW_FEATURE_PBHA_HWU,
+	BASE_HW_FEATURE_END
+};
+
+__attribute__((unused)) static const enum base_hw_feature base_hw_features_tKRx[] = {
+	BASE_HW_FEATURE_FLUSH_REDUCTION,  BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_L2_CONFIG,	  BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_L2_SLICE_HASH,	  BASE_HW_FEATURE_GPU_SLEEP,
+	BASE_HW_FEATURE_CORE_FEATURES,	  BASE_HW_FEATURE_PBHA_HWU,
+	BASE_HW_FEATURE_LARGE_PAGE_ALLOC, BASE_HW_FEATURE_END
+};
+
+
+#endif /* _BASE_HWCONFIG_FEATURES_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_base_hwconfig_issues.h b/drivers/gpu/arm/bifrost/mali_base_hwconfig_issues.h
new file mode 100644
index 000000000000..a61861fcb677
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_base_hwconfig_issues.h
@@ -0,0 +1,497 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+/* AUTOMATICALLY GENERATED FILE. If you want to amend the issues/features,
+ * please update base/tools/hwconfig_generator/hwc_{issues,features}.py
+ * For more information see base/tools/docs/hwconfig_generator.md
+ */
+
+#ifndef _BASE_HWCONFIG_ISSUES_H_
+#define _BASE_HWCONFIG_ISSUES_H_
+
+enum base_hw_issue {
+	BASE_HW_ISSUE_5736,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10682,
+	BASE_HW_ISSUE_11054,
+	BASE_HW_ISSUE_T76X_3953,
+	BASE_HW_ISSUE_TMIX_7891,
+	BASE_HW_ISSUE_TMIX_7940,
+	BASE_HW_ISSUE_TMIX_8042,
+	BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TMIX_8138,
+	BASE_HW_ISSUE_TMIX_8206,
+	BASE_HW_ISSUE_TMIX_8343,
+	BASE_HW_ISSUE_TMIX_8463,
+	BASE_HW_ISSUE_TMIX_8456,
+	BASE_HW_ISSUE_TSIX_1116,
+	BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TMIX_8438,
+	BASE_HW_ISSUE_TNOX_1194,
+	BASE_HW_ISSUE_TGOX_R1_1234,
+	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_TSIX_1792,
+	BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_3076,
+	BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TTRX_3083,
+	BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,
+	BASE_HW_ISSUE_TTRX_3485,
+	BASE_HW_ISSUE_GPU2019_3212,
+	BASE_HW_ISSUE_TURSEHW_1997,
+	BASE_HW_ISSUE_GPU2019_3878,
+	BASE_HW_ISSUE_TURSEHW_2716,
+	BASE_HW_ISSUE_GPU2019_3901,
+	BASE_HW_ISSUE_GPU2021PRO_290,
+	BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_TITANHW_2679,
+	BASE_HW_ISSUE_TITANHW_2922,
+	BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_TITANHW_2952,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((
+	unused)) static const enum base_hw_issue base_hw_issues_generic[] = { BASE_HW_ISSUE_END };
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tMIx_r0p0_05dev0[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_10682,	BASE_HW_ISSUE_11054,
+	BASE_HW_ISSUE_T76X_3953,    BASE_HW_ISSUE_TMIX_7891,	BASE_HW_ISSUE_TMIX_8042,
+	BASE_HW_ISSUE_TMIX_8133,    BASE_HW_ISSUE_TMIX_8138,	BASE_HW_ISSUE_TMIX_8206,
+	BASE_HW_ISSUE_TMIX_8343,    BASE_HW_ISSUE_TMIX_8463,	BASE_HW_ISSUE_TMIX_8456,
+	BASE_HW_ISSUE_TMIX_8438,    BASE_HW_ISSUE_TSIX_2033,	BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_GPU2017_1336, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tMIx_r0p0[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_10682,	BASE_HW_ISSUE_11054,
+	BASE_HW_ISSUE_TMIX_7891,    BASE_HW_ISSUE_TMIX_7940,	BASE_HW_ISSUE_TMIX_8042,
+	BASE_HW_ISSUE_TMIX_8133,    BASE_HW_ISSUE_TMIX_8138,	BASE_HW_ISSUE_TMIX_8206,
+	BASE_HW_ISSUE_TMIX_8343,    BASE_HW_ISSUE_TMIX_8463,	BASE_HW_ISSUE_TMIX_8456,
+	BASE_HW_ISSUE_TMIX_8438,    BASE_HW_ISSUE_TSIX_2033,	BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_GPU2017_1336, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tMIx_r0p1[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_10682,	BASE_HW_ISSUE_11054,
+	BASE_HW_ISSUE_TMIX_7891,    BASE_HW_ISSUE_TMIX_7940,	BASE_HW_ISSUE_TMIX_8042,
+	BASE_HW_ISSUE_TMIX_8133,    BASE_HW_ISSUE_TMIX_8138,	BASE_HW_ISSUE_TMIX_8206,
+	BASE_HW_ISSUE_TMIX_8343,    BASE_HW_ISSUE_TMIX_8463,	BASE_HW_ISSUE_TMIX_8456,
+	BASE_HW_ISSUE_TMIX_8438,    BASE_HW_ISSUE_TSIX_2033,	BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_GPU2017_1336, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tMIx[] = {
+	BASE_HW_ISSUE_5736,	      BASE_HW_ISSUE_9435,      BASE_HW_ISSUE_TMIX_7891,
+	BASE_HW_ISSUE_TMIX_7940,      BASE_HW_ISSUE_TMIX_8042, BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TMIX_8138,      BASE_HW_ISSUE_TMIX_8206, BASE_HW_ISSUE_TMIX_8343,
+	BASE_HW_ISSUE_TMIX_8456,      BASE_HW_ISSUE_TSIX_2033, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tHEx_r0p0[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_10682,	  BASE_HW_ISSUE_11054,
+	BASE_HW_ISSUE_TMIX_7891,    BASE_HW_ISSUE_TMIX_8042,	  BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_921,	  BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tHEx_r0p1[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_10682,	  BASE_HW_ISSUE_11054,
+	BASE_HW_ISSUE_TMIX_7891,    BASE_HW_ISSUE_TMIX_8042,	  BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_921,	  BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tHEx_r0p2[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_10682,	  BASE_HW_ISSUE_11054,
+	BASE_HW_ISSUE_TMIX_7891,    BASE_HW_ISSUE_TMIX_8042,	  BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_921,	  BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tHEx_r0p3[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_10682,	  BASE_HW_ISSUE_TMIX_7891,
+	BASE_HW_ISSUE_TMIX_8042,      BASE_HW_ISSUE_TMIX_8133,	  BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_921,	      BASE_HW_ISSUE_GPU2017_1336, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tHEx[] = {
+	BASE_HW_ISSUE_5736,	    BASE_HW_ISSUE_9435,		  BASE_HW_ISSUE_TMIX_7891,
+	BASE_HW_ISSUE_TMIX_8042,    BASE_HW_ISSUE_TMIX_8133,	  BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tSIx_r0p0[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_11054,	  BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TSIX_1116,    BASE_HW_ISSUE_TSIX_2033,	  BASE_HW_ISSUE_TSIX_1792,
+	BASE_HW_ISSUE_TTRX_921,	    BASE_HW_ISSUE_GPU2017_1336,	  BASE_HW_ISSUE_TTRX_3464,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tSIx_r0p1[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_11054,	  BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TSIX_1116,    BASE_HW_ISSUE_TSIX_2033,	  BASE_HW_ISSUE_TSIX_1792,
+	BASE_HW_ISSUE_TTRX_921,	    BASE_HW_ISSUE_GPU2017_1336,	  BASE_HW_ISSUE_TTRX_3464,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tSIx_r1p0[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_11054,     BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TSIX_1116,      BASE_HW_ISSUE_TSIX_2033, BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_GPU2017_1336,   BASE_HW_ISSUE_TTRX_3464, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tSIx_r1p1[] = {
+	BASE_HW_ISSUE_9435,	 BASE_HW_ISSUE_TMIX_8133,    BASE_HW_ISSUE_TSIX_1116,
+	BASE_HW_ISSUE_TSIX_2033, BASE_HW_ISSUE_TTRX_921,     BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TTRX_3464, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tSIx[] = {
+	BASE_HW_ISSUE_5736,	    BASE_HW_ISSUE_9435,		  BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TSIX_1116,    BASE_HW_ISSUE_TSIX_2033,	  BASE_HW_ISSUE_TTRX_3464,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tDVx_r0p0[] = {
+	BASE_HW_ISSUE_9435,	 BASE_HW_ISSUE_TMIX_8133,    BASE_HW_ISSUE_TSIX_1116,
+	BASE_HW_ISSUE_TSIX_2033, BASE_HW_ISSUE_TTRX_921,     BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TTRX_3464, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tDVx[] = {
+	BASE_HW_ISSUE_5736,	    BASE_HW_ISSUE_9435,		  BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TSIX_1116,    BASE_HW_ISSUE_TSIX_2033,	  BASE_HW_ISSUE_TTRX_3464,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tNOx_r0p0[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TMIX_8133, BASE_HW_ISSUE_TSIX_1116,
+	BASE_HW_ISSUE_TSIX_2033,      BASE_HW_ISSUE_TNOX_1194, BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_GPU2017_1336,   BASE_HW_ISSUE_TTRX_3464, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tNOx[] = {
+	BASE_HW_ISSUE_5736,	    BASE_HW_ISSUE_9435,		  BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TSIX_1116,    BASE_HW_ISSUE_TSIX_2033,	  BASE_HW_ISSUE_TTRX_3464,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tGOx_r0p0[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TMIX_8133, BASE_HW_ISSUE_TSIX_1116,
+	BASE_HW_ISSUE_TSIX_2033,      BASE_HW_ISSUE_TNOX_1194, BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_GPU2017_1336,   BASE_HW_ISSUE_TTRX_3464, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tGOx_r1p0[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TMIX_8133,	  BASE_HW_ISSUE_TSIX_1116,
+	BASE_HW_ISSUE_TSIX_2033,      BASE_HW_ISSUE_TGOX_R1_1234, BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_GPU2017_1336,   BASE_HW_ISSUE_TTRX_3464,	  BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tGOx[] = {
+	BASE_HW_ISSUE_5736,	    BASE_HW_ISSUE_9435,		  BASE_HW_ISSUE_TMIX_8133,
+	BASE_HW_ISSUE_TSIX_1116,    BASE_HW_ISSUE_TSIX_2033,	  BASE_HW_ISSUE_TTRX_3464,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tTRx_r0p0[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,    BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_3076,    BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_TTRX_3414,    BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TTRX_3083,    BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,    BASE_HW_ISSUE_TTRX_3485,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tTRx_r0p1[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,    BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_3076,    BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_TTRX_3414,    BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TTRX_3083,    BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,    BASE_HW_ISSUE_TTRX_3485,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tTRx_r0p2[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_3076,      BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_TTRX_3414,      BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TTRX_3083,      BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,      BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tTRx[] = {
+	BASE_HW_ISSUE_5736,	      BASE_HW_ISSUE_9435,      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_3414, BASE_HW_ISSUE_TTRX_3083,
+	BASE_HW_ISSUE_TTRX_3470,      BASE_HW_ISSUE_TTRX_3464, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tNAx_r0p0[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,    BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_3076,    BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_TTRX_3414,    BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TTRX_3083,    BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,    BASE_HW_ISSUE_TTRX_3485,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tNAx_r0p1[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_3076,      BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_TTRX_3414,      BASE_HW_ISSUE_GPU2017_1336,
+	BASE_HW_ISSUE_TTRX_3083,      BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,      BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tNAx[] = {
+	BASE_HW_ISSUE_5736,	      BASE_HW_ISSUE_9435,      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_3414, BASE_HW_ISSUE_TTRX_3083,
+	BASE_HW_ISSUE_TTRX_3470,      BASE_HW_ISSUE_TTRX_3464, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tBEx_r0p0[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,    BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_921,	    BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,    BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,    BASE_HW_ISSUE_TTRX_3485,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tBEx_r0p1[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_921,	      BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,      BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,      BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tBEx_r1p0[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_921,	      BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,      BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,      BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tBEx_r1p1[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_921,	      BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,      BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,      BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tBEx[] = {
+	BASE_HW_ISSUE_5736,	      BASE_HW_ISSUE_9435,      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_3414, BASE_HW_ISSUE_TTRX_3083,
+	BASE_HW_ISSUE_TTRX_3470,      BASE_HW_ISSUE_TTRX_3464, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_lBEx_r1p0[] = {
+	BASE_HW_ISSUE_9435,	    BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,    BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_921,	    BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,    BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,    BASE_HW_ISSUE_TTRX_3485,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_lBEx_r1p1[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_921,	      BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,      BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,      BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tBAx_r0p0[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_921,	      BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,      BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,      BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tBAx_r1p0[] = {
+	BASE_HW_ISSUE_9435,	      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_921,	      BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,      BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,      BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tBAx[] = {
+	BASE_HW_ISSUE_5736,	      BASE_HW_ISSUE_9435,      BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,      BASE_HW_ISSUE_TTRX_3414, BASE_HW_ISSUE_TTRX_3083,
+	BASE_HW_ISSUE_TTRX_3470,      BASE_HW_ISSUE_TTRX_3464, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tODx_r0p0[] = {
+	BASE_HW_ISSUE_TSIX_2033,      BASE_HW_ISSUE_TTRX_1337,	  BASE_HW_ISSUE_GPU2019_3212,
+	BASE_HW_ISSUE_GPU2019_3878,   BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tODx[] = {
+	BASE_HW_ISSUE_TSIX_2033,      BASE_HW_ISSUE_TTRX_1337,	  BASE_HW_ISSUE_GPU2019_3212,
+	BASE_HW_ISSUE_GPU2019_3878,   BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_TITANHW_2710,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tGRx_r0p0[] = {
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_1337,	BASE_HW_ISSUE_GPU2019_3878,
+	BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tGRx[] = {
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_1337,	BASE_HW_ISSUE_GPU2019_3878,
+	BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tVAx_r0p0[] = {
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_1337,	BASE_HW_ISSUE_GPU2019_3878,
+	BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tVAx[] = {
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_1337,	BASE_HW_ISSUE_GPU2019_3878,
+	BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tTUx_r0p0[] = {
+	BASE_HW_ISSUE_TSIX_2033,      BASE_HW_ISSUE_TTRX_1337,	  BASE_HW_ISSUE_TURSEHW_1997,
+	BASE_HW_ISSUE_GPU2019_3878,   BASE_HW_ISSUE_TURSEHW_2716, BASE_HW_ISSUE_GPU2019_3901,
+	BASE_HW_ISSUE_GPU2021PRO_290, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_TITANHW_2679,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tTUx_r0p1[] = {
+	BASE_HW_ISSUE_TSIX_2033,      BASE_HW_ISSUE_TTRX_1337,	  BASE_HW_ISSUE_TURSEHW_1997,
+	BASE_HW_ISSUE_GPU2019_3878,   BASE_HW_ISSUE_TURSEHW_2716, BASE_HW_ISSUE_GPU2019_3901,
+	BASE_HW_ISSUE_GPU2021PRO_290, BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_TITANHW_2679,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_TITANHW_2922, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tTUx[] = {
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_1337,	BASE_HW_ISSUE_GPU2019_3878,
+	BASE_HW_ISSUE_TURSEHW_2716, BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_GPU2021PRO_290,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_TITANHW_2679, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_TITANHW_2922, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tTUx_r1p0[] = {
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_1337,	BASE_HW_ISSUE_GPU2019_3878,
+	BASE_HW_ISSUE_TURSEHW_2716, BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_GPU2021PRO_290,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_TITANHW_2679, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_TITANHW_2922, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tTUx_r1p1[] = {
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_1337,	BASE_HW_ISSUE_GPU2019_3878,
+	BASE_HW_ISSUE_TURSEHW_2716, BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_GPU2021PRO_290,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_TITANHW_2679, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_TITANHW_2922, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tTUx_r1p2[] = {
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_1337,	BASE_HW_ISSUE_GPU2019_3878,
+	BASE_HW_ISSUE_TURSEHW_2716, BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_GPU2021PRO_290,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_TITANHW_2679, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_TITANHW_2922, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tTUx_r1p3[] = {
+	BASE_HW_ISSUE_TSIX_2033,    BASE_HW_ISSUE_TTRX_1337,	BASE_HW_ISSUE_GPU2019_3878,
+	BASE_HW_ISSUE_TURSEHW_2716, BASE_HW_ISSUE_GPU2019_3901, BASE_HW_ISSUE_GPU2021PRO_290,
+	BASE_HW_ISSUE_TITANHW_2710, BASE_HW_ISSUE_TITANHW_2679, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_TITANHW_2922, BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tTIx[] = {
+	BASE_HW_ISSUE_TSIX_2033,      BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_TURSEHW_2716,   BASE_HW_ISSUE_GPU2021PRO_290,
+	BASE_HW_ISSUE_TITANHW_2710,   BASE_HW_ISSUE_TITANHW_2679,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_TITANHW_2922,
+	BASE_HW_ISSUE_TITANHW_2952,   BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tTIx_r0p0[] = {
+	BASE_HW_ISSUE_TSIX_2033,      BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_TURSEHW_2716,   BASE_HW_ISSUE_GPU2021PRO_290,
+	BASE_HW_ISSUE_TITANHW_2710,   BASE_HW_ISSUE_TITANHW_2679,
+	BASE_HW_ISSUE_GPU2022PRO_148, BASE_HW_ISSUE_TITANHW_2922,
+	BASE_HW_ISSUE_TITANHW_2952,   BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_tKRx_r0p0[] = {
+	BASE_HW_ISSUE_TTRX_1337, BASE_HW_ISSUE_TURSEHW_2716, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+__attribute__((unused)) static const enum base_hw_issue base_hw_issues_model_tKRx[] = {
+	BASE_HW_ISSUE_TTRX_1337, BASE_HW_ISSUE_TURSEHW_2716, BASE_HW_ISSUE_GPU2022PRO_148,
+	BASE_HW_ISSUE_END
+};
+
+
+#endif /* _BASE_HWCONFIG_ISSUES_H_ */
--- a/drivers/gpu/arm/bifrost/mali_csffw.bin
+++ /dev/null

diff --git a/drivers/gpu/arm/bifrost/mali_kbase.h b/drivers/gpu/arm/bifrost/mali_kbase.h
index 4d845ea08adb..29c032adf15f 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -26,6 +26,25 @@
 
 #include <mali_kbase_debug.h>
 
+#include <linux/atomic.h>
+#include <linux/highmem.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/mutex.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#if (KERNEL_VERSION(4, 11, 0) <= LINUX_VERSION_CODE)
+#include <linux/sched/mm.h>
+#endif
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/vmalloc.h>
+#include <linux/wait.h>
+#include <linux/workqueue.h>
+#include <linux/interrupt.h>
+
 #include <uapi/gpu/arm/bifrost/mali_base_kernel.h>
 #include <mali_kbase_linux.h>
 #include <linux/version_compat_defs.h>
@@ -56,39 +75,18 @@
 
 #include "ipa/mali_kbase_ipa.h"
 
-#if MALI_USE_CSF
-#include "csf/mali_kbase_csf.h"
-#endif
-
 #if IS_ENABLED(CONFIG_GPU_TRACEPOINTS)
 #include <trace/events/gpu.h>
 #endif
 
 #include "mali_linux_trace.h"
 
-#include <linux/atomic.h>
-#include <linux/highmem.h>
-#include <linux/hrtimer.h>
-#include <linux/ktime.h>
-#include <linux/list.h>
-#include <linux/mm.h>
-#include <linux/mutex.h>
-#include <linux/rwsem.h>
-#include <linux/sched.h>
-#if (KERNEL_VERSION(4, 11, 0) <= LINUX_VERSION_CODE)
-#include <linux/sched/mm.h>
-#endif
-#include <linux/slab.h>
-#include <linux/spinlock.h>
-#include <linux/vmalloc.h>
-#include <linux/wait.h>
-#include <linux/workqueue.h>
-#include <linux/interrupt.h>
-
 #define KBASE_DRV_NAME "mali"
 #define KBASE_TIMELINE_NAME KBASE_DRV_NAME ".timeline"
 
 #if MALI_USE_CSF
+#include "csf/mali_kbase_csf.h"
+
 /* Physical memory group ID for CSF user I/O.
  */
 #define KBASE_MEM_GROUP_CSF_IO BASE_MEM_GROUP_DEFAULT
@@ -178,16 +176,7 @@ unsigned long kbase_context_get_unmapped_area(struct kbase_context *kctx, const
 					      const unsigned long len, const unsigned long pgoff,
 					      const unsigned long flags);
 
-/**
- * kbase_get_irqs() - Get GPU interrupts from the device tree.
- *
- * @kbdev: The kbase device structure of the device
- *
- * This function must be called once only when a kbase device is initialized.
- *
- * Return: 0 on success. Error code (negative) on failure.
- */
-int kbase_get_irqs(struct kbase_device *kbdev);
+int assign_irqs(struct kbase_device *kbdev);
 
 int kbase_sysfs_init(struct kbase_device *kbdev);
 void kbase_sysfs_term(struct kbase_device *kbdev);
@@ -204,24 +193,22 @@ int kbase_protected_mode_init(struct kbase_device *kbdev);
 void kbase_protected_mode_term(struct kbase_device *kbdev);
 
 /**
- * kbase_device_backend_init() - Performs backend initialization and performs
- * devicetree validation.
+ * kbase_device_pm_init() - Performs power management initialization and
+ * Verifies device tree configurations.
  * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Return: 0 if successful, otherwise a standard Linux error code
- * If -EPERM is returned, it means the device backend is not supported, but
- * device initialization can continue.
  */
-int kbase_device_backend_init(struct kbase_device *kbdev);
+int kbase_device_pm_init(struct kbase_device *kbdev);
 
 /**
- * kbase_device_backend_term() - Performs backend deinitialization and free
- * resources.
+ * kbase_device_pm_term() - Performs power management deinitialization and
+ * Free resources.
  * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Clean up all the resources
  */
-void kbase_device_backend_term(struct kbase_device *kbdev);
+void kbase_device_pm_term(struct kbase_device *kbdev);
 
 int power_control_init(struct kbase_device *kbdev);
 void power_control_term(struct kbase_device *kbdev);
@@ -291,7 +278,7 @@ int kbase_jd_submit(struct kbase_context *kctx, void __user *user_addr, u32 nr_a
  */
 void kbase_jd_done_worker(struct work_struct *data);
 
-void kbase_jd_done(struct kbase_jd_atom *katom, unsigned int slot_nr, ktime_t *end_timestamp,
+void kbase_jd_done(struct kbase_jd_atom *katom, int slot_nr, ktime_t *end_timestamp,
 		   kbasep_js_atom_done_code done_code);
 void kbase_jd_cancel(struct kbase_device *kbdev, struct kbase_jd_atom *katom);
 void kbase_jd_zap_context(struct kbase_context *kctx);
@@ -345,6 +332,21 @@ void kbase_job_done(struct kbase_device *kbdev, u32 done);
 void kbase_job_slot_ctx_priority_check_locked(struct kbase_context *kctx,
 					      struct kbase_jd_atom *katom);
 
+/**
+ * kbase_job_slot_softstop_start_rp() - Soft-stop the atom at the start
+ *                                      of a renderpass.
+ * @kctx: Pointer to a kernel base context.
+ * @reg:  Reference of a growable GPU memory region in the same context.
+ *        Takes ownership of the reference if successful.
+ *
+ * Used to switch to incremental rendering if we have nearly run out of
+ * virtual address space in a growable memory region and the atom currently
+ * executing on a job slot is the tiler job chain at the start of a renderpass.
+ *
+ * Return: 0 if successful, otherwise a negative error code.
+ */
+int kbase_job_slot_softstop_start_rp(struct kbase_context *kctx, struct kbase_va_region *reg);
+
 /**
  * kbase_job_slot_softstop - Soft-stop the specified job slot
  *
@@ -357,7 +359,7 @@ void kbase_job_slot_ctx_priority_check_locked(struct kbase_context *kctx,
  *
  * Where possible any job in the next register is evicted before the soft-stop.
  */
-void kbase_job_slot_softstop(struct kbase_device *kbdev, unsigned int js,
+void kbase_job_slot_softstop(struct kbase_device *kbdev, int js,
 			     struct kbase_jd_atom *target_katom);
 
 void kbase_job_slot_softstop_swflags(struct kbase_device *kbdev, unsigned int js,
@@ -481,7 +483,9 @@ void kbasep_as_do_poke(struct work_struct *work);
  * or a dmb was executed recently (to ensure the value is most up-to-date).
  * However, without a lock the value could change afterwards.
  *
- * Return: False if a suspend is not in progress, true otherwise,
+ * Return:
+ * * false if a suspend is not in progress
+ * * !=false otherwise
  */
 static inline bool kbase_pm_is_suspending(struct kbase_device *kbdev)
 {
@@ -504,20 +508,21 @@ static inline bool kbase_pm_is_resuming(struct kbase_device *kbdev)
 	return kbdev->pm.resuming;
 }
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 /*
  * Check whether a gpu lost is in progress
  *
  * @kbdev: The kbase device structure for the device (must be a valid pointer)
  *
  * Indicates whether a gpu lost has been received and jobs are no longer
- * being scheduled.
+ * being scheduled
  *
- * Return: false if GPU is already lost or if no Arbiter is present (as GPU will
- *         always be present in this case), true otherwise.
+ * Return: false if gpu is lost
+ * Return: != false otherwise
  */
 static inline bool kbase_pm_is_gpu_lost(struct kbase_device *kbdev)
 {
-	return (kbdev->arb.arb_if && ((bool)atomic_read(&kbdev->pm.gpu_lost)));
+	return (atomic_read(&kbdev->pm.gpu_lost) == 0 ? false : true);
 }
 
 /*
@@ -536,8 +541,9 @@ static inline void kbase_pm_set_gpu_lost(struct kbase_device *kbdev, bool gpu_lo
 	const int cur_val = atomic_xchg(&kbdev->pm.gpu_lost, new_val);
 
 	if (new_val != cur_val)
-		KBASE_KTRACE_ADD(kbdev, ARB_GPU_LOST, NULL, (u64)new_val);
+		KBASE_KTRACE_ADD(kbdev, ARB_GPU_LOST, NULL, new_val);
 }
+#endif
 
 /**
  * kbase_pm_is_active - Determine whether the GPU is active
@@ -637,17 +643,16 @@ int kbase_pm_force_mcu_wakeup_after_sleep(struct kbase_device *kbdev);
  *
  * Return: the atom's ID.
  */
-static inline unsigned int kbase_jd_atom_id(struct kbase_context *kctx,
-					    const struct kbase_jd_atom *katom)
+static inline int kbase_jd_atom_id(struct kbase_context *kctx, const struct kbase_jd_atom *katom)
 {
-	unsigned int result;
+	int result;
 
 	KBASE_DEBUG_ASSERT(kctx);
 	KBASE_DEBUG_ASSERT(katom);
 	KBASE_DEBUG_ASSERT(katom->kctx == kctx);
 
 	result = katom - &kctx->jctx.atoms[0];
-	KBASE_DEBUG_ASSERT(result <= BASE_JD_ATOM_COUNT);
+	KBASE_DEBUG_ASSERT(result >= 0 && result <= BASE_JD_ATOM_COUNT);
 	return result;
 }
 
@@ -769,34 +774,118 @@ int kbase_device_pcm_dev_init(struct kbase_device *const kbdev);
  */
 void kbase_device_pcm_dev_term(struct kbase_device *const kbdev);
 
-#if MALI_USE_CSF
+/**
+ * KBASE_DISJOINT_STATE_INTERLEAVED_CONTEXT_COUNT_THRESHOLD - If a job is soft stopped
+ * and the number of contexts is >= this value it is reported as a disjoint event
+ */
+#define KBASE_DISJOINT_STATE_INTERLEAVED_CONTEXT_COUNT_THRESHOLD 2
+
+#if !defined(UINT64_MAX)
+#define UINT64_MAX ((uint64_t)0xFFFFFFFFFFFFFFFFULL)
+#endif
 
 /**
- * kbasep_adjust_prioritized_process() - Adds or removes the specified PID from
- *                                       the list of prioritized processes.
+ * kbase_file_fops_count() - Get the kfile::fops_count value
  *
- * @kbdev: Pointer to the structure for the kbase device
- * @add: True if the process should be prioritized, false otherwise
- * @tgid: The process/thread group ID
+ * @kfile: Pointer to the object representing the mali device file.
+ *
+ * The value is read with kfile::lock held.
  *
- * Return: true if the operation was successful, false otherwise
+ * Return: sampled value of kfile::fops_count.
  */
-bool kbasep_adjust_prioritized_process(struct kbase_device *kbdev, bool add, uint32_t tgid);
+static inline u32 kbase_file_fops_count(struct kbase_file *kfile)
+{
+	u32 fops_count;
 
-#endif /* MALI_USE_CSF */
+	spin_lock(&kfile->lock);
+	fops_count = kfile->fops_count;
+	spin_unlock(&kfile->lock);
+
+	return fops_count;
+}
 
 /**
- * KBASE_DISJOINT_STATE_INTERLEAVED_CONTEXT_COUNT_THRESHOLD - If a job is soft stopped
- * and the number of contexts is >= this value it is reported as a disjoint event
+ * kbase_file_inc_fops_count_unless_closed() - Increment the kfile::fops_count value if the
+ *                                             kfile::owner is still set.
+ *
+ * @kfile: Pointer to the object representing the /dev/malixx device file instance.
+ *
+ * Return: true if the increment was done otherwise false.
  */
-#define KBASE_DISJOINT_STATE_INTERLEAVED_CONTEXT_COUNT_THRESHOLD 2
+static inline bool kbase_file_inc_fops_count_unless_closed(struct kbase_file *kfile)
+{
+	bool count_incremented = false;
 
-#if !defined(UINT64_MAX)
-#define UINT64_MAX ((uint64_t)0xFFFFFFFFFFFFFFFFULL)
-#endif
+	spin_lock(&kfile->lock);
+	if (kfile->owner) {
+		kfile->fops_count++;
+		count_incremented = true;
+	}
+	spin_unlock(&kfile->lock);
+
+	return count_incremented;
+}
 
-#if !defined(UINT32_MAX)
-#define UINT32_MAX ((uint32_t)0xFFFFFFFFU)
+/**
+ * kbase_file_dec_fops_count() - Decrement the kfile::fops_count value
+ *
+ * @kfile: Pointer to the object representing the /dev/malixx device file instance.
+ *
+ * This function shall only be called to decrement kfile::fops_count if a successful call
+ * to kbase_file_inc_fops_count_unless_closed() was made previously by the current thread.
+ *
+ * The function would enqueue the kfile::destroy_kctx_work if the process that originally
+ * created the file instance has closed its copy and no Kbase handled file operations are
+ * in progress and no memory mappings are present for the file instance.
+ */
+static inline void kbase_file_dec_fops_count(struct kbase_file *kfile)
+{
+	spin_lock(&kfile->lock);
+	WARN_ON_ONCE(kfile->fops_count <= 0);
+	kfile->fops_count--;
+	if (unlikely(!kfile->fops_count && !kfile->owner && !kfile->map_count)) {
+		queue_work(system_wq, &kfile->destroy_kctx_work);
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+		wake_up(&kfile->zero_fops_count_wait);
 #endif
+	}
+	spin_unlock(&kfile->lock);
+}
+
+/**
+ * kbase_file_inc_cpu_mapping_count() - Increment the kfile::map_count value.
+ *
+ * @kfile: Pointer to the object representing the /dev/malixx device file instance.
+ *
+ * This function shall be called when the memory mapping on /dev/malixx device file
+ * instance is created. The kbase_file::setup_state shall be KBASE_FILE_COMPLETE.
+ */
+static inline void kbase_file_inc_cpu_mapping_count(struct kbase_file *kfile)
+{
+	spin_lock(&kfile->lock);
+	kfile->map_count++;
+	spin_unlock(&kfile->lock);
+}
+
+/**
+ * kbase_file_dec_cpu_mapping_count() - Decrement the kfile::map_count value
+ *
+ * @kfile: Pointer to the object representing the /dev/malixx device file instance.
+ *
+ * This function is called to decrement kfile::map_count value when the memory mapping
+ * on /dev/malixx device file is closed.
+ * The function would enqueue the kfile::destroy_kctx_work if the process that originally
+ * created the file instance has closed its copy and there are no mappings present and no
+ * Kbase handled file operations are in progress for the file instance.
+ */
+static inline void kbase_file_dec_cpu_mapping_count(struct kbase_file *kfile)
+{
+	spin_lock(&kfile->lock);
+	WARN_ON_ONCE(kfile->map_count <= 0);
+	kfile->map_count--;
+	if (unlikely(!kfile->map_count && !kfile->owner && !kfile->fops_count))
+		queue_work(system_wq, &kfile->destroy_kctx_work);
+	spin_unlock(&kfile->lock);
+}
 
 #endif
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_as_fault_debugfs.h b/drivers/gpu/arm/bifrost/mali_kbase_as_fault_debugfs.h
index 54c60e48feae..b07207ec524f 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_as_fault_debugfs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_as_fault_debugfs.h
@@ -22,8 +22,6 @@
 #ifndef _KBASE_AS_FAULT_DEBUG_FS_H
 #define _KBASE_AS_FAULT_DEBUG_FS_H
 
-#include <mali_kbase.h>
-
 /**
  * kbase_as_fault_debugfs_init() - Add debugfs files for reporting page faults
  *
@@ -37,7 +35,7 @@ void kbase_as_fault_debugfs_init(struct kbase_device *kbdev);
  * @kbdev: Pointer to kbase_device
  * @as_no: The address space the fault occurred on
  */
-static inline void kbase_as_fault_debugfs_new(struct kbase_device *kbdev, unsigned int as_no)
+static inline void kbase_as_fault_debugfs_new(struct kbase_device *kbdev, int as_no)
 {
 #if IS_ENABLED(CONFIG_DEBUG_FS)
 #ifdef CONFIG_MALI_BIFROST_DEBUG
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_cache_policy.c b/drivers/gpu/arm/bifrost/mali_kbase_cache_policy.c
index 92fb703f7f00..4675025baaf8 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_cache_policy.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_cache_policy.c
@@ -23,8 +23,7 @@
  * Cache Policy API.
  */
 
-#include <mali_kbase_cache_policy.h>
-#include <mali_kbase.h>
+#include "mali_kbase_cache_policy.h"
 
 /*
  * The output flags should be a combination of the following values:
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_cache_policy.h b/drivers/gpu/arm/bifrost/mali_kbase_cache_policy.h
index d3d47d14edda..1d9f00c560e7 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_cache_policy.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_cache_policy.h
@@ -26,7 +26,8 @@
 #ifndef _KBASE_CACHE_POLICY_H_
 #define _KBASE_CACHE_POLICY_H_
 
-#include <linux/types.h>
+#include "mali_kbase.h"
+#include <uapi/gpu/arm/bifrost/mali_base_kernel.h>
 
 /**
  * kbase_cache_enabled - Choose the cache policy for a specific region
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_caps.h b/drivers/gpu/arm/bifrost/mali_kbase_caps.h
index 000e30e1ed84..a92569d31f06 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_caps.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_caps.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -33,40 +33,15 @@
  *
  * @MALI_KBASE_CAP_SYSTEM_MONITOR: System Monitor
  * @MALI_KBASE_CAP_JIT_PRESSURE_LIMIT: JIT Pressure limit
- * @MALI_KBASE_CAP_QUERY_MEM_DONT_NEED: BASE_MEM_DONT_NEED is queryable
- * @MALI_KBASE_CAP_QUERY_MEM_GROW_ON_GPF: BASE_MEM_GROW_ON_GPF is queryable
- * @MALI_KBASE_CAP_QUERY_MEM_PROTECTED: BASE_MEM_PROTECTED is queryable
- * @MALI_KBASE_CAP_QUERY_MEM_IMPORT_SYNC_ON_MAP_UNMAP: BASE_MEM_IMPORT_SYNC_ON_MAP_UNMAP is
- *                                                     queryable
- * @MALI_KBASE_CAP_QUERY_MEM_KERNEL_SYNC: BASE_MEM_KERNEL_SYNC is queryable
- * @MALI_KBASE_CAP_QUERY_MEM_SAME_VA: BASE_MEM_SAME_VA is queryable
- * @MALI_KBASE_CAP_REJECT_ALLOC_MEM_DONT_NEED: BASE_MEM_DONT_NEED is not allocatable
- * @MALI_KBASE_CAP_REJECT_ALLOC_MEM_PROTECTED_IN_UNPROTECTED_ALLOCS: BASE_MEM_PROTECTED is not
- *                                                                   allocatable in functions other
- *                                                                   than base_mem_protected
- * @MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_8: BASE_MEM_UNUSED_BIT_8 is not allocatable
- * @MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_19: BASE_MEM_UNUSED_BIT_19 is not allocatable
- * @MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_20: BASE_MEM_UNUSED_BIT_20 is not allocatable
- * @MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_27: BASE_MEM_UNUSED_BIT_27 is not allocatable
+ * @MALI_KBASE_CAP_MEM_GROW_ON_GPF: Memory grow on page fault
+ * @MALI_KBASE_CAP_MEM_PROTECTED: Protected memory
  * @MALI_KBASE_NUM_CAPS: Delimiter
- *
- * New enumerator must not be negative and smaller than @MALI_KBASE_NUM_CAPS.
  */
 enum mali_kbase_cap {
 	MALI_KBASE_CAP_SYSTEM_MONITOR = 0,
 	MALI_KBASE_CAP_JIT_PRESSURE_LIMIT,
-	MALI_KBASE_CAP_QUERY_MEM_DONT_NEED,
-	MALI_KBASE_CAP_QUERY_MEM_GROW_ON_GPF,
-	MALI_KBASE_CAP_QUERY_MEM_PROTECTED,
-	MALI_KBASE_CAP_QUERY_MEM_IMPORT_SYNC_ON_MAP_UNMAP,
-	MALI_KBASE_CAP_QUERY_MEM_KERNEL_SYNC,
-	MALI_KBASE_CAP_QUERY_MEM_SAME_VA,
-	MALI_KBASE_CAP_REJECT_ALLOC_MEM_DONT_NEED,
-	MALI_KBASE_CAP_REJECT_ALLOC_MEM_PROTECTED_IN_UNPROTECTED_ALLOCS,
-	MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_8,
-	MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_19,
-	MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_20,
-	MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_27,
+	MALI_KBASE_CAP_MEM_GROW_ON_GPF,
+	MALI_KBASE_CAP_MEM_PROTECTED,
 	MALI_KBASE_NUM_CAPS
 };
 
@@ -82,67 +57,14 @@ static inline bool mali_kbase_supports_jit_pressure_limit(unsigned long api_vers
 	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_JIT_PRESSURE_LIMIT);
 }
 
-static inline bool mali_kbase_supports_query_mem_dont_need(unsigned long api_version)
-{
-	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_QUERY_MEM_DONT_NEED);
-}
-
-static inline bool mali_kbase_supports_query_mem_grow_on_gpf(unsigned long api_version)
-{
-	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_QUERY_MEM_GROW_ON_GPF);
-}
-
-static inline bool mali_kbase_supports_query_mem_protected(unsigned long api_version)
-{
-	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_QUERY_MEM_PROTECTED);
-}
-
-static inline bool mali_kbase_supports_query_mem_import_sync_on_map_unmap(unsigned long api_version)
-{
-	return mali_kbase_supports_cap(api_version,
-				       MALI_KBASE_CAP_QUERY_MEM_IMPORT_SYNC_ON_MAP_UNMAP);
-}
-
-static inline bool mali_kbase_supports_query_mem_kernel_sync(unsigned long api_version)
-{
-	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_QUERY_MEM_KERNEL_SYNC);
-}
-
-static inline bool mali_kbase_supports_query_mem_same_va(unsigned long api_version)
-{
-	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_QUERY_MEM_SAME_VA);
-}
-
-static inline bool mali_kbase_supports_reject_alloc_mem_dont_need(unsigned long api_version)
-{
-	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_REJECT_ALLOC_MEM_DONT_NEED);
-}
-
-static inline bool
-mali_kbase_supports_reject_alloc_mem_protected_in_unprotected_allocs(unsigned long api_version)
-{
-	return mali_kbase_supports_cap(
-		api_version, MALI_KBASE_CAP_REJECT_ALLOC_MEM_PROTECTED_IN_UNPROTECTED_ALLOCS);
-}
-
-static inline bool mali_kbase_supports_reject_alloc_mem_unused_bit_8(unsigned long api_version)
-{
-	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_8);
-}
-
-static inline bool mali_kbase_supports_reject_alloc_mem_unused_bit_19(unsigned long api_version)
-{
-	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_19);
-}
-
-static inline bool mali_kbase_supports_reject_alloc_mem_unused_bit_20(unsigned long api_version)
+static inline bool mali_kbase_supports_mem_grow_on_gpf(unsigned long api_version)
 {
-	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_20);
+	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_MEM_GROW_ON_GPF);
 }
 
-static inline bool mali_kbase_supports_reject_alloc_mem_unused_bit_27(unsigned long api_version)
+static inline bool mali_kbase_supports_mem_protected(unsigned long api_version)
 {
-	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_REJECT_ALLOC_MEM_UNUSED_BIT_27);
+	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_MEM_PROTECTED);
 }
 
 #endif /* __KBASE_CAPS_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_ccswe.c b/drivers/gpu/arm/bifrost/mali_kbase_ccswe.c
index a3c927d13e6f..c4acbf6881f3 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_ccswe.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_ccswe.c
@@ -32,14 +32,13 @@ static u64 kbasep_ccswe_cycle_at_no_lock(struct kbase_ccswe *self, u64 timestamp
 
 	lockdep_assert_held(&self->access);
 
-	diff_ns = (s64)(timestamp_ns - self->timestamp_ns);
+	diff_ns = timestamp_ns - self->timestamp_ns;
 	gpu_freq = diff_ns > 0 ? self->gpu_freq : self->prev_gpu_freq;
 
 	diff_s = div_s64(diff_ns, NSEC_PER_SEC);
 	diff_ns -= diff_s * NSEC_PER_SEC;
 
-	return self->cycles_elapsed + (u64)diff_s * gpu_freq +
-	       (u64)div_s64(diff_ns * gpu_freq, NSEC_PER_SEC);
+	return self->cycles_elapsed + diff_s * gpu_freq + div_s64(diff_ns * gpu_freq, NSEC_PER_SEC);
 }
 
 void kbase_ccswe_init(struct kbase_ccswe *self)
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_ccswe.h b/drivers/gpu/arm/bifrost/mali_kbase_ccswe.h
index 2b27bafcf7be..ce148ed537c5 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_ccswe.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_ccswe.h
@@ -22,7 +22,6 @@
 #ifndef _KBASE_CCSWE_H_
 #define _KBASE_CCSWE_H_
 
-#include <linux/types.h>
 #include <linux/spinlock.h>
 
 /**
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_config.h b/drivers/gpu/arm/bifrost/mali_kbase_config.h
index 2f9e28aaec9a..d5dd49055b00 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_config.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_config.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -26,9 +26,9 @@
 #ifndef _KBASE_CONFIG_H_
 #define _KBASE_CONFIG_H_
 
-#include <mali_malisw.h>
-
 #include <linux/mm.h>
+#include <mali_malisw.h>
+#include <backend/gpu/mali_kbase_backend_config.h>
 
 /* Forward declaration of struct kbase_device */
 struct kbase_device;
@@ -166,9 +166,8 @@ struct kbase_pm_callback_conf {
 	 *
 	 * The system integrator can decide whether to either do nothing, just switch off
 	 * the clocks to the GPU, or to completely power down the GPU.
-	 * The platform specific private pointer kbase_device::platform_context can be
-	 * accessed and modified in here. It is the platform \em callbacks responsibility
-	 * to initialize and terminate this pointer if used (see @ref kbase_platform_funcs_conf).
+	 * The platform specific private pointer kbase_device::platform_context can be accessed and modified in here. It is the
+	 * platform \em callbacks responsibility to initialize and terminate this pointer if used (see @ref kbase_platform_funcs_conf).
 	 *
 	 * If runtime PM is enabled and @power_runtime_gpu_idle_callback is used
 	 * then this callback should power off the GPU (or switch off the clocks
@@ -180,18 +179,15 @@ struct kbase_pm_callback_conf {
 
 	/** Callback for when the GPU is about to become active and power must be supplied.
 	 *
-	 * This function must not return until the GPU is powered and clocked sufficiently
-	 * for register access to succeed. The return value specifies whether the GPU was
-	 * powered down since the call to power_off_callback.
-	 * If the GPU is in reset state it should return 2, if the GPU state has been lost
-	 * then this function must return 1, otherwise it should return 0.
-	 * The platform specific private pointer kbase_device::platform_context can be
-	 * accessed and modified in here. It is the platform \em callbacks responsibility
-	 * to initialize and terminate this pointer if used (see @ref kbase_platform_funcs_conf).
+	 * This function must not return until the GPU is powered and clocked sufficiently for register access to
+	 * succeed.  The return value specifies whether the GPU was powered down since the call to power_off_callback.
+	 * If the GPU state has been lost then this function must return 1, otherwise it should return 0.
+	 * The platform specific private pointer kbase_device::platform_context can be accessed and modified in here. It is the
+	 * platform \em callbacks responsibility to initialize and terminate this pointer if used (see @ref kbase_platform_funcs_conf).
 	 *
 	 * The return value of the first call to this function is ignored.
 	 *
-	 * @return 2 if GPU in reset state, 1 if the GPU state may have been lost, 0 otherwise.
+	 * @return 1 if the GPU state may have been lost, 0 otherwise.
 	 */
 	int (*power_on_callback)(struct kbase_device *kbdev);
 
@@ -227,11 +223,9 @@ struct kbase_pm_callback_conf {
 
 	/** Callback for handling runtime power management initialization.
 	 *
-	 * The runtime power management callbacks @ref power_runtime_off_callback
-	 * and @ref power_runtime_on_callback will become active from calls made
-	 * to the OS from within this function.
-	 * The runtime calls can be triggered by calls from @ref power_off_callback
-	 * and @ref power_on_callback.
+	 * The runtime power management callbacks @ref power_runtime_off_callback and @ref power_runtime_on_callback
+	 * will become active from calls made to the OS from within this function.
+	 * The runtime calls can be triggered by calls from @ref power_off_callback and @ref power_on_callback.
 	 * Note: for linux the kernel must have CONFIG_PM_RUNTIME enabled to use this feature.
 	 *
 	 * @return 0 on success, else int error code.
@@ -240,9 +234,8 @@ struct kbase_pm_callback_conf {
 
 	/** Callback for handling runtime power management termination.
 	 *
-	 * The runtime power management callbacks @ref power_runtime_off_callback
-	 * and @ref power_runtime_on_callback should no longer be called by the
-	 * OS on completion of this function.
+	 * The runtime power management callbacks @ref power_runtime_off_callback and @ref power_runtime_on_callback
+	 * should no longer be called by the OS on completion of this function.
 	 * Note: for linux the kernel must have CONFIG_PM_RUNTIME enabled to use this feature.
 	 */
 	void (*power_runtime_term_callback)(struct kbase_device *kbdev);
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_config_defaults.h b/drivers/gpu/arm/bifrost/mali_kbase_config_defaults.h
index 7657c25d565c..9dc134373dc3 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_config_defaults.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_config_defaults.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2013-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2013-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -194,22 +194,9 @@ enum {
  */
 #define CSF_CSG_SUSPEND_TIMEOUT_CYCLES (3100000000ull)
 
-/* Waiting timeout in clock cycles for GPU suspend to complete. */
-#define CSF_GPU_SUSPEND_TIMEOUT_CYCLES (CSF_CSG_SUSPEND_TIMEOUT_CYCLES)
-
 /* Waiting timeout in clock cycles for GPU reset to complete. */
 #define CSF_GPU_RESET_TIMEOUT_CYCLES (CSF_CSG_SUSPEND_TIMEOUT_CYCLES * 2)
 
-/* Waiting timeout in clock cycles for a CSG to be terminated.
- *
- * Based on 0.6s timeout at 100MHZ, scaled from 0.1s at 600Mhz GPU frequency
- * which is the timeout defined in FW to wait for iterator to complete the
- * transitioning to DISABLED state.
- * More cycles (0.4s @ 100Mhz = 40000000) are added up to ensure that
- * host timeout is always bigger than FW timeout.
- */
-#define CSF_CSG_TERM_TIMEOUT_CYCLES (100000000)
-
 /* Waiting timeout in clock cycles for GPU firmware to boot.
  *
  * Based on 250ms timeout at 100MHz, scaled from a 50MHz GPU system.
@@ -226,28 +213,12 @@ enum {
  *
  * Based on 10s timeout at 100MHz, scaled from a 50MHz GPU system.
  */
-#if IS_ENABLED(CONFIG_MALI_VECTOR_DUMP)
-/* Set a large value to avoid timing out while vector dumping */
-#define KCPU_FENCE_SIGNAL_TIMEOUT_CYCLES (250000000000ull)
-#elif IS_ENABLED(CONFIG_MALI_IS_FPGA)
+#if IS_ENABLED(CONFIG_MALI_IS_FPGA)
 #define KCPU_FENCE_SIGNAL_TIMEOUT_CYCLES (2500000000ull)
 #else
 #define KCPU_FENCE_SIGNAL_TIMEOUT_CYCLES (1000000000ull)
 #endif
 
-/* Timeout for polling the GPU in clock cycles.
- *
- * Based on 10s timeout based on original MAX_LOOPS value.
- */
-#define IPA_INACTIVE_TIMEOUT_CYCLES (1000000000ull)
-
-/* Timeout for polling the GPU for the MCU status in clock cycles.
- *
- * Based on 120s timeout based on original MAX_LOOPS value.
- */
-#define CSF_FIRMWARE_STOP_TIMEOUT_CYCLES (12000000000ull)
-
-
 /* Waiting timeout for task execution on an endpoint. Based on the
  * DEFAULT_PROGRESS_TIMEOUT.
  *
@@ -255,42 +226,6 @@ enum {
  */
 #define DEFAULT_PROGRESS_TIMEOUT_CYCLES (2500000000ull)
 
-/* MIN value of iterators' suspend timeout*/
-#define CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MIN (200)
-#if CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MIN <= 0
-#error "CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MIN should be larger than 0"
-#endif
-
-/* MAX value of iterators' suspend timeout*/
-#define CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MAX (60000)
-#if CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MAX >= (0xFFFFFFFF)
-#error "CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MAX should be less than U32_MAX"
-#endif
-
-/* Firmware iterators' suspend timeout, default 4000ms. Customer can update this by
- * using debugfs -- csg_suspend_timeout
- */
-#if IS_ENABLED(CONFIG_MALI_REAL_HW) && !IS_ENABLED(CONFIG_MALI_IS_FPGA)
-#define CSG_SUSPEND_TIMEOUT_FIRMWARE_MS (4000)
-#else
-#define CSG_SUSPEND_TIMEOUT_FIRMWARE_MS (31000)
-#endif
-#if (CSG_SUSPEND_TIMEOUT_FIRMWARE_MS < CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MIN) || \
-	(CSG_SUSPEND_TIMEOUT_FIRMWARE_MS > CSG_SUSPEND_TIMEOUT_FIRMWARE_MS_MAX)
-#error "CSG_SUSPEND_TIMEOUT_FIRMWARE_MS is out of range"
-#endif
-
-/* Additional time in milliseconds added to the firmware iterators' suspend timeout,
- * default 100ms
- */
-#define CSG_SUSPEND_TIMEOUT_HOST_ADDED_MS (100)
-
-/* Host side CSG suspend timeout */
-#define CSG_SUSPEND_TIMEOUT_MS (CSG_SUSPEND_TIMEOUT_FIRMWARE_MS + CSG_SUSPEND_TIMEOUT_HOST_ADDED_MS)
-
-/* MAX allowed timeout value(ms) on host side, should be less than ANR timeout */
-#define MAX_TIMEOUT_MS (4500)
-
 #else /* MALI_USE_CSF */
 
 /* A default timeout in clock cycles to be used when an invalid timeout
@@ -315,24 +250,6 @@ enum {
 
 #endif /* !MALI_USE_CSF */
 
-/* Timeout for polling the GPU PRFCNT_ACTIVE bit in clock cycles.
- *
- * Based on 120s timeout at 100MHz, based on original MAX_LOOPS value.
- */
-#define KBASE_PRFCNT_ACTIVE_TIMEOUT_CYCLES (12000000000ull)
-
-/* Timeout for polling the GPU for a cache flush in clock cycles.
- *
- * Based on 120ms timeout at 100MHz, based on original MAX_LOOPS value.
- */
-#define KBASE_CLEAN_CACHE_TIMEOUT_CYCLES (12000000ull)
-
-/* Timeout for polling the GPU for an AS command to complete in clock cycles.
- *
- * Based on 120s timeout at 100MHz, based on original MAX_LOOPS value.
- */
-#define KBASE_AS_INACTIVE_TIMEOUT_CYCLES (12000000000ull)
-
 /* Default timeslice that a context is scheduled in for, in nanoseconds.
  *
  * When a context has used up this amount of time across its jobs, it is
@@ -363,6 +280,14 @@ enum {
  */
 #define DEFAULT_PROGRESS_TIMEOUT ((u64)5 * 500 * 1024 * 1024)
 
+/* Default threshold at which to switch to incremental rendering
+ *
+ * Fraction of the maximum size of an allocation that grows on GPU page fault
+ * that can be used up before the driver switches to incremental rendering,
+ * in 256ths. 0 means disable incremental rendering.
+ */
+#define DEFAULT_IR_THRESHOLD (192)
+
 /* Waiting time in clock cycles for the completion of a MMU operation.
  *
  * Ideally 1.6M GPU cycles required for the L2 cache (512KiB slice) flush.
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_core_linux.c b/drivers/gpu/arm/bifrost/mali_kbase_core_linux.c
index 9cd02fec4a10..6700337b447d 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_core_linux.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_core_linux.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -62,7 +62,9 @@
 #include "csf/mali_kbase_csf_cpu_queue.h"
 #include "csf/mali_kbase_csf_event.h"
 #endif
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 #include "arbiter/mali_kbase_arbiter_pm.h"
+#endif
 
 #include "mali_kbase_cs_experimental.h"
 
@@ -74,7 +76,6 @@
 #if IS_ENABLED(CONFIG_DEBUG_FS)
 #include "mali_kbase_pbha_debugfs.h"
 #endif
-#include "mali_kbase_ioctl_helpers.h"
 
 #include <linux/module.h>
 #include <linux/init.h>
@@ -105,7 +106,6 @@
 #include <linux/clk-provider.h>
 #include <linux/delay.h>
 #include <linux/log2.h>
-#include <linux/mali_hw_access.h>
 
 #include <mali_kbase_config.h>
 
@@ -129,7 +129,7 @@
  * @minor: Kernel minor version
  */
 #define KBASE_API_VERSION(major, minor) \
-	((((major)&0xFFFU) << 20U) | (((minor)&0xFFFU) << 8U) | ((0U & 0xFFU) << 0U))
+	((((major)&0xFFF) << 20) | (((minor)&0xFFF) << 8) | ((0 & 0xFF) << 0))
 
 /**
  * struct mali_kbase_capability_def - kbase capabilities table
@@ -152,29 +152,13 @@ static const struct mali_kbase_capability_def kbase_caps_table[MALI_KBASE_NUM_CA
 #if MALI_USE_CSF
 	{ 1, 0 }, /* SYSTEM_MONITOR */
 	{ 1, 0 }, /* JIT_PRESSURE_LIMIT */
-	{ 1, 22 }, /* QUERY_MEM_DONT_NEED */
-	{ 1, 0 }, /* QUERY_MEM_GROW_ON_GPF */
-	{ 1, 0 }, /* QUERY_MEM_PROTECTED */
-	{ 1, 26 }, /* QUERY_MEM_IMPORT_SYNC_ON_MAP_UNMAP */
-	{ 1, 26 }, /* QUERY_MEM_KERNEL_SYNC */
-	{ 1, 28 }, /* QUERY_MEM_SAME_VA */
-	{ 1, 31 }, /* REJECT_ALLOC_MEM_DONT_NEED */
-	{ 1, 31 }, /* REJECT_ALLOC_MEM_PROTECTED_IN_UNPROTECTED_ALLOCS */
-	{ 1, 31 }, /* REJECT_ALLOC_MEM_UNUSED_BIT_20 */
-	{ 1, 31 } /* REJECT_ALLOC_MEM_UNUSED_BIT_27 */
+	{ 1, 0 }, /* MEM_GROW_ON_GPF */
+	{ 1, 0 } /* MEM_PROTECTED */
 #else
 	{ 11, 15 }, /* SYSTEM_MONITOR */
 	{ 11, 25 }, /* JIT_PRESSURE_LIMIT */
-	{ 11, 40 }, /* QUERY_MEM_DONT_NEED */
-	{ 11, 2 }, /* QUERY_MEM_GROW_ON_GPF */
-	{ 11, 2 }, /* QUERY_MEM_PROTECTED */
-	{ 11, 43 }, /* QUERY_MEM_IMPORT_SYNC_ON_MAP_UNMAP */
-	{ 11, 43 }, /* QUERY_MEM_KERNEL_SYNC */
-	{ 11, 44 }, /* QUERY_MEM_SAME_VA */
-	{ 11, 46 }, /* REJECT_ALLOC_MEM_DONT_NEED */
-	{ 11, 46 }, /* REJECT_ALLOC_MEM_PROTECTED_IN_UNPROTECTED_ALLOCS */
-	{ 11, 46 }, /* REJECT_ALLOC_MEM_UNUSED_BIT_8 */
-	{ 11, 46 } /* REJECT_ALLOC_MEM_UNUSED_BIT_19 */
+	{ 11, 2 }, /* MEM_GROW_ON_GPF */
+	{ 11, 2 } /* MEM_PROTECTED */
 #endif
 };
 
@@ -183,11 +167,13 @@ static const struct mali_kbase_capability_def kbase_caps_table[MALI_KBASE_NUM_CA
 static struct mutex kbase_probe_mutex;
 #endif
 
+static void kbase_file_destroy_kctx_worker(struct work_struct *work);
+
 /**
  * mali_kbase_supports_cap - Query whether a kbase capability is supported
  *
  * @api_version: API version to convert
- * @cap:         Capability to query for - see mali_kbase_caps.h. Shouldn't be negative.
+ * @cap:         Capability to query for - see mali_kbase_caps.h
  *
  * Return: true if the capability is supported
  */
@@ -198,10 +184,13 @@ bool mali_kbase_supports_cap(unsigned long api_version, enum mali_kbase_cap cap)
 
 	struct mali_kbase_capability_def const *cap_def;
 
+	if (WARN_ON(cap < 0))
+		return false;
+
 	if (WARN_ON(cap >= MALI_KBASE_NUM_CAPS))
 		return false;
 
-	cap_def = &kbase_caps_table[cap];
+	cap_def = &kbase_caps_table[(int)cap];
 	required_ver = KBASE_API_VERSION(cap_def->required_major, cap_def->required_minor);
 	supported = (api_version >= required_ver);
 
@@ -223,7 +212,7 @@ bool mali_kbase_supports_cap(unsigned long api_version, enum mali_kbase_cap cap)
  * Return: Address of an object representing a simulated device file, or NULL
  *         on failure.
  *
- * Note: This function shall always be called in Userspace context.
+ * Note: This function always gets called in Userspace context.
  */
 static struct kbase_file *kbase_file_new(struct kbase_device *const kbdev, struct file *const filp)
 {
@@ -235,6 +224,17 @@ static struct kbase_file *kbase_file_new(struct kbase_device *const kbdev, struc
 		kfile->kctx = NULL;
 		kfile->api_version = 0;
 		atomic_set(&kfile->setup_state, KBASE_FILE_NEED_VSN);
+		/* Store the pointer to the file table structure of current process. */
+		kfile->owner = current->files;
+		INIT_WORK(&kfile->destroy_kctx_work, kbase_file_destroy_kctx_worker);
+		spin_lock_init(&kfile->lock);
+		kfile->fops_count = 0;
+		kfile->map_count = 0;
+		typecheck(typeof(kfile->map_count), typeof(current->mm->map_count));
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+		init_waitqueue_head(&kfile->zero_fops_count_wait);
+#endif
+		init_waitqueue_head(&kfile->event_queue);
 	}
 	return kfile;
 }
@@ -313,6 +313,33 @@ static unsigned long kbase_file_get_api_version(struct kbase_file *const kfile)
  */
 static int kbase_file_create_kctx(struct kbase_file *kfile, base_context_create_flags flags);
 
+/**
+ * kbase_file_inc_fops_count_if_allowed - Increment the kfile::fops_count value if the file
+ *                                        operation is allowed for the current process.
+ *
+ * @kfile: Pointer to the object representing the /dev/malixx device file instance.
+ *
+ * The function shall be called at the beginning of certain file operation methods
+ * implemented for @kbase_fops, like ioctl, poll, read and mmap.
+ *
+ * kbase_file_dec_fops_count() shall be called if the increment was done.
+ *
+ * Return: true if the increment was done otherwise false.
+ *
+ * Note: This function shall always be called in Userspace context.
+ */
+static bool kbase_file_inc_fops_count_if_allowed(struct kbase_file *const kfile)
+{
+	/* Disallow file operations from the other process that shares the instance
+	 * of /dev/malixx file i.e. 'kfile' or disallow file operations if parent
+	 * process has closed the file instance.
+	 */
+	if (unlikely(kfile->owner != current->files))
+		return false;
+
+	return kbase_file_inc_fops_count_unless_closed(kfile);
+}
+
 /**
  * kbase_file_get_kctx_if_setup_complete - Get a kernel base context
  *                                         pointer from a device file
@@ -325,6 +352,8 @@ static int kbase_file_create_kctx(struct kbase_file *kfile, base_context_create_
  *
  * Return: Address of the kernel base context associated with the @kfile, or
  *         NULL if no context exists.
+ *
+ * Note: This function shall always be called in Userspace context.
  */
 static struct kbase_context *kbase_file_get_kctx_if_setup_complete(struct kbase_file *const kfile)
 {
@@ -336,37 +365,102 @@ static struct kbase_context *kbase_file_get_kctx_if_setup_complete(struct kbase_
 }
 
 /**
- * kbase_file_delete - Destroy an object representing a device file
+ * kbase_file_destroy_kctx - Destroy the Kbase context created for @kfile.
  *
  * @kfile: A device file created by kbase_file_new()
- *
- * If any context was created for the @kfile then it is destroyed.
  */
-static void kbase_file_delete(struct kbase_file *const kfile)
+static void kbase_file_destroy_kctx(struct kbase_file *const kfile)
 {
-	struct kbase_device *kbdev = NULL;
-
-	if (WARN_ON(!kfile))
+	if (atomic_cmpxchg(&kfile->setup_state, KBASE_FILE_COMPLETE, KBASE_FILE_DESTROY_CTX) !=
+	    KBASE_FILE_COMPLETE)
 		return;
 
-	kfile->filp->private_data = NULL;
-	kbdev = kfile->kbdev;
-
-	if (atomic_read(&kfile->setup_state) == KBASE_FILE_COMPLETE) {
-		struct kbase_context *kctx = kfile->kctx;
-
 #if IS_ENABLED(CONFIG_DEBUG_FS)
-		kbasep_mem_profile_debugfs_remove(kctx);
+	kbasep_mem_profile_debugfs_remove(kfile->kctx);
+	kbase_context_debugfs_term(kfile->kctx);
 #endif
-		kbase_context_debugfs_term(kctx);
 
-		kbase_destroy_context(kctx);
+	kbase_destroy_context(kfile->kctx);
+	dev_dbg(kfile->kbdev->dev, "Deleted kbase context");
+}
+
+/**
+ * kbase_file_destroy_kctx_worker - Work item to destroy the Kbase context.
+ *
+ * @work: Pointer to the kfile::destroy_kctx_work.
+ *
+ * The work item shall only be enqueued if the context termination could not
+ * be done from @kbase_flush().
+ */
+static void kbase_file_destroy_kctx_worker(struct work_struct *work)
+{
+	struct kbase_file *kfile = container_of(work, struct kbase_file, destroy_kctx_work);
+
+	WARN_ON_ONCE(kfile->owner);
+	WARN_ON_ONCE(kfile->map_count);
+	WARN_ON_ONCE(kfile->fops_count);
+
+	kbase_file_destroy_kctx(kfile);
+}
+
+/**
+ * kbase_file_destroy_kctx_on_flush - Try destroy the Kbase context from the flush()
+ *                                    method of @kbase_fops.
+ *
+ * @kfile: A device file created by kbase_file_new()
+ */
+static void kbase_file_destroy_kctx_on_flush(struct kbase_file *const kfile)
+{
+	bool can_destroy_context = false;
+
+	spin_lock(&kfile->lock);
+	kfile->owner = NULL;
+	/* To destroy the context from flush() method, unlike the release()
+	 * method, need to synchronize manually against the other threads in
+	 * the current process that could be operating on the /dev/malixx file.
+	 *
+	 * Only destroy the context if all the memory mappings on the
+	 * /dev/malixx file instance have been closed. If there are mappings
+	 * present then the context would be destroyed later when the last
+	 * mapping is closed.
+	 * Also, only destroy the context if no file operations are in progress.
+	 */
+	can_destroy_context = !kfile->map_count && !kfile->fops_count;
+	spin_unlock(&kfile->lock);
 
-		dev_dbg(kbdev->dev, "deleted base context\n");
+	if (likely(can_destroy_context)) {
+		WARN_ON_ONCE(work_pending(&kfile->destroy_kctx_work));
+		kbase_file_destroy_kctx(kfile);
 	}
+}
 
-	kbase_release_device(kbdev);
+/**
+ * kbase_file_delete - Destroy an object representing a device file
+ *
+ * @kfile: A device file created by kbase_file_new()
+ *
+ * If any context was created for the @kfile and is still alive, then it is destroyed.
+ */
+static void kbase_file_delete(struct kbase_file *const kfile)
+{
+	if (WARN_ON(!kfile))
+		return;
+
+	/* All the CPU mappings on the device file should have been closed */
+	WARN_ON_ONCE(kfile->map_count);
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+	/* There could still be file operations due to the debugfs file (mem_view) */
+	wait_event(kfile->zero_fops_count_wait, !kbase_file_fops_count(kfile));
+#else
+	/* There shall not be any file operations in progress on the device file */
+	WARN_ON_ONCE(kfile->fops_count);
+#endif
 
+	kfile->filp->private_data = NULL;
+	cancel_work_sync(&kfile->destroy_kctx_work);
+	/* Destroy the context if it wasn't done earlier from the flush() method. */
+	kbase_file_destroy_kctx(kfile);
+	kbase_release_device(kfile->kbdev);
 	kfree(kfile);
 }
 
@@ -432,28 +526,26 @@ static struct kbase_device *to_kbase_device(struct device *dev)
 	return dev_get_drvdata(dev);
 }
 
-/**
- * get_irqs - Get interrupts information from the device tree.
- *
- * @kbdev: Kbase device.
- * @pdev:  Platform device of the kbase device
- *
- * Read interrupt number and flag for 'JOB', 'MMU' and 'GPU' interrupts
- * from the device tree and fill them into the struct of the kbase device.
- *
- * Return: 0 on successful reading of all the entries JOB, MMU and GPU interrupts.
- *         -EINVAL on failure for all other cases.
- *
- */
-static int get_irqs(struct kbase_device *kbdev, struct platform_device *pdev)
+int assign_irqs(struct kbase_device *kbdev)
 {
-	int i;
 	static const char *const irq_names_caps[] = { "JOB", "MMU", "GPU" };
 
+#if IS_ENABLED(CONFIG_OF)
+	static const char *const irq_names[] = { "job", "mmu", "gpu" };
+#endif
+
+	struct platform_device *pdev;
+	size_t i;
+
+	if (!kbdev)
+		return -ENODEV;
+
+	pdev = to_platform_device(kbdev->dev);
+
 	for (i = 0; i < ARRAY_SIZE(irq_names_caps); i++) {
-		struct irq_data *irqdata;
 		int irq;
 
+#if IS_ENABLED(CONFIG_OF)
 		/* We recommend using Upper case for the irq names in dts, but if
 		 * there are devices in the world using Lower case then we should
 		 * avoid breaking support for them. So try using names in Upper case
@@ -461,45 +553,24 @@ static int get_irqs(struct kbase_device *kbdev, struct platform_device *pdev)
 		 * we assume there is no IRQ resource specified for the GPU.
 		 */
 		irq = platform_get_irq_byname(pdev, irq_names_caps[i]);
-		if (irq < 0) {
-			static const char *const irq_names[] = { "job", "mmu", "gpu" };
-
+		if (irq < 0)
 			irq = platform_get_irq_byname(pdev, irq_names[i]);
-		}
+#else
+		irq = platform_get_irq(pdev, i);
+#endif /* CONFIG_OF */
 
-		if (irq < 0)
+		if (irq < 0) {
+			dev_err(kbdev->dev, "No IRQ resource '%s'\n", irq_names_caps[i]);
 			return irq;
+		}
 
-		kbdev->irqs[i].irq = (u32)irq;
-		irqdata = irq_get_irq_data((unsigned int)irq);
-		if (likely(irqdata))
-			kbdev->irqs[i].flags = irqd_get_trigger_type(irqdata);
-		else
-			return -EINVAL;
-
-		kbdev->nr_irqs++;
+		kbdev->irqs[i].irq = irq;
+		kbdev->irqs[i].flags = irqd_get_trigger_type(irq_get_irq_data(irq));
 	}
 
 	return 0;
 }
 
-
-int kbase_get_irqs(struct kbase_device *kbdev)
-{
-	int result;
-	struct platform_device *pdev = to_platform_device(kbdev->dev);
-
-	kbdev->nr_irqs = 0;
-	result = get_irqs(kbdev, pdev);
-	if (!result)
-		return result;
-
-	if (result)
-		dev_err(kbdev->dev, "Invalid or No interrupt resources");
-
-	return result;
-}
-
 /* Find a particular kbase device (as specified by minor number), or find the "first" device if -1 is specified */
 struct kbase_device *kbase_find_device(int minor)
 {
@@ -548,19 +619,19 @@ static ssize_t write_ctx_infinite_cache(struct file *f, const char __user *ubuf,
 	else
 		kbase_ctx_flag_clear(kctx, KCTX_INFINITE_CACHE);
 
-	return (ssize_t)size;
+	return size;
 }
 
 static ssize_t read_ctx_infinite_cache(struct file *f, char __user *ubuf, size_t size, loff_t *off)
 {
 	struct kbase_context *kctx = f->private_data;
 	char buf[32];
-	size_t count;
+	int count;
 	bool value;
 
 	value = kbase_ctx_flag(kctx, KCTX_INFINITE_CACHE);
 
-	count = (size_t)scnprintf(buf, sizeof(buf), "%s\n", value ? "Y" : "N");
+	count = scnprintf(buf, sizeof(buf), "%s\n", value ? "Y" : "N");
 
 	return simple_read_from_buffer(ubuf, size, off, buf, count);
 }
@@ -599,19 +670,19 @@ static ssize_t write_ctx_force_same_va(struct file *f, const char __user *ubuf,
 		kbase_ctx_flag_clear(kctx, KCTX_FORCE_SAME_VA);
 	}
 
-	return (ssize_t)size;
+	return size;
 }
 
 static ssize_t read_ctx_force_same_va(struct file *f, char __user *ubuf, size_t size, loff_t *off)
 {
 	struct kbase_context *kctx = f->private_data;
 	char buf[32];
-	size_t count;
+	int count;
 	bool value;
 
 	value = kbase_ctx_flag(kctx, KCTX_FORCE_SAME_VA);
 
-	count = (size_t)scnprintf(buf, sizeof(buf), "%s\n", value ? "Y" : "N");
+	count = scnprintf(buf, sizeof(buf), "%s\n", value ? "Y" : "N");
 
 	return simple_read_from_buffer(ubuf, size, off, buf, count);
 }
@@ -645,8 +716,7 @@ static int kbase_file_create_kctx(struct kbase_file *const kfile,
 
 	kbdev = kfile->kbdev;
 
-	kctx = kbase_create_context(kbdev, in_compat_syscall(), flags, kfile->api_version,
-				    kfile->filp);
+	kctx = kbase_create_context(kbdev, in_compat_syscall(), flags, kfile->api_version, kfile);
 
 	/* if bad flags, will stay stuck in setup mode */
 	if (!kctx)
@@ -690,7 +760,7 @@ static int kbase_open(struct inode *inode, struct file *filp)
 	struct kbase_file *kfile;
 	int ret = 0;
 
-	kbdev = kbase_find_device((int)iminor(inode));
+	kbdev = kbase_find_device(iminor(inode));
 
 	if (!kbdev)
 		return -ENODEV;
@@ -733,6 +803,36 @@ static int kbase_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+/**
+ * kbase_flush - Function implementing the flush() method of @kbase_fops.
+ *
+ * @filp: Pointer to the /dev/malixx device file instance.
+ * @id:   Pointer to the file table structure of current process.
+ *        If @filp is being shared by multiple processes then @id can differ
+ *        from kfile::owner.
+ *
+ * This function is called everytime the copy of @filp is closed. So if 3 processes
+ * are sharing the @filp then this function would be called 3 times and only after
+ * that kbase_release() would get called.
+ *
+ * Return: 0 if successful, otherwise a negative error code.
+ *
+ * Note: This function always gets called in Userspace context when the
+ *       file is closed.
+ */
+static int kbase_flush(struct file *filp, fl_owner_t id)
+{
+	struct kbase_file *const kfile = filp->private_data;
+
+	/* Try to destroy the context if the flush() method has been called for the
+	 * process that created the instance of /dev/malixx file i.e. 'kfile'.
+	 */
+	if (kfile->owner == id)
+		kbase_file_destroy_kctx_on_flush(kfile);
+
+	return 0;
+}
+
 static int kbase_api_set_flags(struct kbase_file *kfile, struct kbase_ioctl_set_flags *flags)
 {
 	int err = 0;
@@ -802,7 +902,7 @@ static int kbase_api_get_gpuprops(struct kbase_file *kfile,
 	}
 
 	if (get_props->size == 0)
-		return (int)kprops->prop_buffer_size;
+		return kprops->prop_buffer_size;
 	if (get_props->size < kprops->prop_buffer_size)
 		return -EINVAL;
 
@@ -810,7 +910,7 @@ static int kbase_api_get_gpuprops(struct kbase_file *kfile,
 			   kprops->prop_buffer_size);
 	if (err)
 		return -EFAULT;
-	return (int)kprops->prop_buffer_size;
+	return kprops->prop_buffer_size;
 }
 
 #if !MALI_USE_CSF
@@ -826,7 +926,7 @@ static int kbase_api_mem_alloc_ex(struct kbase_context *kctx,
 				  union kbase_ioctl_mem_alloc_ex *alloc_ex)
 {
 	struct kbase_va_region *reg;
-	base_mem_alloc_flags flags = alloc_ex->in.flags;
+	u64 flags = alloc_ex->in.flags;
 	u64 gpu_va;
 
 	/* Calls to this function are inherently asynchronous, with respect to
@@ -936,7 +1036,7 @@ static int kbase_api_mem_alloc(struct kbase_context *kctx, union kbase_ioctl_mem
 static int kbase_api_mem_alloc(struct kbase_context *kctx, union kbase_ioctl_mem_alloc *alloc)
 {
 	struct kbase_va_region *reg;
-	base_mem_alloc_flags flags = alloc->in.flags;
+	u64 flags = alloc->in.flags;
 	u64 gpu_va;
 
 	/* Calls to this function are inherently asynchronous, with respect to
@@ -1012,8 +1112,8 @@ static int kbase_api_get_cpu_gpu_timeinfo(struct kbase_context *kctx,
 		timeinfo->out.cycle_counter = cycle_cnt;
 
 	if (flags & BASE_TIMEINFO_MONOTONIC_FLAG) {
-		timeinfo->out.sec = (u64)ts.tv_sec;
-		timeinfo->out.nsec = (u32)ts.tv_nsec;
+		timeinfo->out.sec = ts.tv_sec;
+		timeinfo->out.nsec = ts.tv_nsec;
 	}
 
 	kbase_pm_context_idle(kctx->kbdev);
@@ -1040,14 +1140,14 @@ static int kbase_api_get_ddk_version(struct kbase_context *kctx,
 				     struct kbase_ioctl_get_ddk_version *version)
 {
 	int ret;
-	int len = sizeof(KERNEL_SIDE_DDK_VERSION_STRING);
+	uint len = sizeof(KERNEL_SIDE_DDK_VERSION_STRING);
 
 	CSTD_UNUSED(kctx);
 
 	if (version->version_buffer == 0)
 		return len;
 
-	if (version->size < (u32)len)
+	if (version->size < len)
 		return -EOVERFLOW;
 
 	ret = copy_to_user(u64_to_user_ptr(version->version_buffer), KERNEL_SIDE_DDK_VERSION_STRING,
@@ -1062,6 +1162,16 @@ static int kbase_api_get_ddk_version(struct kbase_context *kctx,
 static int kbase_api_mem_jit_init(struct kbase_context *kctx,
 				  struct kbase_ioctl_mem_jit_init *jit_init)
 {
+	size_t i;
+
+	for (i = 0; i < sizeof(jit_init->padding); i++) {
+		/* Ensure all padding bytes are 0 for potential future
+		 * extension
+		 */
+		if (jit_init->padding[i])
+			return -EINVAL;
+	}
+
 	return kbase_region_tracker_init_jit(kctx, jit_init->va_pages, jit_init->max_allocations,
 					     jit_init->trim_level, jit_init->group_id,
 					     jit_init->phys_pages);
@@ -1127,7 +1237,7 @@ static int kbase_api_mem_commit(struct kbase_context *kctx, struct kbase_ioctl_m
 static int kbase_api_mem_alias(struct kbase_context *kctx, union kbase_ioctl_mem_alias *alias)
 {
 	struct base_mem_aliasing_info *ai;
-	base_mem_alloc_flags flags;
+	u64 flags;
 	int err;
 
 	if (alias->in.nents == 0 || alias->in.nents > BASE_MEM_ALIAS_MAX_ENTS)
@@ -1138,7 +1248,7 @@ static int kbase_api_mem_alias(struct kbase_context *kctx, union kbase_ioctl_mem
 		return -ENOMEM;
 
 	err = copy_from_user(ai, u64_to_user_ptr(alias->in.aliasing_info),
-			     size_mul(sizeof(*ai), alias->in.nents));
+			     sizeof(*ai) * alias->in.nents);
 	if (err) {
 		vfree(ai);
 		return -EFAULT;
@@ -1166,7 +1276,7 @@ static int kbase_api_mem_alias(struct kbase_context *kctx, union kbase_ioctl_mem
 static int kbase_api_mem_import(struct kbase_context *kctx, union kbase_ioctl_mem_import *import)
 {
 	int ret;
-	base_mem_alloc_flags flags = import->in.flags;
+	u64 flags = import->in.flags;
 
 	if (flags & BASEP_MEM_FLAGS_KERNEL_ONLY)
 		return -ENOMEM;
@@ -1278,17 +1388,15 @@ static int kbase_api_sticky_resource_map(struct kbase_context *kctx,
 	if (!map->count || map->count > BASE_EXT_RES_COUNT_MAX)
 		return -EOVERFLOW;
 
-	ret = copy_from_user(gpu_addr, u64_to_user_ptr(map->address),
-			     size_mul(sizeof(u64), map->count));
+	ret = copy_from_user(gpu_addr, u64_to_user_ptr(map->address), sizeof(u64) * map->count);
 
 	if (ret != 0)
 		return -EFAULT;
 
-	down_read(kbase_mem_get_process_mmap_lock());
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	for (i = 0; i < map->count; i++) {
-		if (!kbase_sticky_resource_acquire(kctx, gpu_addr[i], current->mm)) {
+		if (!kbase_sticky_resource_acquire(kctx, gpu_addr[i])) {
 			/* Invalid resource */
 			ret = -EINVAL;
 			break;
@@ -1302,8 +1410,7 @@ static int kbase_api_sticky_resource_map(struct kbase_context *kctx,
 		}
 	}
 
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
-	up_read(kbase_mem_get_process_mmap_lock());
+	kbase_gpu_vm_unlock(kctx);
 
 	return ret;
 }
@@ -1318,13 +1425,12 @@ static int kbase_api_sticky_resource_unmap(struct kbase_context *kctx,
 	if (!unmap->count || unmap->count > BASE_EXT_RES_COUNT_MAX)
 		return -EOVERFLOW;
 
-	ret = copy_from_user(gpu_addr, u64_to_user_ptr(unmap->address),
-			     size_mul(sizeof(u64), unmap->count));
+	ret = copy_from_user(gpu_addr, u64_to_user_ptr(unmap->address), sizeof(u64) * unmap->count);
 
 	if (ret != 0)
 		return -EFAULT;
 
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	for (i = 0; i < unmap->count; i++) {
 		if (!kbase_sticky_resource_release_force(kctx, NULL, gpu_addr[i])) {
@@ -1333,7 +1439,7 @@ static int kbase_api_sticky_resource_unmap(struct kbase_context *kctx,
 		}
 	}
 
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 
 	return ret;
 }
@@ -1391,16 +1497,11 @@ static int kbasep_cs_queue_kick(struct kbase_context *kctx, struct kbase_ioctl_c
 	return kbase_csf_queue_kick(kctx, kick);
 }
 
-static int kbasep_queue_group_clear_faults(struct kbase_context *kctx,
-					   struct kbase_ioctl_queue_group_clear_faults *faults)
-{
-	return kbase_csf_queue_group_clear_faults(kctx, faults);
-}
-
 static int kbasep_cs_queue_group_create_1_6(struct kbase_context *kctx,
 					    union kbase_ioctl_cs_queue_group_create_1_6 *create)
 {
 	int ret;
+	size_t i;
 	union kbase_ioctl_cs_queue_group_create
 		new_create = { .in = {
 				       .tiler_mask = create->in.tiler_mask,
@@ -1413,7 +1514,15 @@ static int kbasep_cs_queue_group_create_1_6(struct kbase_context *kctx,
 				       .compute_max = create->in.compute_max,
 			       } };
 
+	for (i = 0; i < ARRAY_SIZE(create->in.padding); i++) {
+		if (create->in.padding[i] != 0) {
+			dev_warn(kctx->kbdev->dev, "Invalid padding not 0 in queue group create\n");
+			return -EINVAL;
+		}
+	}
+
 	ret = kbase_csf_queue_group_create(kctx, &new_create);
+
 	create->out.group_handle = new_create.out.group_handle;
 	create->out.group_uid = new_create.out.group_uid;
 
@@ -1424,6 +1533,7 @@ static int kbasep_cs_queue_group_create_1_18(struct kbase_context *kctx,
 					     union kbase_ioctl_cs_queue_group_create_1_18 *create)
 {
 	int ret;
+	size_t i;
 	union kbase_ioctl_cs_queue_group_create
 		new_create = { .in = {
 				       .tiler_mask = create->in.tiler_mask,
@@ -1438,7 +1548,15 @@ static int kbasep_cs_queue_group_create_1_18(struct kbase_context *kctx,
 				       .dvs_buf = create->in.dvs_buf,
 			       } };
 
+	for (i = 0; i < ARRAY_SIZE(create->in.padding); i++) {
+		if (create->in.padding[i] != 0) {
+			dev_warn(kctx->kbdev->dev, "Invalid padding not 0 in queue group create\n");
+			return -EINVAL;
+		}
+	}
+
 	ret = kbase_csf_queue_group_create(kctx, &new_create);
+
 	create->out.group_handle = new_create.out.group_handle;
 	create->out.group_uid = new_create.out.group_uid;
 
@@ -1448,8 +1566,6 @@ static int kbasep_cs_queue_group_create_1_18(struct kbase_context *kctx,
 static int kbasep_cs_queue_group_create(struct kbase_context *kctx,
 					union kbase_ioctl_cs_queue_group_create *create)
 {
-	/* create->in.reserved only present pre-TDRX configuration. */
-
 	if (create->in.reserved != 0) {
 		dev_warn(kctx->kbdev->dev, "Invalid reserved field not 0 in queue group create\n");
 		return -EINVAL;
@@ -1565,15 +1681,14 @@ static int kbase_ioctl_cs_get_glb_iface(struct kbase_context *kctx,
 			&param->out.prfcnt_size, &param->out.instr_features);
 
 		if (copy_to_user(user_groups, group_data,
-				 size_mul(MIN(max_group_num, param->out.group_num),
-					  sizeof(*group_data))))
+				 MIN(max_group_num, param->out.group_num) * sizeof(*group_data)))
 			err = -EFAULT;
 	}
 
 	if (!err)
 		if (copy_to_user(user_streams, stream_data,
-				 size_mul(MIN(max_total_stream_num, param->out.total_stream_num),
-					  sizeof(*stream_data))))
+				 MIN(max_total_stream_num, param->out.total_stream_num) *
+					 sizeof(*stream_data)))
 			err = -EFAULT;
 
 	kfree(group_data);
@@ -1597,6 +1712,10 @@ static int kbase_ioctl_read_user_page(struct kbase_context *kctx,
 	if (unlikely(user_page->in.offset != LATEST_FLUSH))
 		return -EINVAL;
 
+	/* Validating padding that must be zero */
+	if (unlikely(user_page->in.padding != 0))
+		return -EINVAL;
+
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	if (!kbdev->pm.backend.gpu_powered)
 		user_page->out.val_lo = POWER_DOWN_LATEST_FLUSH_VALUE;
@@ -1623,33 +1742,83 @@ kbasep_ioctl_context_priority_check(struct kbase_context *kctx,
 	return 0;
 }
 
+#define KBASE_HANDLE_IOCTL(cmd, function, arg)                                         \
+	do {                                                                           \
+		int ret;                                                               \
+		BUILD_BUG_ON(_IOC_DIR(cmd) != _IOC_NONE);                              \
+		dev_dbg(arg->kbdev->dev, "Enter ioctl %s\n", #function);               \
+		ret = function(arg);                                                   \
+		dev_dbg(arg->kbdev->dev, "Return %d from ioctl %s\n", ret, #function); \
+		return ret;                                                            \
+	} while (0)
+
+#define KBASE_HANDLE_IOCTL_IN(cmd, function, type, arg)                                \
+	do {                                                                           \
+		type param;                                                            \
+		int ret, err;                                                          \
+		dev_dbg(arg->kbdev->dev, "Enter ioctl %s\n", #function);               \
+		BUILD_BUG_ON(_IOC_DIR(cmd) != _IOC_WRITE);                             \
+		BUILD_BUG_ON(sizeof(param) != _IOC_SIZE(cmd));                         \
+		err = copy_from_user(&param, uarg, sizeof(param));                     \
+		if (err)                                                               \
+			return -EFAULT;                                                \
+		ret = function(arg, &param);                                           \
+		dev_dbg(arg->kbdev->dev, "Return %d from ioctl %s\n", ret, #function); \
+		return ret;                                                            \
+	} while (0)
+
+#define KBASE_HANDLE_IOCTL_OUT(cmd, function, type, arg)                               \
+	do {                                                                           \
+		type param;                                                            \
+		int ret, err;                                                          \
+		dev_dbg(arg->kbdev->dev, "Enter ioctl %s\n", #function);               \
+		BUILD_BUG_ON(_IOC_DIR(cmd) != _IOC_READ);                              \
+		BUILD_BUG_ON(sizeof(param) != _IOC_SIZE(cmd));                         \
+		memset(&param, 0, sizeof(param));                                      \
+		ret = function(arg, &param);                                           \
+		err = copy_to_user(uarg, &param, sizeof(param));                       \
+		if (err)                                                               \
+			return -EFAULT;                                                \
+		dev_dbg(arg->kbdev->dev, "Return %d from ioctl %s\n", ret, #function); \
+		return ret;                                                            \
+	} while (0)
+
+#define KBASE_HANDLE_IOCTL_INOUT(cmd, function, type, arg)                             \
+	do {                                                                           \
+		type param;                                                            \
+		int ret, err;                                                          \
+		dev_dbg(arg->kbdev->dev, "Enter ioctl %s\n", #function);               \
+		BUILD_BUG_ON(_IOC_DIR(cmd) != (_IOC_WRITE | _IOC_READ));               \
+		BUILD_BUG_ON(sizeof(param) != _IOC_SIZE(cmd));                         \
+		err = copy_from_user(&param, uarg, sizeof(param));                     \
+		if (err)                                                               \
+			return -EFAULT;                                                \
+		ret = function(arg, &param);                                           \
+		err = copy_to_user(uarg, &param, sizeof(param));                       \
+		if (err)                                                               \
+			return -EFAULT;                                                \
+		dev_dbg(arg->kbdev->dev, "Return %d from ioctl %s\n", ret, #function); \
+		return ret;                                                            \
+	} while (0)
+
 static int kbasep_ioctl_set_limited_core_count(
 	struct kbase_context *kctx,
 	struct kbase_ioctl_set_limited_core_count *set_limited_core_count)
 {
 	const u64 shader_core_mask = kbase_pm_get_present_cores(kctx->kbdev, KBASE_PM_CORE_SHADER);
-	const u8 max_core_count = set_limited_core_count->max_core_count;
-	u64 limited_core_mask = 0;
-
-	/* Sanity check to avoid shift-out-of-bounds */
-	if (max_core_count > 64)
-		return -EINVAL;
-	else if (max_core_count == 64)
-		limited_core_mask = UINT64_MAX;
-	else
-		limited_core_mask = ((u64)1 << max_core_count) - 1;
+	const u64 limited_core_mask = ((u64)1 << (set_limited_core_count->max_core_count)) - 1;
 
-	/* At least one shader core must be available after applying the mask */
-	if ((shader_core_mask & limited_core_mask) == 0)
+	if ((shader_core_mask & limited_core_mask) == 0) {
+		/* At least one shader core must be available after applying the mask */
 		return -EINVAL;
+	}
 
 	kctx->limited_core_mask = limited_core_mask;
 	return 0;
 }
 
-static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+static long kbase_kfile_ioctl(struct kbase_file *kfile, unsigned int cmd, unsigned long arg)
 {
-	struct kbase_file *const kfile = filp->private_data;
 	struct kbase_context *kctx = NULL;
 	struct kbase_device *kbdev = kfile->kbdev;
 	void __user *uarg = (void __user *)arg;
@@ -1898,11 +2067,6 @@ static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_KCPU_QUEUE_ENQUEUE, kbasep_kcpu_queue_enqueue,
 				      struct kbase_ioctl_kcpu_queue_enqueue, kctx);
 		break;
-	case KBASE_IOCTL_QUEUE_GROUP_CLEAR_FAULTS:
-		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_QUEUE_GROUP_CLEAR_FAULTS,
-				      kbasep_queue_group_clear_faults,
-				      struct kbase_ioctl_queue_group_clear_faults, kctx);
-		break;
 	case KBASE_IOCTL_CS_TILER_HEAP_INIT:
 		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_CS_TILER_HEAP_INIT, kbasep_cs_tiler_heap_init,
 					 union kbase_ioctl_cs_tiler_heap_init, kctx);
@@ -1953,22 +2117,45 @@ static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 	return -ENOIOCTLCMD;
 }
 
+static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct kbase_file *const kfile = filp->private_data;
+	long ioctl_ret;
+
+	if (unlikely(!kbase_file_inc_fops_count_if_allowed(kfile)))
+		return -EPERM;
+
+	ioctl_ret = kbase_kfile_ioctl(kfile, cmd, arg);
+	kbase_file_dec_fops_count(kfile);
+
+	return ioctl_ret;
+}
+
 #if MALI_USE_CSF
 static ssize_t kbase_read(struct file *filp, char __user *buf, size_t count, loff_t *f_pos)
 {
 	struct kbase_file *const kfile = filp->private_data;
-	struct kbase_context *const kctx = kbase_file_get_kctx_if_setup_complete(kfile);
+	struct kbase_context *kctx;
 	struct base_csf_notification event_data = { .type = BASE_CSF_NOTIFICATION_EVENT };
 	const size_t data_size = sizeof(event_data);
 	bool read_event = false, read_error = false;
+	ssize_t err = 0;
 
 	CSTD_UNUSED(f_pos);
 
-	if (unlikely(!kctx))
+	if (unlikely(!kbase_file_inc_fops_count_if_allowed(kfile)))
 		return -EPERM;
 
-	if (count < data_size)
-		return -ENOBUFS;
+	kctx = kbase_file_get_kctx_if_setup_complete(kfile);
+	if (unlikely(!kctx)) {
+		err = -EPERM;
+		goto out;
+	}
+
+	if (count < data_size) {
+		err = -ENOBUFS;
+		goto out;
+	}
 
 	if (atomic_read(&kctx->event_count))
 		read_event = true;
@@ -1989,29 +2176,41 @@ static ssize_t kbase_read(struct file *filp, char __user *buf, size_t count, lof
 
 	if (copy_to_user(buf, &event_data, data_size) != 0) {
 		dev_warn(kctx->kbdev->dev, "Failed to copy data\n");
-		return -EFAULT;
+		err = -EFAULT;
+		goto out;
 	}
 
 	if (read_event)
 		atomic_set(&kctx->event_count, 0);
 
-	return data_size;
+out:
+	kbase_file_dec_fops_count(kfile);
+	return err ? err : (ssize_t)data_size;
 }
 #else /* MALI_USE_CSF */
 static ssize_t kbase_read(struct file *filp, char __user *buf, size_t count, loff_t *f_pos)
 {
 	struct kbase_file *const kfile = filp->private_data;
-	struct kbase_context *const kctx = kbase_file_get_kctx_if_setup_complete(kfile);
+	struct kbase_context *kctx;
 	struct base_jd_event_v2 uevent;
 	int out_count = 0;
+	ssize_t err = 0;
 
 	CSTD_UNUSED(f_pos);
 
-	if (unlikely(!kctx))
+	if (unlikely(!kbase_file_inc_fops_count_if_allowed(kfile)))
 		return -EPERM;
 
-	if (count < sizeof(uevent))
-		return -ENOBUFS;
+	kctx = kbase_file_get_kctx_if_setup_complete(kfile);
+	if (unlikely(!kctx)) {
+		err = -EPERM;
+		goto out;
+	}
+
+	if (count < sizeof(uevent)) {
+		err = -ENOBUFS;
+		goto out;
+	}
 
 	memset(&uevent, 0, sizeof(uevent));
 
@@ -2020,21 +2219,29 @@ static ssize_t kbase_read(struct file *filp, char __user *buf, size_t count, lof
 			if (out_count > 0)
 				goto out;
 
-			if (filp->f_flags & O_NONBLOCK)
-				return -EAGAIN;
+			if (filp->f_flags & O_NONBLOCK) {
+				err = -EAGAIN;
+				goto out;
+			}
 
-			if (wait_event_interruptible(kctx->event_queue,
-						     kbase_event_pending(kctx)) != 0)
-				return -ERESTARTSYS;
+			if (wait_event_interruptible(kfile->event_queue,
+						     kbase_event_pending(kctx)) != 0) {
+				err = -ERESTARTSYS;
+				goto out;
+			}
 		}
 		if (uevent.event_code == BASE_JD_EVENT_DRV_TERMINATED) {
-			if (out_count == 0)
-				return -EPIPE;
+			if (out_count == 0) {
+				err = -EPIPE;
+				goto out;
+			}
 			goto out;
 		}
 
-		if (copy_to_user(buf, &uevent, sizeof(uevent)) != 0)
-			return -EFAULT;
+		if (copy_to_user(buf, &uevent, sizeof(uevent)) != 0) {
+			err = -EFAULT;
+			goto out;
+		}
 
 		buf += sizeof(uevent);
 		out_count++;
@@ -2042,40 +2249,59 @@ static ssize_t kbase_read(struct file *filp, char __user *buf, size_t count, lof
 	} while (count >= sizeof(uevent));
 
 out:
-	return out_count * sizeof(uevent);
+	kbase_file_dec_fops_count(kfile);
+	return err ? err : (ssize_t)(out_count * sizeof(uevent));
 }
 #endif /* MALI_USE_CSF */
 
 static __poll_t kbase_poll(struct file *filp, poll_table *wait)
 {
 	struct kbase_file *const kfile = filp->private_data;
-	struct kbase_context *const kctx = kbase_file_get_kctx_if_setup_complete(kfile);
+	struct kbase_context *kctx;
+	__poll_t ret = 0;
+
+	if (unlikely(!kbase_file_inc_fops_count_if_allowed(kfile))) {
+#if (KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE)
+		ret = POLLNVAL;
+#else
+		ret = EPOLLNVAL;
+#endif
+		return ret;
+	}
 
+	kctx = kbase_file_get_kctx_if_setup_complete(kfile);
 	if (unlikely(!kctx)) {
 #if (KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE)
-		return POLLERR;
+		ret = POLLERR;
 #else
-		return EPOLLERR;
+		ret = EPOLLERR;
 #endif
+		goto out;
 	}
 
-	poll_wait(filp, &kctx->event_queue, wait);
+	poll_wait(filp, &kfile->event_queue, wait);
 	if (kbase_event_pending(kctx)) {
 #if (KERNEL_VERSION(4, 19, 0) > LINUX_VERSION_CODE)
-		return POLLIN | POLLRDNORM;
+		ret = POLLIN | POLLRDNORM;
 #else
-		return EPOLLIN | EPOLLRDNORM;
+		ret = EPOLLIN | EPOLLRDNORM;
 #endif
 	}
 
-	return 0;
+out:
+	kbase_file_dec_fops_count(kfile);
+	return ret;
 }
 
 void kbase_event_wakeup(struct kbase_context *kctx)
 {
 	KBASE_DEBUG_ASSERT(kctx);
 	dev_dbg(kctx->kbdev->dev, "Waking event queue for context %pK\n", (void *)kctx);
-	wake_up_interruptible(&kctx->event_queue);
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+	if (WARN_ON_ONCE(!kctx->kfile))
+		return;
+#endif
+	wake_up_interruptible(&kctx->kfile->event_queue);
 }
 
 KBASE_EXPORT_TEST_API(kbase_event_wakeup);
@@ -2108,12 +2334,20 @@ KBASE_EXPORT_TEST_API(kbase_event_pending);
 static int kbase_mmap(struct file *const filp, struct vm_area_struct *const vma)
 {
 	struct kbase_file *const kfile = filp->private_data;
-	struct kbase_context *const kctx = kbase_file_get_kctx_if_setup_complete(kfile);
+	struct kbase_context *kctx;
+	int ret;
 
-	if (unlikely(!kctx))
+	if (unlikely(!kbase_file_inc_fops_count_if_allowed(kfile)))
 		return -EPERM;
 
-	return kbase_context_mmap(kctx, vma);
+	kctx = kbase_file_get_kctx_if_setup_complete(kfile);
+	if (likely(kctx))
+		ret = kbase_context_mmap(kctx, vma);
+	else
+		ret = -EPERM;
+
+	kbase_file_dec_fops_count(kfile);
+	return ret;
 }
 
 static int kbase_check_flags(int flags)
@@ -2132,17 +2366,26 @@ static unsigned long kbase_get_unmapped_area(struct file *const filp, const unsi
 					     const unsigned long flags)
 {
 	struct kbase_file *const kfile = filp->private_data;
-	struct kbase_context *const kctx = kbase_file_get_kctx_if_setup_complete(kfile);
+	struct kbase_context *kctx;
+	unsigned long address;
 
-	if (unlikely(!kctx))
+	if (unlikely(!kbase_file_inc_fops_count_if_allowed(kfile)))
 		return -EPERM;
 
-	return kbase_context_get_unmapped_area(kctx, addr, len, pgoff, flags);
+	kctx = kbase_file_get_kctx_if_setup_complete(kfile);
+	if (likely(kctx))
+		address = kbase_context_get_unmapped_area(kctx, addr, len, pgoff, flags);
+	else
+		address = -EPERM;
+
+	kbase_file_dec_fops_count(kfile);
+	return address;
 }
 
 static const struct file_operations kbase_fops = {
 	.owner = THIS_MODULE,
 	.open = kbase_open,
+	.flush = kbase_flush,
 	.release = kbase_release,
 	.read = kbase_read,
 	.poll = kbase_poll,
@@ -2188,15 +2431,13 @@ static ssize_t power_policy_show(struct device *dev, struct device_attribute *at
 
 	for (i = 0; i < policy_count && ret < (ssize_t)PAGE_SIZE; i++) {
 		if (policy_list[i] == current_policy)
-			ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret), "[%s] ",
-					 policy_list[i]->name);
+			ret += scnprintf(buf + ret, PAGE_SIZE - ret, "[%s] ", policy_list[i]->name);
 		else
-			ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret), "%s ",
-					 policy_list[i]->name);
+			ret += scnprintf(buf + ret, PAGE_SIZE - ret, "%s ", policy_list[i]->name);
 	}
 
 	if (ret < (ssize_t)PAGE_SIZE - 1) {
-		ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret), "\n");
+		ret += scnprintf(buf + ret, PAGE_SIZE - ret, "\n");
 	} else {
 		buf[PAGE_SIZE - 2] = '\n';
 		buf[PAGE_SIZE - 1] = '\0';
@@ -2253,7 +2494,7 @@ static ssize_t power_policy_store(struct device *dev, struct device_attribute *a
 
 	kbase_pm_set_policy(kbdev, new_policy);
 
-	return (ssize_t)count;
+	return count;
 }
 
 /*
@@ -2281,9 +2522,6 @@ static ssize_t core_mask_show(struct device *dev, struct device_attribute *attr,
 	struct kbase_device *kbdev;
 	unsigned long flags;
 	ssize_t ret = 0;
-#if !MALI_USE_CSF
-	size_t i;
-#endif
 
 	CSTD_UNUSED(attr);
 
@@ -2295,217 +2533,161 @@ static ssize_t core_mask_show(struct device *dev, struct device_attribute *attr,
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
 #if MALI_USE_CSF
-	ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret), "Current debug core mask : 0x%llX\n",
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "Current debug core mask : 0x%llX\n",
 			 kbdev->pm.debug_core_mask);
-	ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret),
-			 "Current desired core mask : 0x%llX\n", kbase_pm_ca_get_core_mask(kbdev));
-	ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret),
-			 "Current in use core mask : 0x%llX\n", kbdev->pm.backend.shaders_avail);
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "Current desired core mask : 0x%llX\n",
+			 kbase_pm_ca_get_core_mask(kbdev));
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "Current in use core mask : 0x%llX\n",
+			 kbdev->pm.backend.shaders_avail);
 #else
-	for (i = 0; i < BASE_JM_MAX_NR_SLOTS; i++) {
-		if (PAGE_SIZE < ret)
-			goto out_unlock;
-
-		ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret),
-				 "Current core mask (JS%zu) : 0x%llX\n", i,
-				 kbdev->pm.debug_core_mask[i]);
-	}
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "Current core mask (JS0) : 0x%llX\n",
+			 kbdev->pm.debug_core_mask[0]);
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "Current core mask (JS1) : 0x%llX\n",
+			 kbdev->pm.debug_core_mask[1]);
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "Current core mask (JS2) : 0x%llX\n",
+			 kbdev->pm.debug_core_mask[2]);
 #endif /* MALI_USE_CSF */
 
-	ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret), "Available core mask : 0x%llX\n",
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "Available core mask : 0x%llX\n",
 			 kbdev->gpu_props.shader_present);
-#if !MALI_USE_CSF
-out_unlock:
-#endif
+
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 	return ret;
 }
 
+/**
+ * core_mask_store - Store callback for the core_mask sysfs file.
+ *
+ * @dev:	The device with sysfs file is for
+ * @attr:	The attributes of the sysfs file
+ * @buf:	The value written to the sysfs file
+ * @count:	The number of bytes to write to the sysfs file
+ *
+ * This function is called when the core_mask sysfs file is written to.
+ *
+ * Return: @count if the function succeeded. An error code on failure.
+ */
+static ssize_t core_mask_store(struct device *dev, struct device_attribute *attr, const char *buf,
+			       size_t count)
+{
+	struct kbase_device *kbdev;
 #if MALI_USE_CSF
-struct kbase_core_mask {
 	u64 new_core_mask;
-};
-
-static int core_mask_parse(struct kbase_device *const kbdev, const char *const buf,
-			   struct kbase_core_mask *const mask)
-{
-	int err = kstrtou64(buf, 0, &mask->new_core_mask);
-
-	if (err)
-		dev_err(kbdev->dev, "Couldn't process core mask write operation.\n");
-
-	return err;
-}
+#else
+	u64 new_core_mask[3];
+	u64 group_core_mask;
+	int i;
+#endif /* MALI_USE_CSF */
 
-static int core_mask_set(struct kbase_device *kbdev, struct kbase_core_mask *const new_mask)
-{
-	u64 new_core_mask = new_mask->new_core_mask;
-	u64 shader_present;
+	int items;
+	ssize_t err = count;
 	unsigned long flags;
-	int ret = 0;
-
-	kbase_csf_scheduler_lock(kbdev);
-	kbase_pm_lock(kbdev);
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-
-	shader_present = kbdev->gpu_props.shader_present;
+	u64 shader_present;
 
-	if ((new_core_mask & shader_present) != new_core_mask) {
-		dev_err(kbdev->dev,
-			"Invalid requested core mask 0x%llX: Includes non-existent cores (present = 0x%llX)",
-			new_core_mask, shader_present);
-		ret = -EINVAL;
-		goto exit;
-	} else if (!(new_core_mask & shader_present & kbdev->pm.backend.ca_cores_enabled)) {
-		dev_err(kbdev->dev,
-			"Invalid requested core mask 0x%llX: No intersection with currently available cores (present = 0x%llX, CA enabled = 0x%llX)",
-			new_core_mask, kbdev->gpu_props.shader_present,
-			kbdev->pm.backend.ca_cores_enabled);
-		ret = -EINVAL;
-		goto exit;
-	}
+	CSTD_UNUSED(attr);
 
+	kbdev = to_kbase_device(dev);
 
-	if (kbdev->pm.debug_core_mask != new_core_mask)
-		kbase_pm_set_debug_core_mask(kbdev, new_core_mask);
+	if (!kbdev)
+		return -ENODEV;
 
-exit:
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-	kbase_pm_unlock(kbdev);
-	kbase_csf_scheduler_unlock(kbdev);
+#if MALI_USE_CSF
+	items = sscanf(buf, "%llx", &new_core_mask);
 
-	return ret;
-}
+	if (items != 1) {
+		dev_err(kbdev->dev, "Couldn't process core mask write operation.\n"
+				    "Use format <core_mask>\n");
+		err = -EINVAL;
+		goto end;
+	}
 #else
-struct kbase_core_mask {
-	u64 new_core_mask[BASE_JM_MAX_NR_SLOTS];
-};
+	items = sscanf(buf, "%llx %llx %llx", &new_core_mask[0], &new_core_mask[1],
+		       &new_core_mask[2]);
 
-static int core_mask_parse(struct kbase_device *const kbdev, const char *const buf,
-			   struct kbase_core_mask *const mask)
-{
-	int items;
-
-	items = sscanf(buf, "%llx %llx %llx", &mask->new_core_mask[0], &mask->new_core_mask[1],
-		       &mask->new_core_mask[2]);
-
-	if (items != 1 && items != BASE_JM_MAX_NR_SLOTS) {
+	if (items != 1 && items != 3) {
 		dev_err(kbdev->dev, "Couldn't process core mask write operation.\n"
 				    "Use format <core_mask>\n"
 				    "or <core_mask_js0> <core_mask_js1> <core_mask_js2>\n");
-		return -EINVAL;
+		err = -EINVAL;
+		goto end;
 	}
 
-	/* If only one value was provided, set all other core masks equal to the value. */
-	if (items == 1) {
-		size_t i;
+	if (items == 1)
+		new_core_mask[1] = new_core_mask[2] = new_core_mask[0];
+#endif
 
-		for (i = 1; i < BASE_JM_MAX_NR_SLOTS; i++)
-			mask->new_core_mask[i] = mask->new_core_mask[0];
-	}
+	mutex_lock(&kbdev->pm.lock);
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
-	return 0;
-}
+	shader_present = kbdev->gpu_props.shader_present;
 
-static int core_mask_set(struct kbase_device *kbdev, struct kbase_core_mask *const new_mask)
-{
-	u64 shader_present = kbdev->gpu_props.shader_present;
-	u64 group_core_mask = kbdev->gpu_props.coherency_info.group.core_mask;
-	u64 *new_core_mask;
-	unsigned long flags;
-	int ret = 0;
-	size_t i;
+#if MALI_USE_CSF
+	if ((new_core_mask & shader_present) != new_core_mask) {
+		dev_err(dev,
+			"Invalid core mask 0x%llX: Includes non-existent cores (present = 0x%llX)",
+			new_core_mask, shader_present);
+		err = -EINVAL;
+		goto unlock;
 
-	kbase_pm_lock(kbdev);
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	} else if (!(new_core_mask & shader_present & kbdev->pm.backend.ca_cores_enabled)) {
+		dev_err(dev,
+			"Invalid core mask 0x%llX: No intersection with currently available cores (present = 0x%llX, CA enabled = 0x%llX\n",
+			new_core_mask, kbdev->gpu_props.shader_present,
+			kbdev->pm.backend.ca_cores_enabled);
+		err = -EINVAL;
+		goto unlock;
+	}
 
-	new_core_mask = &new_mask->new_core_mask[0];
+	if (kbdev->pm.debug_core_mask != new_core_mask)
+		kbase_pm_set_debug_core_mask(kbdev, new_core_mask);
+#else
+	group_core_mask = kbdev->gpu_props.coherency_info.group.core_mask;
 
-	for (i = 0; i < BASE_JM_MAX_NR_SLOTS; ++i) {
+	for (i = 0; i < 3; ++i) {
 		if ((new_core_mask[i] & shader_present) != new_core_mask[i]) {
-			dev_err(kbdev->dev,
-				"Invalid core mask 0x%llX for JS %zu: Includes non-existent cores (present = 0x%llX)",
+			dev_err(dev,
+				"Invalid core mask 0x%llX for JS %d: Includes non-existent cores (present = 0x%llX)",
 				new_core_mask[i], i, shader_present);
-			ret = -EINVAL;
-			goto exit;
+			err = -EINVAL;
+			goto unlock;
 
 		} else if (!(new_core_mask[i] & shader_present &
 			     kbdev->pm.backend.ca_cores_enabled)) {
-			dev_err(kbdev->dev,
-				"Invalid core mask 0x%llX for JS %zu: No intersection with currently available cores (present = 0x%llX, CA enabled = 0x%llX)",
+			dev_err(dev,
+				"Invalid core mask 0x%llX for JS %d: No intersection with currently available cores (present = 0x%llX, CA enabled = 0x%llX\n",
 				new_core_mask[i], i, kbdev->gpu_props.shader_present,
 				kbdev->pm.backend.ca_cores_enabled);
-			ret = -EINVAL;
-			goto exit;
+			err = -EINVAL;
+			goto unlock;
 		} else if (!(new_core_mask[i] & group_core_mask)) {
-			dev_err(kbdev->dev,
-				"Invalid core mask 0x%llX for JS %zu: No intersection with group 0 core mask 0x%llX",
+			dev_err(dev,
+				"Invalid core mask 0x%llX for JS %d: No intersection with group 0 core mask 0x%llX\n",
 				new_core_mask[i], i, group_core_mask);
-			ret = -EINVAL;
-			goto exit;
+			err = -EINVAL;
+			goto unlock;
 		} else if (!(new_core_mask[i] & kbdev->gpu_props.curr_config.shader_present)) {
-			dev_err(kbdev->dev,
-				"Invalid core mask 0x%llX for JS %zu: No intersection with current core mask 0x%llX",
+			dev_err(dev,
+				"Invalid core mask 0x%llX for JS %d: No intersection with current core mask 0x%llX\n",
 				new_core_mask[i], i, kbdev->gpu_props.curr_config.shader_present);
-			ret = -EINVAL;
-			goto exit;
+			err = -EINVAL;
+			goto unlock;
 		}
 	}
 
-	for (i = 0; i < BASE_JM_MAX_NR_SLOTS; i++) {
-		if (kbdev->pm.debug_core_mask[i] != new_core_mask[i]) {
-			kbase_pm_set_debug_core_mask(kbdev, new_core_mask, BASE_JM_MAX_NR_SLOTS);
-			break;
-		}
+	if (kbdev->pm.debug_core_mask[0] != new_core_mask[0] ||
+	    kbdev->pm.debug_core_mask[1] != new_core_mask[1] ||
+	    kbdev->pm.debug_core_mask[2] != new_core_mask[2]) {
+		kbase_pm_set_debug_core_mask(kbdev, new_core_mask[0], new_core_mask[1],
+					     new_core_mask[2]);
 	}
+#endif /* MALI_USE_CSF */
 
-exit:
+unlock:
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-	kbase_pm_unlock(kbdev);
-
-	return ret;
-}
-
-#endif
-
-/**
- * core_mask_store - Store callback for the core_mask sysfs file.
- *
- * @dev:	The device with sysfs file is for
- * @attr:	The attributes of the sysfs file
- * @buf:	The value written to the sysfs file
- * @count:	The number of bytes to write to the sysfs file
- *
- * This function is called when the core_mask sysfs file is written to.
- *
- * Return: @count if the function succeeded. An error code on failure.
- */
-static ssize_t core_mask_store(struct device *dev, struct device_attribute *attr, const char *buf,
-			       size_t count)
-{
-	struct kbase_device *kbdev;
-	struct kbase_core_mask core_mask = {};
-
-	int err;
-
-	CSTD_UNUSED(attr);
-
-	kbdev = to_kbase_device(dev);
-
-	if (!kbdev)
-		return -ENODEV;
-
-	err = core_mask_parse(kbdev, buf, &core_mask);
-	if (err)
-		return err;
-
-	err = core_mask_set(kbdev, &core_mask);
-
-	if (err)
-		return err;
-
-	return count;
+	mutex_unlock(&kbdev->pm.lock);
+end:
+	return err;
 }
 
 /*
@@ -2552,7 +2734,7 @@ static ssize_t soft_job_timeout_store(struct device *dev, struct device_attribut
 
 	atomic_set(&kbdev->js_data.soft_job_timeout_ms, soft_job_timeout_ms);
 
-	return (ssize_t)count;
+	return count;
 }
 
 /**
@@ -2587,14 +2769,14 @@ static u32 timeout_ms_to_ticks(struct kbase_device *kbdev, long timeout_ms, int
 			       u32 old_ticks)
 {
 	if (timeout_ms > 0) {
-		u64 ticks = (u64)timeout_ms * 1000000ULL;
+		u64 ticks = timeout_ms * 1000000ULL;
 
 		do_div(ticks, kbdev->js_data.scheduling_period_ns);
 		if (!ticks)
 			return 1;
 		return ticks;
 	} else if (timeout_ms < 0) {
-		return (u32)default_ticks;
+		return default_ticks;
 	} else {
 		return old_ticks;
 	}
@@ -2678,7 +2860,7 @@ static ssize_t js_timeouts_store(struct device *dev, struct device_attribute *at
 
 		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
-		return (ssize_t)count;
+		return count;
 	}
 
 	dev_err(kbdev->dev,
@@ -2860,7 +3042,7 @@ static ssize_t js_scheduling_period_store(struct device *dev, struct device_attr
 
 	dev_dbg(kbdev->dev, "JS scheduling period: %dms\n", js_scheduling_period);
 
-	return (ssize_t)count;
+	return count;
 }
 
 /**
@@ -2921,7 +3103,7 @@ static ssize_t js_softstop_always_store(struct device *dev, struct device_attrib
 	kbdev->js_data.softstop_always = (bool)softstop_always;
 	dev_dbg(kbdev->dev, "Support for softstop on a single context: %s\n",
 		(kbdev->js_data.softstop_always) ? "Enabled" : "Disabled");
-	return (ssize_t)count;
+	return count;
 }
 
 static ssize_t js_softstop_always_show(struct device *dev, struct device_attribute *attr,
@@ -3010,8 +3192,7 @@ static ssize_t debug_command_show(struct device *dev, struct device_attribute *a
 		return -ENODEV;
 
 	for (i = 0; i < KBASEP_DEBUG_COMMAND_COUNT && ret < (ssize_t)PAGE_SIZE; i++)
-		ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret), "%s\n",
-				 debug_commands[i].str);
+		ret += scnprintf(buf + ret, PAGE_SIZE - ret, "%s\n", debug_commands[i].str);
 
 	if (ret >= (ssize_t)PAGE_SIZE) {
 		buf[PAGE_SIZE - 2] = '\n';
@@ -3053,7 +3234,7 @@ static ssize_t debug_command_store(struct device *dev, struct device_attribute *
 	for (i = 0; i < KBASEP_DEBUG_COMMAND_COUNT; i++) {
 		if (sysfs_streq(debug_commands[i].str, buf)) {
 			debug_commands[i].func(kbdev);
-			return (ssize_t)count;
+			return count;
 		}
 	}
 
@@ -3107,7 +3288,6 @@ static ssize_t gpuinfo_show(struct device *dev, struct device_attribute *attr, c
 		{ .id = GPU_ID_PRODUCT_LODX, .name = "Mali-G610" },
 		{ .id = GPU_ID_PRODUCT_TGRX, .name = "Mali-G510" },
 		{ .id = GPU_ID_PRODUCT_TVAX, .name = "Mali-G310" },
-		{ .id = GPU_ID_PRODUCT_LTUX, .name = "Mali-G615" },
 		{ .id = GPU_ID_PRODUCT_LTIX, .name = "Mali-G620" },
 		{ .id = GPU_ID_PRODUCT_TKRX, .name = "Mali-TKRX" },
 		{ .id = GPU_ID_PRODUCT_LKRX, .name = "Mali-LKRX" },
@@ -3173,7 +3353,6 @@ static ssize_t gpuinfo_show(struct device *dev, struct device_attribute *attr, c
 		dev_dbg(kbdev->dev, "GPU ID_Name: %s (ID: 0x%x), nr_cores(%u)\n", product_name,
 			product_id, nr_cores);
 	}
-
 #endif /* MALI_USE_CSF */
 
 	return scnprintf(buf, PAGE_SIZE, "%s %d cores r%dp%d 0x%08X\n", product_name,
@@ -3214,10 +3393,10 @@ static ssize_t dvfs_period_store(struct device *dev, struct device_attribute *at
 		return -EINVAL;
 	}
 
-	kbdev->pm.dvfs_period = (u32)dvfs_period;
+	kbdev->pm.dvfs_period = dvfs_period;
 	dev_dbg(kbdev->dev, "DVFS period: %dms\n", dvfs_period);
 
-	return (ssize_t)count;
+	return count;
 }
 
 /**
@@ -3274,8 +3453,12 @@ int kbase_pm_gpu_freq_init(struct kbase_device *kbdev)
 		/* convert found frequency to KHz */
 		found_freq /= 1000;
 
-		/* always use the lowest freqency from opp table */
-		lowest_freq_khz = found_freq;
+		/* If lowest frequency in OPP table is still higher
+		 * than the reference, then keep the reference frequency
+		 * as the one to use for scaling .
+		 */
+		if (found_freq < lowest_freq_khz)
+			lowest_freq_khz = found_freq;
 	}
 #else
 	dev_err(kbdev->dev, "No operating-points-v2 node or operating-points property in DT");
@@ -3349,7 +3532,7 @@ static ssize_t pm_poweroff_store(struct device *dev, struct device_attribute *at
 	if (poweroff_gpu_ticks != 0)
 		dev_warn(kbdev->dev, "Separate GPU poweroff delay no longer supported.\n");
 
-	return (ssize_t)count;
+	return count;
 }
 
 /**
@@ -3435,7 +3618,7 @@ static ssize_t reset_timeout_store(struct device *dev, struct device_attribute *
 	kbdev->reset_timeout_ms = reset_timeout;
 	dev_dbg(kbdev->dev, "Reset timeout: %ums\n", reset_timeout);
 
-	return (ssize_t)count;
+	return count;
 }
 
 /**
@@ -3652,13 +3835,13 @@ static DEVICE_ATTR_RW(lp_mem_pool_max_size);
 
 /**
  * show_simplified_mem_pool_max_size - Show the maximum size for the memory
- *                                     pool 0 of small (4KiB/16KiB/64KiB) pages.
+ *                                     pool 0 of small (4KiB) pages.
  * @dev:  The device this sysfs file is for.
  * @attr: The attributes of the sysfs file.
  * @buf:  The output buffer to receive the max size.
  *
  * This function is called to get the maximum size for the memory pool 0 of
- * small pages. It is assumed that the maximum size value is same for
+ * small (4KiB) pages. It is assumed that the maximum size value is same for
  * all the pools.
  *
  * Return: The number of bytes output to @buf.
@@ -3679,14 +3862,14 @@ static ssize_t show_simplified_mem_pool_max_size(struct device *dev, struct devi
 
 /**
  * set_simplified_mem_pool_max_size - Set the same maximum size for all the
- *                                    memory pools of small (4KiB/16KiB/64KiB) pages.
+ *                                    memory pools of small (4KiB) pages.
  * @dev:   The device with sysfs file is for
  * @attr:  The attributes of the sysfs file
  * @buf:   The value written to the sysfs file
  * @count: The number of bytes written to the sysfs file
  *
  * This function is called to set the same maximum size for all the memory
- * pools of small pages.
+ * pools of small (4KiB) pages.
  *
  * Return: The number of bytes output to @buf.
  */
@@ -3695,7 +3878,7 @@ static ssize_t set_simplified_mem_pool_max_size(struct device *dev, struct devic
 {
 	struct kbase_device *const kbdev = to_kbase_device(dev);
 	unsigned long new_size;
-	size_t gid;
+	int gid;
 	int err;
 
 	CSTD_UNUSED(attr);
@@ -3710,7 +3893,7 @@ static ssize_t set_simplified_mem_pool_max_size(struct device *dev, struct devic
 	for (gid = 0; gid < MEMORY_GROUP_MANAGER_NR_GROUPS; ++gid)
 		kbase_mem_pool_debugfs_set_max_size(kbdev->mem_pools.small, gid, (size_t)new_size);
 
-	return (ssize_t)count;
+	return count;
 }
 
 static DEVICE_ATTR(max_size, 0600, show_simplified_mem_pool_max_size,
@@ -3762,7 +3945,7 @@ static ssize_t set_simplified_lp_mem_pool_max_size(struct device *dev,
 {
 	struct kbase_device *const kbdev = to_kbase_device(dev);
 	unsigned long new_size;
-	size_t gid;
+	int gid;
 	int err;
 
 	CSTD_UNUSED(attr);
@@ -3777,7 +3960,7 @@ static ssize_t set_simplified_lp_mem_pool_max_size(struct device *dev,
 	for (gid = 0; gid < MEMORY_GROUP_MANAGER_NR_GROUPS; ++gid)
 		kbase_mem_pool_debugfs_set_max_size(kbdev->mem_pools.large, gid, (size_t)new_size);
 
-	return (ssize_t)count;
+	return count;
 }
 
 static DEVICE_ATTR(lp_max_size, 0600, show_simplified_lp_mem_pool_max_size,
@@ -3785,15 +3968,15 @@ static DEVICE_ATTR(lp_max_size, 0600, show_simplified_lp_mem_pool_max_size,
 
 /**
  * show_simplified_ctx_default_max_size - Show the default maximum size for the
- *                                        memory pool 0 of small (4KiB/16KiB/64KiB) pages.
+ *                                        memory pool 0 of small (4KiB) pages.
  * @dev:  The device this sysfs file is for.
  * @attr: The attributes of the sysfs file.
  * @buf:  The output buffer to receive the pool size.
  *
  * This function is called to get the default ctx maximum size for the memory
- * pool 0 of small pages. It is assumed that maximum size value is same
+ * pool 0 of small (4KiB) pages. It is assumed that maximum size value is same
  * for all the pools. The maximum size for the pool of large (2MiB) pages will
- * be same as max size of the pool of small pages in terms of bytes.
+ * be same as max size of the pool of small (4KiB) pages in terms of bytes.
  *
  * Return: The number of bytes output to @buf.
  */
@@ -3850,7 +4033,7 @@ static ssize_t set_simplified_ctx_default_max_size(struct device *dev,
 
 	kbase_mem_pool_group_config_set_max_size(&kbdev->mem_pool_defaults, (size_t)new_size);
 
-	return (ssize_t)count;
+	return count;
 }
 
 static DEVICE_ATTR(ctx_default_max_size, 0600, show_simplified_ctx_default_max_size,
@@ -3920,7 +4103,7 @@ static ssize_t js_ctx_scheduling_mode_store(struct device *dev, struct device_at
 	}
 
 	if (new_js_ctx_scheduling_mode == kbdev->js_ctx_scheduling_mode)
-		return (ssize_t)count;
+		return count;
 
 	mutex_lock(&kbdev->kctx_list_lock);
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
@@ -3937,7 +4120,7 @@ static ssize_t js_ctx_scheduling_mode_store(struct device *dev, struct device_at
 
 	dev_dbg(kbdev->dev, "JS ctx scheduling mode: %u\n", new_js_ctx_scheduling_mode);
 
-	return (ssize_t)count;
+	return count;
 }
 
 static DEVICE_ATTR_RW(js_ctx_scheduling_mode);
@@ -3993,7 +4176,7 @@ static ssize_t update_serialize_jobs_setting(struct kbase_device *kbdev, const c
 		return -EINVAL;
 	}
 
-	return (ssize_t)count;
+	return count;
 }
 
 #if IS_ENABLED(CONFIG_DEBUG_FS)
@@ -4108,15 +4291,15 @@ static ssize_t show_serialize_jobs_sysfs(struct device *dev, struct device_attri
 
 	for (i = 0; i < NR_SERIALIZE_JOBS_SETTINGS; i++) {
 		if (kbdev->serialize_jobs == serialize_jobs_settings[i].setting)
-			ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret), "[%s]",
+			ret += scnprintf(buf + ret, PAGE_SIZE - ret, "[%s]",
 					 serialize_jobs_settings[i].name);
 		else
-			ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret), "%s ",
+			ret += scnprintf(buf + ret, PAGE_SIZE - ret, "%s ",
 					 serialize_jobs_settings[i].name);
 	}
 
 	if (ret < (ssize_t)(PAGE_SIZE - 1)) {
-		ret += scnprintf(buf + ret, (size_t)(PAGE_SIZE - ret), "\n");
+		ret += scnprintf(buf + ret, PAGE_SIZE - ret, "\n");
 	} else {
 		buf[PAGE_SIZE - 2] = '\n';
 		buf[PAGE_SIZE - 1] = '\0';
@@ -4258,7 +4441,7 @@ static int kbase_common_reg_map(struct kbase_device *kbdev)
 		goto out_region;
 	}
 
-	kbdev->reg = mali_ioremap(kbdev->reg_start, kbdev->reg_size);
+	kbdev->reg = ioremap(kbdev->reg_start, kbdev->reg_size);
 	if (!kbdev->reg) {
 		dev_err(kbdev->dev, "Can't remap register window\n");
 		err = -EINVAL;
@@ -4276,7 +4459,7 @@ static int kbase_common_reg_map(struct kbase_device *kbdev)
 static void kbase_common_reg_unmap(struct kbase_device *const kbdev)
 {
 	if (kbdev->reg) {
-		mali_iounmap(kbdev->reg);
+		iounmap(kbdev->reg);
 		release_mem_region(kbdev->reg_start, kbdev->reg_size);
 		kbdev->reg = NULL;
 		kbdev->reg_start = 0;
@@ -4327,7 +4510,7 @@ void registers_unmap(struct kbase_device *kbdev)
 	kbase_common_reg_unmap(kbdev);
 }
 
-#if defined(CONFIG_OF)
+#if defined(CONFIG_MALI_ARBITER_SUPPORT) && defined(CONFIG_OF)
 
 static bool kbase_is_pm_enabled(const struct device_node *gpu_node)
 {
@@ -4354,6 +4537,17 @@ static bool kbase_is_pm_enabled(const struct device_node *gpu_node)
 	return is_pm_enable;
 }
 
+static bool kbase_is_pv_enabled(const struct device_node *gpu_node)
+{
+	const void *arbiter_if_node;
+
+	arbiter_if_node = of_get_property(gpu_node, "arbiter-if", NULL);
+	if (!arbiter_if_node)
+		arbiter_if_node = of_get_property(gpu_node, "arbiter_if", NULL);
+
+	return arbiter_if_node ? true : false;
+}
+
 static bool kbase_is_full_coherency_enabled(const struct device_node *gpu_node)
 {
 	const void *coherency_dts;
@@ -4367,61 +4561,72 @@ static bool kbase_is_full_coherency_enabled(const struct device_node *gpu_node)
 	}
 	return false;
 }
-#endif /* defined(CONFIG_OF) */
 
-int kbase_device_backend_init(struct kbase_device *kbdev)
+#endif /* CONFIG_MALI_ARBITER_SUPPORT && CONFIG_OF */
+
+int kbase_device_pm_init(struct kbase_device *kbdev)
 {
 	int err = 0;
 
-#if defined(CONFIG_OF)
-	/*
-	 * Attempt to initialize arbitration.
-	 * If the platform is not suitable for arbitration, return -EPERM.
-	 * The device initialization should not fail but kbase will
-	 * not support arbitration.
-	 */
-	if (kbase_is_pm_enabled(kbdev->dev->of_node)) {
-		/* Arbitration AND power management invalid */
-		dev_dbg(kbdev->dev, "Arbitration not supported with power management");
-		return -EPERM;
-	}
-
-	if (kbase_is_full_coherency_enabled(kbdev->dev->of_node)) {
-		/* Arbitration AND full coherency invalid */
-		dev_dbg(kbdev->dev, "Arbitration not supported with full coherency");
-		return -EPERM;
-	}
-
-	err = kbase_arbiter_pm_early_init(kbdev);
-	if (err == 0) {
-#if !MALI_USE_CSF
-		u32 product_model;
+#if defined(CONFIG_MALI_ARBITER_SUPPORT) && defined(CONFIG_OF)
+	u32 product_model;
 
-		/*
-		 * Attempt to obtain and parse gpu_id in the event an external AW module
-		 * is used for messaging. We should have access to GPU at this point.
-		 */
-		if (kbdev->gpu_props.gpu_id.arch_major == 0)
+	if (kbase_is_pv_enabled(kbdev->dev->of_node)) {
+		dev_info(kbdev->dev, "Arbitration interface enabled\n");
+		if (kbase_is_pm_enabled(kbdev->dev->of_node)) {
+			/* Arbitration AND power management invalid */
+			dev_err(kbdev->dev,
+				"Invalid combination of arbitration AND power management\n");
+			return -EPERM;
+		}
+		if (kbase_is_full_coherency_enabled(kbdev->dev->of_node)) {
+			/* Arbitration AND full coherency invalid */
+			dev_err(kbdev->dev,
+				"Invalid combination of arbitration AND full coherency\n");
+			return -EPERM;
+		}
+		err = kbase_arbiter_pm_early_init(kbdev);
+		if (err == 0) {
+			/* Check if Arbitration is running on
+			 * supported GPU platform
+			 */
+			kbase_pm_register_access_enable(kbdev);
 			kbase_gpuprops_parse_gpu_id(&kbdev->gpu_props.gpu_id,
 						    kbase_reg_get_gpu_id(kbdev));
-
-		product_model = kbdev->gpu_props.gpu_id.product_model;
-		if (product_model != GPU_ID_PRODUCT_TGOX && product_model != GPU_ID_PRODUCT_TNOX &&
-		    product_model != GPU_ID_PRODUCT_TBAX) {
-			kbase_arbiter_pm_early_term(kbdev);
-			dev_dbg(kbdev->dev, "GPU platform not suitable for arbitration");
-			return -EPERM;
+			kbase_pm_register_access_disable(kbdev);
+			product_model = kbdev->gpu_props.gpu_id.product_model;
+
+			if (product_model != GPU_ID_PRODUCT_TGOX &&
+			    product_model != GPU_ID_PRODUCT_TNOX &&
+			    product_model != GPU_ID_PRODUCT_TBAX) {
+				kbase_arbiter_pm_early_term(kbdev);
+				dev_err(kbdev->dev, "GPU platform not suitable for arbitration\n");
+				return -EPERM;
+			}
 		}
-#endif /* !MALI_USE_CSF */
-		dev_info(kbdev->dev, "Arbitration interface enabled");
+	} else {
+		kbdev->arb.arb_if = NULL;
+		kbdev->arb.arb_dev = NULL;
+		err = power_control_init(kbdev);
 	}
-#endif /* defined(CONFIG_OF) */
+#else
+	err = power_control_init(kbdev);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT && CONFIG_OF */
 	return err;
 }
 
-void kbase_device_backend_term(struct kbase_device *kbdev)
+void kbase_device_pm_term(struct kbase_device *kbdev)
 {
-	kbase_arbiter_pm_early_term(kbdev);
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+#if IS_ENABLED(CONFIG_OF)
+	if (kbase_is_pv_enabled(kbdev->dev->of_node))
+		kbase_arbiter_pm_early_term(kbdev);
+	else
+		power_control_term(kbdev);
+#endif /* CONFIG_OF */
+#else
+	power_control_term(kbdev);
+#endif
 }
 
 int power_control_init(struct kbase_device *kbdev)
@@ -4486,7 +4691,7 @@ int power_control_init(struct kbase_device *kbdev)
 	 * operating with a partial initialization of clocks.
 	 */
 	for (i = 0; i < BASE_MAX_NR_CLOCKS_REGULATORS; i++) {
-		kbdev->clocks[i] = of_clk_get(kbdev->dev->of_node, (int)i);
+		kbdev->clocks[i] = of_clk_get(kbdev->dev->of_node, i);
 		if (IS_ERR(kbdev->clocks[i])) {
 			err = PTR_ERR(kbdev->clocks[i]);
 			kbdev->clocks[i] = NULL;
@@ -4834,12 +5039,11 @@ static struct dentry *init_debugfs(struct kbase_device *kbdev)
 		return dentry;
 	}
 
-
 	dentry = debugfs_ctx_defaults_init(kbdev);
 	if (IS_ERR_OR_NULL(dentry))
 		return dentry;
 
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE)) {
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_PROTECTED_DEBUG_MODE)) {
 		dentry = debugfs_create_file("protected_debug_mode", 0444,
 					     kbdev->mali_debugfs_directory, kbdev,
 					     &fops_protected_debug_mode);
@@ -5021,93 +5225,6 @@ int kbase_device_coherency_init(struct kbase_device *kbdev)
 
 
 #if MALI_USE_CSF
-
-bool kbasep_adjust_prioritized_process(struct kbase_device *kbdev, bool add, uint32_t tgid)
-{
-	struct kbase_context *kctx;
-	bool found_contexts = false;
-
-	mutex_lock(&kbdev->kctx_list_lock);
-	list_for_each_entry(kctx, &kbdev->kctx_list, kctx_list_link) {
-		if (kctx->tgid == tgid) {
-			if (add)
-				dev_dbg(kbdev->dev,
-					"Adding context %pK of process %u to prioritized list\n",
-					(void *)kctx, tgid);
-			else
-				dev_dbg(kbdev->dev,
-					"Removing context %pK of process %u from prioritized list\n",
-					(void *)kctx, tgid);
-			atomic_set(&kctx->prioritized, add);
-			found_contexts = true;
-		}
-	}
-	mutex_unlock(&kbdev->kctx_list_lock);
-
-	if (found_contexts)
-		kbase_csf_scheduler_kick(kbdev);
-
-	return found_contexts;
-}
-
-static ssize_t add_prioritized_process_store(struct device *dev, struct device_attribute *attr,
-					     const char *buf, size_t count)
-{
-	struct kbase_device *kbdev;
-	int ret;
-	unsigned int tgid;
-
-	CSTD_UNUSED(attr);
-
-	kbdev = to_kbase_device(dev);
-	if (!kbdev)
-		return -ENODEV;
-
-	ret = kstrtouint(buf, 0, &tgid);
-	if (ret || tgid == 0) {
-		dev_err(kbdev->dev, "Invalid PID specified\n");
-		return -EINVAL;
-	}
-
-	if (unlikely(!kbasep_adjust_prioritized_process(kbdev, true, tgid))) {
-		dev_err(kbdev->dev, "Non-existent PID specified\n");
-		return -EINVAL;
-	}
-
-	return count;
-}
-
-static DEVICE_ATTR_WO(add_prioritized_process);
-
-static ssize_t remove_prioritized_process_store(struct device *dev, struct device_attribute *attr,
-						const char *buf, size_t count)
-{
-	struct kbase_device *kbdev;
-	int ret;
-	unsigned int tgid;
-
-	CSTD_UNUSED(attr);
-
-	kbdev = to_kbase_device(dev);
-	if (!kbdev)
-		return -ENODEV;
-
-	ret = kstrtouint(buf, 0, &tgid);
-	if (ret || tgid == 0) {
-		dev_err(kbdev->dev, "Invalid PID specified\n");
-		return -EINVAL;
-	}
-
-	if (unlikely(!kbasep_adjust_prioritized_process(kbdev, false, tgid))) {
-		dev_err(kbdev->dev, "Non-existent PID specified\n");
-		return -EINVAL;
-	}
-
-	return count;
-}
-
-static DEVICE_ATTR_WO(remove_prioritized_process);
-
 /**
  * csg_scheduling_period_store - Store callback for the csg_scheduling_period
  * sysfs file.
@@ -5147,7 +5264,7 @@ static ssize_t csg_scheduling_period_store(struct device *dev, struct device_att
 	dev_dbg(kbdev->dev, "CSG scheduling period: %ums\n", csg_scheduling_period);
 	kbase_csf_scheduler_unlock(kbdev);
 
-	return (ssize_t)count;
+	return count;
 }
 
 /**
@@ -5211,16 +5328,16 @@ static ssize_t fw_timeout_store(struct device *dev, struct device_attribute *att
 			"Couldn't process fw_timeout write operation.\n"
 			"Use format 'fw_timeout_ms', and fw_timeout_ms > 0\n"
 			"Default fw_timeout: %u",
-			kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT));
+			kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_PING_TIMEOUT));
 		return -EINVAL;
 	}
 
 	kbase_csf_scheduler_lock(kbdev);
-	kbase_device_set_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT, fw_timeout);
+	kbdev->csf.fw_timeout_ms = fw_timeout;
 	kbase_csf_scheduler_unlock(kbdev);
 	dev_dbg(kbdev->dev, "Firmware timeout: %ums\n", fw_timeout);
 
-	return (ssize_t)count;
+	return count;
 }
 
 /**
@@ -5244,7 +5361,7 @@ static ssize_t fw_timeout_show(struct device *dev, struct device_attribute *attr
 	if (!kbdev)
 		return -ENODEV;
 
-	ret = scnprintf(buf, PAGE_SIZE, "%u\n", kbase_get_timeout_ms(kbdev, CSF_FIRMWARE_TIMEOUT));
+	ret = scnprintf(buf, PAGE_SIZE, "%u\n", kbdev->csf.fw_timeout_ms);
 
 	return ret;
 }
@@ -5270,7 +5387,7 @@ static ssize_t idle_hysteresis_time_store(struct device *dev, struct device_attr
 					  const char *buf, size_t count)
 {
 	struct kbase_device *kbdev;
-	u32 dur_us = 0;
+	u32 dur = 0;
 
 	CSTD_UNUSED(attr);
 
@@ -5278,7 +5395,7 @@ static ssize_t idle_hysteresis_time_store(struct device *dev, struct device_attr
 	if (!kbdev)
 		return -ENODEV;
 
-	if (kstrtou32(buf, 0, &dur_us)) {
+	if (kstrtou32(buf, 0, &dur)) {
 		dev_err(kbdev->dev, "Couldn't process idle_hysteresis_time write operation.\n"
 				    "Use format <idle_hysteresis_time>\n");
 		return -EINVAL;
@@ -5287,9 +5404,9 @@ static ssize_t idle_hysteresis_time_store(struct device *dev, struct device_attr
 	/* In sysFs, The unit of the input value of idle_hysteresis_time is us.
 	 * But the unit of the input parameter of this function is ns, so multiply by 1000
 	 */
-	kbase_csf_firmware_set_gpu_idle_hysteresis_time(kbdev, (u64)dur_us * NSEC_PER_USEC);
+	kbase_csf_firmware_set_gpu_idle_hysteresis_time(kbdev, dur * NSEC_PER_USEC);
 
-	return (ssize_t)count;
+	return count;
 }
 
 /**
@@ -5299,7 +5416,7 @@ static ssize_t idle_hysteresis_time_store(struct device *dev, struct device_attr
  * @attr: The attributes of the sysfs file.
  * @buf:  The output buffer to receive the GPU information.
  *
- * This function is called to get the current idle hysteresis duration in us.
+ * This function is called to get the current idle hysteresis duration in ms.
  *
  * Return: The number of bytes output to @buf.
  */
@@ -5308,7 +5425,7 @@ static ssize_t idle_hysteresis_time_show(struct device *dev, struct device_attri
 {
 	struct kbase_device *kbdev;
 	ssize_t ret;
-	u64 dur_us;
+	u32 dur;
 
 	CSTD_UNUSED(attr);
 
@@ -5316,9 +5433,9 @@ static ssize_t idle_hysteresis_time_show(struct device *dev, struct device_attri
 	if (!kbdev)
 		return -ENODEV;
 
-	/* The unit of return value of idle_hysteresis_time_show is us, So divide by 1000 */
-	dur_us = div_u64(kbase_csf_firmware_get_gpu_idle_hysteresis_time(kbdev), NSEC_PER_USEC);
-	ret = scnprintf(buf, PAGE_SIZE, "%u\n", (u32)dur_us);
+	/* The unit of return value of idle_hysteresis_time_show is us, So divide by 1000.*/
+	dur = kbase_csf_firmware_get_gpu_idle_hysteresis_time(kbdev) / NSEC_PER_USEC;
+	ret = scnprintf(buf, PAGE_SIZE, "%u\n", dur);
 
 	return ret;
 }
@@ -5345,19 +5462,19 @@ static ssize_t idle_hysteresis_time_ns_store(struct device *dev, struct device_a
 					     const char *buf, size_t count)
 {
 	struct kbase_device *kbdev;
-	u64 dur_ns = 0;
+	u32 dur = 0;
 
 	kbdev = to_kbase_device(dev);
 	if (!kbdev)
 		return -ENODEV;
 
-	if (kstrtou64(buf, 0, &dur_ns)) {
+	if (kstrtou32(buf, 0, &dur)) {
 		dev_err(kbdev->dev, "Couldn't process idle_hysteresis_time_ns write operation.\n"
 				    "Use format <idle_hysteresis_time_ns>\n");
 		return -EINVAL;
 	}
 
-	kbase_csf_firmware_set_gpu_idle_hysteresis_time(kbdev, dur_ns);
+	kbase_csf_firmware_set_gpu_idle_hysteresis_time(kbdev, dur);
 
 	return count;
 }
@@ -5379,14 +5496,14 @@ static ssize_t idle_hysteresis_time_ns_show(struct device *dev, struct device_at
 {
 	struct kbase_device *kbdev;
 	ssize_t ret;
-	u64 dur_ns;
+	u32 dur;
 
 	kbdev = to_kbase_device(dev);
 	if (!kbdev)
 		return -ENODEV;
 
-	dur_ns = kbase_csf_firmware_get_gpu_idle_hysteresis_time(kbdev);
-	ret = scnprintf(buf, PAGE_SIZE, "%llu\n", dur_ns);
+	dur = kbase_csf_firmware_get_gpu_idle_hysteresis_time(kbdev);
+	ret = scnprintf(buf, PAGE_SIZE, "%u\n", dur);
 
 	return ret;
 }
@@ -5410,16 +5527,16 @@ static ssize_t mcu_shader_pwroff_timeout_show(struct device *dev, struct device_
 					      char *const buf)
 {
 	struct kbase_device *kbdev = dev_get_drvdata(dev);
-	u64 pwroff_us;
+	u32 pwroff;
 
 	CSTD_UNUSED(attr);
 
 	if (!kbdev)
 		return -ENODEV;
 
-	/* The unit of return value of the function is us, So divide by 1000 */
-	pwroff_us = div_u64(kbase_csf_firmware_get_mcu_core_pwroff_time(kbdev), NSEC_PER_USEC);
-	return scnprintf(buf, PAGE_SIZE, "%u\n", (u32)pwroff_us);
+	/* The unit of return value of the function is us, So divide by 1000.*/
+	pwroff = kbase_csf_firmware_get_mcu_core_pwroff_time(kbdev) / NSEC_PER_USEC;
+	return scnprintf(buf, PAGE_SIZE, "%u\n", pwroff);
 }
 
 /**
@@ -5440,7 +5557,7 @@ static ssize_t mcu_shader_pwroff_timeout_store(struct device *dev, struct device
 					       const char *buf, size_t count)
 {
 	struct kbase_device *kbdev = dev_get_drvdata(dev);
-	u32 dur_us;
+	u32 dur;
 
 	const struct kbase_pm_policy *current_policy;
 	bool always_on;
@@ -5450,20 +5567,20 @@ static ssize_t mcu_shader_pwroff_timeout_store(struct device *dev, struct device
 	if (!kbdev)
 		return -ENODEV;
 
-	if (kstrtou32(buf, 0, &dur_us))
+	if (kstrtouint(buf, 0, &dur))
 		return -EINVAL;
 
 	current_policy = kbase_pm_get_policy(kbdev);
 	always_on = current_policy == &kbase_pm_always_on_policy_ops;
-	if (dur_us == 0 && !always_on)
+	if (dur == 0 && !always_on)
 		return -EINVAL;
 
 	/* In sysFs, The unit of the input value of mcu_shader_pwroff_timeout is us.
 	 * But the unit of the input parameter of this function is ns, so multiply by 1000
 	 */
-	kbase_csf_firmware_set_mcu_core_pwroff_time(kbdev, (u64)dur_us * NSEC_PER_USEC);
+	kbase_csf_firmware_set_mcu_core_pwroff_time(kbdev, dur * NSEC_PER_USEC);
 
-	return (ssize_t)count;
+	return count;
 }
 
 static DEVICE_ATTR_RW(mcu_shader_pwroff_timeout);
@@ -5485,13 +5602,13 @@ static ssize_t mcu_shader_pwroff_timeout_ns_show(struct device *dev, struct devi
 						 char *const buf)
 {
 	struct kbase_device *kbdev = dev_get_drvdata(dev);
-	u64 pwroff_ns;
+	u32 pwroff;
 
 	if (!kbdev)
 		return -ENODEV;
 
-	pwroff_ns = kbase_csf_firmware_get_mcu_core_pwroff_time(kbdev);
-	return scnprintf(buf, PAGE_SIZE, "%llu\n", pwroff_ns);
+	pwroff = kbase_csf_firmware_get_mcu_core_pwroff_time(kbdev);
+	return scnprintf(buf, PAGE_SIZE, "%u\n", pwroff);
 }
 
 /**
@@ -5512,7 +5629,7 @@ static ssize_t mcu_shader_pwroff_timeout_ns_store(struct device *dev, struct dev
 						  const char *buf, size_t count)
 {
 	struct kbase_device *kbdev = dev_get_drvdata(dev);
-	u64 dur_ns;
+	u32 dur;
 
 	const struct kbase_pm_policy *current_policy;
 	bool always_on;
@@ -5520,15 +5637,15 @@ static ssize_t mcu_shader_pwroff_timeout_ns_store(struct device *dev, struct dev
 	if (!kbdev)
 		return -ENODEV;
 
-	if (kstrtou64(buf, 0, &dur_ns))
+	if (kstrtouint(buf, 0, &dur))
 		return -EINVAL;
 
 	current_policy = kbase_pm_get_policy(kbdev);
 	always_on = current_policy == &kbase_pm_always_on_policy_ops;
-	if (dur_ns == 0 && !always_on)
+	if (dur == 0 && !always_on)
 		return -EINVAL;
 
-	kbase_csf_firmware_set_mcu_core_pwroff_time(kbdev, dur_ns);
+	kbase_csf_firmware_set_mcu_core_pwroff_time(kbdev, dur);
 
 	return count;
 }
@@ -5563,8 +5680,6 @@ static struct attribute *kbase_attrs[] = {
 	&dev_attr_js_scheduling_period.attr,
 #else
 	&dev_attr_csg_scheduling_period.attr,
-	&dev_attr_add_prioritized_process.attr,
-	&dev_attr_remove_prioritized_process.attr,
 	&dev_attr_fw_timeout.attr,
 	&dev_attr_idle_hysteresis_time.attr,
 	&dev_attr_idle_hysteresis_time_ns.attr,
@@ -5718,19 +5833,15 @@ static int kbase_platform_device_probe(struct platform_device *pdev)
 #endif
 
 		dev_info(kbdev->dev, "Probed as %s\n", dev_name(kbdev->mdev.this_device));
-		if (PAGE_SHIFT != 12)
-			dev_warn(kbdev->dev, "Experimental feature: %s with Page Size of %luKiB",
-				 dev_name(kbdev->mdev.this_device), PAGE_SIZE / 1024);
-
 		kbase_increment_device_id();
 #if (KERNEL_VERSION(5, 3, 0) <= LINUX_VERSION_CODE)
 		mutex_unlock(&kbase_probe_mutex);
 #endif
-		if (kbase_has_arbiter(kbdev)) {
-			mutex_lock(&kbdev->pm.lock);
-			kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_GPU_INITIALIZED_EVT);
-			mutex_unlock(&kbdev->pm.lock);
-		}
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+		mutex_lock(&kbdev->pm.lock);
+		kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_GPU_INITIALIZED_EVT);
+		mutex_unlock(&kbdev->pm.lock);
+#endif
 	}
 
 	return err;
@@ -5948,8 +6059,8 @@ static int kbase_device_runtime_idle(struct device *dev)
 /* The power management operations for the platform driver.
  */
 static const struct dev_pm_ops kbase_pm_ops = {
-	SYSTEM_SLEEP_PM_OPS(kbase_device_suspend,
-			    kbase_device_resume)
+	.suspend = kbase_device_suspend,
+	.resume = kbase_device_resume,
 #ifdef KBASE_PM_RUNTIME
 	.runtime_suspend = kbase_device_runtime_suspend,
 	.runtime_resume = kbase_device_runtime_resume,
@@ -6039,8 +6150,8 @@ void kbase_trace_mali_pm_status(u32 dev_id, u32 event, u64 value)
 void kbase_trace_mali_job_slots_event(u32 dev_id, u32 event, const struct kbase_context *kctx,
 				      u8 atom_id)
 {
-	trace_mali_job_slots_event(dev_id, event, (kctx != NULL ? (u32)kctx->tgid : 0U),
-				   (kctx != NULL ? (u32)kctx->pid : 0U), atom_id);
+	trace_mali_job_slots_event(dev_id, event, (kctx != NULL ? kctx->tgid : 0),
+				   (kctx != NULL ? kctx->pid : 0), atom_id);
 }
 
 void kbase_trace_mali_page_fault_insert_pages(u32 dev_id, int event, u32 value)
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_cs_experimental.h b/drivers/gpu/arm/bifrost/mali_kbase_cs_experimental.h
index e6222979b72c..0c794e2e90bc 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_cs_experimental.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_cs_experimental.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -30,6 +30,9 @@
  */
 static inline void mali_kbase_print_cs_experimental(void)
 {
+#if MALI_INCREMENTAL_RENDERING_JM
+	pr_info("mali_kbase: INCREMENTAL_RENDERING_JM (experimental) enabled");
+#endif /* MALI_INCREMENTAL_RENDERING_JM */
 }
 
 #endif /* _KBASE_CS_EXPERIMENTAL_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.c b/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.c
index 41f8c9ca2bf8..871d7d0b8395 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.c
@@ -45,7 +45,7 @@ static int kbase_ktrace_get_ctx_refcnt(struct kbase_context *kctx)
 
 int kbase_ctx_sched_init(struct kbase_device *kbdev)
 {
-	int as_present = (1 << kbdev->nr_hw_address_spaces) - 1;
+	int as_present = (1U << kbdev->nr_hw_address_spaces) - 1;
 
 	/* These two must be recalculated if nr_hw_address_spaces changes
 	 * (e.g. for HW workarounds)
@@ -133,8 +133,7 @@ int kbase_ctx_sched_retain_ctx(struct kbase_context *kctx)
 				}
 				kctx->as_nr = free_as;
 				kbdev->as_to_kctx[free_as] = kctx;
-				KBASE_TLSTREAM_TL_KBASE_CTX_ASSIGN_AS(kbdev, kctx->id,
-								      (u32)free_as);
+				KBASE_TLSTREAM_TL_KBASE_CTX_ASSIGN_AS(kbdev, kctx->id, free_as);
 				kbase_mmu_update(kbdev, &kctx->mmu, kctx->as_nr);
 			}
 		} else {
@@ -199,7 +198,7 @@ void kbase_ctx_sched_release_ctx(struct kbase_context *kctx)
 		}
 	}
 
-	KBASE_KTRACE_ADD(kbdev, SCHED_RELEASE_CTX, kctx, (u64)new_ref_count);
+	KBASE_KTRACE_ADD(kbdev, SCHED_RELEASE_CTX, kctx, new_ref_count);
 }
 
 void kbase_ctx_sched_remove_ctx(struct kbase_context *kctx)
@@ -341,7 +340,7 @@ bool kbase_ctx_sched_inc_refcount_nolock(struct kbase_context *kctx)
 
 		kbase_ctx_sched_retain_ctx_refcount(kctx);
 		KBASE_KTRACE_ADD(kctx->kbdev, SCHED_RETAIN_CTX_NOLOCK, kctx,
-				 (u64)kbase_ktrace_get_ctx_refcnt(kctx));
+				 kbase_ktrace_get_ctx_refcnt(kctx));
 		result = true;
 	}
 
@@ -409,7 +408,7 @@ bool kbase_ctx_sched_inc_refcount_if_as_valid(struct kbase_context *kctx)
 			kbdev->as_free &= ~(1u << kctx->as_nr);
 
 		KBASE_KTRACE_ADD(kbdev, SCHED_RETAIN_CTX_NOLOCK, kctx,
-				 (u64)kbase_ktrace_get_ctx_refcnt(kctx));
+				 kbase_ktrace_get_ctx_refcnt(kctx));
 		added_ref = true;
 	}
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.h b/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.h
index fd1b82471d26..397724267fdf 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.h
@@ -22,10 +22,7 @@
 #ifndef _KBASE_CTX_SCHED_H_
 #define _KBASE_CTX_SCHED_H_
 
-#include <linux/types.h>
-
-struct kbase_context;
-struct kbase_device;
+#include <mali_kbase.h>
 
 /**
  * DOC: The Context Scheduler manages address space assignment and reference
@@ -63,7 +60,7 @@ int kbase_ctx_sched_init(struct kbase_device *kbdev);
 void kbase_ctx_sched_term(struct kbase_device *kbdev);
 
 /**
- * kbase_ctx_sched_init_ctx - Initialize per-context data fields for scheduling
+ * kbase_ctx_sched_ctx_init - Initialize per-context data fields for scheduling
  * @kctx: The context to initialize
  *
  * This must be called during context initialization before any other context
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_debug.c b/drivers/gpu/arm/bifrost/mali_kbase_debug.c
index 680ba0184215..77442feb1fda 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_debug.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_debug.c
@@ -19,8 +19,7 @@
  *
  */
 
-#include <mali_kbase_debug.h>
-#include <mali_kbase_linux.h>
+#include <mali_kbase.h>
 
 static struct kbasep_debug_assert_cb kbasep_debug_assert_registered_cb = { NULL, NULL };
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_debug.h b/drivers/gpu/arm/bifrost/mali_kbase_debug.h
index d4edb1d6f0e9..876ecdd5c617 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_debug.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_debug.h
@@ -22,10 +22,7 @@
 #ifndef _KBASE_DEBUG_H
 #define _KBASE_DEBUG_H
 
-#include <mali_malisw.h>
-
 #include <linux/bug.h>
-#include <linux/module.h>
 
 /** @brief If equals to 0, a trace containing the file, line, and function will be displayed before each message. */
 #define KBASE_DEBUG_SKIP_TRACE 0
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_debug_job_fault.h b/drivers/gpu/arm/bifrost/mali_kbase_debug_job_fault.h
index ee1228155621..0d7d2be28575 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_debug_job_fault.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_debug_job_fault.h
@@ -22,10 +22,8 @@
 #ifndef _KBASE_DEBUG_JOB_FAULT_H
 #define _KBASE_DEBUG_JOB_FAULT_H
 
-#include <linux/types.h>
-
-struct kbase_context;
-struct kbase_device;
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
 
 #define REGISTER_DUMP_TERMINATION_FLAG 0xFFFFFFFF
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_allocs.c b/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_allocs.c
index 4b7f6a186ac0..c92fb9e0957e 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_allocs.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_allocs.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2022-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -58,7 +58,7 @@ static void debug_zone_mem_allocs_show(struct kbase_reg_zone *zone, struct seq_f
 	for (p = rb_first(rbtree); p; p = rb_next(p)) {
 		reg = rb_entry(p, struct kbase_va_region, rblink);
 		if (!(reg->flags & KBASE_REG_FREE)) {
-			seq_printf(sfile, "%16llx, %16zx, %16zx, %8llx, %s\n",
+			seq_printf(sfile, "%16llx, %16zx, %16zx, %8lx, %s\n",
 				   reg->start_pfn << PAGE_SHIFT, reg->nr_pages << PAGE_SHIFT,
 				   kbase_reg_current_backed_size(reg) << PAGE_SHIFT, reg->flags,
 				   type_names[reg->gpu_alloc->type]);
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_allocs.h b/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_allocs.h
index 28df0eaba0bf..8cf69c2cbaf9 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_allocs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_allocs.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -22,7 +22,7 @@
 #ifndef _KBASE_DEBUG_MEM_ALLOCS_H
 #define _KBASE_DEBUG_MEM_ALLOCS_H
 
-struct kbase_context;
+#include <mali_kbase.h>
 
 /**
  * kbase_debug_mem_allocs_init() - Initialize the mem_allocs debugfs file
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_view.c b/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_view.c
index 48469cdcc34e..eb587bd7f6e9 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_view.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_view.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2013-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2013-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -83,7 +83,7 @@ static void *debug_mem_start(struct seq_file *m, loff_t *_pos)
 			if (!data)
 				return NULL;
 			data->lh = &map->node;
-			data->offset = (size_t)pos;
+			data->offset = pos;
 			return data;
 		}
 	}
@@ -237,11 +237,7 @@ static int debug_mem_open(struct inode *i, struct file *file)
 	int ret;
 	enum kbase_memory_zone idx;
 
-#if (KERNEL_VERSION(6, 7, 0) > LINUX_VERSION_CODE)
-	if (get_file_rcu(kctx->filp) == 0)
-#else
-	if (get_file_rcu(&kctx->filp) == 0)
-#endif
+	if (!kbase_file_inc_fops_count_unless_closed(kctx->kfile))
 		return -ENOENT;
 
 	/* Check if file was opened in write mode. GPU memory contents
@@ -301,7 +297,7 @@ static int debug_mem_open(struct inode *i, struct file *file)
 	}
 	seq_release(i, file);
 open_fail:
-	fput(kctx->filp);
+	kbase_file_dec_fops_count(kctx->kfile);
 
 	return ret;
 }
@@ -331,7 +327,7 @@ static int debug_mem_release(struct inode *inode, struct file *file)
 		kfree(mem_data);
 	}
 
-	fput(kctx->filp);
+	kbase_file_dec_fops_count(kctx->kfile);
 
 	return 0;
 }
@@ -364,7 +360,7 @@ static ssize_t debug_mem_write(struct file *file, const char __user *ubuf, size_
 	kctx->mem_view_column_width = column_width;
 	kbase_gpu_vm_unlock(kctx);
 
-	return (ssize_t)count;
+	return count;
 }
 
 static const struct file_operations kbase_debug_mem_view_fops = { .owner = THIS_MODULE,
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_zones.h b/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_zones.h
index 275c863ec159..acf349b608d5 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_zones.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_debug_mem_zones.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -22,7 +22,7 @@
 #ifndef _KBASE_DEBUG_MEM_ZONES_H
 #define _KBASE_DEBUG_MEM_ZONES_H
 
-struct kbase_context;
+#include <mali_kbase.h>
 
 /**
  * kbase_debug_mem_zones_init() - Initialize the mem_zones sysfs file
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_debugfs_helper.c b/drivers/gpu/arm/bifrost/mali_kbase_debugfs_helper.c
index 0686f32ea08d..69e715caf7cc 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_debugfs_helper.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_debugfs_helper.c
@@ -48,8 +48,7 @@
 static int set_attr_from_string(char *const buf, void *const array, size_t const nelems,
 				kbase_debugfs_helper_set_attr_fn *const set_attr_fn)
 {
-	size_t index;
-	int err = 0;
+	size_t index, err = 0;
 	char *ptr = buf;
 
 	for (index = 0; index < nelems && *ptr; ++index) {
@@ -176,8 +175,8 @@ ssize_t kbase_debugfs_helper_get_attr_to_string(char *const buf, size_t const si
 		if (index == (nelems - 1))
 			postfix = "\n";
 
-		total += scnprintf(buf + total, size - (size_t)total, "%zu%s",
-				   get_attr_fn(array, index), postfix);
+		total += scnprintf(buf + total, size - total, "%zu%s", get_attr_fn(array, index),
+				   postfix);
 	}
 
 	return total;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_defs.h b/drivers/gpu/arm/bifrost/mali_kbase_defs.h
index b97df15f7a17..9fbcde665250 100755
--- a/drivers/gpu/arm/bifrost/mali_kbase_defs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_defs.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2011-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -28,12 +28,11 @@
 #define _KBASE_DEFS_H_
 
 #include <mali_kbase_config.h>
-#include <mali_kbase_hwconfig_features.h>
-#include <mali_kbase_hwconfig_issues.h>
+#include <mali_base_hwconfig_features.h>
+#include <mali_base_hwconfig_issues.h>
 #include <mali_kbase_mem_lowlevel.h>
 #include <mmu/mali_kbase_mmu_hw.h>
 #include <backend/gpu/mali_kbase_instr_defs.h>
-#include <backend/gpu/mali_kbase_pm_defs.h>
 #include <mali_kbase_pm.h>
 #include <mali_kbase_reg_track.h>
 #include <mali_kbase_gpuprops_types.h>
@@ -46,12 +45,20 @@
 #include <hwcnt/backend/mali_kbase_hwcnt_backend_jm_watchdog.h>
 #endif
 
-#include "debug/mali_kbase_debug_ktrace_defs.h"
-
 #include <protected_mode_switcher.h>
+
+#include <linux/atomic.h>
+#include <linux/mempool.h>
+#include <linux/slab.h>
+#include <linux/file.h>
+#include <linux/sizes.h>
 #include <linux/version_compat_defs.h>
 
 
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+#include <linux/debugfs.h>
+#endif /* CONFIG_DEBUG_FS */
+
 #ifdef CONFIG_MALI_BIFROST_DEVFREQ
 #include <linux/devfreq.h>
 #endif /* CONFIG_MALI_BIFROST_DEVFREQ */
@@ -60,20 +67,16 @@
 #include <linux/devfreq_cooling.h>
 #endif
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 #include <arbiter/mali_kbase_arbiter_defs.h>
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
+#include <linux/clk.h>
+#include <linux/regulator/consumer.h>
 #include <linux/memory_group_manager.h>
 #include <soc/rockchip/rockchip_opp_select.h>
 
-#include <linux/atomic.h>
-#include <linux/mempool.h>
-#include <linux/notifier.h>
-#include <linux/slab.h>
-#include <linux/file.h>
-#include <linux/sizes.h>
-#include <linux/clk.h>
-#include <linux/debugfs.h>
-#include <linux/regulator/consumer.h>
+#include "debug/mali_kbase_debug_ktrace_defs.h"
 
 /** Number of milliseconds before we time out on a GPU soft/hard reset */
 #define RESET_TIMEOUT 500
@@ -173,11 +176,16 @@ struct kbase_gpu_metrics {
  *
  * @link:                    Links the object in kbase_device::gpu_metrics::active_list
  *                           or kbase_device::gpu_metrics::inactive_list.
- * @active_start_time:       Records the time at which the application first became
+ * @first_active_start_time: Records the time at which the application first became
+ *                           active in the current work period.
+ * @last_active_start_time:  Records the time at which the application last became
  *                           active in the current work period.
- * @active_end_time:         Records the time at which the application last became
- *                           inactive in the current work period, or the time of the end of
- *                           previous work period if the application remained active.
+ * @last_active_end_time:    Records the time at which the application last became
+ *                           inactive in the current work period.
+ * @total_active:            Tracks the time for which application has been active
+ *                           in the current work period.
+ * @prev_wp_active_end_time: Records the time at which the application last became
+ *                           inactive in the previous work period.
  * @aid:                     Unique identifier for an application.
  * @kctx_count:              Counter to keep a track of the number of Kbase contexts
  *                           created for an application. There may be multiple Kbase
@@ -185,14 +193,19 @@ struct kbase_gpu_metrics {
  *                           metrics context.
  * @active_cnt:              Counter that is updated every time the GPU activity starts
  *                           and ends in the current work period for an application.
+ * @flags:                   Flags to track the state of GPU metrics context.
  */
 struct kbase_gpu_metrics_ctx {
 	struct list_head link;
-	u64 active_start_time;
-	u64 active_end_time;
+	u64 first_active_start_time;
+	u64 last_active_start_time;
+	u64 last_active_end_time;
+	u64 total_active;
+	u64 prev_wp_active_end_time;
 	unsigned int aid;
 	unsigned int kctx_count;
 	u8 active_cnt;
+	u8 flags;
 };
 #endif
 
@@ -292,33 +305,24 @@ struct kbase_fault {
 #define MAX_PAGES_FOR_FREE_PGDS ((size_t)9)
 
 /* Maximum number of pointers to free PGDs */
-#define MAX_FREE_PGDS ((PAGE_SIZE / sizeof(phys_addr_t)) * MAX_PAGES_FOR_FREE_PGDS)
+#define MAX_FREE_PGDS ((PAGE_SIZE / sizeof(struct page *)) * MAX_PAGES_FOR_FREE_PGDS)
 
 /**
  * struct kbase_mmu_table  - object representing a set of GPU page tables
- * @mmu_lock:                Lock to serialize the accesses made to multi level GPU
- *                           page tables
- * @pgd:                     Physical address of the page allocated for the top
- *                           level page table of the context, this is used for
- *                           MMU HW programming as the address translation will
- *                           start from the top level page table.
- * @group_id:                A memory group ID to be passed to a platform-specific
- *                           memory group manager.
- *                           Valid range is 0..(MEMORY_GROUP_MANAGER_NR_GROUPS-1).
- * @kctx:                    If this set of MMU tables belongs to a context then
- *                           this is a back-reference to the context, otherwise
- *                           it is NULL.
- * @scratch_mem:             Scratch memory used for MMU operations, which are
- *                           serialized by the @mmu_lock.
- * @pgd_pages_list:          List head to link all 16K/64K pages allocated for the PGDs of mmut.
- *                           These pages will be used to allocate 4KB PGD pages for
- *                           the GPU page table.
- *                           Linked with &kbase_page_metadata.data.pt_mapped.pgd_link.
- * @last_allocated_pgd_page: Pointer to PGD page from where the last sub page
- *                           was allocated for mmut.
- * @last_freed_pgd_page:     Pointer to PGD page to which the last freed 4K sub page
- *                           was returned for mmut.
- * @num_free_pgd_sub_pages:  The total number of free 4K PGD pages in the mmut.
+ * @mmu_lock:             Lock to serialize the accesses made to multi level GPU
+ *                        page tables
+ * @pgd:                  Physical address of the page allocated for the top
+ *                        level page table of the context, this is used for
+ *                        MMU HW programming as the address translation will
+ *                        start from the top level page table.
+ * @group_id:             A memory group ID to be passed to a platform-specific
+ *                        memory group manager.
+ *                        Valid range is 0..(MEMORY_GROUP_MANAGER_NR_GROUPS-1).
+ * @kctx:                 If this set of MMU tables belongs to a context then
+ *                        this is a back-reference to the context, otherwise
+ *                        it is NULL.
+ * @scratch_mem:          Scratch memory used for MMU operations, which are
+ *                        serialized by the @mmu_lock.
  */
 struct kbase_mmu_table {
 	struct mutex mmu_lock;
@@ -336,7 +340,7 @@ struct kbase_mmu_table {
 			 * @levels: Array of PGD pages, large enough to copy one PGD
 			 *          for each level of the MMU table.
 			 */
-			u64 levels[MIDGARD_MMU_BOTTOMLEVEL][GPU_PAGE_SIZE / sizeof(u64)];
+			u64 levels[MIDGARD_MMU_BOTTOMLEVEL][PAGE_SIZE / sizeof(u64)];
 		} teardown_pages;
 		/**
 		 * @free_pgds: Scratch memory used for insertion, update and teardown
@@ -345,18 +349,11 @@ struct kbase_mmu_table {
 		 */
 		struct {
 			/** @pgds: Array of pointers to PGDs to free. */
-			phys_addr_t pgds[MAX_FREE_PGDS];
+			struct page *pgds[MAX_FREE_PGDS];
 			/** @head_index: Index of first free element in the PGDs array. */
 			size_t head_index;
 		} free_pgds;
 	} scratch_mem;
-
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	struct list_head pgd_pages_list;
-	struct page *last_allocated_pgd_page;
-	struct page *last_freed_pgd_page;
-	u32 num_free_pgd_sub_pages;
-#endif
 };
 
 #if MALI_USE_CSF
@@ -382,9 +379,14 @@ static inline int kbase_as_has_page_fault(struct kbase_as *as, struct kbase_faul
  *
  * @used_pages:   Tracks usage of OS shared memory. Updated when OS memory is
  *                allocated/freed.
+ * @ir_threshold: Fraction of the maximum size of an allocation that grows
+ *                on GPU page fault that can be used before the driver
+ *                switches to incremental rendering, in 1/256ths.
+ *                0 means disabled.
  */
 struct kbasep_mem_device {
 	atomic_t used_pages;
+	atomic_t ir_threshold;
 };
 
 struct kbase_clk_rate_listener;
@@ -489,7 +491,9 @@ struct kbase_pm_device_data {
 #if MALI_USE_CSF
 	bool runtime_active;
 #endif
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	atomic_t gpu_lost;
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 	wait_queue_head_t zero_active_count_wait;
 	wait_queue_head_t resume_wait;
 
@@ -505,8 +509,10 @@ struct kbase_pm_device_data {
 	void (*callback_power_runtime_term)(struct kbase_device *kbdev);
 	u32 dvfs_period;
 	struct kbase_pm_backend_data backend;
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	struct kbase_arbiter_vm_state *arb_vm_state;
 	atomic_t gpu_users_waiting;
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 	struct kbase_clk_rate_trace_manager clk_rtm;
 };
 
@@ -516,11 +522,8 @@ struct kbase_pm_device_data {
  * @cur_size:                  Number of free pages currently in the pool (may exceed
  *                             @max_size in some corner cases)
  * @max_size:                  Maximum number of free pages in the pool
- * @order:                     order = 0 refers to a pool of small pages
- *                             order != 0 refers to a pool of 2 MB pages, so
- *                             order = 9 (when small page size is 4KB,  2^9 *  4KB = 2 MB)
- *                             order = 7 (when small page size is 16KB, 2^7 * 16KB = 2 MB)
- *                             order = 5 (when small page size is 64KB, 2^5 * 64KB = 2 MB)
+ * @order:                     order = 0 refers to a pool of 4 KB pages
+ *                             order = 9 refers to a pool of 2 MB pages (2^9 * 4KB = 2 MB)
  * @group_id:                  A memory group ID to be passed to a platform-specific
  *                             memory group manager, if present. Immutable.
  *                             Valid range is 0..(MEMORY_GROUP_MANAGER_NR_GROUPS-1).
@@ -547,7 +550,7 @@ struct kbase_mem_pool {
 	u8 group_id;
 	spinlock_t pool_lock;
 	struct list_head page_list;
-	DEFINE_KBASE_SHRINKER reclaim;
+	struct shrinker reclaim;
 	atomic_t isolation_in_progress_cnt;
 
 	struct kbase_mem_pool *next_pool;
@@ -559,14 +562,14 @@ struct kbase_mem_pool {
 /**
  * struct kbase_mem_pool_group - a complete set of physical memory pools.
  *
- * @small: Array of objects containing the state for pools of small size
+ * @small: Array of objects containing the state for pools of 4 KiB size
  *         physical pages.
  * @large: Array of objects containing the state for pools of 2 MiB size
  *         physical pages.
  *
  * Memory pools are used to allow efficient reallocation of previously-freed
  * physical pages. A pair of memory pools is initialized for each physical
- * memory group: one for small pages and one for 2 MiB pages. These arrays
+ * memory group: one for 4 KiB pages and one for 2 MiB pages. These arrays
  * should be indexed by physical memory group ID, the meaning of which is
  * defined by the systems integrator.
  */
@@ -589,7 +592,7 @@ struct kbase_mem_pool_config {
  * struct kbase_mem_pool_group_config - Initial configuration for a complete
  *                                      set of physical memory pools
  *
- * @small: Array of initial configuration for pools of small pages.
+ * @small: Array of initial configuration for pools of 4 KiB pages.
  * @large: Array of initial configuration for pools of 2 MiB pages.
  *
  * This array should be indexed by physical memory group ID, the meaning
@@ -774,7 +777,6 @@ struct kbase_mem_migrate {
  *                         power management, cache etc.)
  * @irqs.irq:              irq number
  * @irqs.flags:            irq flags
- * @nr_irqs:               The number of interrupt entries.
  * @clocks:                Pointer to the input clock resources referenced by
  *                         the GPU device node.
  * @scmi_clk:              Pointer to the input scmi clock resources
@@ -830,8 +832,8 @@ struct kbase_mem_migrate {
  *                         group manager if no platform-specific memory group
  *                         manager was retrieved through device tree.
  * @mmu_unresponsive:      Flag to indicate MMU is not responding.
- *                         Set if a MMU command isn't completed within the
- *                         MMU_AS_INACTIVE_WAIT_TIMEOUT scaled timeout.
+ *                         Set if a MMU command isn't completed within
+ *                         &kbase_device:mmu_or_gpu_cache_op_wait_time_ms.
  *                         Clear by kbase_ctx_sched_restore_all_as() after GPU reset completes.
  * @as:                    Array of objects representing address spaces of GPU.
  * @as_to_kctx:            Array of pointers to struct kbase_context, having
@@ -839,6 +841,8 @@ struct kbase_mem_migrate {
  * @as_free:               Bitpattern of free/available GPU address spaces.
  * @mmu_mask_change:       Lock to serialize the access to MMU interrupt mask
  *                         register used in the handling of Bus & Page faults.
+ * @pagesize_2mb:          Boolean to determine whether 2MiB page sizes are
+ *                         supported and used where possible.
  * @gpu_props:             Object containing complete information about the
  *                         configuration/properties of GPU HW device in use.
  * @hw_issues_mask:        List of SW workarounds for HW issues
@@ -949,7 +953,7 @@ struct kbase_mem_migrate {
  * @ipa.last_sample_time:  Records the time when counters, used for dynamic
  *                         energy estimation, were last sampled.
  * @previous_frequency:    Previous frequency of GPU clock used for
- *                         KBASE_HW_ISSUE_GPU2017_1336 workaround, This clock is
+ *                         BASE_HW_ISSUE_GPU2017_1336 workaround, This clock is
  *                         restored when L2 is powered on.
  * @job_fault_debug:       Flag to control the dumping of debug data for job faults,
  *                         set when the 'job_fault' debugfs file is opened.
@@ -983,6 +987,13 @@ struct kbase_mem_migrate {
  *                         backend specific data for HW access layer.
  * @faults_pending:        Count of page/bus faults waiting for bottom half processing
  *                         via workqueues.
+ * @mmu_hw_operation_in_progress: Set before sending the MMU command and is
+ *                         cleared after the command is complete. Whilst this
+ *                         flag is set, the write to L2_PWROFF register will be
+ *                         skipped which is needed to workaround the HW issue
+ *                         GPU2019-3878. PM state machine is invoked after
+ *                         clearing this flag and @hwaccess_lock is used to
+ *                         serialize the access.
  * @mmu_page_migrate_in_progress: Set before starting a MMU page migration transaction
  *                         and cleared after the transaction completes. PM L2 state is
  *                         prevented from entering powering up/down transitions when the
@@ -1071,13 +1082,11 @@ struct kbase_mem_migrate {
  *                          KCPU queue. These structures may outlive kbase module
  *                          itself. Therefore, in such a case, a warning should be
  *                          be produced.
- * @va_region_slab:         kmem_cache (slab) for allocated @kbase_va_region structures.
- * @page_metadata_slab:     kmem_cache (slab) for allocated @kbase_page_metadata structures.
+ * @mmu_or_gpu_cache_op_wait_time_ms: Maximum waiting time in ms for the completion of
+ *                          a cache operation via MMU_AS_CONTROL or GPU_CONTROL.
+ * @va_region_slab:         kmem_cache (slab) for allocated kbase_va_region structures.
  * @fence_signal_timeout_enabled: Global flag for whether fence signal timeout tracking
  *                                is enabled.
- * @pcm_prioritized_process_nb: Notifier block for the Priority Control Manager
- *                              driver, this is used to be informed of the
- *                              changes in the list of prioritized processes.
  */
 struct kbase_device {
 	u32 hw_quirks_sc;
@@ -1097,10 +1106,9 @@ struct kbase_device {
 		size_t size;
 	} regmap;
 	struct {
-		u32 irq;
-		u32 flags;
+		int irq;
+		int flags;
 	} irqs[3];
-	u32 nr_irqs;
 
 	struct clk *clocks[BASE_MAX_NR_CLOCKS_REGULATORS];
 	unsigned int nr_clocks;
@@ -1135,10 +1143,12 @@ struct kbase_device {
 
 	spinlock_t mmu_mask_change;
 
+	bool pagesize_2mb;
+
 	struct kbase_gpu_props gpu_props;
 
-	unsigned long hw_issues_mask[(KBASE_HW_ISSUE_END + BITS_PER_LONG - 1) / BITS_PER_LONG];
-	unsigned long hw_features_mask[(KBASE_HW_FEATURE_END + BITS_PER_LONG - 1) / BITS_PER_LONG];
+	unsigned long hw_issues_mask[(BASE_HW_ISSUE_END + BITS_PER_LONG - 1) / BITS_PER_LONG];
+	unsigned long hw_features_mask[(BASE_HW_FEATURE_END + BITS_PER_LONG - 1) / BITS_PER_LONG];
 
 	struct {
 		atomic_t count;
@@ -1154,12 +1164,6 @@ struct kbase_device {
 	 */
 	u8 pbha_propagate_bits;
 
-	/**
-	 * @mma_wa_id: The PBHA ID to use for the PBHA OVERRIDE based workaround for MMA violation.
-	 *
-	 */
-	u32 mma_wa_id;
-
 #if MALI_USE_CSF
 	struct kbase_hwcnt_backend_csf_if hwcnt_backend_csf_if_fw;
 #else
@@ -1251,6 +1255,7 @@ struct kbase_device {
 	atomic_t job_fault_debug;
 #endif /* !MALI_USE_CSF */
 
+#if IS_ENABLED(CONFIG_DEBUG_FS)
 	struct dentry *mali_debugfs_directory;
 	struct dentry *debugfs_ctx_directory;
 	struct dentry *debugfs_instr_directory;
@@ -1272,6 +1277,7 @@ struct kbase_device {
 		u32 reg_offset;
 	} regs_dump_debugfs_data;
 #endif /* !MALI_CUSTOMER_RELEASE */
+#endif /* CONFIG_DEBUG_FS */
 
 	atomic_t ctx_num;
 
@@ -1283,6 +1289,9 @@ struct kbase_device {
 
 	atomic_t faults_pending;
 
+#if MALI_USE_CSF
+	bool mmu_hw_operation_in_progress;
+#endif
 	bool mmu_page_migrate_in_progress;
 	bool poweroff_pending;
 
@@ -1357,12 +1366,14 @@ struct kbase_device {
 	struct {
 		struct kbase_context *kctx;
 		u64 jc;
-		u32 slot;
+		int slot;
 		u64 flags;
 	} dummy_job_wa;
 	bool dummy_job_wa_loaded;
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	struct kbase_arbiter_device arb;
+#endif
 	/* Priority Control Manager device */
 	struct priority_control_manager_device *pcm_dev;
 
@@ -1386,10 +1397,8 @@ struct kbase_device {
 #if MALI_USE_CSF && IS_ENABLED(CONFIG_SYNC_FILE)
 	atomic_t live_fence_metadata;
 #endif
+	u32 mmu_or_gpu_cache_op_wait_time_ms;
 	struct kmem_cache *va_region_slab;
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	struct kmem_cache *page_metadata_slab;
-#endif
 
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
 	/**
@@ -1400,8 +1409,6 @@ struct kbase_device {
 #if MALI_USE_CSF
 	atomic_t fence_signal_timeout_enabled;
 #endif
-
-	struct notifier_block pcm_prioritized_process_nb;
 };
 
 /**
@@ -1418,6 +1425,9 @@ struct kbase_device {
  * @KBASE_FILE_COMPLETE:        Indicates if the setup for context has
  *                              completed, i.e. flags have been set for the
  *                              context.
+ * @KBASE_FILE_DESTROY_CTX:     Indicates that destroying of context has begun or
+ *                              is complete. This state can only be reached after
+ *                              @KBASE_FILE_COMPLETE.
  *
  * The driver allows only limited interaction with user-space until setup
  * is complete.
@@ -1427,7 +1437,8 @@ enum kbase_file_state {
 	KBASE_FILE_VSN_IN_PROGRESS,
 	KBASE_FILE_NEED_CTX,
 	KBASE_FILE_CTX_IN_PROGRESS,
-	KBASE_FILE_COMPLETE
+	KBASE_FILE_COMPLETE,
+	KBASE_FILE_DESTROY_CTX
 };
 
 /**
@@ -1437,6 +1448,12 @@ enum kbase_file_state {
  *                       allocated from the probe method of the Mali driver.
  * @filp:                Pointer to the struct file corresponding to device file
  *                       /dev/malixx instance, passed to the file's open method.
+ * @owner:               Pointer to the file table structure of a process that
+ *                       created the instance of /dev/malixx device file. Set to
+ *                       NULL when that process closes the file instance. No more
+ *                       file operations would be allowed once set to NULL.
+ *                       It would be updated only in the Userspace context, i.e.
+ *                       when @kbase_open or @kbase_flush is called.
  * @kctx:                Object representing an entity, among which GPU is
  *                       scheduled and which gets its own GPU address space.
  *                       Invalid until @setup_state is KBASE_FILE_COMPLETE.
@@ -1445,13 +1462,44 @@ enum kbase_file_state {
  *                       @setup_state is KBASE_FILE_NEED_CTX.
  * @setup_state:         Initialization state of the file. Values come from
  *                       the kbase_file_state enumeration.
+ * @destroy_kctx_work:   Work item for destroying the @kctx, enqueued only when
+ *                       @fops_count and @map_count becomes zero after /dev/malixx
+ *                       file was previously closed by the @owner.
+ * @lock:                Lock to serialize the access to members like @owner, @fops_count,
+ *                       @map_count.
+ * @fops_count:          Counter that is incremented at the beginning of a method
+ *                       defined for @kbase_fops and is decremented at the end.
+ *                       So the counter keeps a track of the file operations in progress
+ *                       for /dev/malixx file, that are being handled by the Kbase.
+ *                       The counter is needed to defer the context termination as
+ *                       Userspace can close the /dev/malixx file and flush() method
+ *                       can get called when some other file operation is in progress.
+ * @map_count:           Counter to keep a track of the memory mappings present on
+ *                       /dev/malixx file instance. The counter is needed to defer the
+ *                       context termination as Userspace can close the /dev/malixx
+ *                       file and flush() method can get called when mappings are still
+ *                       present.
+ * @zero_fops_count_wait: Waitqueue used to wait for the @fops_count to become 0.
+ *                        Currently needed only for the "mem_view" debugfs file.
+ * @event_queue:          Wait queue used for blocking the thread, which consumes
+ *                        the base_jd_event corresponding to an atom, when there
+ *                        are no more posted events.
  */
 struct kbase_file {
 	struct kbase_device *kbdev;
 	struct file *filp;
+	fl_owner_t owner;
 	struct kbase_context *kctx;
 	unsigned long api_version;
 	atomic_t setup_state;
+	struct work_struct destroy_kctx_work;
+	spinlock_t lock;
+	int fops_count;
+	int map_count;
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+	wait_queue_head_t zero_fops_count_wait;
+#endif
+	wait_queue_head_t event_queue;
 };
 #if MALI_JIT_PRESSURE_LIMIT_BASE
 /**
@@ -1512,11 +1560,6 @@ struct kbase_file {
  * @KCTX_JPL_ENABLED: Set when JIT physical page limit is less than JIT virtual
  * address page limit, so we must take care to not exceed the physical limit
  *
- * @KCTX_PAGE_FAULT_REPORT_SKIP: Set when the GPU page fault handler is not
- * allowed to allocate a physical page due to the process exit or context
- * termination. It is used to suppress the error messages that ensue because
- * the page fault didn't get handled.
- *
  * All members need to be separate bits. This enum is intended for use in a
  * bitmask where multiple values get OR-ed together.
  */
@@ -1537,7 +1580,6 @@ enum kbase_context_flags {
 	KCTX_PULLED_SINCE_ACTIVE_JS2 = 1U << 14,
 	KCTX_AS_DISABLED_ON_FAULT = 1U << 15,
 	KCTX_JPL_ENABLED = 1U << 16,
-	KCTX_PAGE_FAULT_REPORT_SKIP = 1U << 17,
 };
 #else
 /**
@@ -1596,11 +1638,6 @@ enum kbase_context_flags {
  * refcount for the context drops to 0 or on when the address spaces are
  * re-enabled on GPU reset or power cycle.
  *
- * @KCTX_PAGE_FAULT_REPORT_SKIP: Set when the GPU page fault handler is not
- * allowed to allocate a physical page due to the process exit or context
- * termination. It is used to suppress the error messages that ensue because
- * the page fault didn't get handled.
- *
  * All members need to be separate bits. This enum is intended for use in a
  * bitmask where multiple values get OR-ed together.
  */
@@ -1620,21 +1657,20 @@ enum kbase_context_flags {
 	KCTX_PULLED_SINCE_ACTIVE_JS1 = 1U << 13,
 	KCTX_PULLED_SINCE_ACTIVE_JS2 = 1U << 14,
 	KCTX_AS_DISABLED_ON_FAULT = 1U << 15,
-	KCTX_PAGE_FAULT_REPORT_SKIP = 1U << 16,
 };
 #endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 struct kbase_sub_alloc {
 	struct list_head link;
 	struct page *page;
-	DECLARE_BITMAP(sub_pages, NUM_PAGES_IN_2MB_LARGE_PAGE);
+	DECLARE_BITMAP(sub_pages, SZ_2M / SZ_4K);
 };
 
 /**
  * struct kbase_context - Kernel base context
  *
- * @filp:                 Pointer to the struct file corresponding to device file
- *                        /dev/malixx instance, passed to the file's open method.
+ * @kfile:                Pointer to the object representing the /dev/malixx device
+ *                        file instance.
  * @kbdev:                Pointer to the Kbase device for which the context is created.
  * @kctx_list_link:       Node into Kbase device list of contexts.
  * @mmu:                  Structure holding details of the MMU tables for this
@@ -1667,7 +1703,7 @@ struct kbase_sub_alloc {
  * @mem_partials_lock:    Lock for protecting the operations done on the elements
  *                        added to @mem_partials list.
  * @mem_partials:         List head for the list of large pages, 2MB in size, which
- *                        have been split into small pages and are used partially
+ *                        have been split into 4 KB pages and are used partially
  *                        for the allocations >= 2 MB in size.
  * @reg_lock:             Lock used for GPU virtual address space management operations,
  *                        like adding/freeing a memory region in the address space.
@@ -1687,9 +1723,6 @@ struct kbase_sub_alloc {
  *                        used in conjunction with @cookies bitmask mainly for
  *                        providing a mechansim to have the same value for CPU &
  *                        GPU virtual address.
- * @event_queue:          Wait queue used for blocking the thread, which consumes
- *                        the base_jd_event corresponding to an atom, when there
- *                        are no more posted events.
  * @tgid:                 Thread group ID of the process whose thread created
  *                        the context (by calling KBASE_IOCTL_VERSION_CHECK or
  *                        KBASE_IOCTL_SET_FLAGS, depending on the @api_version).
@@ -1700,17 +1733,10 @@ struct kbase_sub_alloc {
  *                        which actually created the context. This is usually,
  *                        but not necessarily, the same as the thread which
  *                        opened the device file /dev/malixx instance.
- * @prioritized:          Indicate whether work items originating from this
- *                        context should be treated with a higher priority
- *                        level relative to work items with the same priority
- *                        from other contexts. This value could change multiple
- *                        times over the life time of the context, such as when
- *                        an application becomes foreground or goes to the
- *                        background.
  * @csf:                  kbase csf context
  * @jctx:                 object encapsulating all the Job dispatcher related state,
  *                        including the array of atoms.
- * @used_pages:           Keeps a track of the number of small physical pages in use
+ * @used_pages:           Keeps a track of the number of 4KB physical pages in use
  *                        for the context.
  * @nonmapped_pages:      Updated in the same way as @used_pages, except for the case
  *                        when special tracking page is freed by userspace where it
@@ -1765,7 +1791,7 @@ struct kbase_sub_alloc {
  *                        on this descriptor for the Userspace created contexts so that
  *                        Kbase can safely access it to update the memory usage counters.
  *                        The reference is dropped on context termination.
- * @gpu_va_end:           End address of the GPU va space (in small page units)
+ * @gpu_va_end:           End address of the GPU va space (in 4KB page units)
  * @running_total_tiler_heap_nr_chunks: Running total of number of chunks in all
  *                        tiler heaps of the kbase context.
  * @running_total_tiler_heap_memory: Running total of the tiler heap memory in the
@@ -1901,7 +1927,7 @@ struct kbase_sub_alloc {
  * is made on the device file.
  */
 struct kbase_context {
-	struct file *filp;
+	struct kbase_file *kfile;
 	struct kbase_device *kbdev;
 	struct list_head kctx_list_link;
 	struct kbase_mmu_table mmu;
@@ -1953,18 +1979,15 @@ struct kbase_context {
 	DECLARE_BITMAP(cookies, BITS_PER_LONG);
 	struct kbase_va_region *pending_regions[BITS_PER_LONG];
 
-	wait_queue_head_t event_queue;
 	pid_t tgid;
 	pid_t pid;
-	atomic_t prioritized;
 	atomic_t used_pages;
 	atomic_t nonmapped_pages;
 	atomic_t permanent_mapped_pages;
 
 	struct kbase_mem_pool_group mem_pools;
 
-	DEFINE_KBASE_SHRINKER reclaim;
-
+	struct shrinker reclaim;
 	struct list_head evict_list;
 	atomic_t evict_nents;
 
@@ -2139,20 +2162,14 @@ static inline u64 kbase_get_lock_region_min_size_log2(struct kbase_gpu_props con
 	return 15; /* 32 kB */
 }
 
-/**
- * kbase_has_arbiter - Check whether GPU has an arbiter.
- *
- * @kbdev: KBase device.
- *
- * Return: True if there is an arbiter, False otherwise.
- */
-static inline bool kbase_has_arbiter(struct kbase_device *kbdev)
-{
-	return (bool)kbdev->arb.arb_if;
-}
-
 /* Conversion helpers for setting up high resolution timers */
 #define HR_TIMER_DELAY_MSEC(x) (ns_to_ktime(((u64)(x)) * 1000000U))
 #define HR_TIMER_DELAY_NSEC(x) (ns_to_ktime(x))
 
+/* Maximum number of loops polling the GPU for a cache flush before we assume it must have completed */
+#define KBASE_CLEAN_CACHE_MAX_LOOPS 100000
+/* Maximum number of loops polling the GPU for an AS command to complete before we assume the GPU has hung */
+#define KBASE_AS_INACTIVE_MAX_LOOPS 100000000
+/* Maximum number of loops polling the GPU PRFCNT_ACTIVE bit before we assume the GPU has hung */
+#define KBASE_PRFCNT_ACTIVE_MAX_LOOPS 100000000
 #endif /* _KBASE_DEFS_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_disjoint_events.c b/drivers/gpu/arm/bifrost/mali_kbase_disjoint_events.c
index 305b38c343e6..dc2df465f3b2 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_disjoint_events.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_disjoint_events.c
@@ -74,6 +74,6 @@ u32 kbase_disjoint_event_get(struct kbase_device *kbdev)
 {
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 
-	return (u32)atomic_read(&kbdev->disjoint_event.count);
+	return atomic_read(&kbdev->disjoint_event.count);
 }
 KBASE_EXPORT_TEST_API(kbase_disjoint_event_get);
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.c b/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.c
index 7b578c81af60..4b322e62f2de 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -53,7 +53,7 @@ struct wa_blob {
 	u32 blob_offset;
 } __packed;
 
-static bool within_range(const u8 *base, const u8 *end, off_t off, size_t sz)
+static bool in_range(const u8 *base, const u8 *end, off_t off, size_t sz)
 {
 	return !((size_t)(end - base - off) < sz);
 }
@@ -80,7 +80,39 @@ static u32 wait_any(struct kbase_device *kbdev, off_t offset, u32 bits)
 	return (val & bits);
 }
 
-static inline int run_job(struct kbase_device *kbdev, int as, u32 slot, u64 cores, u64 jc)
+static int wait(struct kbase_device *kbdev, off_t offset, u64 bits, bool set)
+{
+	int loop;
+	const int timeout = 100;
+	u64 val;
+	u64 target = 0;
+
+	if (set)
+		target = bits;
+
+	for (loop = 0; loop < timeout; loop++) {
+		if (kbase_reg_is_size64(kbdev, offset))
+			val = kbase_reg_read64(kbdev, offset);
+		else
+			val = kbase_reg_read32(kbdev, offset);
+
+		if ((val & bits) == target)
+			break;
+
+		udelay(10);
+	}
+
+	if (loop == timeout) {
+		dev_err(kbdev->dev,
+			"Timeout reading register 0x%lx, bits 0x%llx, last read was 0x%llx\n",
+			(unsigned long)offset, bits, val);
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static inline int run_job(struct kbase_device *kbdev, int as, int slot, u64 cores, u64 jc)
 {
 	u32 done;
 
@@ -88,7 +120,7 @@ static inline int run_job(struct kbase_device *kbdev, int as, u32 slot, u64 core
 	kbase_reg_write64(kbdev, JOB_SLOT_OFFSET(slot, HEAD_NEXT), jc);
 	kbase_reg_write64(kbdev, JOB_SLOT_OFFSET(slot, AFFINITY_NEXT), cores);
 	kbase_reg_write32(kbdev, JOB_SLOT_OFFSET(slot, CONFIG_NEXT),
-			  JS_CONFIG_DISABLE_DESCRIPTOR_WR_BK | (unsigned int)as);
+			  JS_CONFIG_DISABLE_DESCRIPTOR_WR_BK | as);
 
 	/* go */
 	kbase_reg_write32(kbdev, JOB_SLOT_OFFSET(slot, COMMAND_NEXT), JS_COMMAND_START);
@@ -99,7 +131,7 @@ static inline int run_job(struct kbase_device *kbdev, int as, u32 slot, u64 core
 	kbase_reg_write32(kbdev, JOB_CONTROL_ENUM(JOB_IRQ_CLEAR), done);
 
 	if (done != (1ul << slot)) {
-		dev_err(kbdev->dev, "Failed to run WA job on slot %u cores 0x%llx: done 0x%lx\n",
+		dev_err(kbdev->dev, "Failed to run WA job on slot %d cores 0x%llx: done 0x%lx\n",
 			slot, (unsigned long long)cores, (unsigned long)done);
 		dev_err(kbdev->dev, "JS_STATUS on failure: 0x%x\n",
 			kbase_reg_read32(kbdev, JOB_SLOT_OFFSET(slot, STATUS)));
@@ -114,14 +146,12 @@ static inline int run_job(struct kbase_device *kbdev, int as, u32 slot, u64 core
 int kbase_dummy_job_wa_execute(struct kbase_device *kbdev, u64 cores)
 {
 	int as;
-	u32 slot;
+	int slot;
 	u64 jc;
 	int failed = 0;
 	int runs = 0;
 	u32 old_gpu_mask;
 	u32 old_job_mask;
-	u64 val;
-	const u32 timeout_us = 10000;
 
 	if (!kbdev)
 		return -EFAULT;
@@ -144,8 +174,7 @@ int kbase_dummy_job_wa_execute(struct kbase_device *kbdev, u64 cores)
 
 	if (kbdev->dummy_job_wa.flags & KBASE_DUMMY_JOB_WA_FLAG_WAIT_POWERUP) {
 		/* wait for power-ups */
-		kbase_reg_poll64_timeout(kbdev, GPU_CONTROL_ENUM(SHADER_READY), val,
-					 (val & cores) == cores, 10, timeout_us, false);
+		wait(kbdev, GPU_CONTROL_ENUM(SHADER_READY), cores, true);
 	}
 
 	if (kbdev->dummy_job_wa.flags & KBASE_DUMMY_JOB_WA_FLAG_SERIALIZE) {
@@ -176,10 +205,8 @@ int kbase_dummy_job_wa_execute(struct kbase_device *kbdev, u64 cores)
 		kbase_reg_write64(kbdev, GPU_CONTROL_ENUM(SHADER_PWROFF), cores);
 
 		/* wait for power off complete */
-		kbase_reg_poll64_timeout(kbdev, GPU_CONTROL_ENUM(SHADER_READY), val, !(val & cores),
-					 10, timeout_us, false);
-		kbase_reg_poll64_timeout(kbdev, GPU_CONTROL_ENUM(SHADER_PWRTRANS), val,
-					 !(val & cores), 10, timeout_us, false);
+		wait(kbdev, GPU_CONTROL_ENUM(SHADER_READY), cores, false);
+		wait(kbdev, GPU_CONTROL_ENUM(SHADER_PWRTRANS), cores, false);
 
 		kbase_reg_write32(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_CLEAR), U32_MAX);
 	}
@@ -218,7 +245,7 @@ static bool wa_blob_load_needed(struct kbase_device *kbdev)
 	if (of_machine_is_compatible("arm,juno"))
 		return false;
 
-	if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_TTRX_3485))
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TTRX_3485))
 		return true;
 
 	return false;
@@ -270,7 +297,7 @@ int kbase_dummy_job_wa_load(struct kbase_device *kbdev)
 
 	dev_dbg(kbdev->dev, "Loaded firmware of size %zu bytes\n", firmware->size);
 
-	if (!within_range(fw, fw_end, 0, sizeof(*header))) {
+	if (!in_range(fw, fw_end, 0, sizeof(*header))) {
 		dev_err(kbdev->dev, "WA too small\n");
 		goto bad_fw;
 	}
@@ -289,7 +316,7 @@ int kbase_dummy_job_wa_load(struct kbase_device *kbdev)
 		goto bad_fw;
 	}
 
-	if (!within_range(fw, fw_end, header->info_offset, sizeof(*v2_info))) {
+	if (!in_range(fw, fw_end, header->info_offset, sizeof(*v2_info))) {
 		dev_err(kbdev->dev, "WA info offset out of bounds\n");
 		goto bad_fw;
 	}
@@ -311,18 +338,18 @@ int kbase_dummy_job_wa_load(struct kbase_device *kbdev)
 	while (blob_offset) {
 		const struct wa_blob *blob;
 		size_t nr_pages;
-		base_mem_alloc_flags flags;
+		u64 flags;
 		u64 gpu_va;
 		struct kbase_va_region *va_region;
 
-		if (!within_range(fw, fw_end, blob_offset, sizeof(*blob))) {
+		if (!in_range(fw, fw_end, blob_offset, sizeof(*blob))) {
 			dev_err(kbdev->dev, "Blob offset out-of-range: 0x%lx\n",
 				(unsigned long)blob_offset);
 			goto bad_fw;
 		}
 
 		blob = (const struct wa_blob *)(fw + blob_offset);
-		if (!within_range(fw, fw_end, blob->payload_offset, blob->size)) {
+		if (!in_range(fw, fw_end, blob->payload_offset, blob->size)) {
 			dev_err(kbdev->dev, "Payload out-of-bounds\n");
 			goto bad_fw;
 		}
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_fence.c b/drivers/gpu/arm/bifrost/mali_kbase_fence.c
index febf2fd5643c..023bc6715224 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_fence.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_fence.c
@@ -25,14 +25,20 @@
 #include <mali_kbase_fence.h>
 #include <mali_kbase.h>
 
-#include <linux/version_compat_defs.h>
-
 /* Spin lock protecting all Mali fences as fence->lock. */
 static DEFINE_SPINLOCK(kbase_fence_lock);
 
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+struct fence *kbase_fence_out_new(struct kbase_jd_atom *katom)
+#else
 struct dma_fence *kbase_fence_out_new(struct kbase_jd_atom *katom)
+#endif
 {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence;
+#else
 	struct dma_fence *fence;
+#endif
 
 	WARN_ON(katom->dma_fence.fence);
 
@@ -41,7 +47,7 @@ struct dma_fence *kbase_fence_out_new(struct kbase_jd_atom *katom)
 		return NULL;
 
 	dma_fence_init(fence, &kbase_fence_ops, &kbase_fence_lock, katom->dma_fence.context,
-		       (u64)atomic_inc_return(&katom->dma_fence.seqno));
+		       atomic_inc_return(&katom->dma_fence.seqno));
 
 	katom->dma_fence.fence = fence;
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_fence.h b/drivers/gpu/arm/bifrost/mali_kbase_fence.h
index d45a0fec4104..f170e95b39d9 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_fence.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_fence.h
@@ -29,43 +29,14 @@
 
 #if IS_ENABLED(CONFIG_SYNC_FILE)
 
-#include "mali_kbase.h"
-
 #include <linux/list.h>
+#include "mali_kbase.h"
+#include "mali_kbase_refcount_defs.h"
 #include <linux/version_compat_defs.h>
 
 #if MALI_USE_CSF
-/* Number of digits needed to express the max value of given unsigned type.
- *
- * Details: The number of digits needed to express the max value of given type is log10(t_max) + 1
- * sizeof(t) == log2(t_max)/8
- * log10(t_max) == log2(t_max) / log2(10)
- * log2(t_max) == sizeof(type) * 8
- * 1/log2(10) is approx (1233 >> 12)
- * Hence, number of digits for given type == log10(t_max) + 1 == sizeof(type) * 8 * (1233 >> 12) + 1
- */
-#define MAX_DIGITS_FOR_UNSIGNED_TYPE(t) ((((sizeof(t) * BITS_PER_BYTE) * 1233) >> 12) + 1)
-
-/* Number of digits needed to express the max value of given signed type,
- * including the sign character,
- */
-#define MAX_DIGITS_FOR_SIGNED_TYPE(t) (MAX_DIGITS_FOR_UNSIGNED_TYPE(t) + 1)
-
-/* Max number of characters for id member of kbase_device struct. */
-#define MAX_KBDEV_ID_LEN MAX_DIGITS_FOR_UNSIGNED_TYPE(u32)
-/* Max number of characters for tgid member of kbase_context struct. */
-#define MAX_KCTX_TGID_LEN MAX_DIGITS_FOR_SIGNED_TYPE(pid_t)
-/* Max number of characters for id member of kbase_context struct. */
-#define MAX_KCTX_ID_LEN MAX_DIGITS_FOR_UNSIGNED_TYPE(u32)
-/* Max number of characters for fence_context member of kbase_kcpu_command_queue struct. */
-#define MAX_KCTX_QUEUE_FENCE_CTX_LEN MAX_DIGITS_FOR_UNSIGNED_TYPE(u64)
-/* Max number of characters for timeline name fixed format, including null character. */
-#define FIXED_FORMAT_LEN (9)
-
 /* Maximum number of characters in DMA fence timeline name. */
-#define MAX_TIMELINE_NAME                                                                        \
-	(MAX_KBDEV_ID_LEN + MAX_KCTX_TGID_LEN + MAX_KCTX_ID_LEN + MAX_KCTX_QUEUE_FENCE_CTX_LEN + \
-	 FIXED_FORMAT_LEN)
+#define MAX_TIMELINE_NAME (32)
 
 /**
  * struct kbase_kcpu_dma_fence_meta - Metadata structure for dma fence objects containing
@@ -81,7 +52,7 @@
 struct kbase_kcpu_dma_fence_meta {
 	kbase_refcount_t refcount;
 	struct kbase_device *kbdev;
-	u32 kctx_id;
+	int kctx_id;
 	char timeline_name[MAX_TIMELINE_NAME];
 };
 
@@ -93,12 +64,20 @@ struct kbase_kcpu_dma_fence_meta {
  * @metadata:  Pointer to metadata structure.
  */
 struct kbase_kcpu_dma_fence {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence base;
+#else
 	struct dma_fence base;
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0) */
 	struct kbase_kcpu_dma_fence_meta *metadata;
 };
 #endif
 
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+extern const struct fence_ops kbase_fence_ops;
+#else
 extern const struct dma_fence_ops kbase_fence_ops;
+#endif
 
 /**
  * kbase_fence_out_new() - Creates a new output fence and puts it on the atom
@@ -106,7 +85,11 @@ extern const struct dma_fence_ops kbase_fence_ops;
  *
  * Return: A new fence object on success, NULL on failure.
  */
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+struct fence *kbase_fence_out_new(struct kbase_jd_atom *katom);
+#else
 struct dma_fence *kbase_fence_out_new(struct kbase_jd_atom *katom);
+#endif
 
 #if IS_ENABLED(CONFIG_SYNC_FILE)
 /**
@@ -214,7 +197,11 @@ static inline int kbase_fence_out_signal(struct kbase_jd_atom *katom, int status
 #define kbase_fence_get(fence_info) dma_fence_get((fence_info)->fence)
 
 #if MALI_USE_CSF
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+static inline struct kbase_kcpu_dma_fence *kbase_kcpu_dma_fence_get(struct fence *fence)
+#else
 static inline struct kbase_kcpu_dma_fence *kbase_kcpu_dma_fence_get(struct dma_fence *fence)
+#endif
 {
 	if (fence->ops == &kbase_fence_ops)
 		return (struct kbase_kcpu_dma_fence *)fence;
@@ -230,7 +217,11 @@ static inline void kbase_kcpu_dma_fence_meta_put(struct kbase_kcpu_dma_fence_met
 	}
 }
 
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+static inline void kbase_kcpu_dma_fence_put(struct fence *fence)
+#else
 static inline void kbase_kcpu_dma_fence_put(struct dma_fence *fence)
+#endif
 {
 	struct kbase_kcpu_dma_fence *kcpu_fence = kbase_kcpu_dma_fence_get(fence);
 
@@ -243,7 +234,11 @@ static inline void kbase_kcpu_dma_fence_put(struct dma_fence *fence)
  * kbase_fence_put() - Releases a reference to a fence
  * @fence: Fence to release reference for.
  */
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+static inline void kbase_fence_put(struct fence *fence)
+#else
 static inline void kbase_fence_put(struct dma_fence *fence)
+#endif
 {
 	dma_fence_put(fence);
 }
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_fence_ops.c b/drivers/gpu/arm/bifrost/mali_kbase_fence_ops.c
index b10e69cea565..7315da8b89f9 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_fence_ops.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_fence_ops.c
@@ -63,7 +63,7 @@ static void kbase_fence_fence_value_str(struct dma_fence *fence, char *str, int
 	else
 		format = "%llu";
 
-	if (unlikely(!scnprintf(str, (size_t)size, format, fence->seqno)))
+	if (unlikely(!scnprintf(str, size, format, fence->seqno)))
 		pr_err("Fail to encode fence seqno to string");
 }
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_gpu_memory_debugfs.h b/drivers/gpu/arm/bifrost/mali_kbase_gpu_memory_debugfs.h
index 6993693cd8da..6ad773658554 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_gpu_memory_debugfs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_gpu_memory_debugfs.h
@@ -27,11 +27,8 @@
 #ifndef _KBASE_GPU_MEMORY_DEBUGFS_H
 #define _KBASE_GPU_MEMORY_DEBUGFS_H
 
-#include <linux/compiler_types.h>
-#include <linux/types.h>
-
-struct kbase_io_history;
-struct kbase_device;
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
 
 /* kbase_io_history_add - add new entry to the register access history
  *
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_gpu_metrics.c b/drivers/gpu/arm/bifrost/mali_kbase_gpu_metrics.c
index 60ad1c272f84..8e00c0dc830e 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_gpu_metrics.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_gpu_metrics.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -20,14 +20,25 @@
  */
 
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
-
 #include "mali_power_gpu_work_period_trace.h"
 #include <mali_kbase_gpu_metrics.h>
 #include <mali_kbase_config_defaults.h>
-#include <mali_kbase.h>
 
-#include <linux/module.h>
-#include <linux/slab.h>
+/**
+ * enum gpu_metrics_ctx_flags - Flags for the GPU metrics context
+ *
+ * @ACTIVE_INTERVAL_IN_WP: Flag set when the application first becomes active in
+ *                         the current work period.
+ *
+ * @INSIDE_ACTIVE_LIST:    Flag to track if object is in kbase_device::gpu_metrics::active_list
+ *
+ * All members need to be separate bits. This enum is intended for use in a
+ * bitmask where multiple values get OR-ed together.
+ */
+enum gpu_metrics_ctx_flags {
+	ACTIVE_INTERVAL_IN_WP = 1 << 0,
+	INSIDE_ACTIVE_LIST = 1 << 1,
+};
 
 static unsigned long gpu_metrics_tp_emit_interval_ns = DEFAULT_GPU_METRICS_TP_EMIT_INTERVAL_NS;
 
@@ -35,40 +46,74 @@ module_param(gpu_metrics_tp_emit_interval_ns, ulong, 0444);
 MODULE_PARM_DESC(gpu_metrics_tp_emit_interval_ns,
 		 "Time interval in nano seconds at which GPU metrics tracepoints are emitted");
 
+static inline bool gpu_metrics_ctx_flag(struct kbase_gpu_metrics_ctx *gpu_metrics_ctx,
+					enum gpu_metrics_ctx_flags flag)
+{
+	return (gpu_metrics_ctx->flags & flag);
+}
+
+static inline void gpu_metrics_ctx_flag_set(struct kbase_gpu_metrics_ctx *gpu_metrics_ctx,
+					    enum gpu_metrics_ctx_flags flag)
+{
+	gpu_metrics_ctx->flags |= flag;
+}
+
+static inline void gpu_metrics_ctx_flag_clear(struct kbase_gpu_metrics_ctx *gpu_metrics_ctx,
+					      enum gpu_metrics_ctx_flags flag)
+{
+	gpu_metrics_ctx->flags &= ~flag;
+}
+
 static inline void validate_tracepoint_data(struct kbase_gpu_metrics_ctx *gpu_metrics_ctx,
 					    u64 start_time, u64 end_time, u64 total_active)
 {
 #if 0
+	WARN(total_active > NSEC_PER_SEC, "total_active %llu > 1 second for aid %u active_cnt %u",
+	     total_active, gpu_metrics_ctx->aid, gpu_metrics_ctx->active_cnt);
+
 	WARN(start_time >= end_time, "start_time %llu >= end_time %llu for aid %u active_cnt %u",
 	     start_time, end_time, gpu_metrics_ctx->aid, gpu_metrics_ctx->active_cnt);
 
 	WARN(total_active > (end_time - start_time),
 	     "total_active %llu > end_time %llu - start_time %llu for aid %u active_cnt %u",
 	     total_active, end_time, start_time, gpu_metrics_ctx->aid, gpu_metrics_ctx->active_cnt);
+
+	WARN(gpu_metrics_ctx->prev_wp_active_end_time > start_time,
+	     "prev_wp_active_end_time %llu > start_time %llu for aid %u active_cnt %u",
+	     gpu_metrics_ctx->prev_wp_active_end_time, start_time, gpu_metrics_ctx->aid,
+	     gpu_metrics_ctx->active_cnt);
 #endif
 }
 
 static void emit_tracepoint_for_active_gpu_metrics_ctx(
 	struct kbase_device *kbdev, struct kbase_gpu_metrics_ctx *gpu_metrics_ctx, u64 current_time)
 {
-	const u64 start_time = gpu_metrics_ctx->active_start_time;
-	u64 total_active, end_time = current_time;
+	const u64 start_time = gpu_metrics_ctx->first_active_start_time;
+	u64 total_active = gpu_metrics_ctx->total_active;
+	u64 end_time;
 
 	/* Check if the GPU activity is currently ongoing */
 	if (gpu_metrics_ctx->active_cnt) {
 		/* The following check is to handle the race on CSF GPUs that can happen between
 		 * the draining of trace buffer and FW emitting the ACT=1 event .
 		 */
-		if (unlikely(end_time == start_time))
-			end_time++;
-		gpu_metrics_ctx->active_start_time = end_time;
+		if (unlikely(current_time == gpu_metrics_ctx->last_active_start_time))
+			current_time++;
+		end_time = current_time;
+		total_active += end_time - gpu_metrics_ctx->last_active_start_time;
+
+		gpu_metrics_ctx->first_active_start_time = current_time;
+		gpu_metrics_ctx->last_active_start_time = current_time;
+	} else {
+		end_time = gpu_metrics_ctx->last_active_end_time;
+		gpu_metrics_ctx_flag_clear(gpu_metrics_ctx, ACTIVE_INTERVAL_IN_WP);
 	}
 
-	total_active = end_time - start_time;
 	trace_gpu_work_period(kbdev->id, gpu_metrics_ctx->aid, start_time, end_time, total_active);
 
 	validate_tracepoint_data(gpu_metrics_ctx, start_time, end_time, total_active);
-	gpu_metrics_ctx->active_end_time = end_time;
+	gpu_metrics_ctx->prev_wp_active_end_time = end_time;
+	gpu_metrics_ctx->total_active = 0;
 }
 
 void kbase_gpu_metrics_ctx_put(struct kbase_device *kbdev,
@@ -81,8 +126,7 @@ void kbase_gpu_metrics_ctx_put(struct kbase_device *kbdev,
 	if (gpu_metrics_ctx->kctx_count)
 		return;
 
-	/* Generate a tracepoint if there's still activity */
-	if (gpu_metrics_ctx->active_cnt)
+	if (gpu_metrics_ctx_flag(gpu_metrics_ctx, ACTIVE_INTERVAL_IN_WP))
 		emit_tracepoint_for_active_gpu_metrics_ctx(kbdev, gpu_metrics_ctx,
 							   ktime_get_raw_ns());
 
@@ -117,11 +161,12 @@ struct kbase_gpu_metrics_ctx *kbase_gpu_metrics_ctx_get(struct kbase_device *kbd
 void kbase_gpu_metrics_ctx_init(struct kbase_device *kbdev,
 				struct kbase_gpu_metrics_ctx *gpu_metrics_ctx, unsigned int aid)
 {
-	gpu_metrics_ctx->active_start_time = 0;
-	gpu_metrics_ctx->active_end_time = 0;
 	gpu_metrics_ctx->aid = aid;
+	gpu_metrics_ctx->total_active = 0;
 	gpu_metrics_ctx->kctx_count = 1;
 	gpu_metrics_ctx->active_cnt = 0;
+	gpu_metrics_ctx->prev_wp_active_end_time = 0;
+	gpu_metrics_ctx->flags = 0;
 	list_add_tail(&gpu_metrics_ctx->link, &kbdev->gpu_metrics.inactive_list);
 }
 
@@ -130,9 +175,17 @@ void kbase_gpu_metrics_ctx_start_activity(struct kbase_context *kctx, u64 timest
 	struct kbase_gpu_metrics_ctx *gpu_metrics_ctx = kctx->gpu_metrics_ctx;
 
 	gpu_metrics_ctx->active_cnt++;
-	if (gpu_metrics_ctx->active_cnt == 1) {
-		gpu_metrics_ctx->active_start_time = timestamp_ns;
+	if (gpu_metrics_ctx->active_cnt == 1)
+		gpu_metrics_ctx->last_active_start_time = timestamp_ns;
+
+	if (!gpu_metrics_ctx_flag(gpu_metrics_ctx, ACTIVE_INTERVAL_IN_WP)) {
+		gpu_metrics_ctx->first_active_start_time = timestamp_ns;
+		gpu_metrics_ctx_flag_set(gpu_metrics_ctx, ACTIVE_INTERVAL_IN_WP);
+	}
+
+	if (!gpu_metrics_ctx_flag(gpu_metrics_ctx, INSIDE_ACTIVE_LIST)) {
 		list_move_tail(&gpu_metrics_ctx->link, &kctx->kbdev->gpu_metrics.active_list);
+		gpu_metrics_ctx_flag_set(gpu_metrics_ctx, INSIDE_ACTIVE_LIST);
 	}
 }
 
@@ -143,22 +196,22 @@ void kbase_gpu_metrics_ctx_end_activity(struct kbase_context *kctx, u64 timestam
 	if (WARN_ON_ONCE(!gpu_metrics_ctx->active_cnt))
 		return;
 
-	/* Do not emit tracepoint if GPU activity still continues. */
 	if (--gpu_metrics_ctx->active_cnt)
 		return;
 
-	if (likely(timestamp_ns > gpu_metrics_ctx->active_start_time)) {
-		emit_tracepoint_for_active_gpu_metrics_ctx(kctx->kbdev, gpu_metrics_ctx,
-							   timestamp_ns);
+	if (likely(timestamp_ns > gpu_metrics_ctx->last_active_start_time)) {
+		gpu_metrics_ctx->last_active_end_time = timestamp_ns;
+		gpu_metrics_ctx->total_active +=
+			timestamp_ns - gpu_metrics_ctx->last_active_start_time;
 		return;
 	}
 
 	/* Due to conversion from system timestamp to CPU timestamp (which involves rounding)
 	 * the value for start and end timestamp could come as same on CSF GPUs.
 	 */
-	if (timestamp_ns == gpu_metrics_ctx->active_start_time) {
-		emit_tracepoint_for_active_gpu_metrics_ctx(kctx->kbdev, gpu_metrics_ctx,
-							   timestamp_ns + 1);
+	if (timestamp_ns == gpu_metrics_ctx->last_active_start_time) {
+		gpu_metrics_ctx->last_active_end_time = timestamp_ns + 1;
+		gpu_metrics_ctx->total_active += 1;
 		return;
 	}
 
@@ -166,9 +219,12 @@ void kbase_gpu_metrics_ctx_end_activity(struct kbase_context *kctx, u64 timestam
 	 * visible to the Kbase even though the system timestamp value sampled by FW was less than
 	 * the system timestamp value sampled by Kbase just before the draining of trace buffer.
 	 */
-	if (gpu_metrics_ctx->active_end_time == gpu_metrics_ctx->active_start_time) {
-		emit_tracepoint_for_active_gpu_metrics_ctx(kctx->kbdev, gpu_metrics_ctx,
-							   gpu_metrics_ctx->active_end_time + 1);
+	if (gpu_metrics_ctx->last_active_start_time == gpu_metrics_ctx->first_active_start_time &&
+	    gpu_metrics_ctx->prev_wp_active_end_time == gpu_metrics_ctx->first_active_start_time) {
+		WARN_ON_ONCE(gpu_metrics_ctx->total_active);
+		gpu_metrics_ctx->last_active_end_time =
+			gpu_metrics_ctx->prev_wp_active_end_time + 1;
+		gpu_metrics_ctx->total_active = 1;
 		return;
 	}
 
@@ -181,12 +237,15 @@ void kbase_gpu_metrics_emit_tracepoint(struct kbase_device *kbdev, u64 ts)
 	struct kbase_gpu_metrics_ctx *gpu_metrics_ctx, *tmp;
 
 	list_for_each_entry_safe(gpu_metrics_ctx, tmp, &gpu_metrics->active_list, link) {
-		if (gpu_metrics_ctx->active_cnt) {
-			emit_tracepoint_for_active_gpu_metrics_ctx(kbdev, gpu_metrics_ctx, ts);
+		if (!gpu_metrics_ctx_flag(gpu_metrics_ctx, ACTIVE_INTERVAL_IN_WP)) {
+			WARN_ON(!gpu_metrics_ctx_flag(gpu_metrics_ctx, INSIDE_ACTIVE_LIST));
+			WARN_ON(gpu_metrics_ctx->active_cnt);
+			list_move_tail(&gpu_metrics_ctx->link, &gpu_metrics->inactive_list);
+			gpu_metrics_ctx_flag_clear(gpu_metrics_ctx, INSIDE_ACTIVE_LIST);
 			continue;
 		}
 
-		list_move_tail(&gpu_metrics_ctx->link, &gpu_metrics->inactive_list);
+		emit_tracepoint_for_active_gpu_metrics_ctx(kbdev, gpu_metrics_ctx, ts);
 	}
 }
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_gpu_metrics.h b/drivers/gpu/arm/bifrost/mali_kbase_gpu_metrics.h
index 658cf1c164c5..c89e25996f52 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_gpu_metrics.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_gpu_metrics.h
@@ -27,12 +27,7 @@
 #define _KBASE_GPU_METRICS_H_
 
 #if IS_ENABLED(CONFIG_MALI_TRACE_POWER_GPU_WORK_PERIOD)
-
-#include <linux/types.h>
-
-struct kbase_device;
-struct kbase_context;
-struct kbase_gpu_metrics_ctx;
+#include <mali_kbase.h>
 
 /**
  * kbase_gpu_metrics_get_tp_emit_interval() - Return the trace point emission interval.
@@ -106,7 +101,7 @@ void kbase_gpu_metrics_ctx_init(struct kbase_device *kbdev,
  * @kctx:         Pointer to the Kbase context contributing data to the GPU metrics context.
  * @timestamp_ns: CPU timestamp at which the GPU activity started.
  *
- * The provided timestamp is used as the "start_time_ns" for the
+ * The provided timestamp would be later used as the "start_time_ns" for the
  * power/gpu_work_period tracepoint if this is the first GPU activity for the GPU
  * metrics context in the current work period.
  *
@@ -122,9 +117,9 @@ void kbase_gpu_metrics_ctx_start_activity(struct kbase_context *kctx, u64 timest
  * @kctx:         Pointer to the Kbase context contributing data to the GPU metrics context.
  * @timestamp_ns: CPU timestamp at which the GPU activity ended.
  *
- * The provided timestamp is used as the "end_time_ns" for the power/gpu_work_period
- * tracepoint if this is the last GPU activity for the GPU metrics context
- * in the current work period.
+ * The provided timestamp would be later used as the "end_time_ns" for the
+ * power/gpu_work_period tracepoint if this is the last GPU activity for the GPU
+ * metrics context in the current work period.
  *
  * Note: The caller must appropriately serialize the call to this function with the
  *       call to other GPU metrics functions declared in this file.
@@ -138,8 +133,8 @@ void kbase_gpu_metrics_ctx_end_activity(struct kbase_context *kctx, u64 timestam
  * @kbdev: Pointer to the GPU device.
  * @ts:    Timestamp at which the tracepoint is being emitted.
  *
- * This function would loop through all GPU metrics contexts in the active list and
- * emit a power/gpu_work_period tracepoint if the GPU work in the context still active.
+ * This function would loop through all the active GPU metrics contexts and emit a
+ * power/gpu_work_period tracepoint for them.
  * The GPU metrics context that is found to be inactive since the last tracepoint
  * was emitted would be moved to the inactive list.
  * The current work period would be considered as over and a new work period would
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.c b/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.c
index 9719580837cc..190800394292 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2011-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -326,8 +326,6 @@ void kbase_gpuprops_term(struct kbase_device *kbdev)
 static u8 override_l2_size;
 module_param(override_l2_size, byte, 0000);
 MODULE_PARM_DESC(override_l2_size, "Override L2 size config for testing");
-/* Minimum L2 cache size - LOG2(1KiB) */
-#define OVERRIDE_L2_SIZE_MIN_LOG2 (10)
 
 static u8 override_l2_hash;
 module_param(override_l2_hash, byte, 0000);
@@ -357,7 +355,6 @@ enum l2_config_override_result {
 /**
  * kbase_read_l2_config_from_dt - Read L2 configuration
  * @kbdev: The kbase device for which to get the L2 configuration.
- * @regdump: Pointer to struct kbase_gpuprops_regdump structure.
  *
  * Check for L2 configuration overrides in module parameters and device tree.
  * Override values in module parameters take priority over override values in
@@ -367,16 +364,9 @@ enum l2_config_override_result {
  *         overridden, L2_CONFIG_OVERRIDE_NONE if no overrides are provided.
  *         L2_CONFIG_OVERRIDE_FAIL otherwise.
  */
-static enum l2_config_override_result
-kbase_read_l2_config_from_dt(struct kbase_device *const kbdev,
-			     struct kbasep_gpuprops_regdump *regdump)
+static enum l2_config_override_result kbase_read_l2_config_from_dt(struct kbase_device *const kbdev)
 {
 	struct device_node *np = kbdev->dev->of_node;
-	/*
-	 * CACHE_SIZE bit fields in L2_FEATURES register, default value after the reset/powerup
-	 * holds the maximum size of the cache that can be programmed in L2_CONFIG register.
-	 */
-	const u8 l2_size_max = L2_FEATURES_CACHE_SIZE_GET(regdump->l2_features);
 
 	if (!np)
 		return L2_CONFIG_OVERRIDE_NONE;
@@ -386,13 +376,6 @@ kbase_read_l2_config_from_dt(struct kbase_device *const kbdev,
 	else if (of_property_read_u8(np, "l2-size", &kbdev->l2_size_override))
 		kbdev->l2_size_override = 0;
 
-	if (kbdev->l2_size_override != 0 && (kbdev->l2_size_override < OVERRIDE_L2_SIZE_MIN_LOG2 ||
-					     kbdev->l2_size_override > l2_size_max)) {
-		dev_err(kbdev->dev, "Invalid Cache Size in %s",
-			override_l2_size ? "Module parameters" : "Device tree node");
-		return L2_CONFIG_OVERRIDE_FAIL;
-	}
-
 	/* Check overriding value is supported, if not will result in
 	 * undefined behavior.
 	 */
@@ -437,11 +420,11 @@ int kbase_gpuprops_update_l2_features(struct kbase_device *kbdev)
 {
 	int err = 0;
 
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_L2_CONFIG)) {
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_L2_CONFIG)) {
 		struct kbasep_gpuprops_regdump *regdump = &PRIV_DATA_REGDUMP(kbdev);
 
 		/* Check for L2 cache size & hash overrides */
-		switch (kbase_read_l2_config_from_dt(kbdev, regdump)) {
+		switch (kbase_read_l2_config_from_dt(kbdev)) {
 		case L2_CONFIG_OVERRIDE_FAIL:
 			err = -EIO;
 			goto exit;
@@ -637,6 +620,7 @@ static void kbase_populate_user_data(struct kbase_device *kbdev, struct gpu_prop
 	data->raw_props.coherency_mode = kprops->coherency_mode;
 
 	/* Properties (mostly) from raw register values */
+	/* For compatibility, we are passing the lower 32-bits of the gpu_id */
 	data->raw_props.gpu_id = regdump->gpu_id;
 
 	{
@@ -665,7 +649,7 @@ static void kbase_populate_user_data(struct kbase_device *kbdev, struct gpu_prop
 	data->l2_props.log2_cache_size = KBASE_UBFX64(regdump->l2_features, 16U, 8);
 	data->coherency_info.coherency = regdump->mem_features;
 
-	data->tiler_props.bin_size_bytes = 1U << KBASE_UBFX64(regdump->tiler_features, 0U, 6);
+	data->tiler_props.bin_size_bytes = 1 << KBASE_UBFX64(regdump->tiler_features, 0U, 6);
 	data->tiler_props.max_active_levels = KBASE_UBFX32(regdump->tiler_features, 8U, 4);
 
 	if (regdump->thread_max_workgroup_size == 0)
@@ -699,7 +683,7 @@ static void kbase_populate_user_data(struct kbase_device *kbdev, struct gpu_prop
 		data->thread_props.max_thread_group_split = THREAD_MTGS_DEFAULT;
 	}
 
-	if (!kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_THREAD_GROUP_SPLIT))
+	if (!kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_THREAD_GROUP_SPLIT))
 		data->thread_props.max_thread_group_split = 0;
 
 	/* Raw Register Values */
@@ -744,7 +728,7 @@ int kbase_gpuprops_populate_user_buffer(struct kbase_device *kbdev)
 
 	for (i = 0; i < count; i++) {
 		/* 4 bytes for the ID, and the size of the property */
-		size += (u32)(4 + gpu_property_mapping[i].size);
+		size += 4 + gpu_property_mapping[i].size;
 	}
 
 	kprops->prop_buffer_size = size;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.h b/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.h
index 46857858c80d..093f9680ece6 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.h
@@ -26,13 +26,10 @@
 #ifndef _KBASE_GPUPROPS_H_
 #define _KBASE_GPUPROPS_H_
 
-#include <linux/types.h>
+#include "mali_kbase_gpuprops_types.h"
 
 /* Forward definition - see mali_kbase.h */
 struct kbase_device;
-struct max_config_props;
-struct curr_config_props;
-struct kbase_gpu_id_props;
 
 /**
  * KBASE_UBFX32 - Extracts bits from a 32-bit bitfield.
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_gwt.c b/drivers/gpu/arm/bifrost/mali_kbase_gwt.c
index 99558b82ba7b..79c3d5129324 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_gwt.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_gwt.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -19,21 +19,13 @@
  *
  */
 
-#include <mali_kbase_gwt.h>
-#include <mmu/mali_kbase_mmu.h>
-#include <mali_kbase.h>
-#include <mali_kbase_defs.h>
-#include <mali_malisw.h>
-
-#include <linux/rbtree.h>
+#include "mali_kbase_gwt.h"
 #include <linux/list_sort.h>
-#include <linux/module.h>
 
 static inline void kbase_gpu_gwt_setup_page_permission(struct kbase_context *kctx,
-						       unsigned long flag,
-						       struct kbase_reg_zone *zone)
+						       unsigned long flag, struct rb_node *node)
 {
-	struct rb_node *rbnode = rb_first(&zone->reg_rbtree);
+	struct rb_node *rbnode = node;
 
 	while (rbnode) {
 		struct kbase_va_region *reg;
@@ -56,15 +48,17 @@ static inline void kbase_gpu_gwt_setup_page_permission(struct kbase_context *kct
 
 static void kbase_gpu_gwt_setup_pages(struct kbase_context *kctx, unsigned long flag)
 {
-	kbase_gpu_gwt_setup_page_permission(kctx, flag, &kctx->reg_zone[SAME_VA_ZONE]);
-	kbase_gpu_gwt_setup_page_permission(kctx, flag, &kctx->reg_zone[CUSTOM_VA_ZONE]);
+	kbase_gpu_gwt_setup_page_permission(kctx, flag,
+					    rb_first(&kctx->reg_zone[SAME_VA_ZONE].reg_rbtree));
+	kbase_gpu_gwt_setup_page_permission(kctx, flag,
+					    rb_first(&kctx->reg_zone[CUSTOM_VA_ZONE].reg_rbtree));
 }
 
 int kbase_gpu_gwt_start(struct kbase_context *kctx)
 {
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 	if (kctx->gwt_enabled) {
-		kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+		kbase_gpu_vm_unlock(kctx);
 		return -EBUSY;
 	}
 
@@ -90,7 +84,7 @@ int kbase_gpu_gwt_start(struct kbase_context *kctx)
 
 	kbase_gpu_gwt_setup_pages(kctx, ~KBASE_REG_GPU_WR);
 
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 	return 0;
 }
 
@@ -178,10 +172,6 @@ int kbase_gpu_gwt_dump(struct kbase_context *kctx, union kbase_ioctl_cinstr_gwt_
 	__user void *user_addr = (__user void *)(uintptr_t)gwt_dump->in.addr_buffer;
 	__user void *user_sizes = (__user void *)(uintptr_t)gwt_dump->in.size_buffer;
 
-	/* We don't have any valid user space buffer to copy the write modified addresses. */
-	if (!gwt_dump->in.len || !gwt_dump->in.addr_buffer || !gwt_dump->in.size_buffer)
-		return -EINVAL;
-
 	kbase_gpu_vm_lock(kctx);
 
 	if (!kctx->gwt_enabled) {
@@ -190,6 +180,14 @@ int kbase_gpu_gwt_dump(struct kbase_context *kctx, union kbase_ioctl_cinstr_gwt_
 		return -EPERM;
 	}
 
+	if (!gwt_dump->in.len || !gwt_dump->in.addr_buffer || !gwt_dump->in.size_buffer) {
+		kbase_gpu_vm_unlock(kctx);
+		/* We don't have any valid user space buffer to copy the
+		 * write modified addresses.
+		 */
+		return -EINVAL;
+	}
+
 	if (list_empty(&kctx->gwt_snapshot_list) && !list_empty(&kctx->gwt_current_list)) {
 		list_replace_init(&kctx->gwt_current_list, &kctx->gwt_snapshot_list);
 
@@ -223,14 +221,14 @@ int kbase_gpu_gwt_dump(struct kbase_context *kctx, union kbase_ioctl_cinstr_gwt_
 
 		if (count) {
 			err = copy_to_user((user_addr + (ubuf_count * sizeof(u64))),
-					   (void *)addr_buffer, size_mul(count, sizeof(u64)));
+					   (void *)addr_buffer, count * sizeof(u64));
 			if (err) {
 				dev_err(kctx->kbdev->dev, "Copy to user failure\n");
 				kbase_gpu_vm_unlock(kctx);
 				return err;
 			}
 			err = copy_to_user((user_sizes + (ubuf_count * sizeof(u64))),
-					   (void *)num_page_buffer, size_mul(count, sizeof(u64)));
+					   (void *)num_page_buffer, count * sizeof(u64));
 			if (err) {
 				dev_err(kctx->kbdev->dev, "Copy to user failure\n");
 				kbase_gpu_vm_unlock(kctx);
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_gwt.h b/drivers/gpu/arm/bifrost/mali_kbase_gwt.h
index ee7f3bf6b0ac..e184375d0589 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_gwt.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_gwt.h
@@ -22,10 +22,9 @@
 #if !defined(_KBASE_GWT_H)
 #define _KBASE_GWT_H
 
+#include <mali_kbase.h>
 #include <uapi/gpu/arm/bifrost/mali_kbase_ioctl.h>
 
-struct kbase_context;
-
 /**
  * kbase_gpu_gwt_start - Start the GPU write tracking
  * @kctx: Pointer to kernel context
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hw.c b/drivers/gpu/arm/bifrost/mali_kbase_hw.c
index e04aad2422c7..dd0873f03125 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hw.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hw.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2012-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -23,8 +23,8 @@
  * Run-time work-arounds helpers
  */
 
-#include <mali_kbase_hwconfig_features.h>
-#include <mali_kbase_hwconfig_issues.h>
+#include <mali_base_hwconfig_features.h>
+#include <mali_base_hwconfig_issues.h>
 #include <hw_access/mali_kbase_hw_access_regmap.h>
 #include "mali_kbase.h"
 #include "mali_kbase_hw.h"
@@ -92,7 +92,7 @@ void kbase_hw_set_features_mask(struct kbase_device *kbdev)
 		break;
 	}
 
-	for (; *features != KBASE_HW_FEATURE_END; features++)
+	for (; *features != BASE_HW_FEATURE_END; features++)
 		set_bit(*features, &kbdev->hw_features_mask[0]);
 
 #if defined(CONFIG_MALI_VECTOR_DUMP)
@@ -103,8 +103,8 @@ void kbase_hw_set_features_mask(struct kbase_device *kbdev)
 	 * in the implementation of flush reduction optimization due to
 	 * unclear or ambiguous ARCH spec.
 	 */
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_CLEAN_ONLY_SAFE))
-		clear_bit(KBASE_HW_FEATURE_FLUSH_REDUCTION, &kbdev->hw_features_mask[0]);
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_CLEAN_ONLY_SAFE))
+		clear_bit(BASE_HW_FEATURE_FLUSH_REDUCTION, &kbdev->hw_features_mask[0]);
 #endif
 }
 
@@ -113,7 +113,7 @@ void kbase_hw_set_features_mask(struct kbase_device *kbdev)
  * @kbdev: Device pointer
  *
  * Return: pointer to an array of hardware issues, terminated by
- * KBASE_HW_ISSUE_END.
+ * BASE_HW_ISSUE_END.
  *
  * In debugging versions of the driver, unknown versions of a known GPU will
  * be treated as the most recent known version not later than the actual
@@ -205,8 +205,6 @@ static const enum base_hw_issue *kbase_hw_get_issues_for_new_id(struct kbase_dev
 		  { { GPU_ID_VERSION_MAKE(0, 0, 0), base_hw_issues_tBAx_r0p0 },
 		    { GPU_ID_VERSION_MAKE(0, 0, 1), base_hw_issues_tBAx_r0p0 },
 		    { GPU_ID_VERSION_MAKE(0, 0, 2), base_hw_issues_tBAx_r0p0 },
-		    { GPU_ID_VERSION_MAKE(0, 1, 0), base_hw_issues_tBAx_r0p1 },
-		    { GPU_ID_VERSION_MAKE(0, 2, 0), base_hw_issues_tBAx_r0p2 },
 		    { U32_MAX, NULL } } },
 
 		{ GPU_ID_PRODUCT_TODX,
@@ -225,8 +223,6 @@ static const enum base_hw_issue *kbase_hw_get_issues_for_new_id(struct kbase_dev
 
 		{ GPU_ID_PRODUCT_TVAX,
 		  { { GPU_ID_VERSION_MAKE(0, 0, 0), base_hw_issues_tVAx_r0p0 },
-		    { GPU_ID_VERSION_MAKE(0, 0, 5), base_hw_issues_tVAx_r0p0 },
-		    { GPU_ID_VERSION_MAKE(0, 1, 0), base_hw_issues_tVAx_r0p1 },
 		    { U32_MAX, NULL } } },
 
 		{ GPU_ID_PRODUCT_TTUX,
@@ -248,21 +244,17 @@ static const enum base_hw_issue *kbase_hw_get_issues_for_new_id(struct kbase_dev
 
 		{ GPU_ID_PRODUCT_TTIX,
 		  { { GPU_ID_VERSION_MAKE(0, 0, 0), base_hw_issues_tTIx_r0p0 },
-		    { GPU_ID_VERSION_MAKE(0, 1, 0), base_hw_issues_tTIx_r0p1 },
 		    { U32_MAX, NULL } } },
 
 		{ GPU_ID_PRODUCT_LTIX,
 		  { { GPU_ID_VERSION_MAKE(0, 0, 0), base_hw_issues_tTIx_r0p0 },
-		    { GPU_ID_VERSION_MAKE(0, 1, 0), base_hw_issues_tTIx_r0p1 },
 		    { U32_MAX, NULL } } },
 
 		{ GPU_ID_PRODUCT_TKRX,
 		  { { GPU_ID_VERSION_MAKE(0, 0, 0), base_hw_issues_tKRx_r0p0 },
-		    { GPU_ID_VERSION_MAKE(0, 1, 0), base_hw_issues_tKRx_r0p1 },
 		    { U32_MAX, NULL } } },
 		{ GPU_ID_PRODUCT_LKRX,
 		  { { GPU_ID_VERSION_MAKE(0, 0, 0), base_hw_issues_tKRx_r0p0 },
-		    { GPU_ID_VERSION_MAKE(0, 1, 0), base_hw_issues_tKRx_r0p1 },
 		    { U32_MAX, NULL } } },
 	};
 
@@ -336,8 +328,6 @@ static const enum base_hw_issue *kbase_hw_get_issues_for_new_id(struct kbase_dev
 			gpu_id->version_id = fallback_version;
 		}
 	}
-
-
 	return issues;
 }
 
@@ -424,7 +414,7 @@ int kbase_hw_set_issues_mask(struct kbase_device *kbdev)
 		 gpu_id->product_major, gpu_id->arch_major, gpu_id->arch_minor, gpu_id->arch_rev,
 		 gpu_id->version_major, gpu_id->version_minor, gpu_id->version_status);
 
-	for (; *issues != KBASE_HW_ISSUE_END; issues++)
+	for (; *issues != BASE_HW_ISSUE_END; issues++)
 		set_bit(*issues, &kbdev->hw_issues_mask[0]);
 
 	return 0;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hw.h b/drivers/gpu/arm/bifrost/mali_kbase_hw.h
index f14e5fb6d9ab..44e1ee4a4a50 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hw.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hw.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2012-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -23,8 +23,8 @@
  * DOC: Run-time work-arounds helpers
  */
 
-#ifndef _MALI_KBASE_HW_H_
-#define _MALI_KBASE_HW_H_
+#ifndef _KBASE_HW_H_
+#define _KBASE_HW_H_
 
 #include "mali_kbase_defs.h"
 
@@ -47,7 +47,7 @@
  * @kbdev: Device pointer
  */
 #define kbase_hw_has_l2_slice_hash_feature(kbdev) \
-	test_bit(KBASE_HW_FEATURE_L2_SLICE_HASH, &(kbdev)->hw_features_mask[0])
+	test_bit(BASE_HW_FEATURE_L2_SLICE_HASH, &(kbdev)->hw_features_mask[0])
 
 /**
  * kbase_hw_set_issues_mask - Set the hardware issues mask based on the GPU ID
@@ -73,4 +73,4 @@ int kbase_hw_set_issues_mask(struct kbase_device *kbdev);
  */
 void kbase_hw_set_features_mask(struct kbase_device *kbdev);
 
-#endif /* _MALI_KBASE_HW_H_ */
+#endif /* _KBASE_HW_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_defs.h b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_defs.h
index 96aeb6188a05..62a6ec51b17f 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_defs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_defs.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014, 2016-2018, 2020-2021 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -26,7 +26,7 @@
 #ifndef _KBASE_HWACCESS_DEFS_H_
 #define _KBASE_HWACCESS_DEFS_H_
 
-#include <backend/gpu/mali_kbase_defs.h>
+#include <backend/gpu/mali_kbase_jm_defs.h>
 
 /**
  * struct kbase_hwaccess_data - object encapsulating the GPU backend specific
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_jm.h b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_jm.h
index ed91019ff74c..93003754820d 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_jm.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_jm.h
@@ -194,7 +194,7 @@ int kbase_backend_nr_atoms_on_slot(struct kbase_device *kbdev, unsigned int js);
  *
  * Return: Number of atoms currently on slot @js that are currently on the GPU.
  */
-u32 kbase_backend_nr_atoms_submitted(struct kbase_device *kbdev, unsigned int js);
+int kbase_backend_nr_atoms_submitted(struct kbase_device *kbdev, unsigned int js);
 
 /**
  * kbase_backend_ctx_count_changed() - Number of contexts ready to submit jobs
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_pm.h b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_pm.h
index 982547d16022..fc43a3fcd69f 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_pm.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_pm.h
@@ -26,13 +26,13 @@
 #ifndef _KBASE_HWACCESS_PM_H_
 #define _KBASE_HWACCESS_PM_H_
 
-#include <linux/types.h>
+#include <hw_access/mali_kbase_hw_access_regmap.h>
 #include <linux/atomic.h>
 
+#include <backend/gpu/mali_kbase_pm_defs.h>
+
 /* Forward definition - see mali_kbase.h */
 struct kbase_device;
-struct kbase_pm_policy;
-struct kbase_pm_ca_policy;
 
 /* Functions common to all HW access backends */
 
@@ -129,14 +129,14 @@ void kbase_pm_set_debug_core_mask(struct kbase_device *kbdev, u64 new_core_mask)
  * kbase_pm_set_debug_core_mask - Set the debug core mask.
  *
  * @kbdev: The kbase device structure for the device (must be a valid pointer)
- * @new_core_mask: The core mask to use, as an array where each element refers
- *                 to a job slot.
- * @new_core_mask_size: Number of elements in the core mask array.
+ * @new_core_mask_js0: The core mask to use for job slot 0
+ * @new_core_mask_js1: The core mask to use for job slot 1
+ * @new_core_mask_js2: The core mask to use for job slot 2
  *
  * This determines which cores the power manager is allowed to use.
  */
-void kbase_pm_set_debug_core_mask(struct kbase_device *kbdev, u64 *new_core_mask,
-				  size_t new_core_mask_size);
+void kbase_pm_set_debug_core_mask(struct kbase_device *kbdev, u64 new_core_mask_js0,
+				  u64 new_core_mask_js1, u64 new_core_mask_js2);
 #endif /* MALI_USE_CSF */
 
 /**
@@ -199,8 +199,7 @@ void kbase_pm_set_policy(struct kbase_device *kbdev, const struct kbase_pm_polic
  *
  * Return: The number of policies
  */
-size_t kbase_pm_list_policies(struct kbase_device *kbdev,
-			      const struct kbase_pm_policy *const **list);
+int kbase_pm_list_policies(struct kbase_device *kbdev, const struct kbase_pm_policy *const **list);
 
 /**
  * kbase_pm_protected_mode_enable() - Enable protected mode
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_time.h b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_time.h
index 222ff2001e56..8e5a8137be45 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_time.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_time.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2014, 2018-2021, 2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014, 2018-2021, 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -27,8 +27,7 @@
  *
  * @multiplier:		Numerator of the converter's fraction.
  * @divisor:		Denominator of the converter's fraction.
- * @gpu_timestamp_offset: Cached CPU to GPU TS offset computed whenever whole system
- *                        enters into standby mode where CPU Monotonic time is suspend.
+ * @offset:		Converter's offset term.
  * @device_scaled_timeouts: Timeouts in milliseconds that were scaled to be
  *                          consistent with the minimum MCU frequency. This
  *                          array caches the results of all of the conversions
@@ -56,7 +55,7 @@ struct kbase_backend_time {
 #if MALI_USE_CSF
 	u64 multiplier;
 	u64 divisor;
-	s64 gpu_timestamp_offset;
+	s64 offset;
 #endif
 	unsigned int device_scaled_timeouts[KBASE_TIMEOUT_SELECTOR_COUNT];
 };
@@ -71,40 +70,6 @@ struct kbase_backend_time {
  * Return: The CPU timestamp.
  */
 u64 __maybe_unused kbase_backend_time_convert_gpu_to_cpu(struct kbase_device *kbdev, u64 gpu_ts);
-
-/**
- * kbase_backend_update_gpu_timestamp_offset() - Updates GPU timestamp offset register with the
- *                                               cached value.
- *
- * @kbdev:	Kbase device pointer
- *
- * Compute the new cached value for GPU timestamp offset if the previously cached value has been
- * invalidated and update the GPU timestamp offset register with the cached value.
- */
-void kbase_backend_update_gpu_timestamp_offset(struct kbase_device *kbdev);
-
-/**
- * kbase_backend_invalidate_gpu_timestamp_offset() - Invalidate cached GPU timestamp offset value
- *
- * @kbdev:	Kbase device pointer
- *
- * This function invalidates cached GPU timestamp offset value whenever system suspend
- * is about to happen where CPU TS counter will be stopped.
- */
-void kbase_backend_invalidate_gpu_timestamp_offset(struct kbase_device *kbdev);
-
-#if MALI_UNIT_TEST
-/**
- * kbase_backend_read_gpu_timestamp_offset_reg() - Read GPU TIMESTAMP OFFSET Register
- *
- * @kbdev:	Kbase device pointer
- *
- * This function read GPU TIMESTAMP OFFSET Register with proper register access
- *
- * Return: GPU TIMESTAMP OFFSET Register value, as unsigned 64 bit value
- */
-u64 kbase_backend_read_gpu_timestamp_offset_reg(struct kbase_device *kbdev);
-#endif
 #endif
 
 /**
@@ -184,15 +149,6 @@ unsigned int kbase_get_timeout_ms(struct kbase_device *kbdev, enum kbase_timeout
  */
 u64 kbase_backend_get_cycle_cnt(struct kbase_device *kbdev);
 
-/**
- * kbase_arch_timer_get_cntfrq - Get system timestamp counter frequency.
- *
- * @kbdev: Instance of a GPU platform device.
- *
- * Return: Frequency in Hz
- */
-u64 kbase_arch_timer_get_cntfrq(struct kbase_device *kbdev);
-
 /**
  * kbase_backend_time_init() - Initialize system timestamp converter.
  *
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwconfig_features.h b/drivers/gpu/arm/bifrost/mali_kbase_hwconfig_features.h
deleted file mode 100644
index 265cb9585cc6..000000000000
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwconfig_features.h
+++ /dev/null
@@ -1,158 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
-/*
- *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-#ifndef _KBASE_HWCONFIG_FEATURES_H_
-#define _KBASE_HWCONFIG_FEATURES_H_
-
-#include <linux/version_compat_defs.h>
-
-enum base_hw_feature {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION,
-	KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_TLS_HASHING,
-	KBASE_HW_FEATURE_THREAD_GROUP_SPLIT,
-	KBASE_HW_FEATURE_CLEAN_ONLY_SAFE,
-	KBASE_HW_FEATURE_IDVS_GROUP_SIZE,
-	KBASE_HW_FEATURE_L2_CONFIG,
-	KBASE_HW_FEATURE_L2_SLICE_HASH,
-	KBASE_HW_FEATURE_GPU_SLEEP,
-	KBASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER,
-	KBASE_HW_FEATURE_CORE_FEATURES,
-	KBASE_HW_FEATURE_PBHA_HWU,
-	KBASE_HW_FEATURE_LARGE_PAGE_ALLOC,
-	KBASE_HW_FEATURE_THREAD_TLS_ALLOC,
-	KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_generic[] = {
-	KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tMIx[] = {
-	KBASE_HW_FEATURE_THREAD_GROUP_SPLIT, KBASE_HW_FEATURE_FLUSH_REDUCTION, KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tHEx[] = {
-	KBASE_HW_FEATURE_THREAD_GROUP_SPLIT, KBASE_HW_FEATURE_FLUSH_REDUCTION,
-	KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE, KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tSIx[] = {
-	KBASE_HW_FEATURE_THREAD_GROUP_SPLIT, KBASE_HW_FEATURE_FLUSH_REDUCTION,
-	KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE, KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tDVx[] = {
-	KBASE_HW_FEATURE_THREAD_GROUP_SPLIT, KBASE_HW_FEATURE_FLUSH_REDUCTION,
-	KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE, KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tNOx[] = {
-	KBASE_HW_FEATURE_THREAD_GROUP_SPLIT,   KBASE_HW_FEATURE_FLUSH_REDUCTION,
-	KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE, KBASE_HW_FEATURE_TLS_HASHING,
-	KBASE_HW_FEATURE_IDVS_GROUP_SIZE,      KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tGOx[] = {
-	KBASE_HW_FEATURE_THREAD_GROUP_SPLIT,   KBASE_HW_FEATURE_FLUSH_REDUCTION,
-	KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE, KBASE_HW_FEATURE_TLS_HASHING,
-	KBASE_HW_FEATURE_IDVS_GROUP_SIZE,      KBASE_HW_FEATURE_CORE_FEATURES,
-	KBASE_HW_FEATURE_THREAD_TLS_ALLOC,     KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tTRx[] = {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION,	 KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_IDVS_GROUP_SIZE,	 KBASE_HW_FEATURE_CLEAN_ONLY_SAFE,
-	KBASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER, KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tNAx[] = {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION,	 KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_IDVS_GROUP_SIZE,	 KBASE_HW_FEATURE_CLEAN_ONLY_SAFE,
-	KBASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER, KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tBEx[] = {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION,
-	KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_IDVS_GROUP_SIZE,
-	KBASE_HW_FEATURE_L2_CONFIG,
-	KBASE_HW_FEATURE_CLEAN_ONLY_SAFE,
-	KBASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER,
-	KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tBAx[] = {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION,
-	KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_IDVS_GROUP_SIZE,
-	KBASE_HW_FEATURE_L2_CONFIG,
-	KBASE_HW_FEATURE_CLEAN_ONLY_SAFE,
-	KBASE_HW_FEATURE_FLUSH_INV_SHADER_OTHER,
-	KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tODx[] = {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION, KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_L2_CONFIG, KBASE_HW_FEATURE_CLEAN_ONLY_SAFE, KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tGRx[] = {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION, KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_L2_CONFIG,	  KBASE_HW_FEATURE_CLEAN_ONLY_SAFE,
-	KBASE_HW_FEATURE_CORE_FEATURES,	  KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tVAx[] = {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION, KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_L2_CONFIG,	  KBASE_HW_FEATURE_CLEAN_ONLY_SAFE,
-	KBASE_HW_FEATURE_CORE_FEATURES,	  KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tTUx[] = {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION, KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_L2_CONFIG,	  KBASE_HW_FEATURE_CLEAN_ONLY_SAFE,
-	KBASE_HW_FEATURE_L2_SLICE_HASH,	  KBASE_HW_FEATURE_GPU_SLEEP,
-	KBASE_HW_FEATURE_CORE_FEATURES,	  KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tTIx[] = {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION,
-	KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_L2_CONFIG,
-	KBASE_HW_FEATURE_CLEAN_ONLY_SAFE,
-	KBASE_HW_FEATURE_L2_SLICE_HASH,
-	KBASE_HW_FEATURE_GPU_SLEEP,
-	KBASE_HW_FEATURE_CORE_FEATURES,
-	KBASE_HW_FEATURE_PBHA_HWU,
-	KBASE_HW_FEATURE_END
-};
-
-__maybe_unused static const enum base_hw_feature base_hw_features_tKRx[] = {
-	KBASE_HW_FEATURE_FLUSH_REDUCTION,  KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
-	KBASE_HW_FEATURE_L2_CONFIG,	   KBASE_HW_FEATURE_CLEAN_ONLY_SAFE,
-	KBASE_HW_FEATURE_L2_SLICE_HASH,	   KBASE_HW_FEATURE_GPU_SLEEP,
-	KBASE_HW_FEATURE_CORE_FEATURES,	   KBASE_HW_FEATURE_PBHA_HWU,
-	KBASE_HW_FEATURE_LARGE_PAGE_ALLOC, KBASE_HW_FEATURE_END
-};
-
-
-#endif /* _KBASE_HWCONFIG_FEATURES_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwconfig_issues.h b/drivers/gpu/arm/bifrost/mali_kbase_hwconfig_issues.h
deleted file mode 100644
index b1a3a41b232b..000000000000
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwconfig_issues.h
+++ /dev/null
@@ -1,609 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
-/*
- *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-#ifndef _KBASE_HWCONFIG_ISSUES_H_
-#define _KBASE_HWCONFIG_ISSUES_H_
-
-#include <linux/version_compat_defs.h>
-
-enum base_hw_issue {
-	KBASE_HW_ISSUE_5736,
-	KBASE_HW_ISSUE_9435,
-	KBASE_HW_ISSUE_10682,
-	KBASE_HW_ISSUE_11054,
-	KBASE_HW_ISSUE_T76X_3953,
-	KBASE_HW_ISSUE_TMIX_7891,
-	KBASE_HW_ISSUE_TMIX_7940,
-	KBASE_HW_ISSUE_TMIX_8042,
-	KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TMIX_8138,
-	KBASE_HW_ISSUE_TMIX_8206,
-	KBASE_HW_ISSUE_TMIX_8343,
-	KBASE_HW_ISSUE_TMIX_8463,
-	KBASE_HW_ISSUE_TMIX_8456,
-	KBASE_HW_ISSUE_TSIX_1116,
-	KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TMIX_8438,
-	KBASE_HW_ISSUE_TNOX_1194,
-	KBASE_HW_ISSUE_TGOX_R1_1234,
-	KBASE_HW_ISSUE_TTRX_1337,
-	KBASE_HW_ISSUE_TSIX_1792,
-	KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_3076,
-	KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TTRX_3083,
-	KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TTRX_3485,
-	KBASE_HW_ISSUE_GPU2019_3212,
-	KBASE_HW_ISSUE_TURSEHW_1997,
-	KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_TURSEHW_2716,
-	KBASE_HW_ISSUE_GPU2019_3901,
-	KBASE_HW_ISSUE_GPU2021PRO_290,
-	KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_TITANHW_2679,
-	KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2922,
-	KBASE_HW_ISSUE_TITANHW_2952,
-	KBASE_HW_ISSUE_KRAKEHW_2151,
-	KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_KRAKEHW_2269,
-	KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_generic[] = { KBASE_HW_ISSUE_END };
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tMIx_r0p0_05dev0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_10682,	  KBASE_HW_ISSUE_11054,
-	KBASE_HW_ISSUE_T76X_3953,    KBASE_HW_ISSUE_TMIX_7891,	  KBASE_HW_ISSUE_TMIX_8042,
-	KBASE_HW_ISSUE_TMIX_8133,    KBASE_HW_ISSUE_TMIX_8138,	  KBASE_HW_ISSUE_TMIX_8206,
-	KBASE_HW_ISSUE_TMIX_8343,    KBASE_HW_ISSUE_TMIX_8463,	  KBASE_HW_ISSUE_TMIX_8456,
-	KBASE_HW_ISSUE_TMIX_8438,    KBASE_HW_ISSUE_TSIX_2033,	  KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_GPU2017_1336, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tMIx_r0p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_10682,	  KBASE_HW_ISSUE_11054,
-	KBASE_HW_ISSUE_TMIX_7891,    KBASE_HW_ISSUE_TMIX_7940,	  KBASE_HW_ISSUE_TMIX_8042,
-	KBASE_HW_ISSUE_TMIX_8133,    KBASE_HW_ISSUE_TMIX_8138,	  KBASE_HW_ISSUE_TMIX_8206,
-	KBASE_HW_ISSUE_TMIX_8343,    KBASE_HW_ISSUE_TMIX_8463,	  KBASE_HW_ISSUE_TMIX_8456,
-	KBASE_HW_ISSUE_TMIX_8438,    KBASE_HW_ISSUE_TSIX_2033,	  KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_GPU2017_1336, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tMIx_r0p1[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_10682,	  KBASE_HW_ISSUE_11054,
-	KBASE_HW_ISSUE_TMIX_7891,    KBASE_HW_ISSUE_TMIX_7940,	  KBASE_HW_ISSUE_TMIX_8042,
-	KBASE_HW_ISSUE_TMIX_8133,    KBASE_HW_ISSUE_TMIX_8138,	  KBASE_HW_ISSUE_TMIX_8206,
-	KBASE_HW_ISSUE_TMIX_8343,    KBASE_HW_ISSUE_TMIX_8463,	  KBASE_HW_ISSUE_TMIX_8456,
-	KBASE_HW_ISSUE_TMIX_8438,    KBASE_HW_ISSUE_TSIX_2033,	  KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_GPU2017_1336, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tMIx[] = {
-	KBASE_HW_ISSUE_5736,	       KBASE_HW_ISSUE_9435,	    KBASE_HW_ISSUE_TMIX_7891,
-	KBASE_HW_ISSUE_TMIX_7940,      KBASE_HW_ISSUE_TMIX_8042,    KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TMIX_8138,      KBASE_HW_ISSUE_TMIX_8206,    KBASE_HW_ISSUE_TMIX_8343,
-	KBASE_HW_ISSUE_TMIX_8456,      KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tHEx_r0p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_10682,	    KBASE_HW_ISSUE_11054,
-	KBASE_HW_ISSUE_TMIX_7891,    KBASE_HW_ISSUE_TMIX_8042,	    KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_921,	    KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tHEx_r0p1[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_10682,	    KBASE_HW_ISSUE_11054,
-	KBASE_HW_ISSUE_TMIX_7891,    KBASE_HW_ISSUE_TMIX_8042,	    KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_921,	    KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tHEx_r0p2[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_10682,	    KBASE_HW_ISSUE_11054,
-	KBASE_HW_ISSUE_TMIX_7891,    KBASE_HW_ISSUE_TMIX_8042,	    KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_921,	    KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tHEx_r0p3[] = {
-	KBASE_HW_ISSUE_9435,	       KBASE_HW_ISSUE_10682,	    KBASE_HW_ISSUE_TMIX_7891,
-	KBASE_HW_ISSUE_TMIX_8042,      KBASE_HW_ISSUE_TMIX_8133,    KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_921,       KBASE_HW_ISSUE_GPU2017_1336, KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tHEx[] = {
-	KBASE_HW_ISSUE_5736,	     KBASE_HW_ISSUE_9435,	    KBASE_HW_ISSUE_TMIX_7891,
-	KBASE_HW_ISSUE_TMIX_8042,    KBASE_HW_ISSUE_TMIX_8133,	    KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tSIx_r0p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_11054,	    KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_1116,    KBASE_HW_ISSUE_TSIX_2033,	    KBASE_HW_ISSUE_TSIX_1792,
-	KBASE_HW_ISSUE_TTRX_921,     KBASE_HW_ISSUE_GPU2017_1336,   KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tSIx_r0p1[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_11054,	    KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_1116,    KBASE_HW_ISSUE_TSIX_2033,	    KBASE_HW_ISSUE_TSIX_1792,
-	KBASE_HW_ISSUE_TTRX_921,     KBASE_HW_ISSUE_GPU2017_1336,   KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tSIx_r1p0[] = {
-	KBASE_HW_ISSUE_9435,	       KBASE_HW_ISSUE_11054,	    KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_1116,      KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_GPU2017_1336,   KBASE_HW_ISSUE_TTRX_3464,    KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tSIx_r1p1[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_TMIX_8133,	  KBASE_HW_ISSUE_TSIX_1116,
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_921,	  KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TTRX_3464,    KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tSIx[] = {
-	KBASE_HW_ISSUE_5736,	     KBASE_HW_ISSUE_9435,	    KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_1116,    KBASE_HW_ISSUE_TSIX_2033,	    KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tDVx_r0p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_TMIX_8133,	  KBASE_HW_ISSUE_TSIX_1116,
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_921,	  KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TTRX_3464,    KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tDVx[] = {
-	KBASE_HW_ISSUE_5736,	     KBASE_HW_ISSUE_9435,	    KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_1116,    KBASE_HW_ISSUE_TSIX_2033,	    KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tNOx_r0p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_1116,    KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TNOX_1194,    KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_GPU2017_1336, KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tNOx[] = {
-	KBASE_HW_ISSUE_5736,	     KBASE_HW_ISSUE_9435,
-	KBASE_HW_ISSUE_TMIX_8133,    KBASE_HW_ISSUE_TSIX_1116,
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tGOx_r0p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_1116,    KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TNOX_1194,    KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_GPU2017_1336, KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tGOx_r1p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_TMIX_8133,
-	KBASE_HW_ISSUE_TSIX_1116,    KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TGOX_R1_1234, KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_GPU2017_1336, KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tGOx[] = {
-	KBASE_HW_ISSUE_5736,	     KBASE_HW_ISSUE_9435,
-	KBASE_HW_ISSUE_TMIX_8133,    KBASE_HW_ISSUE_TSIX_1116,
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTRx_r0p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_3076,    KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_TTRX_3414,    KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TTRX_3083,    KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,    KBASE_HW_ISSUE_TTRX_3485,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTRx_r0p1[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_3076,    KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_TTRX_3414,    KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TTRX_3083,    KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,    KBASE_HW_ISSUE_TTRX_3485,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTRx_r0p2[] = {
-	KBASE_HW_ISSUE_9435,
-	KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,
-	KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_3076,
-	KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TTRX_3083,
-	KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tTRx[] = {
-	KBASE_HW_ISSUE_5736,	       KBASE_HW_ISSUE_9435,	    KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,      KBASE_HW_ISSUE_TTRX_3414,    KBASE_HW_ISSUE_TTRX_3083,
-	KBASE_HW_ISSUE_TTRX_3470,      KBASE_HW_ISSUE_TTRX_3464,    KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tNAx_r0p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_3076,    KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_TTRX_3414,    KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TTRX_3083,    KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,    KBASE_HW_ISSUE_TTRX_3485,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tNAx_r0p1[] = {
-	KBASE_HW_ISSUE_9435,
-	KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,
-	KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_3076,
-	KBASE_HW_ISSUE_TTRX_921,
-	KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_GPU2017_1336,
-	KBASE_HW_ISSUE_TTRX_3083,
-	KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tNAx[] = {
-	KBASE_HW_ISSUE_5736,	       KBASE_HW_ISSUE_9435,	    KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,      KBASE_HW_ISSUE_TTRX_3414,    KBASE_HW_ISSUE_TTRX_3083,
-	KBASE_HW_ISSUE_TTRX_3470,      KBASE_HW_ISSUE_TTRX_3464,    KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tBEx_r0p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_921,     KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_TTRX_3083,    KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,    KBASE_HW_ISSUE_TTRX_3485,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tBEx_r0p1[] = {
-	KBASE_HW_ISSUE_9435,	       KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,      KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_921,       KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_TTRX_3083,      KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,      KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tBEx_r1p0[] = {
-	KBASE_HW_ISSUE_9435,	       KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,      KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_921,       KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_TTRX_3083,      KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,      KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tBEx_r1p1[] = {
-	KBASE_HW_ISSUE_9435,	       KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,      KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_921,       KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_TTRX_3083,      KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,      KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tBEx[] = {
-	KBASE_HW_ISSUE_5736,	     KBASE_HW_ISSUE_9435,
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,
-	KBASE_HW_ISSUE_TTRX_3414,    KBASE_HW_ISSUE_TTRX_3083,
-	KBASE_HW_ISSUE_TTRX_3470,    KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_lBEx_r1p0[] = {
-	KBASE_HW_ISSUE_9435,	     KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_921,     KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_TTRX_3083,    KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,    KBASE_HW_ISSUE_TTRX_3485,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_lBEx_r1p1[] = {
-	KBASE_HW_ISSUE_9435,	       KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,      KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_921,       KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_TTRX_3083,      KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,      KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tBAx_r0p0[] = {
-	KBASE_HW_ISSUE_9435,	       KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,      KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_921,       KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_TTRX_3083,      KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,      KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tBAx_r0p1[] = {
-	KBASE_HW_ISSUE_9435,	       KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,      KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_921,       KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_TTRX_3083,      KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,      KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tBAx_r0p2[] = {
-	KBASE_HW_ISSUE_9435,	       KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,      KBASE_HW_ISSUE_TTRX_2968_TTRX_3162,
-	KBASE_HW_ISSUE_TTRX_921,       KBASE_HW_ISSUE_TTRX_3414,
-	KBASE_HW_ISSUE_TTRX_3083,      KBASE_HW_ISSUE_TTRX_3470,
-	KBASE_HW_ISSUE_TTRX_3464,      KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tBAx[] = {
-	KBASE_HW_ISSUE_5736,	     KBASE_HW_ISSUE_9435,
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,
-	KBASE_HW_ISSUE_TTRX_3414,    KBASE_HW_ISSUE_TTRX_3083,
-	KBASE_HW_ISSUE_TTRX_3470,    KBASE_HW_ISSUE_TTRX_3464,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tODx_r0p0[] = {
-	KBASE_HW_ISSUE_TSIX_2033,      KBASE_HW_ISSUE_TTRX_1337,
-	KBASE_HW_ISSUE_GPU2019_3212,   KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_GPU2019_3901,   KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tODx[] = {
-	KBASE_HW_ISSUE_TSIX_2033,      KBASE_HW_ISSUE_TTRX_1337,
-	KBASE_HW_ISSUE_GPU2019_3212,   KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_GPU2019_3901,   KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tGRx_r0p0[] = {
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,	  KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_GPU2019_3901, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tGRx[] = {
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,	  KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_GPU2019_3901, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tVAx_r0p0[] = {
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,	  KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_GPU2019_3901, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tVAx_r0p1[] = {
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,	  KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_GPU2019_3901, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tVAx[] = {
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,	  KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_GPU2019_3901, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTUx_r0p0[] = {
-	KBASE_HW_ISSUE_TSIX_2033,      KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TURSEHW_1997,
-	KBASE_HW_ISSUE_GPU2019_3878,   KBASE_HW_ISSUE_TURSEHW_2716, KBASE_HW_ISSUE_GPU2019_3901,
-	KBASE_HW_ISSUE_GPU2021PRO_290, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_TITANHW_2679,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTUx_r0p1[] = {
-	KBASE_HW_ISSUE_TSIX_2033,      KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TURSEHW_1997,
-	KBASE_HW_ISSUE_GPU2019_3878,   KBASE_HW_ISSUE_TURSEHW_2716, KBASE_HW_ISSUE_GPU2019_3901,
-	KBASE_HW_ISSUE_GPU2021PRO_290, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_TITANHW_2679,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2922, KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_TURSEHW_2934,   KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tTUx[] = {
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,	  KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_TURSEHW_2716, KBASE_HW_ISSUE_GPU2019_3901, KBASE_HW_ISSUE_GPU2021PRO_290,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_TITANHW_2679, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2922, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTUx_r1p0[] = {
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,	  KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_TURSEHW_2716, KBASE_HW_ISSUE_GPU2019_3901, KBASE_HW_ISSUE_GPU2021PRO_290,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_TITANHW_2679, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2922, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTUx_r1p1[] = {
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,	  KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_TURSEHW_2716, KBASE_HW_ISSUE_GPU2019_3901, KBASE_HW_ISSUE_GPU2021PRO_290,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_TITANHW_2679, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2922, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTUx_r1p2[] = {
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,	  KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_TURSEHW_2716, KBASE_HW_ISSUE_GPU2019_3901, KBASE_HW_ISSUE_GPU2021PRO_290,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_TITANHW_2679, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2922, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTUx_r1p3[] = {
-	KBASE_HW_ISSUE_TSIX_2033,    KBASE_HW_ISSUE_TTRX_1337,	  KBASE_HW_ISSUE_GPU2019_3878,
-	KBASE_HW_ISSUE_TURSEHW_2716, KBASE_HW_ISSUE_GPU2019_3901, KBASE_HW_ISSUE_GPU2021PRO_290,
-	KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_TITANHW_2679, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2922, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tTIx[] = {
-	KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,
-	KBASE_HW_ISSUE_TURSEHW_2716,
-	KBASE_HW_ISSUE_GPU2021PRO_290,
-	KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_TITANHW_2679,
-	KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2922,
-	KBASE_HW_ISSUE_TITANHW_2952,
-	KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTIx_r0p0[] = {
-	KBASE_HW_ISSUE_TSIX_2033,
-	KBASE_HW_ISSUE_TTRX_1337,
-	KBASE_HW_ISSUE_TURSEHW_2716,
-	KBASE_HW_ISSUE_GPU2021PRO_290,
-	KBASE_HW_ISSUE_TITANHW_2710,
-	KBASE_HW_ISSUE_TITANHW_2679,
-	KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_TITANHW_2922,
-	KBASE_HW_ISSUE_TITANHW_2952,
-	KBASE_HW_ISSUE_TITANHW_2938,
-	KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tTIx_r0p1[] = {
-	KBASE_HW_ISSUE_TSIX_2033,      KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TURSEHW_2716,
-	KBASE_HW_ISSUE_GPU2021PRO_290, KBASE_HW_ISSUE_TITANHW_2710, KBASE_HW_ISSUE_TITANHW_2679,
-	KBASE_HW_ISSUE_GPU2022PRO_148, KBASE_HW_ISSUE_TITANHW_2938, KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321,   KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tKRx_r0p0[] = {
-	KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TURSEHW_2716, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_KRAKEHW_2151, KBASE_HW_ISSUE_KRAKEHW_2269, KBASE_HW_ISSUE_TITANHW_2922,
-	KBASE_HW_ISSUE_TURSEHW_2934, KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_tKRx_r0p1[] = {
-	KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TURSEHW_2716, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_KRAKEHW_2269, KBASE_HW_ISSUE_TURSEHW_2934, KBASE_HW_ISSUE_KRAKEHW_2321,
-	KBASE_HW_ISSUE_END
-};
-
-__maybe_unused static const enum base_hw_issue base_hw_issues_model_tKRx[] = {
-	KBASE_HW_ISSUE_TTRX_1337,    KBASE_HW_ISSUE_TURSEHW_2716, KBASE_HW_ISSUE_GPU2022PRO_148,
-	KBASE_HW_ISSUE_KRAKEHW_2151, KBASE_HW_ISSUE_KRAKEHW_2269, KBASE_HW_ISSUE_TURSEHW_2934,
-	KBASE_HW_ISSUE_KRAKEHW_2321, KBASE_HW_ISSUE_END
-};
-
-
-#endif /* _KBASE_HWCONFIG_ISSUES_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_ioctl_helpers.h b/drivers/gpu/arm/bifrost/mali_kbase_ioctl_helpers.h
deleted file mode 100644
index e87925bab9b0..000000000000
--- a/drivers/gpu/arm/bifrost/mali_kbase_ioctl_helpers.h
+++ /dev/null
@@ -1,542 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
-/*
- *
- * (C) COPYRIGHT 2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-#ifndef _KBASE_IOCTL_HELPERS_H_
-#define _KBASE_IOCTL_HELPERS_H_
-
-#include <uapi/gpu/arm/bifrost/mali_kbase_ioctl.h>
-
-/* Macro for IOCTLs that don't have IOCTL struct */
-#define KBASE_HANDLE_IOCTL(cmd, function, arg)                                         \
-	do {                                                                           \
-		int ret;                                                               \
-		BUILD_BUG_ON(_IOC_DIR(cmd) != _IOC_NONE);                              \
-		dev_dbg(arg->kbdev->dev, "Enter ioctl %s\n", #function);               \
-		ret = function(arg);                                                   \
-		dev_dbg(arg->kbdev->dev, "Return %d from ioctl %s\n", ret, #function); \
-		return ret;                                                            \
-	} while (0)
-
-/* Macro for IOCTLs that have input IOCTL struct */
-#define KBASE_HANDLE_IOCTL_IN(cmd, function, type, arg)                                \
-	do {                                                                           \
-		type param;                                                            \
-		int ret, err;                                                          \
-		dev_dbg(arg->kbdev->dev, "Enter ioctl %s\n", #function);               \
-		BUILD_BUG_ON(_IOC_DIR(cmd) != _IOC_WRITE);                             \
-		BUILD_BUG_ON(sizeof(param) != _IOC_SIZE(cmd));                         \
-		err = copy_from_user(&param, uarg, sizeof(param));                     \
-		if (unlikely(err))                                                     \
-			return -EFAULT;                                                \
-		err = check_padding_##cmd(&param);                                     \
-		if (unlikely(err))                                                     \
-			return -EINVAL;                                                \
-		ret = function(arg, &param);                                           \
-		dev_dbg(arg->kbdev->dev, "Return %d from ioctl %s\n", ret, #function); \
-		return ret;                                                            \
-	} while (0)
-
-/* Macro for IOCTLs that have output IOCTL struct */
-#define KBASE_HANDLE_IOCTL_OUT(cmd, function, type, arg)                               \
-	do {                                                                           \
-		type param;                                                            \
-		int ret, err;                                                          \
-		dev_dbg(arg->kbdev->dev, "Enter ioctl %s\n", #function);               \
-		BUILD_BUG_ON(_IOC_DIR(cmd) != _IOC_READ);                              \
-		BUILD_BUG_ON(sizeof(param) != _IOC_SIZE(cmd));                         \
-		memset(&param, 0, sizeof(param));                                      \
-		ret = function(arg, &param);                                           \
-		err = copy_to_user(uarg, &param, sizeof(param));                       \
-		if (unlikely(err))                                                     \
-			return -EFAULT;                                                \
-		dev_dbg(arg->kbdev->dev, "Return %d from ioctl %s\n", ret, #function); \
-		return ret;                                                            \
-	} while (0)
-
-/* Macro for IOCTLs that have input and output IOCTL struct */
-#define KBASE_HANDLE_IOCTL_INOUT(cmd, function, type, arg)                             \
-	do {                                                                           \
-		type param;                                                            \
-		int ret, err;                                                          \
-		dev_dbg(arg->kbdev->dev, "Enter ioctl %s\n", #function);               \
-		BUILD_BUG_ON(_IOC_DIR(cmd) != (_IOC_WRITE | _IOC_READ));               \
-		BUILD_BUG_ON(sizeof(param) != _IOC_SIZE(cmd));                         \
-		err = copy_from_user(&param, uarg, sizeof(param));                     \
-		if (unlikely(err))                                                     \
-			return -EFAULT;                                                \
-		err = check_padding_##cmd(&param);                                     \
-		if (unlikely(err))                                                     \
-			return -EINVAL;                                                \
-		ret = function(arg, &param);                                           \
-		err = copy_to_user(uarg, &param, sizeof(param));                       \
-		if (unlikely(err))                                                     \
-			return -EFAULT;                                                \
-		dev_dbg(arg->kbdev->dev, "Return %d from ioctl %s\n", ret, #function); \
-		return ret;                                                            \
-	} while (0)
-
-/* Inline functions to check padding bytes in the input IOCTL struct.
- * Return 0 if all padding bytes are zero, non-zero otherwise.
- */
-static inline int check_padding_KBASE_IOCTL_VERSION_CHECK(struct kbase_ioctl_version_check *p)
-{
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_VERSION_CHECK_RESERVED(struct kbase_ioctl_version_check *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_SET_FLAGS(struct kbase_ioctl_set_flags *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_GET_GPUPROPS(struct kbase_ioctl_get_gpuprops *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_ALLOC(union kbase_ioctl_mem_alloc *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_QUERY(union kbase_ioctl_mem_query *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_FREE(struct kbase_ioctl_mem_free *p)
-{
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_HWCNT_READER_SETUP(struct kbase_ioctl_hwcnt_reader_setup *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_HWCNT_SET(struct kbase_ioctl_hwcnt_values *p)
-{
-	return p->padding;
-}
-
-static inline int check_padding_KBASE_IOCTL_GET_DDK_VERSION(struct kbase_ioctl_get_ddk_version *p)
-{
-	return p->padding;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_JIT_INIT(struct kbase_ioctl_mem_jit_init *p)
-{
-	size_t i;
-
-	for (i = 0; i < ARRAY_SIZE(p->padding); i++) {
-		if (p->padding[i])
-			return -1;
-	}
-
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_SYNC(struct kbase_ioctl_mem_sync *p)
-{
-	size_t i;
-
-	/*
-	 * Checking p->padding is deferred till the support window for backward-compatibility ends.
-	 * GPUCORE-42000 will add the checking.
-	 *
-	 * To avoid the situation with old version of base which might not set padding bytes as 0,
-	 * padding bytes are set as zero here on behalf on user space.
-	 */
-	for (i = 0; i < ARRAY_SIZE(p->padding); i++)
-		p->padding[i] = 0;
-
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_MEM_FIND_CPU_OFFSET(union kbase_ioctl_mem_find_cpu_offset *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_TLSTREAM_ACQUIRE(struct kbase_ioctl_tlstream_acquire *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_COMMIT(struct kbase_ioctl_mem_commit *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_ALIAS(union kbase_ioctl_mem_alias *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_IMPORT(union kbase_ioctl_mem_import *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_FLAGS_CHANGE(struct kbase_ioctl_mem_flags_change *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_STREAM_CREATE(struct kbase_ioctl_stream_create *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_FENCE_VALIDATE(struct kbase_ioctl_fence_validate *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_PROFILE_ADD(struct kbase_ioctl_mem_profile_add *p)
-{
-	return p->padding;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_STICKY_RESOURCE_MAP(struct kbase_ioctl_sticky_resource_map *p)
-{
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_STICKY_RESOURCE_UNMAP(struct kbase_ioctl_sticky_resource_unmap *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_FIND_GPU_START_AND_OFFSET(
-	union kbase_ioctl_mem_find_gpu_start_and_offset *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_CINSTR_GWT_DUMP(union kbase_ioctl_cinstr_gwt_dump *p)
-{
-	/*
-	 * Checking p->padding is deferred till the support window for backward-compatibility ends.
-	 * GPUCORE-42000 will add the checking.
-	 *
-	 * To avoid the situation with old version of base which might not set padding bytes as 0,
-	 * padding bytes are set as zero here on behalf on user space.
-	 */
-	p->in.padding = 0;
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_EXEC_INIT(struct kbase_ioctl_mem_exec_init *p)
-{
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_GET_CPU_GPU_TIMEINFO(union kbase_ioctl_get_cpu_gpu_timeinfo *p)
-{
-	size_t i;
-
-	for (i = 0; i < ARRAY_SIZE(p->in.paddings); i++) {
-		if (p->in.paddings[i])
-			return -1;
-	}
-
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_CONTEXT_PRIORITY_CHECK(struct kbase_ioctl_context_priority_check *p)
-{
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_SET_LIMITED_CORE_COUNT(struct kbase_ioctl_set_limited_core_count *p)
-{
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_KINSTR_PRFCNT_ENUM_INFO(struct kbase_ioctl_kinstr_prfcnt_enum_info *p)
-{
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_KINSTR_PRFCNT_SETUP(union kbase_ioctl_kinstr_prfcnt_setup *p)
-{
-	return 0;
-}
-
-#if MALI_UNIT_TEST
-#endif /* MALI_UNIT_TEST */
-
-#if MALI_USE_CSF
-
-static inline int
-check_padding_KBASE_IOCTL_CS_QUEUE_REGISTER(struct kbase_ioctl_cs_queue_register *p)
-{
-	size_t i;
-
-	for (i = 0; i < ARRAY_SIZE(p->padding); i++) {
-		if (p->padding[i])
-			return -1;
-	}
-
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_CS_QUEUE_KICK(struct kbase_ioctl_cs_queue_kick *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_CS_QUEUE_BIND(union kbase_ioctl_cs_queue_bind *p)
-{
-	size_t i;
-
-	/*
-	 * Checking p->padding is deferred till the support window for backward-compatibility ends.
-	 * GPUCORE-42000 will add the checking.
-	 *
-	 * To avoid the situation with old version of base which might not set padding bytes as 0,
-	 * padding bytes are set as zero here on behalf on user space.
-	 */
-	for (i = 0; i < ARRAY_SIZE(p->in.padding); i++)
-		p->in.padding[i] = 0;
-
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_CS_QUEUE_REGISTER_EX(struct kbase_ioctl_cs_queue_register_ex *p)
-{
-	size_t i;
-
-	for (i = 0; i < ARRAY_SIZE(p->padding); i++) {
-		if (p->padding[i])
-			return -1;
-	}
-
-	for (i = 0; i < ARRAY_SIZE(p->ex_padding); i++) {
-		if (p->ex_padding[i])
-			return -1;
-	}
-
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_CS_QUEUE_TERMINATE(struct kbase_ioctl_cs_queue_terminate *p)
-{
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_CS_QUEUE_GROUP_CREATE_1_6(union kbase_ioctl_cs_queue_group_create_1_6 *p)
-{
-	size_t i;
-
-	for (i = 0; i < ARRAY_SIZE(p->in.padding); i++) {
-		if (p->in.padding[i])
-			return -1;
-	}
-
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_CS_QUEUE_GROUP_CREATE_1_18(
-	union kbase_ioctl_cs_queue_group_create_1_18 *p)
-{
-	size_t i;
-
-	for (i = 0; i < ARRAY_SIZE(p->in.padding); i++) {
-		if (p->in.padding[i])
-			return -1;
-	}
-
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_CS_QUEUE_GROUP_CREATE(union kbase_ioctl_cs_queue_group_create *p)
-{
-	size_t i;
-
-	for (i = 0; i < ARRAY_SIZE(p->in.padding); i++) {
-		if (p->in.padding[i])
-			return -1;
-	}
-
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE(struct kbase_ioctl_cs_queue_group_term *p)
-{
-	size_t i;
-
-	for (i = 0; i < ARRAY_SIZE(p->padding); i++) {
-		if (p->padding[i])
-			return -1;
-	}
-
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_KCPU_QUEUE_DELETE(struct kbase_ioctl_kcpu_queue_delete *p)
-{
-	size_t i;
-
-	/*
-	 * Checking p->padding is deferred till the support window for backward-compatibility ends.
-	 * GPUCORE-42000 will add the checking.
-	 *
-	 * To avoid the situation with old version of base which might not set padding bytes as 0,
-	 * padding bytes are set as zero here on behalf on user space.
-	 */
-	for (i = 0; i < ARRAY_SIZE(p->padding); i++)
-		p->padding[i] = 0;
-
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_KCPU_QUEUE_ENQUEUE(struct kbase_ioctl_kcpu_queue_enqueue *p)
-{
-	size_t i;
-
-	/*
-	 * Checking p->padding is deferred till the support window for backward-compatibility ends.
-	 * GPUCORE-42000 will add the checking.
-	 *
-	 * To avoid the situation with old version of base which might not set padding bytes as 0,
-	 * padding bytes are set as zero here on behalf on user space.
-	 */
-	for (i = 0; i < ARRAY_SIZE(p->padding); i++)
-		p->padding[i] = 0;
-
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_CS_TILER_HEAP_INIT(union kbase_ioctl_cs_tiler_heap_init *p)
-{
-	return p->in.padding;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_CS_TILER_HEAP_INIT_1_13(union kbase_ioctl_cs_tiler_heap_init_1_13 *p)
-{
-	return p->in.padding;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_CS_TILER_HEAP_TERM(struct kbase_ioctl_cs_tiler_heap_term *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_CS_GET_GLB_IFACE(union kbase_ioctl_cs_get_glb_iface *p)
-{
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_CS_CPU_QUEUE_DUMP(struct kbase_ioctl_cs_cpu_queue_info *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_MEM_ALLOC_EX(union kbase_ioctl_mem_alloc_ex *p)
-{
-	size_t i;
-
-	for (i = 0; i < ARRAY_SIZE(p->in.extra); i++) {
-		if (p->in.extra[i])
-			return -1;
-	}
-
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_READ_USER_PAGE(union kbase_ioctl_read_user_page *p)
-{
-	return p->in.padding;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_QUEUE_GROUP_CLEAR_FAULTS(struct kbase_ioctl_queue_group_clear_faults *p)
-{
-	size_t i;
-
-	/*
-	 * Checking p->padding is deferred till the support window for backward-compatibility ends.
-	 * GPUCORE-42000 will add the checking.
-	 *
-	 * To avoid the situation with old version of base which might not set padding bytes as 0,
-	 * padding bytes are set as zero here on behalf on user space.
-	 */
-	for (i = 0; i < ARRAY_SIZE(p->padding); i++)
-		p->padding[i] = 0;
-
-	return 0;
-}
-
-#else /* MALI_USE_CSF */
-
-static inline int check_padding_KBASE_IOCTL_JOB_SUBMIT(struct kbase_ioctl_job_submit *p)
-{
-	return 0;
-}
-
-static inline int
-check_padding_KBASE_IOCTL_SOFT_EVENT_UPDATE(struct kbase_ioctl_soft_event_update *p)
-{
-	return 0;
-}
-
-static inline int check_padding_KBASE_IOCTL_KINSTR_JM_FD(union kbase_kinstr_jm_fd *p)
-{
-	size_t i;
-
-	for (i = 0; i < ARRAY_SIZE(p->in.padding); i++) {
-		if (p->in.padding[i])
-			return -1;
-	}
-
-	return 0;
-}
-
-#endif /* !MALI_USE_CSF */
-
-#endif /* _KBASE_IOCTL_HELPERS_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_jd.c b/drivers/gpu/arm/bifrost/mali_kbase_jd.c
index 4da7fa377bd7..f66529485975 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_jd.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_jd.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -39,6 +39,9 @@
 #include <mali_kbase_hwaccess_jm.h>
 #include <tl/mali_kbase_tracepoints.h>
 #include <mali_linux_trace.h>
+
+#include <mali_kbase_cs_experimental.h>
+
 #include <mali_kbase_caps.h>
 
 /* Return whether katom will run on the GPU or not. Currently only soft jobs and
@@ -206,7 +209,7 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom,
 	}
 
 	if (copy_from_user(input_extres, get_compat_pointer(katom->kctx, user_atom->extres_list),
-			   size_mul(sizeof(*input_extres), katom->nr_extres)) != 0) {
+			   sizeof(*input_extres) * katom->nr_extres) != 0) {
 		err = -EINVAL;
 		goto failed_input_copy;
 	}
@@ -221,8 +224,7 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom,
 		struct kbase_va_region *reg;
 
 		reg = kbase_region_tracker_find_region_enclosing_address(
-			katom->kctx,
-			user_res->ext_resource & ~(__u64)BASE_EXT_RES_ACCESS_EXCLUSIVE);
+			katom->kctx, user_res->ext_resource & ~BASE_EXT_RES_ACCESS_EXCLUSIVE);
 		/* did we find a matching region object? */
 		if (unlikely(kbase_is_region_invalid_or_free(reg))) {
 			/* roll back */
@@ -684,16 +686,17 @@ static void jd_trace_atom_submit(struct kbase_context *const kctx,
 {
 	struct kbase_device *const kbdev = kctx->kbdev;
 
-	KBASE_TLSTREAM_TL_NEW_ATOM(kbdev, katom, (u32)kbase_jd_atom_id(kctx, katom));
+	KBASE_TLSTREAM_TL_NEW_ATOM(kbdev, katom, kbase_jd_atom_id(kctx, katom));
 	KBASE_TLSTREAM_TL_RET_ATOM_CTX(kbdev, katom, kctx);
 	if (priority)
-		KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITY(kbdev, katom, (u32)*priority);
+		KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITY(kbdev, katom, *priority);
 	KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(kbdev, katom, TL_ATOM_STATE_IDLE);
 	kbase_kinstr_jm_atom_queue(katom);
 }
 
 static bool jd_submit_atom(struct kbase_context *const kctx,
 			   const struct base_jd_atom *const user_atom,
+			   const struct base_jd_fragment *const user_jc_incr,
 			   struct kbase_jd_atom *const katom)
 {
 	struct kbase_device *kbdev = kctx->kbdev;
@@ -751,6 +754,8 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 	}
 #endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
+	katom->renderpass_id = user_atom->renderpass_id;
+
 	/* Implicitly sets katom->protected_state.enter as well. */
 	katom->protected_state.exit = KBASE_ATOM_EXIT_PROTECTED_CHECK;
 
@@ -869,7 +874,20 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 	/* Create a new atom. */
 	jd_trace_atom_submit(kctx, katom, &katom->sched_priority);
 
-	if (!katom->jc && (katom->core_req & BASE_JD_REQ_ATOM_TYPE) != BASE_JD_REQ_DEP) {
+#if !MALI_INCREMENTAL_RENDERING_JM
+	/* Reject atoms for incremental rendering if not supported */
+	if (katom->core_req & (BASE_JD_REQ_START_RENDERPASS | BASE_JD_REQ_END_RENDERPASS)) {
+		dev_err(kctx->kbdev->dev, "Rejecting atom with unsupported core_req 0x%x\n",
+			katom->core_req);
+		katom->event_code = BASE_JD_EVENT_JOB_INVALID;
+		return kbase_jd_done_nolock(katom, true);
+	}
+#endif /* !MALI_INCREMENTAL_RENDERING_JM */
+
+	if (katom->core_req & BASE_JD_REQ_END_RENDERPASS) {
+		WARN_ON(katom->jc != 0);
+		katom->jc_fragment = *user_jc_incr;
+	} else if (!katom->jc && (katom->core_req & BASE_JD_REQ_ATOM_TYPE) != BASE_JD_REQ_DEP) {
 		/* Reject atoms with job chain = NULL, as these cause issues
 		 * with soft-stop
 		 */
@@ -999,7 +1017,8 @@ int kbase_jd_submit(struct kbase_context *kctx, void __user *user_addr, u32 nr_a
 	struct kbase_device *kbdev;
 	u32 latest_flush;
 
-	bool jd_atom_is_v2 = (stride == sizeof(struct base_jd_atom_v2));
+	bool jd_atom_is_v2 = (stride == sizeof(struct base_jd_atom_v2) ||
+			      stride == offsetof(struct base_jd_atom_v2, renderpass_id));
 
 	CSTD_UNUSED(uk6_atom);
 
@@ -1015,7 +1034,10 @@ int kbase_jd_submit(struct kbase_context *kctx, void __user *user_addr, u32 nr_a
 		return -EINVAL;
 	}
 
-	if (stride != sizeof(struct base_jd_atom_v2) && stride != sizeof(struct base_jd_atom)) {
+	if (stride != offsetof(struct base_jd_atom_v2, renderpass_id) &&
+	    stride != sizeof(struct base_jd_atom_v2) &&
+	    stride != offsetof(struct base_jd_atom, renderpass_id) &&
+	    stride != sizeof(struct base_jd_atom)) {
 		dev_err(kbdev->dev,
 			"Stride %u passed to job_submit isn't supported by the kernel\n", stride);
 		return -EINVAL;
@@ -1034,6 +1056,7 @@ int kbase_jd_submit(struct kbase_context *kctx, void __user *user_addr, u32 nr_a
 		struct base_jd_atom user_atom = {
 			.seq_nr = 0,
 		};
+		struct base_jd_fragment user_jc_incr;
 		struct kbase_jd_atom *katom;
 
 		if (unlikely(jd_atom_is_v2)) {
@@ -1058,6 +1081,44 @@ int kbase_jd_submit(struct kbase_context *kctx, void __user *user_addr, u32 nr_a
 			}
 		}
 
+		if (stride == offsetof(struct base_jd_atom_v2, renderpass_id)) {
+			dev_dbg(kbdev->dev, "No renderpass ID: use 0\n");
+			user_atom.renderpass_id = 0;
+		} else {
+			/* Ensure all padding bytes are 0 for potential future
+			 * extension
+			 */
+			size_t j;
+
+			dev_dbg(kbdev->dev, "Renderpass ID is %d\n", user_atom.renderpass_id);
+			for (j = 0; j < sizeof(user_atom.padding); j++) {
+				if (user_atom.padding[j]) {
+					dev_err(kbdev->dev, "Bad padding byte %zu: %d\n", j,
+						user_atom.padding[j]);
+					err = -EINVAL;
+					break;
+				}
+			}
+			if (err)
+				break;
+		}
+
+		/* In this case 'jc' is the CPU address of a struct
+		 * instead of a GPU address of a job chain.
+		 */
+		if (user_atom.core_req & BASE_JD_REQ_END_RENDERPASS) {
+			if (copy_from_user(&user_jc_incr, u64_to_user_ptr(user_atom.jc),
+					   sizeof(user_jc_incr))) {
+				dev_err(kbdev->dev,
+					"Invalid jc address 0x%llx passed to job_submit\n",
+					user_atom.jc);
+				err = -EFAULT;
+				break;
+			}
+			dev_dbg(kbdev->dev, "Copied IR jobchain addresses\n");
+			user_atom.jc = 0;
+		}
+
 		user_addr = (void __user *)((uintptr_t)user_addr + stride);
 
 		mutex_lock(&jctx->lock);
@@ -1110,7 +1171,8 @@ int kbase_jd_submit(struct kbase_context *kctx, void __user *user_addr, u32 nr_a
 			mutex_lock(&jctx->lock);
 		}
 		KBASE_TLSTREAM_TL_JD_SUBMIT_ATOM_START(kbdev, katom);
-		need_to_try_schedule_context |= jd_submit_atom(kctx, &user_atom, katom);
+		need_to_try_schedule_context |=
+			jd_submit_atom(kctx, &user_atom, &user_jc_incr, katom);
 		KBASE_TLSTREAM_TL_JD_SUBMIT_ATOM_END(kbdev, katom);
 		/* Register a completed job as a disjoint event when the GPU is in a disjoint state
 		 * (ie. being reset).
@@ -1196,8 +1258,7 @@ void kbase_jd_done_worker(struct work_struct *data)
 		return;
 	}
 
-	if ((katom->event_code != BASE_JD_EVENT_DONE) && !kbase_ctx_flag(katom->kctx, KCTX_DYING) &&
-	    !kbase_ctx_flag(katom->kctx, KCTX_PAGE_FAULT_REPORT_SKIP))
+	if ((katom->event_code != BASE_JD_EVENT_DONE) && (!kbase_ctx_flag(katom->kctx, KCTX_DYING)))
 		if (!kbase_is_quick_reset_enabled(kbdev))
 			dev_err(kbdev->dev, "t6xx: GPU fault 0x%02lx from job slot %d\n",
 				(unsigned long)katom->event_code, katom->slot_nr);
@@ -1384,7 +1445,7 @@ static void jd_cancel_worker(struct work_struct *data)
  *   This can be called safely from atomic context.
  *   The caller must hold kbdev->hwaccess_lock
  */
-void kbase_jd_done(struct kbase_jd_atom *katom, unsigned int slot_nr, ktime_t *end_timestamp,
+void kbase_jd_done(struct kbase_jd_atom *katom, int slot_nr, ktime_t *end_timestamp,
 		   kbasep_js_atom_done_code done_code)
 {
 	struct kbase_context *kctx;
@@ -1516,6 +1577,9 @@ int kbase_jd_init(struct kbase_context *kctx)
 #endif
 	}
 
+	for (i = 0; i < BASE_JD_RP_COUNT; i++)
+		kctx->jctx.renderpasses[i].state = KBASE_JD_RP_COMPLETE;
+
 	mutex_init(&kctx->jctx.lock);
 
 	init_waitqueue_head(&kctx->jctx.zero_jobs_wait);
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_jm.c b/drivers/gpu/arm/bifrost/mali_kbase_jm.c
index cac12df225a0..15b0706e82d0 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_jm.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_jm.c
@@ -70,13 +70,13 @@ u32 kbase_jm_kick(struct kbase_device *kbdev, u32 js_mask)
 	dev_dbg(kbdev->dev, "JM kick slot mask 0x%x\n", js_mask);
 
 	while (js_mask) {
-		unsigned int js = (unsigned int)ffs((int)js_mask) - 1U;
+		unsigned int js = ffs(js_mask) - 1;
 		int nr_jobs_to_submit = kbase_backend_slot_free(kbdev, js);
 
 		if (kbase_jm_next_job(kbdev, js, nr_jobs_to_submit))
-			ret_mask |= (1U << js);
+			ret_mask |= (1 << js);
 
-		js_mask &= ~(1U << js);
+		js_mask &= ~(1 << js);
 	}
 
 	dev_dbg(kbdev->dev, "Can still submit to mask 0x%x\n", ret_mask);
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_jm.h b/drivers/gpu/arm/bifrost/mali_kbase_jm.h
index fb0e69a0aee3..977bcc8dcb92 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_jm.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_jm.h
@@ -49,7 +49,7 @@ u32 kbase_jm_kick(struct kbase_device *kbdev, u32 js_mask);
  */
 static inline u32 kbase_jm_kick_all(struct kbase_device *kbdev)
 {
-	return kbase_jm_kick(kbdev, (1U << kbdev->gpu_props.num_job_slots) - 1U);
+	return kbase_jm_kick(kbdev, (1 << kbdev->gpu_props.num_job_slots) - 1);
 }
 
 /**
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_js.c b/drivers/gpu/arm/bifrost/mali_kbase_js.c
index d42fde37db2a..1dca014c82d4 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_js.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_js.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2011-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -161,7 +161,7 @@ static inline int gpu_metrics_ctx_init(struct kbase_context *kctx)
 	put_cred(cred);
 
 	/* Return early if this is not a Userspace created context */
-	if (unlikely(!kctx->filp))
+	if (unlikely(!kctx->kfile))
 		return 0;
 
 	/* Serialize against the other threads trying to create/destroy Kbase contexts. */
@@ -200,7 +200,7 @@ static inline void gpu_metrics_ctx_term(struct kbase_context *kctx)
 	unsigned long flags;
 
 	/* Return early if this is not a Userspace created context */
-	if (unlikely(!kctx->filp))
+	if (unlikely(!kctx->kfile))
 		return;
 
 	/* Serialize against the other threads trying to create/destroy Kbase contexts. */
@@ -333,6 +333,19 @@ static void jsctx_queue_foreach_prio(struct kbase_context *kctx, unsigned int js
 
 		rb_erase(node, &queue->runnable_tree);
 		callback(kctx->kbdev, entry);
+
+		/* Runnable end-of-renderpass atoms can also be in the linked
+		 * list of atoms blocked on cross-slot dependencies. Remove them
+		 * to avoid calling the callback twice.
+		 */
+		if (entry->atom_flags & KBASE_KATOM_FLAG_JSCTX_IN_X_DEP_LIST) {
+			WARN_ON(!(entry->core_req & BASE_JD_REQ_END_RENDERPASS));
+			dev_dbg(kctx->kbdev->dev, "Del runnable atom %pK from X_DEP list\n",
+				(void *)entry);
+
+			list_del(&entry->queue);
+			entry->atom_flags &= ~KBASE_KATOM_FLAG_JSCTX_IN_X_DEP_LIST;
+		}
 	}
 
 	while (!list_empty(&queue->x_dep_head)) {
@@ -831,7 +844,7 @@ static bool kbase_jsctx_slot_prio_is_blocked(struct kbase_context *kctx, unsigne
 	/* all bits of sched_prio or higher, with sched_prio = 0 being the
 	 * highest priority
 	 */
-	higher_prios_mask = (prio_bit << 1u) - 1u;
+	higher_prios_mask = (prio_bit << 1) - 1u;
 	return (slot_tracking->blocked & higher_prios_mask) != 0u;
 }
 
@@ -959,7 +972,7 @@ static bool kbase_js_ctx_list_add_pullable_nolock(struct kbase_device *kbdev,
 			atomic_inc(&kbdev->js_data.nr_contexts_runnable);
 		}
 	}
-	kctx->slots_pullable |= (1UL << js);
+	kctx->slots_pullable |= (1 << js);
 
 	return ret;
 }
@@ -1000,7 +1013,7 @@ static bool kbase_js_ctx_list_add_pullable_head_nolock(struct kbase_device *kbde
 			atomic_inc(&kbdev->js_data.nr_contexts_runnable);
 		}
 	}
-	kctx->slots_pullable |= (1UL << js);
+	kctx->slots_pullable |= (1 << js);
 
 	return ret;
 }
@@ -1070,7 +1083,7 @@ static bool kbase_js_ctx_list_add_unpullable_nolock(struct kbase_device *kbdev,
 			atomic_dec(&kbdev->js_data.nr_contexts_runnable);
 		}
 	}
-	kctx->slots_pullable &= ~(1UL << js);
+	kctx->slots_pullable &= ~(1 << js);
 
 	return ret;
 }
@@ -1111,7 +1124,7 @@ static bool kbase_js_ctx_list_remove_nolock(struct kbase_device *kbdev, struct k
 			atomic_dec(&kbdev->js_data.nr_contexts_runnable);
 		}
 	}
-	kctx->slots_pullable &= ~(1UL << js);
+	kctx->slots_pullable &= ~(1 << js);
 
 	return ret;
 }
@@ -1217,7 +1230,7 @@ static bool kbase_js_ctx_pullable(struct kbase_context *kctx, unsigned int js, b
 		dev_dbg(kbdev->dev, "JS: Atom %pK is blocked in js_ctx_pullable\n", (void *)katom);
 		return false; /* next atom blocked */
 	}
-	if (katom->atom_flags & KBASE_KATOM_FLAG_X_DEP_BLOCKED) {
+	if (kbase_js_atom_blocked_on_x_dep(katom)) {
 		if (katom->x_pre_dep->gpu_rb_state == KBASE_ATOM_GPU_RB_NOT_IN_SLOT_RB ||
 		    katom->x_pre_dep->will_fail_event_code) {
 			dev_dbg(kbdev->dev,
@@ -1358,6 +1371,9 @@ static bool kbase_js_dep_validate(struct kbase_context *kctx, struct kbase_jd_at
 				    (dep_atom->status != KBASE_JD_ATOM_STATE_UNUSED)) {
 					katom->atom_flags |= KBASE_KATOM_FLAG_X_DEP_BLOCKED;
 
+					dev_dbg(kbdev->dev, "Set X_DEP flag on atom %pK\n",
+						(void *)katom);
+
 					katom->x_pre_dep = dep_atom;
 					dep_atom->x_post_dep = katom;
 					if (kbase_jd_katom_dep_type(&katom->dep[i]) ==
@@ -1393,7 +1409,7 @@ void kbase_js_set_ctx_priority(struct kbase_context *kctx, int new_priority)
 	/* Move kctx to the pullable/upullable list as per the new priority */
 	if (new_priority != kctx->priority) {
 		for (js = 0; js < kbdev->gpu_props.num_job_slots; js++) {
-			if (kctx->slots_pullable & (1UL << js))
+			if (kctx->slots_pullable & (1 << js))
 				list_move_tail(&kctx->jctx.sched_info.ctx.ctx_list_entry[js],
 					       &kbdev->js_data.ctx_list_pullable[js][new_priority]);
 			else
@@ -1431,12 +1447,110 @@ void kbase_js_update_ctx_priority(struct kbase_context *kctx)
 }
 KBASE_EXPORT_TEST_API(kbase_js_update_ctx_priority);
 
+/**
+ * js_add_start_rp() - Add an atom that starts a renderpass to the job scheduler
+ * @start_katom: Pointer to the atom to be added.
+ * Return: 0 if successful or a negative value on failure.
+ */
+static int js_add_start_rp(struct kbase_jd_atom *const start_katom)
+{
+	struct kbase_context *const kctx = start_katom->kctx;
+	struct kbase_jd_renderpass *rp;
+	struct kbase_device *const kbdev = kctx->kbdev;
+	unsigned long flags;
+
+	lockdep_assert_held(&kctx->jctx.lock);
+
+	if (WARN_ON(!(start_katom->core_req & BASE_JD_REQ_START_RENDERPASS)))
+		return -EINVAL;
+
+	if (start_katom->core_req & BASE_JD_REQ_END_RENDERPASS)
+		return -EINVAL;
+
+	compiletime_assert((1ull << (sizeof(start_katom->renderpass_id) * 8)) <=
+				   ARRAY_SIZE(kctx->jctx.renderpasses),
+			   "Should check invalid access to renderpasses");
+
+	rp = &kctx->jctx.renderpasses[start_katom->renderpass_id];
+
+	if (rp->state != KBASE_JD_RP_COMPLETE)
+		return -EINVAL;
+
+	dev_dbg(kctx->kbdev->dev, "JS add start atom %pK of RP %d\n", (void *)start_katom,
+		start_katom->renderpass_id);
+
+	/* The following members are read when updating the job slot
+	 * ringbuffer/fifo therefore they require additional locking.
+	 */
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
+	rp->state = KBASE_JD_RP_START;
+	rp->start_katom = start_katom;
+	rp->end_katom = NULL;
+	INIT_LIST_HEAD(&rp->oom_reg_list);
+
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	return 0;
+}
+
+/**
+ * js_add_end_rp() - Add an atom that ends a renderpass to the job scheduler
+ * @end_katom: Pointer to the atom to be added.
+ * Return: 0 if successful or a negative value on failure.
+ */
+static int js_add_end_rp(struct kbase_jd_atom *const end_katom)
+{
+	struct kbase_context *const kctx = end_katom->kctx;
+	struct kbase_jd_renderpass *rp;
+	struct kbase_device *const kbdev = kctx->kbdev;
+
+	lockdep_assert_held(&kctx->jctx.lock);
+
+	if (WARN_ON(!(end_katom->core_req & BASE_JD_REQ_END_RENDERPASS)))
+		return -EINVAL;
+
+	if (end_katom->core_req & BASE_JD_REQ_START_RENDERPASS)
+		return -EINVAL;
+
+	compiletime_assert((1ull << (sizeof(end_katom->renderpass_id) * 8)) <=
+				   ARRAY_SIZE(kctx->jctx.renderpasses),
+			   "Should check invalid access to renderpasses");
+
+	rp = &kctx->jctx.renderpasses[end_katom->renderpass_id];
+
+	dev_dbg(kbdev->dev, "JS add end atom %pK in state %d of RP %d\n", (void *)end_katom,
+		(int)rp->state, end_katom->renderpass_id);
+
+	if (rp->state == KBASE_JD_RP_COMPLETE)
+		return -EINVAL;
+
+	if (rp->end_katom == NULL) {
+		/* We can't be in a retry state until the fragment job chain
+		 * has completed.
+		 */
+		unsigned long flags;
+
+		WARN_ON(rp->state == KBASE_JD_RP_RETRY);
+		WARN_ON(rp->state == KBASE_JD_RP_RETRY_PEND_OOM);
+		WARN_ON(rp->state == KBASE_JD_RP_RETRY_OOM);
+
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		rp->end_katom = end_katom;
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	} else
+		WARN_ON(rp->end_katom != end_katom);
+
+	return 0;
+}
+
 bool kbasep_js_add_job(struct kbase_context *kctx, struct kbase_jd_atom *atom)
 {
 	unsigned long flags;
 	struct kbasep_js_kctx_info *js_kctx_info;
 	struct kbase_device *kbdev;
 	struct kbasep_js_device_data *js_devdata;
+	int err = 0;
 
 	bool enqueue_required = false;
 	bool timer_sync = false;
@@ -1452,6 +1566,17 @@ bool kbasep_js_add_job(struct kbase_context *kctx, struct kbase_jd_atom *atom)
 	mutex_lock(&js_devdata->queue_mutex);
 	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
 
+	if (atom->core_req & BASE_JD_REQ_START_RENDERPASS)
+		err = js_add_start_rp(atom);
+	else if (atom->core_req & BASE_JD_REQ_END_RENDERPASS)
+		err = js_add_end_rp(atom);
+
+	if (err < 0) {
+		atom->event_code = BASE_JD_EVENT_JOB_INVALID;
+		atom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+		goto out_unlock;
+	}
+
 	/*
 	 * Begin Runpool transaction
 	 */
@@ -1520,7 +1645,7 @@ bool kbasep_js_add_job(struct kbase_context *kctx, struct kbase_jd_atom *atom)
 	 * kick the job manager to attempt to fast-start the atom
 	 */
 	if (enqueue_required && kctx == kbdev->hwaccess.active_kctx[atom->slot_nr])
-		kbase_jm_try_kick(kbdev, 1UL << atom->slot_nr);
+		kbase_jm_try_kick(kbdev, 1 << atom->slot_nr);
 
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 	if (timer_sync)
@@ -1735,7 +1860,10 @@ kbasep_js_runpool_release_ctx_internal(struct kbase_device *kbdev, struct kbase_
 			kbasep_js_ctx_attr_ctx_release_atom(kbdev, kctx, katom_retained_state);
 
 	if (new_ref_count == 2 && kbase_ctx_flag(kctx, KCTX_PRIVILEGED) &&
-	    !kbase_pm_is_gpu_lost(kbdev) && !kbase_pm_is_suspending(kbdev)) {
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	    !kbase_pm_is_gpu_lost(kbdev) &&
+#endif
+	    !kbase_pm_is_suspending(kbdev)) {
 		/* Context is kept scheduled into an address space even when
 		 * there are no jobs, in this case we have to handle the
 		 * situation where all jobs have been evicted from the GPU and
@@ -1752,9 +1880,12 @@ kbasep_js_runpool_release_ctx_internal(struct kbase_device *kbdev, struct kbase_
 	 * which was previously acquired by kbasep_js_schedule_ctx().
 	 */
 	if (new_ref_count == 1 && (!kbasep_js_is_submit_allowed(js_devdata, kctx) ||
-				   kbase_pm_is_gpu_lost(kbdev) || kbase_pm_is_suspending(kbdev))) {
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+				   kbase_pm_is_gpu_lost(kbdev) ||
+#endif
+				   kbase_pm_is_suspending(kbdev))) {
 		int num_slots = kbdev->gpu_props.num_job_slots;
-		unsigned int slot;
+		int slot;
 
 		/* Last reference, and we've been told to remove this context
 		 * from the Run Pool
@@ -2058,7 +2189,11 @@ static bool kbasep_js_schedule_ctx(struct kbase_device *kbdev, struct kbase_cont
 	 * of it being called strictly after the suspend flag is set, and will
 	 * wait for this lock to drop)
 	 */
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	if (kbase_pm_is_suspending(kbdev) || kbase_pm_is_gpu_lost(kbdev)) {
+#else
+	if (kbase_pm_is_suspending(kbdev)) {
+#endif
 		/* Cause it to leave at some later point */
 		bool retained;
 		CSTD_UNUSED(retained);
@@ -2132,6 +2267,7 @@ void kbasep_js_schedule_privileged_ctx(struct kbase_device *kbdev, struct kbase_
 	js_devdata = &kbdev->js_data;
 	js_kctx_info = &kctx->jctx.sched_info;
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	/* This should only happen in response to a system call
 	 * from a user-space thread.
 	 * In a non-arbitrated environment this can never happen
@@ -2143,10 +2279,18 @@ void kbasep_js_schedule_privileged_ctx(struct kbase_device *kbdev, struct kbase_
 	 * the wait event for KCTX_SCHEDULED, since no context
 	 * can be scheduled until we have the GPU again.
 	 */
-	if (!kbase_has_arbiter(kbdev)) {
+	if (kbdev->arb.arb_if == NULL)
 		if (WARN_ON(kbase_pm_is_suspending(kbdev)))
 			return;
-	}
+#else
+	/* This should only happen in response to a system call
+	 * from a user-space thread.
+	 * In a non-arbitrated environment this can never happen
+	 * whilst suspending.
+	 */
+	if (WARN_ON(kbase_pm_is_suspending(kbdev)))
+		return;
+#endif
 
 	mutex_lock(&js_devdata->queue_mutex);
 	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
@@ -2258,8 +2402,7 @@ void kbasep_js_suspend(struct kbase_device *kbdev)
 void kbasep_js_resume(struct kbase_device *kbdev)
 {
 	struct kbasep_js_device_data *js_devdata;
-	unsigned int js;
-	int prio;
+	int js, prio;
 
 	KBASE_DEBUG_ASSERT(kbdev);
 	js_devdata = &kbdev->js_data;
@@ -2272,63 +2415,63 @@ void kbasep_js_resume(struct kbase_device *kbdev)
 			struct kbase_context *kctx, *n;
 			unsigned long flags;
 
-			if (kbase_has_arbiter(kbdev)) {
-				spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-
-				list_for_each_entry_safe(
-					kctx, n, &kbdev->js_data.ctx_list_unpullable[js][prio],
-					jctx.sched_info.ctx.ctx_list_entry[js]) {
-					struct kbasep_js_kctx_info *js_kctx_info;
-					bool timer_sync = false;
+#ifndef CONFIG_MALI_ARBITER_SUPPORT
+			spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
-					/* Drop lock so we can take kctx mutexes */
-					spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+			list_for_each_entry_safe(kctx, n,
+						 &kbdev->js_data.ctx_list_unpullable[js][prio],
+						 jctx.sched_info.ctx.ctx_list_entry[js]) {
+				struct kbasep_js_kctx_info *js_kctx_info;
+				bool timer_sync = false;
 
-					js_kctx_info = &kctx->jctx.sched_info;
+				/* Drop lock so we can take kctx mutexes */
+				spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
-					mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
-					mutex_lock(&js_devdata->runpool_mutex);
-					spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+				js_kctx_info = &kctx->jctx.sched_info;
 
-					if (!kbase_ctx_flag(kctx, KCTX_SCHEDULED) &&
-					    kbase_js_ctx_pullable(kctx, js, false))
-						timer_sync = kbase_js_ctx_list_add_pullable_nolock(
-							kbdev, kctx, js);
+				mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+				mutex_lock(&js_devdata->runpool_mutex);
+				spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
-					spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+				if (!kbase_ctx_flag(kctx, KCTX_SCHEDULED) &&
+				    kbase_js_ctx_pullable(kctx, js, false))
+					timer_sync = kbase_js_ctx_list_add_pullable_nolock(
+						kbdev, kctx, js);
 
-					if (timer_sync)
-						kbase_backend_ctx_count_changed(kbdev);
+				spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
-					mutex_unlock(&js_devdata->runpool_mutex);
-					mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+				if (timer_sync)
+					kbase_backend_ctx_count_changed(kbdev);
 
-					/* Take lock before accessing list again */
-					spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-				}
-				spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-			} else {
-				bool timer_sync = false;
+				mutex_unlock(&js_devdata->runpool_mutex);
+				mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
 
+				/* Take lock before accessing list again */
 				spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+			}
+			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+#else
+			bool timer_sync = false;
 
-				list_for_each_entry_safe(
-					kctx, n, &kbdev->js_data.ctx_list_unpullable[js][prio],
-					jctx.sched_info.ctx.ctx_list_entry[js]) {
-					if (!kbase_ctx_flag(kctx, KCTX_SCHEDULED) &&
-					    kbase_js_ctx_pullable(kctx, js, false))
-						timer_sync |= kbase_js_ctx_list_add_pullable_nolock(
-							kbdev, kctx, js);
-				}
+			spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
-				spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+			list_for_each_entry_safe(kctx, n,
+						 &kbdev->js_data.ctx_list_unpullable[js][prio],
+						 jctx.sched_info.ctx.ctx_list_entry[js]) {
+				if (!kbase_ctx_flag(kctx, KCTX_SCHEDULED) &&
+				    kbase_js_ctx_pullable(kctx, js, false))
+					timer_sync |= kbase_js_ctx_list_add_pullable_nolock(
+						kbdev, kctx, js);
+			}
 
-				if (timer_sync) {
-					mutex_lock(&js_devdata->runpool_mutex);
-					kbase_backend_ctx_count_changed(kbdev);
-					mutex_unlock(&js_devdata->runpool_mutex);
-				}
+			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+			if (timer_sync) {
+				mutex_lock(&js_devdata->runpool_mutex);
+				kbase_backend_ctx_count_changed(kbdev);
+				mutex_unlock(&js_devdata->runpool_mutex);
 			}
+#endif
 		}
 	}
 	mutex_unlock(&js_devdata->queue_mutex);
@@ -2371,7 +2514,7 @@ static unsigned int kbase_js_get_slot(struct kbase_device *kbdev, struct kbase_j
 
 bool kbase_js_dep_resolved_submit(struct kbase_context *kctx, struct kbase_jd_atom *katom)
 {
-	bool enqueue_required;
+	bool enqueue_required, add_required = true;
 
 	katom->slot_nr = kbase_js_get_slot(kctx->kbdev, katom);
 
@@ -2381,7 +2524,10 @@ bool kbase_js_dep_resolved_submit(struct kbase_context *kctx, struct kbase_jd_at
 	/* If slot will transition from unpullable to pullable then add to
 	 * pullable list
 	 */
-	enqueue_required = jsctx_rb_none_to_pull(kctx, katom->slot_nr);
+	if (jsctx_rb_none_to_pull(kctx, katom->slot_nr))
+		enqueue_required = true;
+	else
+		enqueue_required = false;
 
 	if ((katom->atom_flags & KBASE_KATOM_FLAG_X_DEP_BLOCKED) ||
 	    (katom->pre_dep &&
@@ -2394,9 +2540,15 @@ bool kbase_js_dep_resolved_submit(struct kbase_context *kctx, struct kbase_jd_at
 
 		list_add_tail(&katom->queue, &queue->x_dep_head);
 		katom->atom_flags |= KBASE_KATOM_FLAG_JSCTX_IN_X_DEP_LIST;
-		enqueue_required = false;
+		if (kbase_js_atom_blocked_on_x_dep(katom)) {
+			enqueue_required = false;
+			add_required = false;
+		}
 	} else {
 		dev_dbg(kctx->kbdev->dev, "Atom %pK not added to X_DEP list\n", (void *)katom);
+	}
+
+	if (add_required) {
 		/* Check if there are lower priority jobs to soft stop */
 		kbase_job_slot_ctx_priority_check_locked(kctx, katom);
 
@@ -2422,22 +2574,30 @@ bool kbase_js_dep_resolved_submit(struct kbase_context *kctx, struct kbase_jd_at
  */
 static void kbase_js_move_to_tree(struct kbase_jd_atom *katom)
 {
-	lockdep_assert_held(&katom->kctx->kbdev->hwaccess_lock);
+	struct kbase_context *const kctx = katom->kctx;
+
+	lockdep_assert_held(&kctx->kbdev->hwaccess_lock);
 
 	while (katom) {
 		WARN_ON(!(katom->atom_flags & KBASE_KATOM_FLAG_JSCTX_IN_X_DEP_LIST));
 
-		if (!(katom->atom_flags & KBASE_KATOM_FLAG_X_DEP_BLOCKED)) {
-			dev_dbg(katom->kctx->kbdev->dev,
+		if (!kbase_js_atom_blocked_on_x_dep(katom)) {
+			dev_dbg(kctx->kbdev->dev,
 				"Del atom %pK from X_DEP list in js_move_to_tree\n", (void *)katom);
 
 			list_del(&katom->queue);
 			katom->atom_flags &= ~KBASE_KATOM_FLAG_JSCTX_IN_X_DEP_LIST;
-			jsctx_tree_add(katom->kctx, katom);
-			katom->atom_flags |= KBASE_KATOM_FLAG_JSCTX_IN_TREE;
+			/* For incremental rendering, an end-of-renderpass atom
+			 * may have had its dependency on start-of-renderpass
+			 * ignored and may therefore already be in the tree.
+			 */
+			if (!(katom->atom_flags & KBASE_KATOM_FLAG_JSCTX_IN_TREE)) {
+				jsctx_tree_add(kctx, katom);
+				katom->atom_flags |= KBASE_KATOM_FLAG_JSCTX_IN_TREE;
+			}
 		} else {
-			dev_dbg(katom->kctx->kbdev->dev,
-				"Atom %pK blocked on x-dep in js_move_to_tree\n", (void *)katom);
+			dev_dbg(kctx->kbdev->dev, "Atom %pK blocked on x-dep in js_move_to_tree\n",
+				(void *)katom);
 			break;
 		}
 
@@ -2454,7 +2614,7 @@ static void kbase_js_move_to_tree(struct kbase_jd_atom *katom)
  *
  * Remove all post dependencies of an atom from the context ringbuffers.
  *
- * The original atom's event_code will be propagated to all dependent atoms.
+ * The original atom's event_code will be propogated to all dependent atoms.
  *
  * Context: Caller must hold the HW access lock
  */
@@ -2510,7 +2670,11 @@ struct kbase_jd_atom *kbase_js_pull(struct kbase_context *kctx, unsigned int js)
 		dev_dbg(kbdev->dev, "JS: No submit allowed for kctx %pK\n", (void *)kctx);
 		return NULL;
 	}
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	if (kbase_pm_is_suspending(kbdev) || kbase_pm_is_gpu_lost(kbdev))
+#else
+	if (kbase_pm_is_suspending(kbdev))
+#endif
 		return NULL;
 
 	katom = jsctx_rb_peek(kctx, js);
@@ -2540,7 +2704,7 @@ struct kbase_jd_atom *kbase_js_pull(struct kbase_context *kctx, unsigned int js)
 			return NULL;
 	}
 
-	if (katom->atom_flags & KBASE_KATOM_FLAG_X_DEP_BLOCKED) {
+	if (kbase_js_atom_blocked_on_x_dep(katom)) {
 		if (katom->x_pre_dep->gpu_rb_state == KBASE_ATOM_GPU_RB_NOT_IN_SLOT_RB ||
 		    katom->x_pre_dep->will_fail_event_code) {
 			dev_dbg(kbdev->dev,
@@ -2558,7 +2722,7 @@ struct kbase_jd_atom *kbase_js_pull(struct kbase_context *kctx, unsigned int js)
 	}
 
 	KBASE_KTRACE_ADD_JM_SLOT_INFO(kbdev, JS_PULL_JOB, kctx, katom, katom->jc, js,
-				      (u64)katom->sched_priority);
+				      katom->sched_priority);
 	kbase_ctx_flag_set(kctx, KCTX_PULLED);
 	kbase_ctx_flag_set(kctx, (KCTX_PULLED_SINCE_ACTIVE_JS0 << js));
 
@@ -2580,6 +2744,190 @@ struct kbase_jd_atom *kbase_js_pull(struct kbase_context *kctx, unsigned int js)
 	return katom;
 }
 
+/**
+ * js_return_of_start_rp() - Handle soft-stop of an atom that starts a
+ *                           renderpass
+ * @start_katom: Pointer to the start-of-renderpass atom that was soft-stopped
+ *
+ * This function is called to switch to incremental rendering if the tiler job
+ * chain at the start of a renderpass has used too much memory. It prevents the
+ * tiler job being pulled for execution in the job scheduler again until the
+ * next phase of incremental rendering is complete.
+ *
+ * If the end-of-renderpass atom is already in the job scheduler (because a
+ * previous attempt at tiling used too much memory during the same renderpass)
+ * then it is unblocked; otherwise, it is run by handing it to the scheduler.
+ */
+static void js_return_of_start_rp(struct kbase_jd_atom *const start_katom)
+{
+	struct kbase_context *const kctx = start_katom->kctx;
+	struct kbase_device *const kbdev = kctx->kbdev;
+	struct kbase_jd_renderpass *rp;
+	struct kbase_jd_atom *end_katom;
+	unsigned long flags;
+
+	lockdep_assert_held(&kctx->jctx.lock);
+
+	if (WARN_ON(!(start_katom->core_req & BASE_JD_REQ_START_RENDERPASS)))
+		return;
+
+	compiletime_assert((1ull << (sizeof(start_katom->renderpass_id) * 8)) <=
+				   ARRAY_SIZE(kctx->jctx.renderpasses),
+			   "Should check invalid access to renderpasses");
+
+	rp = &kctx->jctx.renderpasses[start_katom->renderpass_id];
+
+	if (WARN_ON(rp->start_katom != start_katom))
+		return;
+
+	dev_dbg(kctx->kbdev->dev, "JS return start atom %pK in state %d of RP %d\n",
+		(void *)start_katom, (int)rp->state, start_katom->renderpass_id);
+
+	if (WARN_ON(rp->state == KBASE_JD_RP_COMPLETE))
+		return;
+
+	/* The tiler job might have been soft-stopped for some reason other
+	 * than running out of memory.
+	 */
+	if (rp->state == KBASE_JD_RP_START || rp->state == KBASE_JD_RP_RETRY) {
+		dev_dbg(kctx->kbdev->dev, "JS return isn't OOM in state %d of RP %d\n",
+			(int)rp->state, start_katom->renderpass_id);
+		return;
+	}
+
+	dev_dbg(kctx->kbdev->dev, "JS return confirm OOM in state %d of RP %d\n", (int)rp->state,
+		start_katom->renderpass_id);
+
+	if (WARN_ON(rp->state != KBASE_JD_RP_PEND_OOM && rp->state != KBASE_JD_RP_RETRY_PEND_OOM))
+		return;
+
+	/* Prevent the tiler job being pulled for execution in the
+	 * job scheduler again.
+	 */
+	dev_dbg(kbdev->dev, "Blocking start atom %pK\n", (void *)start_katom);
+	atomic_inc(&start_katom->blocked);
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
+	rp->state = (rp->state == KBASE_JD_RP_PEND_OOM) ? KBASE_JD_RP_OOM : KBASE_JD_RP_RETRY_OOM;
+
+	/* Was the fragment job chain submitted to kbase yet? */
+	end_katom = rp->end_katom;
+	if (end_katom) {
+		dev_dbg(kctx->kbdev->dev, "JS return add end atom %pK\n", (void *)end_katom);
+
+		if (rp->state == KBASE_JD_RP_RETRY_OOM) {
+			/* Allow the end of the renderpass to be pulled for
+			 * execution again to continue incremental rendering.
+			 */
+			dev_dbg(kbdev->dev, "Unblocking end atom %pK\n", (void *)end_katom);
+			atomic_dec(&end_katom->blocked);
+			WARN_ON(!(end_katom->atom_flags & KBASE_KATOM_FLAG_JSCTX_IN_TREE));
+			WARN_ON(end_katom->status != KBASE_JD_ATOM_STATE_IN_JS);
+
+			kbase_js_ctx_list_add_pullable_nolock(kbdev, kctx, end_katom->slot_nr);
+
+			/* Expect the fragment job chain to be scheduled without
+			 * further action because this function is called when
+			 * returning an atom to the job scheduler ringbuffer.
+			 */
+			end_katom = NULL;
+		} else {
+			WARN_ON(end_katom->status != KBASE_JD_ATOM_STATE_QUEUED &&
+				end_katom->status != KBASE_JD_ATOM_STATE_IN_JS);
+		}
+	}
+
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	if (end_katom)
+		kbase_jd_dep_clear_locked(end_katom);
+}
+
+/**
+ * js_return_of_end_rp() - Handle completion of an atom that ends a renderpass
+ * @end_katom: Pointer to the end-of-renderpass atom that was completed
+ *
+ * This function is called to continue incremental rendering if the tiler job
+ * chain at the start of a renderpass used too much memory. It resets the
+ * mechanism for detecting excessive memory usage then allows the soft-stopped
+ * tiler job chain to be pulled for execution again.
+ *
+ * The start-of-renderpass atom must already been submitted to kbase.
+ */
+static void js_return_of_end_rp(struct kbase_jd_atom *const end_katom)
+{
+	struct kbase_context *const kctx = end_katom->kctx;
+	struct kbase_device *const kbdev = kctx->kbdev;
+	struct kbase_jd_renderpass *rp;
+	struct kbase_jd_atom *start_katom;
+	unsigned long flags;
+
+	lockdep_assert_held(&kctx->jctx.lock);
+
+	if (WARN_ON(!(end_katom->core_req & BASE_JD_REQ_END_RENDERPASS)))
+		return;
+
+	compiletime_assert((1ull << (sizeof(end_katom->renderpass_id) * 8)) <=
+				   ARRAY_SIZE(kctx->jctx.renderpasses),
+			   "Should check invalid access to renderpasses");
+
+	rp = &kctx->jctx.renderpasses[end_katom->renderpass_id];
+
+	if (WARN_ON(rp->end_katom != end_katom))
+		return;
+
+	dev_dbg(kctx->kbdev->dev, "JS return end atom %pK in state %d of RP %d\n",
+		(void *)end_katom, (int)rp->state, end_katom->renderpass_id);
+
+	if (WARN_ON(rp->state != KBASE_JD_RP_OOM && rp->state != KBASE_JD_RP_RETRY_OOM))
+		return;
+
+	/* Reduce the number of mapped pages in the memory regions that
+	 * triggered out-of-memory last time so that we can detect excessive
+	 * memory usage again.
+	 */
+	kbase_gpu_vm_lock(kctx);
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
+	while (!list_empty(&rp->oom_reg_list)) {
+		struct kbase_va_region *reg =
+			list_first_entry(&rp->oom_reg_list, struct kbase_va_region, link);
+
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+		dev_dbg(kbdev->dev, "Reset backing to %zu pages for region %pK\n",
+			reg->threshold_pages, (void *)reg);
+
+		if (!WARN_ON(reg->flags & KBASE_REG_VA_FREED))
+			kbase_mem_shrink(kctx, reg, reg->threshold_pages);
+
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		dev_dbg(kbdev->dev, "Deleting region %pK from list\n", (void *)reg);
+		list_del_init(&reg->link);
+		kbase_va_region_alloc_put(kctx, reg);
+	}
+
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	kbase_gpu_vm_unlock(kctx);
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	rp->state = KBASE_JD_RP_RETRY;
+	dev_dbg(kbdev->dev, "Changed state to %d for retry\n", rp->state);
+
+	/* Allow the start of the renderpass to be pulled for execution again
+	 * to begin/continue incremental rendering.
+	 */
+	start_katom = rp->start_katom;
+	if (!WARN_ON(!start_katom)) {
+		dev_dbg(kbdev->dev, "Unblocking start atom %pK\n", (void *)start_katom);
+		atomic_dec(&start_katom->blocked);
+		(void)kbase_js_ctx_list_add_pullable_head_nolock(kbdev, kctx, start_katom->slot_nr);
+	}
+
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+}
+
 static void js_return_worker(struct work_struct *data)
 {
 	struct kbase_jd_atom *katom = container_of(data, struct kbase_jd_atom, work);
@@ -2588,7 +2936,7 @@ static void js_return_worker(struct work_struct *data)
 	struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
 	struct kbasep_js_kctx_info *js_kctx_info = &kctx->jctx.sched_info;
 	struct kbasep_js_atom_retained_state retained_state;
-	unsigned int js = katom->slot_nr;
+	int js = katom->slot_nr;
 	bool slot_became_unblocked;
 	bool timer_sync = false;
 	bool context_idle = false;
@@ -2600,7 +2948,9 @@ static void js_return_worker(struct work_struct *data)
 		katom->event_code);
 
 	KBASE_KTRACE_ADD_JM(kbdev, JS_RETURN_WORKER, kctx, katom, katom->jc, 0);
-	KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTSTOP_EX(kbdev, katom);
+
+	if (katom->event_code != BASE_JD_EVENT_END_RP_DONE)
+		KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTSTOP_EX(kbdev, katom);
 
 	kbase_backend_complete_wq(kbdev, katom);
 
@@ -2609,7 +2959,8 @@ static void js_return_worker(struct work_struct *data)
 	mutex_lock(&js_devdata->queue_mutex);
 	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
 
-	atomic_dec(&katom->blocked);
+	if (katom->event_code != BASE_JD_EVENT_END_RP_DONE)
+		atomic_dec(&katom->blocked);
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
@@ -2640,8 +2991,8 @@ static void js_return_worker(struct work_struct *data)
 		}
 
 		if (kctx->as_nr != KBASEP_AS_NR_INVALID && !kbase_ctx_flag(kctx, KCTX_DYING)) {
-			unsigned int num_slots = kbdev->gpu_props.num_job_slots;
-			unsigned int slot;
+			int num_slots = kbdev->gpu_props.num_job_slots;
+			int slot;
 
 			if (!kbasep_js_is_submit_allowed(js_devdata, kctx))
 				kbasep_js_set_submit_allowed(js_devdata, kctx);
@@ -2674,6 +3025,16 @@ static void js_return_worker(struct work_struct *data)
 	mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
 	mutex_unlock(&js_devdata->queue_mutex);
 
+	if (katom->core_req & BASE_JD_REQ_START_RENDERPASS) {
+		mutex_lock(&kctx->jctx.lock);
+		js_return_of_start_rp(katom);
+		mutex_unlock(&kctx->jctx.lock);
+	} else if (katom->event_code == BASE_JD_EVENT_END_RP_DONE) {
+		mutex_lock(&kctx->jctx.lock);
+		js_return_of_end_rp(katom);
+		mutex_unlock(&kctx->jctx.lock);
+	}
+
 	dev_dbg(kbdev->dev, "JS: retained state %s finished",
 		kbasep_js_has_atom_finished(&retained_state) ? "has" : "hasn't");
 
@@ -2709,6 +3070,144 @@ void kbase_js_unpull(struct kbase_context *kctx, struct kbase_jd_atom *katom)
 	queue_work(kctx->jctx.job_done_wq, &katom->work);
 }
 
+/**
+ * js_complete_start_rp() - Handle completion of atom that starts a renderpass
+ * @kctx:        Context pointer
+ * @start_katom: Pointer to the atom that completed
+ *
+ * Put any references to virtual memory regions that might have been added by
+ * kbase_job_slot_softstop_start_rp() because the tiler job chain completed
+ * despite any pending soft-stop request.
+ *
+ * If the atom that just completed was soft-stopped during a previous attempt to
+ * run it then there should be a blocked end-of-renderpass atom waiting for it,
+ * which we must unblock to process the output of the tiler job chain.
+ *
+ * Return: true if caller should call kbase_backend_ctx_count_changed()
+ */
+static bool js_complete_start_rp(struct kbase_context *kctx,
+				 struct kbase_jd_atom *const start_katom)
+{
+	struct kbase_device *const kbdev = kctx->kbdev;
+	struct kbase_jd_renderpass *rp;
+	bool timer_sync = false;
+
+	lockdep_assert_held(&kctx->jctx.lock);
+
+	if (WARN_ON(!(start_katom->core_req & BASE_JD_REQ_START_RENDERPASS)))
+		return false;
+
+	compiletime_assert((1ull << (sizeof(start_katom->renderpass_id) * 8)) <=
+				   ARRAY_SIZE(kctx->jctx.renderpasses),
+			   "Should check invalid access to renderpasses");
+
+	rp = &kctx->jctx.renderpasses[start_katom->renderpass_id];
+
+	if (WARN_ON(rp->start_katom != start_katom))
+		return false;
+
+	dev_dbg(kctx->kbdev->dev, "Start atom %pK is done in state %d of RP %d\n",
+		(void *)start_katom, (int)rp->state, start_katom->renderpass_id);
+
+	if (WARN_ON(rp->state == KBASE_JD_RP_COMPLETE))
+		return false;
+
+	if (rp->state == KBASE_JD_RP_PEND_OOM || rp->state == KBASE_JD_RP_RETRY_PEND_OOM) {
+		unsigned long flags;
+
+		dev_dbg(kctx->kbdev->dev, "Start atom %pK completed before soft-stop\n",
+			(void *)start_katom);
+
+		kbase_gpu_vm_lock(kctx);
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
+		while (!list_empty(&rp->oom_reg_list)) {
+			struct kbase_va_region *reg =
+				list_first_entry(&rp->oom_reg_list, struct kbase_va_region, link);
+
+			WARN_ON(reg->flags & KBASE_REG_VA_FREED);
+			dev_dbg(kctx->kbdev->dev, "Deleting region %pK from list\n", (void *)reg);
+			list_del_init(&reg->link);
+			kbase_va_region_alloc_put(kctx, reg);
+		}
+
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+		kbase_gpu_vm_unlock(kctx);
+	} else {
+		dev_dbg(kctx->kbdev->dev, "Start atom %pK did not exceed memory threshold\n",
+			(void *)start_katom);
+
+		WARN_ON(rp->state != KBASE_JD_RP_START && rp->state != KBASE_JD_RP_RETRY);
+	}
+
+	if (rp->state == KBASE_JD_RP_RETRY || rp->state == KBASE_JD_RP_RETRY_PEND_OOM) {
+		struct kbase_jd_atom *const end_katom = rp->end_katom;
+
+		if (!WARN_ON(!end_katom)) {
+			unsigned long flags;
+
+			/* Allow the end of the renderpass to be pulled for
+			 * execution again to continue incremental rendering.
+			 */
+			dev_dbg(kbdev->dev, "Unblocking end atom %pK!\n", (void *)end_katom);
+			atomic_dec(&end_katom->blocked);
+
+			spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+			timer_sync = kbase_js_ctx_list_add_pullable_nolock(kbdev, kctx,
+									   end_katom->slot_nr);
+			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+		}
+	}
+
+	return timer_sync;
+}
+
+/**
+ * js_complete_end_rp() - Handle final completion of atom that ends a renderpass
+ * @kctx:      Context pointer
+ * @end_katom: Pointer to the atom that completed for the last time
+ *
+ * This function must only be called if the renderpass actually completed
+ * without the tiler job chain at the start using too much memory; otherwise
+ * completion of the end-of-renderpass atom is handled similarly to a soft-stop.
+ */
+static void js_complete_end_rp(struct kbase_context *kctx, struct kbase_jd_atom *const end_katom)
+{
+	struct kbase_device *const kbdev = kctx->kbdev;
+	unsigned long flags;
+	struct kbase_jd_renderpass *rp;
+
+	lockdep_assert_held(&kctx->jctx.lock);
+
+	if (WARN_ON(!(end_katom->core_req & BASE_JD_REQ_END_RENDERPASS)))
+		return;
+
+	compiletime_assert((1ull << (sizeof(end_katom->renderpass_id) * 8)) <=
+				   ARRAY_SIZE(kctx->jctx.renderpasses),
+			   "Should check invalid access to renderpasses");
+
+	rp = &kctx->jctx.renderpasses[end_katom->renderpass_id];
+
+	if (WARN_ON(rp->end_katom != end_katom))
+		return;
+
+	dev_dbg(kbdev->dev, "End atom %pK is done in state %d of RP %d\n", (void *)end_katom,
+		(int)rp->state, end_katom->renderpass_id);
+
+	if (WARN_ON(rp->state == KBASE_JD_RP_COMPLETE) || WARN_ON(rp->state == KBASE_JD_RP_OOM) ||
+	    WARN_ON(rp->state == KBASE_JD_RP_RETRY_OOM))
+		return;
+
+	/* Rendering completed without running out of memory.
+	 */
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	WARN_ON(!list_empty(&rp->oom_reg_list));
+	rp->state = KBASE_JD_RP_COMPLETE;
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	dev_dbg(kbdev->dev, "Renderpass %d is complete\n", end_katom->renderpass_id);
+}
+
 bool kbase_js_complete_atom_wq(struct kbase_context *kctx, struct kbase_jd_atom *katom)
 {
 	struct kbasep_js_kctx_info *js_kctx_info;
@@ -2716,14 +3215,21 @@ bool kbase_js_complete_atom_wq(struct kbase_context *kctx, struct kbase_jd_atom
 	struct kbase_device *kbdev;
 	unsigned long flags;
 	bool timer_sync = false;
-	unsigned int atom_slot;
+	int atom_slot;
 	bool context_idle = false;
 	int prio = katom->sched_priority;
 
 	kbdev = kctx->kbdev;
 	atom_slot = katom->slot_nr;
 
-	dev_dbg(kbdev->dev, "%s for atom %pK (s:%u)\n", __func__, (void *)katom, atom_slot);
+	dev_dbg(kbdev->dev, "%s for atom %pK (s:%d)\n", __func__, (void *)katom, atom_slot);
+
+	/* Update the incremental rendering state machine.
+	 */
+	if (katom->core_req & BASE_JD_REQ_START_RENDERPASS)
+		timer_sync |= js_complete_start_rp(kctx, katom);
+	else if (katom->core_req & BASE_JD_REQ_END_RENDERPASS)
+		js_complete_end_rp(kctx, katom);
 
 	js_kctx_info = &kctx->jctx.sched_info;
 	js_devdata = &kbdev->js_data;
@@ -2754,7 +3260,7 @@ bool kbase_js_complete_atom_wq(struct kbase_context *kctx, struct kbase_jd_atom
 		 */
 		if (slot_became_unblocked) {
 			dev_dbg(kbdev->dev,
-				"kctx %pK is no longer blocked from submitting on slot %u at priority %d or higher\n",
+				"kctx %pK is no longer blocked from submitting on slot %d at priority %d or higher\n",
 				(void *)kctx, atom_slot, prio);
 
 			if (kbase_js_ctx_pullable(kctx, atom_slot, true))
@@ -2813,6 +3319,61 @@ bool kbase_js_complete_atom_wq(struct kbase_context *kctx, struct kbase_jd_atom
 	return context_idle;
 }
 
+/**
+ * js_end_rp_is_complete() - Check whether an atom that ends a renderpass has
+ *                           completed for the last time.
+ *
+ * @end_katom: Pointer to the atom that completed on the hardware.
+ *
+ * An atom that ends a renderpass may be run on the hardware several times
+ * before notifying userspace or allowing dependent atoms to be executed.
+ *
+ * This function is used to decide whether or not to allow end-of-renderpass
+ * atom completion. It only returns false if the atom at the start of the
+ * renderpass was soft-stopped because it used too much memory during the most
+ * recent attempt at tiling.
+ *
+ * Return: True if the atom completed for the last time.
+ */
+static bool js_end_rp_is_complete(struct kbase_jd_atom *const end_katom)
+{
+	struct kbase_context *const kctx = end_katom->kctx;
+	struct kbase_device *const kbdev = kctx->kbdev;
+	struct kbase_jd_renderpass *rp;
+
+	lockdep_assert_held(&kctx->kbdev->hwaccess_lock);
+
+	if (WARN_ON(!(end_katom->core_req & BASE_JD_REQ_END_RENDERPASS)))
+		return true;
+
+	compiletime_assert((1ull << (sizeof(end_katom->renderpass_id) * 8)) <=
+				   ARRAY_SIZE(kctx->jctx.renderpasses),
+			   "Should check invalid access to renderpasses");
+
+	rp = &kctx->jctx.renderpasses[end_katom->renderpass_id];
+
+	if (WARN_ON(rp->end_katom != end_katom))
+		return true;
+
+	dev_dbg(kbdev->dev, "JS complete end atom %pK in state %d of RP %d\n", (void *)end_katom,
+		(int)rp->state, end_katom->renderpass_id);
+
+	if (WARN_ON(rp->state == KBASE_JD_RP_COMPLETE))
+		return true;
+
+	/* Failure of end-of-renderpass atoms must not return to the
+	 * start of the renderpass.
+	 */
+	if (end_katom->event_code != BASE_JD_EVENT_DONE)
+		return true;
+
+	if (rp->state != KBASE_JD_RP_OOM && rp->state != KBASE_JD_RP_RETRY_OOM)
+		return true;
+
+	dev_dbg(kbdev->dev, "Suppressing end atom completion\n");
+	return false;
+}
+
 struct kbase_jd_atom *kbase_js_complete_atom(struct kbase_jd_atom *katom, ktime_t *end_timestamp)
 {
 	struct kbase_device *kbdev;
@@ -2825,6 +3386,12 @@ struct kbase_jd_atom *kbase_js_complete_atom(struct kbase_jd_atom *katom, ktime_
 
 	lockdep_assert_held(&kctx->kbdev->hwaccess_lock);
 
+	if ((katom->core_req & BASE_JD_REQ_END_RENDERPASS) && !js_end_rp_is_complete(katom)) {
+		katom->event_code = BASE_JD_EVENT_END_RP_DONE;
+		kbase_js_unpull(kctx, katom);
+		return NULL;
+	}
+
 	if (katom->will_fail_event_code)
 		katom->event_code = katom->will_fail_event_code;
 
@@ -2874,6 +3441,70 @@ struct kbase_jd_atom *kbase_js_complete_atom(struct kbase_jd_atom *katom, ktime_
 	return NULL;
 }
 
+/**
+ * kbase_js_atom_blocked_on_x_dep - Decide whether to ignore a cross-slot
+ *                                  dependency
+ * @katom:	Pointer to an atom in the slot ringbuffer
+ *
+ * A cross-slot dependency is ignored if necessary to unblock incremental
+ * rendering. If the atom at the start of a renderpass used too much memory
+ * and was soft-stopped then the atom at the end of a renderpass is submitted
+ * to hardware regardless of its dependency on the start-of-renderpass atom.
+ * This can happen multiple times for the same pair of atoms.
+ *
+ * Return: true to block the atom or false to allow it to be submitted to
+ *         hardware
+ */
+bool kbase_js_atom_blocked_on_x_dep(struct kbase_jd_atom *const katom)
+{
+	struct kbase_context *const kctx = katom->kctx;
+	struct kbase_device *kbdev = kctx->kbdev;
+	struct kbase_jd_renderpass *rp;
+
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	if (!(katom->atom_flags & KBASE_KATOM_FLAG_X_DEP_BLOCKED)) {
+		dev_dbg(kbdev->dev, "Atom %pK is not blocked on a cross-slot dependency",
+			(void *)katom);
+		return false;
+	}
+
+	if (!(katom->core_req & BASE_JD_REQ_END_RENDERPASS)) {
+		dev_dbg(kbdev->dev, "Atom %pK is blocked on a cross-slot dependency",
+			(void *)katom);
+		return true;
+	}
+
+	compiletime_assert((1ull << (sizeof(katom->renderpass_id) * 8)) <=
+				   ARRAY_SIZE(kctx->jctx.renderpasses),
+			   "Should check invalid access to renderpasses");
+
+	rp = &kctx->jctx.renderpasses[katom->renderpass_id];
+	/* We can read a subset of renderpass state without holding
+	 * higher-level locks (but not end_katom, for example).
+	 */
+
+	WARN_ON(rp->state == KBASE_JD_RP_COMPLETE);
+
+	dev_dbg(kbdev->dev, "End atom has cross-slot dep in state %d\n", (int)rp->state);
+
+	if (rp->state != KBASE_JD_RP_OOM && rp->state != KBASE_JD_RP_RETRY_OOM)
+		return true;
+
+	/* Tiler ran out of memory so allow the fragment job chain to run
+	 * if it only depends on the tiler job chain.
+	 */
+	if (katom->x_pre_dep != rp->start_katom) {
+		dev_dbg(kbdev->dev, "Dependency is on %pK not start atom %pK\n",
+			(void *)katom->x_pre_dep, (void *)rp->start_katom);
+		return true;
+	}
+
+	dev_dbg(kbdev->dev, "Ignoring cross-slot dep on atom %pK\n", (void *)katom->x_pre_dep);
+
+	return false;
+}
+
 void kbase_js_sched(struct kbase_device *kbdev, unsigned int js_mask)
 {
 	struct kbasep_js_device_data *js_devdata;
@@ -2898,7 +3529,7 @@ void kbase_js_sched(struct kbase_device *kbdev, unsigned int js_mask)
 	}
 
 	while (js_mask) {
-		js = (unsigned int)ffs((int)js_mask) - 1;
+		js = ffs(js_mask) - 1;
 
 		while (1) {
 			struct kbase_context *kctx;
@@ -2908,7 +3539,7 @@ void kbase_js_sched(struct kbase_device *kbdev, unsigned int js_mask)
 			kctx = kbase_js_ctx_list_pop_head(kbdev, js);
 
 			if (!kctx) {
-				js_mask &= ~(1UL << js);
+				js_mask &= ~(1 << js);
 				dev_dbg(kbdev->dev, "No kctx on pullable list (s:%u)\n", js);
 				break;
 			}
@@ -2960,7 +3591,7 @@ void kbase_js_sched(struct kbase_device *kbdev, unsigned int js_mask)
 				}
 
 				/* No more jobs can be submitted on this slot */
-				js_mask &= ~(1UL << js);
+				js_mask &= ~(1 << js);
 				break;
 			}
 			mutex_lock(&kctx->jctx.sched_info.ctx.jsctx_mutex);
@@ -2968,9 +3599,9 @@ void kbase_js_sched(struct kbase_device *kbdev, unsigned int js_mask)
 
 			kbase_ctx_flag_clear(kctx, KCTX_PULLED);
 
-			if (!kbase_jm_kick(kbdev, 1UL << js)) {
+			if (!kbase_jm_kick(kbdev, 1 << js)) {
 				dev_dbg(kbdev->dev, "No more jobs can be submitted (s:%u)\n", js);
-				js_mask &= ~(1UL << js);
+				js_mask &= ~(1 << js);
 			}
 			if (!kbase_ctx_flag(kctx, KCTX_PULLED)) {
 				bool pullable;
@@ -3018,7 +3649,7 @@ void kbase_js_sched(struct kbase_device *kbdev, unsigned int js_mask)
 				}
 				mutex_unlock(&kctx->jctx.sched_info.ctx.jsctx_mutex);
 
-				js_mask &= ~(1UL << js);
+				js_mask &= ~(1 << js);
 				break; /* Could not run atoms on this slot */
 			}
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.c b/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.c
index 87085912bd6c..c6a66be98178 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -247,7 +247,7 @@ static int reader_changes_init(struct reader_changes *const changes, const size_
 	changes->threshold =
 		min(((size_t)(changes->size)) / 4, ((size_t)(PAGE_SIZE)) / sizeof(*changes->data));
 
-	return (int)changes->size;
+	return changes->size;
 }
 
 /**
@@ -516,8 +516,7 @@ static ssize_t reader_changes_copy_to_user(struct reader_changes *const changes,
 	do {
 		changes_tail = changes->tail;
 		changes_count = reader_changes_count_locked(changes);
-		read_size =
-			min(size_mul(changes_count, entry_size), buffer_size & ~(entry_size - 1));
+		read_size = min(changes_count * entry_size, buffer_size & ~(entry_size - 1));
 
 		if (!read_size)
 			break;
@@ -527,7 +526,7 @@ static ssize_t reader_changes_copy_to_user(struct reader_changes *const changes,
 
 		buffer += read_size;
 		buffer_size -= read_size;
-		ret += (ssize_t)read_size;
+		ret += read_size;
 		changes_tail = (changes_tail + read_size / entry_size) & (changes->size - 1);
 		smp_store_release(&changes->tail, changes_tail);
 	} while (read_size);
@@ -744,6 +743,7 @@ int kbase_kinstr_jm_get_fd(struct kbase_kinstr_jm *const ctx, union kbase_kinstr
 	size_t const change_size = sizeof(struct kbase_kinstr_jm_atom_state_change);
 	int status;
 	int fd;
+	size_t i;
 
 	if (!ctx || !jm_fd_arg)
 		return -EINVAL;
@@ -753,6 +753,10 @@ int kbase_kinstr_jm_get_fd(struct kbase_kinstr_jm *const ctx, union kbase_kinstr
 	if (!is_power_of_2(in->count))
 		return -EINVAL;
 
+	for (i = 0; i < sizeof(in->padding); ++i)
+		if (in->padding[i])
+			return -EINVAL;
+
 	status = reader_init(&reader, ctx, in->count);
 	if (status < 0)
 		return status;
@@ -827,14 +831,14 @@ void kbasep_kinstr_jm_atom_hw_submit(struct kbase_jd_atom *const katom)
 {
 	struct kbase_context *const kctx = katom->kctx;
 	struct kbase_device *const kbdev = kctx->kbdev;
-	const unsigned int slot = katom->slot_nr;
+	const int slot = katom->slot_nr;
 	struct kbase_jd_atom *const submitted = kbase_gpu_inspect(kbdev, slot, 0);
 
 	BUILD_BUG_ON(SLOT_RB_SIZE != 2);
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
-	if (WARN_ON(slot >= GPU_MAX_JOB_SLOTS))
+	if (WARN_ON(slot < 0 || slot >= GPU_MAX_JOB_SLOTS))
 		return;
 	if (WARN_ON(!submitted))
 		return;
@@ -847,7 +851,7 @@ void kbasep_kinstr_jm_atom_hw_release(struct kbase_jd_atom *const katom)
 {
 	struct kbase_context *const kctx = katom->kctx;
 	struct kbase_device *const kbdev = kctx->kbdev;
-	const unsigned int slot = katom->slot_nr;
+	const int slot = katom->slot_nr;
 	struct kbase_jd_atom *const submitted = kbase_gpu_inspect(kbdev, slot, 0);
 	struct kbase_jd_atom *const queued = kbase_gpu_inspect(kbdev, slot, 1);
 
@@ -855,7 +859,7 @@ void kbasep_kinstr_jm_atom_hw_release(struct kbase_jd_atom *const katom)
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
-	if (WARN_ON(slot >= GPU_MAX_JOB_SLOTS))
+	if (WARN_ON(slot < 0 || slot >= GPU_MAX_JOB_SLOTS))
 		return;
 	if (WARN_ON(!submitted))
 		return;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_kinstr_prfcnt.c b/drivers/gpu/arm/bifrost/mali_kbase_kinstr_prfcnt.c
index ee7ea1487cf4..a934948f11ee 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_kinstr_prfcnt.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_kinstr_prfcnt.c
@@ -114,7 +114,7 @@ struct kbase_kinstr_prfcnt_sample_array {
  * @scope:        Scope of performance counters to capture.
  * @buffer_count: Number of buffers used to store samples.
  * @period_ns:    Sampling period, in nanoseconds, or 0 if manual mode.
- * @enable_cm:    Requested counter selection bitmasks.
+ * @phys_em:      Enable map used by the GPU.
  */
 struct kbase_kinstr_prfcnt_client_config {
 	u8 prfcnt_mode;
@@ -122,7 +122,7 @@ struct kbase_kinstr_prfcnt_client_config {
 	u8 scope;
 	u16 buffer_count;
 	u64 period_ns;
-	struct kbase_hwcnt_enable_cm enable_cm;
+	struct kbase_hwcnt_physical_enable_map phys_em;
 };
 
 /**
@@ -502,7 +502,8 @@ int kbasep_kinstr_prfcnt_set_block_meta_items(struct kbase_hwcnt_enable_map *ena
 		return -EINVAL;
 
 	metadata = dst->metadata;
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		u8 *dst_blk;
 		blk_stt_t hw_blk_stt;
 
@@ -555,7 +556,7 @@ static void kbasep_kinstr_prfcnt_set_sample_metadata(struct kbase_kinstr_prfcnt_
 	/* PRFCNT_SAMPLE_META_TYPE_SAMPLE must be the first item */
 	ptr_md->hdr.item_type = PRFCNT_SAMPLE_META_TYPE_SAMPLE;
 	ptr_md->hdr.item_version = PRFCNT_READER_API_VERSION;
-	ptr_md->u.sample_md.seq = (u64)atomic_read(&cli->write_idx);
+	ptr_md->u.sample_md.seq = atomic_read(&cli->write_idx);
 	ptr_md->u.sample_md.flags = cli->sample_flags;
 
 	/* Place the PRFCNT_SAMPLE_META_TYPE_CLOCK optionally as the 2nd */
@@ -591,8 +592,8 @@ static void kbasep_kinstr_prfcnt_set_sample_metadata(struct kbase_kinstr_prfcnt_
  * @ts_end_ns:    Time stamp for the end point of the sample dump.
  */
 static void kbasep_kinstr_prfcnt_client_output_sample(struct kbase_kinstr_prfcnt_client *cli,
-						      int buf_idx, u64 user_data, u64 ts_start_ns,
-						      u64 ts_end_ns)
+						      unsigned int buf_idx, u64 user_data,
+						      u64 ts_start_ns, u64 ts_end_ns)
 {
 	struct kbase_hwcnt_dump_buffer *dump_buf;
 	struct kbase_hwcnt_dump_buffer *tmp_buf = &cli->tmp_buf;
@@ -639,8 +640,8 @@ static int kbasep_kinstr_prfcnt_client_dump(struct kbase_kinstr_prfcnt_client *c
 	int ret;
 	u64 ts_start_ns = 0;
 	u64 ts_end_ns = 0;
-	int write_idx;
-	int read_idx;
+	unsigned int write_idx;
+	unsigned int read_idx;
 	size_t available_samples_count;
 
 	WARN_ON(!cli);
@@ -657,7 +658,7 @@ static int kbasep_kinstr_prfcnt_client_dump(struct kbase_kinstr_prfcnt_client *c
 	WARN_ON(available_samples_count < 1);
 	/* Reserve one slot to store the implicit sample taken on CMD_STOP */
 	available_samples_count -= 1;
-	if ((size_t)(write_idx - read_idx) == available_samples_count) {
+	if (write_idx - read_idx == available_samples_count) {
 		/* For periodic sampling, the current active dump
 		 * will be accumulated in the next sample, when
 		 * a buffer becomes available.
@@ -696,8 +697,8 @@ static int kbasep_kinstr_prfcnt_client_start(struct kbase_kinstr_prfcnt_client *
 {
 	int ret;
 	u64 tm_start, tm_end;
-	int write_idx;
-	int read_idx;
+	unsigned int write_idx;
+	unsigned int read_idx;
 	size_t available_samples_count;
 
 	WARN_ON(!cli);
@@ -713,11 +714,11 @@ static int kbasep_kinstr_prfcnt_client_start(struct kbase_kinstr_prfcnt_client *
 	/* Check whether there is space to store atleast an implicit sample
 	 * corresponding to CMD_STOP.
 	 */
-	available_samples_count = cli->sample_count - (size_t)(write_idx - read_idx);
+	available_samples_count = cli->sample_count - (write_idx - read_idx);
 	if (!available_samples_count)
 		return -EBUSY;
 
-	kbase_hwcnt_gpu_enable_map_from_cm(&cli->enable_map, &cli->config.enable_cm);
+	kbase_hwcnt_gpu_enable_map_from_physical(&cli->enable_map, &cli->config.phys_em);
 
 	/* Enable all the available clk_enable_map. */
 	cli->enable_map.clk_enable_map = (1ull << cli->kinstr_ctx->metadata->clk_cnt) - 1;
@@ -746,9 +747,10 @@ static int kbasep_kinstr_prfcnt_client_stop(struct kbase_kinstr_prfcnt_client *c
 	int ret;
 	u64 tm_start = 0;
 	u64 tm_end = 0;
+	struct kbase_hwcnt_physical_enable_map phys_em = { 0 };
 	size_t available_samples_count;
-	int write_idx;
-	int read_idx;
+	unsigned int write_idx;
+	unsigned int read_idx;
 
 	WARN_ON(!cli);
 	lockdep_assert_held(&cli->cmd_sync_lock);
@@ -759,14 +761,13 @@ static int kbasep_kinstr_prfcnt_client_stop(struct kbase_kinstr_prfcnt_client *c
 
 	mutex_lock(&cli->kinstr_ctx->lock);
 
-	/* Set enable map to 0 */
-	kbase_hwcnt_gpu_enable_map_from_cm(&cli->enable_map, &(struct kbase_hwcnt_enable_cm){});
+	kbase_hwcnt_gpu_enable_map_from_physical(&cli->enable_map, &phys_em);
 
 	/* Check whether one has the buffer to hold the last sample */
 	write_idx = atomic_read(&cli->write_idx);
 	read_idx = atomic_read(&cli->read_idx);
 
-	available_samples_count = cli->sample_count - (size_t)(write_idx - read_idx);
+	available_samples_count = cli->sample_count - (write_idx - read_idx);
 
 	ret = kbase_hwcnt_virtualizer_client_set_counters(cli->hvcli, &cli->enable_map, &tm_start,
 							  &tm_end, &cli->tmp_buf);
@@ -778,7 +779,7 @@ static int kbasep_kinstr_prfcnt_client_stop(struct kbase_kinstr_prfcnt_client *c
 	if (!WARN_ON(!available_samples_count)) {
 		write_idx %= cli->sample_arr.sample_count;
 		/* Handle the last stop sample */
-		kbase_hwcnt_gpu_enable_map_from_cm(&cli->enable_map, &cli->config.enable_cm);
+		kbase_hwcnt_gpu_enable_map_from_physical(&cli->enable_map, &cli->config.phys_em);
 		/* As this is a stop sample, mark it as MANUAL */
 		kbasep_kinstr_prfcnt_client_output_sample(cli, write_idx, user_data, tm_start,
 							  tm_end);
@@ -821,7 +822,7 @@ static int kbasep_kinstr_prfcnt_client_sync_dump(struct kbase_kinstr_prfcnt_clie
 
 static int kbasep_kinstr_prfcnt_client_discard(struct kbase_kinstr_prfcnt_client *cli)
 {
-	int write_idx;
+	unsigned int write_idx;
 
 	WARN_ON(!cli);
 	lockdep_assert_held(&cli->cmd_sync_lock);
@@ -881,9 +882,9 @@ int kbasep_kinstr_prfcnt_cmd(struct kbase_kinstr_prfcnt_client *cli,
 static int kbasep_kinstr_prfcnt_get_sample(struct kbase_kinstr_prfcnt_client *cli,
 					   struct prfcnt_sample_access *sample_access)
 {
-	int write_idx;
-	int read_idx;
-	int fetch_idx;
+	unsigned int write_idx;
+	unsigned int read_idx;
+	unsigned int fetch_idx;
 	u64 sample_offset_bytes;
 	struct prfcnt_metadata *sample_meta;
 	int err = 0;
@@ -916,7 +917,7 @@ static int kbasep_kinstr_prfcnt_get_sample(struct kbase_kinstr_prfcnt_client *cl
 
 	read_idx %= cli->sample_arr.sample_count;
 	sample_meta = cli->sample_arr.samples[read_idx].sample_meta;
-	sample_offset_bytes = (u64)((u8 *)sample_meta - cli->sample_arr.user_buf);
+	sample_offset_bytes = (u8 *)sample_meta - cli->sample_arr.user_buf;
 
 	sample_access->sequence = sample_meta->u.sample_md.seq;
 	sample_access->sample_offset_bytes = sample_offset_bytes;
@@ -932,9 +933,9 @@ static int kbasep_kinstr_prfcnt_get_sample(struct kbase_kinstr_prfcnt_client *cl
 static int kbasep_kinstr_prfcnt_put_sample(struct kbase_kinstr_prfcnt_client *cli,
 					   struct prfcnt_sample_access *sample_access)
 {
-	int write_idx;
-	int read_idx;
-	int fetch_idx;
+	unsigned int write_idx;
+	unsigned int read_idx;
+	unsigned int fetch_idx;
 	u64 sample_offset_bytes;
 	int err = 0;
 
@@ -948,8 +949,8 @@ static int kbasep_kinstr_prfcnt_put_sample(struct kbase_kinstr_prfcnt_client *cl
 	}
 
 	read_idx %= cli->sample_arr.sample_count;
-	sample_offset_bytes = (u64)((u8 *)cli->sample_arr.samples[read_idx].sample_meta -
-				    cli->sample_arr.user_buf);
+	sample_offset_bytes =
+		(u8 *)cli->sample_arr.samples[read_idx].sample_meta - cli->sample_arr.user_buf;
 
 	if (sample_access->sample_offset_bytes != sample_offset_bytes) {
 		err = -EINVAL;
@@ -1147,7 +1148,8 @@ size_t kbasep_kinstr_prfcnt_get_sample_md_count(const struct kbase_hwcnt_metadat
 	if (!metadata)
 		return 0;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		/* Skip unavailable, non-enabled or reserved blocks */
 		if (kbase_kinstr_is_block_type_reserved(metadata, blk) ||
 		    !kbase_hwcnt_metadata_block_instance_avail(metadata, blk, blk_inst) ||
@@ -1457,7 +1459,8 @@ static bool prfcnt_block_supported(const struct kbase_hwcnt_metadata *metadata,
 {
 	size_t blk, blk_inst;
 
-	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst) {
+	kbase_hwcnt_metadata_for_each_block(metadata, blk, blk_inst)
+	{
 		const enum kbase_hwcnt_gpu_v5_block_type blk_type =
 			kbase_hwcnt_metadata_block_type(metadata, blk);
 		const enum prfcnt_block_type prfcnt_block_type =
@@ -1469,15 +1472,15 @@ static bool prfcnt_block_supported(const struct kbase_hwcnt_metadata *metadata,
 	return false;
 }
 
-static void kbasep_kinstr_prfcnt_block_enable_req_to_cfg(uint64_t *cfg_cm,
-							 const uint64_t *enable_mask)
+static void kbasep_kinstr_prfcnt_block_enable_to_physical(uint32_t *phys_em,
+							  const uint64_t *enable_mask)
 {
-	/* Adding a baseline value '0xF' to cfg_cm on any type that has been requested. This
-	 * ensures the block states will always be reflected in the client's
+	/* Adding a baseline phys_em value '1' on any type that has been requested. This
+	 * ensures the phys_em block states will always be reflected in the client's
 	 * sample outputs, even when the client provided an all zero value mask.
 	 */
-	cfg_cm[0] |= (0xF | enable_mask[0]);
-	cfg_cm[1] |= enable_mask[1];
+	*phys_em |=
+		(1 | kbase_hwcnt_backend_gpu_block_map_to_physical(enable_mask[0], enable_mask[1]));
 }
 
 /**
@@ -1555,28 +1558,28 @@ kbasep_kinstr_prfcnt_parse_request_enable(const struct prfcnt_request_enable *re
 	 */
 	switch (req_enable->block_type) {
 	case PRFCNT_BLOCK_TYPE_FE:
-		kbasep_kinstr_prfcnt_block_enable_req_to_cfg(config->enable_cm.fe_bm,
-							     req_enable->enable_mask);
+		kbasep_kinstr_prfcnt_block_enable_to_physical(&config->phys_em.fe_bm,
+							      req_enable->enable_mask);
 		break;
 	case PRFCNT_BLOCK_TYPE_TILER:
-		kbasep_kinstr_prfcnt_block_enable_req_to_cfg(config->enable_cm.tiler_bm,
-							     req_enable->enable_mask);
+		kbasep_kinstr_prfcnt_block_enable_to_physical(&config->phys_em.tiler_bm,
+							      req_enable->enable_mask);
 		break;
 	case PRFCNT_BLOCK_TYPE_MEMORY:
-		kbasep_kinstr_prfcnt_block_enable_req_to_cfg(config->enable_cm.mmu_l2_bm,
-							     req_enable->enable_mask);
+		kbasep_kinstr_prfcnt_block_enable_to_physical(&config->phys_em.mmu_l2_bm,
+							      req_enable->enable_mask);
 		break;
 	case PRFCNT_BLOCK_TYPE_SHADER_CORE:
-		kbasep_kinstr_prfcnt_block_enable_req_to_cfg(config->enable_cm.shader_bm,
-							     req_enable->enable_mask);
+		kbasep_kinstr_prfcnt_block_enable_to_physical(&config->phys_em.shader_bm,
+							      req_enable->enable_mask);
 		break;
 	case PRFCNT_BLOCK_TYPE_FW:
-		kbasep_kinstr_prfcnt_block_enable_req_to_cfg(config->enable_cm.fw_bm,
-							     req_enable->enable_mask);
+		kbasep_kinstr_prfcnt_block_enable_to_physical(&config->phys_em.fw_bm,
+							      req_enable->enable_mask);
 		break;
 	case PRFCNT_BLOCK_TYPE_CSG:
-		kbasep_kinstr_prfcnt_block_enable_req_to_cfg(config->enable_cm.csg_bm,
-							     req_enable->enable_mask);
+		kbasep_kinstr_prfcnt_block_enable_to_physical(&config->phys_em.csg_bm,
+							      req_enable->enable_mask);
 		break;
 	default:
 		err = -EINVAL;
@@ -1775,8 +1778,8 @@ int kbasep_kinstr_prfcnt_client_create(struct kbase_kinstr_prfcnt_context *kinst
 			break;
 
 		case KINSTR_PRFCNT_DUMP_BUFFER:
-			kbase_hwcnt_gpu_enable_map_from_cm(&cli->enable_map,
-							   &cli->config.enable_cm);
+			kbase_hwcnt_gpu_enable_map_from_physical(&cli->enable_map,
+								 &cli->config.phys_em);
 
 			cli->sample_count = cli->config.buffer_count;
 			cli->sample_size =
@@ -1803,8 +1806,8 @@ int kbasep_kinstr_prfcnt_client_create(struct kbase_kinstr_prfcnt_context *kinst
 			/* Set enable map to be 0 to prevent virtualizer to init and kick the
 			 * backend to count.
 			 */
-			kbase_hwcnt_gpu_enable_map_from_cm(&cli->enable_map,
-							   &(struct kbase_hwcnt_enable_cm){});
+			kbase_hwcnt_gpu_enable_map_from_physical(
+				&cli->enable_map, &(struct kbase_hwcnt_physical_enable_map){ 0 });
 
 			err = kbase_hwcnt_virtualizer_client_create(kinstr_ctx->hvirt,
 								    &cli->enable_map, &cli->hvcli);
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_linux.h b/drivers/gpu/arm/bifrost/mali_kbase_linux.h
index cb55d4b417c4..9195be347e2b 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_linux.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_linux.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -35,13 +35,8 @@
 
 #if IS_ENABLED(MALI_KERNEL_TEST_API)
 #define KBASE_EXPORT_TEST_API(func) EXPORT_SYMBOL(func)
-/* Note: due to the 2-layer macro translation, using the NULL _etype does not
- * compile, and one workaround is to use ERRNO_NULL instead.
- */
-#define KBASE_ALLOW_ERROR_INJECTION_TEST_API(func, etype) ALLOW_ERROR_INJECTION(func, etype)
 #else
 #define KBASE_EXPORT_TEST_API(func)
-#define KBASE_ALLOW_ERROR_INJECTION_TEST_API(func, etype)
 #endif
 
 #define KBASE_EXPORT_SYMBOL(func) EXPORT_SYMBOL(func)
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem.c b/drivers/gpu/arm/bifrost/mali_kbase_mem.c
index 1436d8290ebc..329de89812c0 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -35,7 +35,6 @@
 #include <mali_kbase_config.h>
 #include <mali_kbase.h>
 #include <mali_kbase_reg_track.h>
-#include <mali_kbase_caps.h>
 #include <hw_access/mali_kbase_hw_access_regmap.h>
 #include <mali_kbase_cache_policy.h>
 #include <mali_kbase_hw.h>
@@ -43,20 +42,13 @@
 #include <mali_kbase_native_mgm.h>
 #include <mali_kbase_mem_pool_group.h>
 #include <mmu/mali_kbase_mmu.h>
+#include <mali_kbase_config_defaults.h>
 #include <mali_kbase_trace_gpu_mem.h>
 #include <linux/version_compat_defs.h>
 
-/* Static key used to determine if large pages are enabled or not */
-static DEFINE_STATIC_KEY_FALSE(large_pages_static_key);
-
 #define VA_REGION_SLAB_NAME_PREFIX "va-region-slab-"
 #define VA_REGION_SLAB_NAME_SIZE (DEVNAME_SIZE + sizeof(VA_REGION_SLAB_NAME_PREFIX) + 1)
 
-#if GPU_PAGES_PER_CPU_PAGE > 1
-#define PAGE_METADATA_SLAB_NAME_PREFIX "page-metadata-slab-"
-#define PAGE_METADATA_SLAB_NAME_SIZE (DEVNAME_SIZE + sizeof(PAGE_METADATA_SLAB_NAME_PREFIX) + 1)
-#endif
-
 #if MALI_JIT_PRESSURE_LIMIT_BASE
 
 /*
@@ -130,7 +122,7 @@ static int get_large_page_conf(char *buffer, const struct kernel_param *kp)
 		break;
 	}
 
-	return scnprintf(buffer, PAGE_SIZE, "%s\n", out);
+	return sprintf(buffer, "%s\n", out);
 }
 
 static const struct kernel_param_ops large_page_config_params = {
@@ -151,21 +143,21 @@ MODULE_PARM_DESC(large_page_conf, "User override for large page usage on support
 static void kbasep_mem_page_size_init(struct kbase_device *kbdev)
 {
 	if (!IS_ENABLED(CONFIG_LARGE_PAGE_SUPPORT)) {
+		kbdev->pagesize_2mb = false;
 		dev_info(kbdev->dev, "Large page support was disabled at compile-time!");
 		return;
 	}
 
 	switch (large_page_conf) {
 	case LARGE_PAGE_AUTO: {
-		if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_LARGE_PAGE_ALLOC))
-			static_branch_inc(&large_pages_static_key);
+		kbdev->pagesize_2mb = kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_LARGE_PAGE_ALLOC);
 		dev_info(kbdev->dev, "Large page allocation set to %s after hardware feature check",
-			 static_branch_unlikely(&large_pages_static_key) ? "true" : "false");
+			 kbdev->pagesize_2mb ? "true" : "false");
 		break;
 	}
 	case LARGE_PAGE_ON: {
-		static_branch_inc(&large_pages_static_key);
-		if (!kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_LARGE_PAGE_ALLOC))
+		kbdev->pagesize_2mb = true;
+		if (!kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_LARGE_PAGE_ALLOC))
 			dev_warn(kbdev->dev,
 				 "Enabling large page allocations on unsupporting GPU!");
 		else
@@ -173,10 +165,12 @@ static void kbasep_mem_page_size_init(struct kbase_device *kbdev)
 		break;
 	}
 	case LARGE_PAGE_OFF: {
+		kbdev->pagesize_2mb = false;
 		dev_info(kbdev->dev, "Large page allocation override: turned off\n");
 		break;
 	}
 	default: {
+		kbdev->pagesize_2mb = false;
 		dev_info(kbdev->dev, "Invalid large page override, turning off large pages\n");
 		break;
 	}
@@ -186,31 +180,25 @@ static void kbasep_mem_page_size_init(struct kbase_device *kbdev)
 	 * so that userspace could read it to figure out the state of the configuration
 	 * if necessary.
 	 */
-	if (static_branch_unlikely(&large_pages_static_key))
+	if (kbdev->pagesize_2mb)
 		large_page_conf = LARGE_PAGE_ON;
 	else
 		large_page_conf = LARGE_PAGE_OFF;
 }
 
-inline bool kbase_is_large_pages_enabled(void)
-{
-	return static_branch_unlikely(&large_pages_static_key);
-}
-KBASE_EXPORT_TEST_API(kbase_is_large_pages_enabled);
-
 int kbase_mem_init(struct kbase_device *kbdev)
 {
 	int err = 0;
+	struct kbasep_mem_device *memdev;
 	char va_region_slab_name[VA_REGION_SLAB_NAME_SIZE];
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	char page_metadata_slab_name[PAGE_METADATA_SLAB_NAME_SIZE];
-#endif
 #if IS_ENABLED(CONFIG_OF)
 	struct device_node *mgm_node = NULL;
 #endif
 
 	KBASE_DEBUG_ASSERT(kbdev);
 
+	memdev = &kbdev->memdev;
+
 	kbasep_mem_page_size_init(kbdev);
 
 	scnprintf(va_region_slab_name, VA_REGION_SLAB_NAME_SIZE, VA_REGION_SLAB_NAME_PREFIX "%s",
@@ -224,17 +212,6 @@ int kbase_mem_init(struct kbase_device *kbdev)
 		return -ENOMEM;
 	}
 
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	scnprintf(page_metadata_slab_name, PAGE_METADATA_SLAB_NAME_SIZE,
-		  PAGE_METADATA_SLAB_NAME_PREFIX "%s", kbdev->devname);
-	kbdev->page_metadata_slab = kmem_cache_create(
-		page_metadata_slab_name, sizeof(struct kbase_page_metadata), 0, 0, NULL);
-	if (kbdev->page_metadata_slab == NULL) {
-		dev_err(kbdev->dev, "Failed to create page_metadata_slab");
-		return -ENOMEM;
-	}
-#endif
-
 	kbase_mem_migrate_init(kbdev);
 	kbase_mem_pool_group_config_set_max_size(&kbdev->mem_pool_defaults,
 						 KBASE_MEM_POOL_MAX_SIZE_KCTX);
@@ -244,6 +221,12 @@ int kbase_mem_init(struct kbase_device *kbdev)
 	kbdev->dma_buf_root = RB_ROOT;
 	mutex_init(&kbdev->dma_buf_lock);
 
+#ifdef IR_THRESHOLD
+	atomic_set(&memdev->ir_threshold, IR_THRESHOLD);
+#else
+	atomic_set(&memdev->ir_threshold, DEFAULT_IR_THRESHOLD);
+#endif
+
 	kbdev->mgm_dev = &kbase_native_mgm_dev;
 
 #if IS_ENABLED(CONFIG_OF)
@@ -309,10 +292,6 @@ void kbase_mem_term(struct kbase_device *kbdev)
 
 	kbase_mem_migrate_term(kbdev);
 
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	kmem_cache_destroy(kbdev->page_metadata_slab);
-	kbdev->page_metadata_slab = NULL;
-#endif
 	kmem_cache_destroy(kbdev->va_region_slab);
 	kbdev->va_region_slab = NULL;
 
@@ -333,7 +312,7 @@ int kbase_gpu_mmap(struct kbase_context *kctx, struct kbase_va_region *reg, u64
 	size_t i = 0;
 	unsigned long attr;
 	unsigned long mask = ~KBASE_REG_MEMATTR_MASK;
-	unsigned long gwt_mask = ~0UL;
+	unsigned long gwt_mask = ~0;
 	int group_id;
 	struct kbase_mem_phy_alloc *alloc;
 
@@ -545,20 +524,15 @@ int kbase_gpu_munmap(struct kbase_context *kctx, struct kbase_va_region *reg)
 		switch (alloc->imported.user_buf.state) {
 		case KBASE_USER_BUF_STATE_GPU_MAPPED: {
 			alloc->imported.user_buf.current_mapping_usage_count = 0;
-			kbase_mem_phy_alloc_ref_read(alloc) ?
-				      kbase_user_buf_from_gpu_mapped_to_pinned(kctx, reg) :
-				      kbase_user_buf_from_gpu_mapped_to_empty(kctx, reg);
+			kbase_user_buf_from_gpu_mapped_to_empty(kctx, reg);
 			break;
 		}
 		case KBASE_USER_BUF_STATE_DMA_MAPPED: {
-			kbase_mem_phy_alloc_ref_read(alloc) ?
-				      kbase_user_buf_from_dma_mapped_to_pinned(kctx, reg) :
-				      kbase_user_buf_from_dma_mapped_to_empty(kctx, reg);
+			kbase_user_buf_from_dma_mapped_to_empty(kctx, reg);
 			break;
 		}
 		case KBASE_USER_BUF_STATE_PINNED: {
-			if (!kbase_mem_phy_alloc_ref_read(alloc))
-				kbase_user_buf_from_pinned_to_empty(kctx, reg);
+			kbase_user_buf_from_pinned_to_empty(kctx, reg);
 			break;
 		}
 		case KBASE_USER_BUF_STATE_EMPTY: {
@@ -697,12 +671,10 @@ void kbase_sync_single(struct kbase_context *kctx, struct tagged_addr t_cpu_pa,
 	if (likely(cpu_pa == gpu_pa)) {
 		dma_addr_t dma_addr;
 
-		WARN_ON(!cpu_page);
+		BUG_ON(!cpu_page);
+		BUG_ON(offset + size > PAGE_SIZE);
 
-		if ((size_t)offset + size > PAGE_SIZE)
-			dev_warn(kctx->kbdev->dev, "Size and offset exceed page size");
-
-		dma_addr = kbase_dma_addr_from_tagged(t_cpu_pa) + (dma_addr_t)offset;
+		dma_addr = kbase_dma_addr_from_tagged(t_cpu_pa) + offset;
 
 		if (sync_fn == KBASE_SYNC_TO_CPU)
 			dma_sync_single_for_cpu(kctx->kbdev->dev, dma_addr, size,
@@ -720,7 +692,7 @@ void kbase_sync_single(struct kbase_context *kctx, struct tagged_addr t_cpu_pa,
 			return;
 
 		gpu_page = pfn_to_page(PFN_DOWN(gpu_pa));
-		dma_addr = kbase_dma_addr_from_tagged(t_gpu_pa) + (dma_addr_t)offset;
+		dma_addr = kbase_dma_addr_from_tagged(t_gpu_pa) + offset;
 
 		if (sync_fn == KBASE_SYNC_TO_DEVICE) {
 			src = ((unsigned char *)kbase_kmap(cpu_page)) + offset;
@@ -741,105 +713,19 @@ void kbase_sync_single(struct kbase_context *kctx, struct tagged_addr t_cpu_pa,
 	}
 }
 
-static int kbase_get_sync_scope_params(struct kbase_context *kctx, unsigned long start, size_t size,
-				       u64 *page_off, u64 *page_cnt, u64 *offset)
-{
-	u64 tmp_off;
-	struct kbase_cpu_mapping *map =
-		kbasep_find_enclosing_cpu_mapping(kctx, start, size, &tmp_off);
-
-	if (!map) {
-		dev_dbg(kctx->kbdev->dev, "%s: Can't find CPU mapping 0x%016lX", __func__, start);
-		return -EINVAL;
-	}
-
-	*page_off = tmp_off >> PAGE_SHIFT;
-	tmp_off &= ~PAGE_MASK;
-	*page_cnt = (size + tmp_off + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
-	*offset = tmp_off;
-
-	return 0;
-}
-
-static int kbase_sync_imported_user_buf(struct kbase_context *kctx, struct kbase_va_region *reg,
-					struct basep_syncset *sset, enum kbase_sync_type sync_fn)
-{
-	unsigned long start = (uintptr_t)sset->user_addr;
-	size_t size = (size_t)sset->size;
-	dma_addr_t *dma_addr = reg->gpu_alloc->imported.user_buf.dma_addrs;
-	u64 page_off = 0, page_count = 0, offset = 0;
-	u64 i;
-	size_t sz;
-	int err;
-
-	lockdep_assert_held(&kctx->reg_lock);
-
-	if (sync_fn != KBASE_SYNC_TO_CPU && sync_fn != KBASE_SYNC_TO_DEVICE) {
-		dev_dbg(kctx->kbdev->dev, "%s: Unknown kbase sync_fn type!", __func__);
-		return -EINVAL;
-	}
-
-	/* Early return if the imported user_buffer is not yet mapped to GPU */
-	if (reg->gpu_alloc->imported.user_buf.state != KBASE_USER_BUF_STATE_GPU_MAPPED)
-		return -EINVAL;
-
-	err = kbase_get_sync_scope_params(kctx, start, size, &page_off, &page_count, &offset);
-	if (err)
-		return err;
-
-	/* Check the sync is inside the imported range */
-	if ((page_off >= reg->gpu_alloc->nents) ||
-	    ((page_off + page_count) > reg->gpu_alloc->nents))
-		return -EINVAL;
-
-	dma_addr = reg->gpu_alloc->imported.user_buf.dma_addrs;
-	/* Sync first page */
-	sz = MIN(((size_t)PAGE_SIZE - offset), size);
-	if (sync_fn == KBASE_SYNC_TO_CPU)
-		dma_sync_single_for_cpu(kctx->kbdev->dev, dma_addr[page_off] + offset, sz,
-					DMA_BIDIRECTIONAL);
-	else
-		dma_sync_single_for_device(kctx->kbdev->dev, dma_addr[page_off] + offset, sz,
-					   DMA_BIDIRECTIONAL);
-
-	/* Calculate the size for last page */
-	sz = ((start + size - 1) & ~PAGE_MASK) + 1;
-
-	/* Sync middle pages (if any) */
-	for (i = 1; page_count > 2 && i < page_count - 1; i++) {
-		if (sync_fn == KBASE_SYNC_TO_CPU)
-			dma_sync_single_for_cpu(kctx->kbdev->dev, dma_addr[page_off + i], PAGE_SIZE,
-						DMA_BIDIRECTIONAL);
-		else
-			dma_sync_single_for_device(kctx->kbdev->dev, dma_addr[page_off + i],
-						   PAGE_SIZE, DMA_BIDIRECTIONAL);
-	}
-
-	/* Sync last page (if any) */
-	if (page_count > 1) {
-		i = page_off + page_count - 1;
-		if (sync_fn == KBASE_SYNC_TO_CPU)
-			dma_sync_single_for_cpu(kctx->kbdev->dev, dma_addr[i], sz,
-						DMA_BIDIRECTIONAL);
-		else
-			dma_sync_single_for_device(kctx->kbdev->dev, dma_addr[i], sz,
-						   DMA_BIDIRECTIONAL);
-	}
-
-	return 0;
-}
-
 static int kbase_do_syncset(struct kbase_context *kctx, struct basep_syncset *sset,
 			    enum kbase_sync_type sync_fn)
 {
 	int err = 0;
 	struct kbase_va_region *reg;
+	struct kbase_cpu_mapping *map;
 	unsigned long start;
 	size_t size;
 	struct tagged_addr *cpu_pa;
 	struct tagged_addr *gpu_pa;
-	u64 page_off = 0, page_count = 0, offset = 0;
+	u64 page_off, page_count;
 	u64 i;
+	u64 offset;
 	size_t sz;
 
 	kbase_os_mem_map_lock(kctx);
@@ -862,10 +748,7 @@ static int kbase_do_syncset(struct kbase_context *kctx, struct basep_syncset *ss
 	 * memory may be cached.
 	 */
 	if (kbase_mem_is_imported(reg->gpu_alloc->type)) {
-		if (reg->gpu_alloc->type == KBASE_MEM_TYPE_IMPORTED_USER_BUF)
-			err = kbase_sync_imported_user_buf(kctx, reg, sset, sync_fn);
-		else
-			err = kbase_sync_imported_umm(kctx, reg, sync_fn);
+		err = kbase_mem_do_sync_imported(kctx, reg, sync_fn);
 		goto out_unlock;
 	}
 
@@ -875,10 +758,17 @@ static int kbase_do_syncset(struct kbase_context *kctx, struct basep_syncset *ss
 	start = (uintptr_t)sset->user_addr;
 	size = (size_t)sset->size;
 
-	err = kbase_get_sync_scope_params(kctx, start, size, &page_off, &page_count, &offset);
-	if (err)
+	map = kbasep_find_enclosing_cpu_mapping(kctx, start, size, &offset);
+	if (!map) {
+		dev_warn(kctx->kbdev->dev, "Can't find CPU mapping 0x%016lX for VA 0x%016llX",
+			 start, sset->mem_handle.basep.handle);
+		err = -EINVAL;
 		goto out_unlock;
+	}
 
+	page_off = offset >> PAGE_SHIFT;
+	offset &= ~PAGE_MASK;
+	page_count = (size + offset + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
 	cpu_pa = kbase_get_cpu_phy_pages(reg);
 	gpu_pa = kbase_get_gpu_phy_pages(reg);
 
@@ -887,6 +777,7 @@ static int kbase_do_syncset(struct kbase_context *kctx, struct basep_syncset *ss
 		err = -EINVAL;
 		goto out_unlock;
 	}
+
 	if (page_off >= reg->gpu_alloc->nents) {
 		/* Start of sync range is outside the physically backed region
 		 * so nothing to do
@@ -897,7 +788,7 @@ static int kbase_do_syncset(struct kbase_context *kctx, struct basep_syncset *ss
 	/* Sync first page */
 	sz = MIN(((size_t)PAGE_SIZE - offset), size);
 
-	kbase_sync_single(kctx, cpu_pa[page_off], gpu_pa[page_off], (off_t)offset, sz, sync_fn);
+	kbase_sync_single(kctx, cpu_pa[page_off], gpu_pa[page_off], offset, sz, sync_fn);
 
 	/* Calculate the size for last page */
 	sz = ((start + size - 1) & ~PAGE_MASK) + 1;
@@ -1051,10 +942,10 @@ int kbase_mem_free(struct kbase_context *kctx, u64 gpu_addr)
 			__func__);
 		return -EINVAL;
 	}
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	if (gpu_addr >= BASE_MEM_COOKIE_BASE && gpu_addr < BASE_MEM_FIRST_FREE_ADDRESS) {
-		unsigned int cookie = PFN_DOWN(gpu_addr - BASE_MEM_COOKIE_BASE);
+		int cookie = PFN_DOWN(gpu_addr - BASE_MEM_COOKIE_BASE);
 
 		reg = kctx->pending_regions[cookie];
 		if (!reg) {
@@ -1090,7 +981,7 @@ int kbase_mem_free(struct kbase_context *kctx, u64 gpu_addr)
 	}
 
 out_unlock:
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 	return err;
 }
 
@@ -1235,7 +1126,6 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
 	 * to satisfy the memory allocation request.
 	 */
 	size_t nr_pages_to_account = 0;
-	size_t nr_pages_from_partials = 0;
 
 	if (WARN_ON(alloc->type != KBASE_MEM_TYPE_NATIVE) ||
 	    WARN_ON(alloc->imported.native.kctx == NULL) ||
@@ -1266,15 +1156,15 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
 	/* Check if we have enough pages requested so we can allocate a large
 	 * page (512 * 4KB = 2MB )
 	 */
-	if (kbase_is_large_pages_enabled() && nr_left >= NUM_PAGES_IN_2MB_LARGE_PAGE) {
-		size_t nr_lp = nr_left / NUM_PAGES_IN_2MB_LARGE_PAGE;
+	if (kbdev->pagesize_2mb && nr_left >= NUM_4K_PAGES_IN_2MB_PAGE) {
+		int nr_lp = nr_left / NUM_4K_PAGES_IN_2MB_PAGE;
 
 		res = kbase_mem_pool_alloc_pages(&kctx->mem_pools.large[alloc->group_id],
-						 nr_lp * NUM_PAGES_IN_2MB_LARGE_PAGE, tp, true,
+						 nr_lp * NUM_4K_PAGES_IN_2MB_PAGE, tp, true,
 						 kctx->task);
 
 		if (res > 0) {
-			nr_left -= (size_t)res;
+			nr_left -= res;
 			tp += res;
 			nr_pages_to_account += res;
 		}
@@ -1285,19 +1175,17 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
 			spin_lock(&kctx->mem_partials_lock);
 
 			list_for_each_entry_safe(sa, temp_sa, &kctx->mem_partials, link) {
-				unsigned int pidx = 0;
+				int pidx = 0;
 
 				while (nr_left) {
-					pidx = find_next_zero_bit(
-						sa->sub_pages, NUM_PAGES_IN_2MB_LARGE_PAGE, pidx);
+					pidx = find_next_zero_bit(sa->sub_pages,
+								  NUM_4K_PAGES_IN_2MB_PAGE, pidx);
 					bitmap_set(sa->sub_pages, pidx, 1);
 					*tp++ = as_tagged_tag(page_to_phys(sa->page + pidx),
 							      FROM_PARTIAL);
 					nr_left--;
-					nr_pages_from_partials++;
 
-					if (bitmap_full(sa->sub_pages,
-							NUM_PAGES_IN_2MB_LARGE_PAGE)) {
+					if (bitmap_full(sa->sub_pages, NUM_4K_PAGES_IN_2MB_PAGE)) {
 						/* unlink from partial list when full */
 						list_del_init(&sa->link);
 						break;
@@ -1310,7 +1198,7 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
 		/* only if we actually have a chunk left <512. If more it indicates
 		 * that we couldn't allocate a 2MB above, so no point to retry here.
 		 */
-		if (nr_left > 0 && nr_left < NUM_PAGES_IN_2MB_LARGE_PAGE) {
+		if (nr_left > 0 && nr_left < NUM_4K_PAGES_IN_2MB_PAGE) {
 			/* create a new partial and suballocate the rest from it */
 			struct page *np = NULL;
 
@@ -1341,10 +1229,10 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
 
 				/* store pointers back to the control struct */
 				np->lru.next = (void *)sa;
-				for (p = np; p < np + NUM_PAGES_IN_2MB_LARGE_PAGE; p++)
+				for (p = np; p < np + NUM_4K_PAGES_IN_2MB_PAGE; p++)
 					p->lru.prev = (void *)np;
 				INIT_LIST_HEAD(&sa->link);
-				bitmap_zero(sa->sub_pages, NUM_PAGES_IN_2MB_LARGE_PAGE);
+				bitmap_zero(sa->sub_pages, NUM_4K_PAGES_IN_2MB_PAGE);
 				sa->page = np;
 
 				for (i = 0; i < nr_left; i++)
@@ -1357,7 +1245,7 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
 				 * for the whole of the large page, and not just for the
 				 * sub-pages that have been used.
 				 */
-				nr_pages_to_account += NUM_PAGES_IN_2MB_LARGE_PAGE;
+				nr_pages_to_account += NUM_4K_PAGES_IN_2MB_PAGE;
 
 				/* expose for later use */
 				spin_lock(&kctx->mem_partials_lock);
@@ -1393,7 +1281,7 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
 alloc_failed:
 	/* The first step of error recovery is freeing any allocation that
 	 * might have succeeded. The function can be in this condition only
-	 * in one case: it tried to allocate a combination of 2 MB and small
+	 * in one case: it tried to allocate a combination of 2 MB and 4 kB
 	 * pages but only the former step succeeded. In this case, calculate
 	 * the number of 2 MB pages to release and free them.
 	 */
@@ -1402,18 +1290,11 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
 
 		alloc->nents += nr_pages_to_free;
 		kbase_free_phy_pages_helper(alloc, nr_pages_to_free);
-
-		/* Notice that the sub-pages from "partials" are not subtracted
-		 * from the counter by the free pages helper, because they just go
-		 * back to the "partials" they belong to, therefore they must be
-		 * subtracted from the counter here.
-		 */
-		nr_left += nr_pages_from_partials;
 	}
 
 	/* Undo the preliminary memory accounting that was done early on
-	 * in the function. If only small pages are used: nr_left is equal
-	 * to nr_pages_requested. If a combination of 2 MB and small pages was
+	 * in the function. If only 4 kB pages are used: nr_left is equal
+	 * to nr_pages_requested. If a combination of 2 MB and 4 kB was
 	 * attempted: nr_pages_requested is equal to the sum of nr_left
 	 * and nr_pages_to_free, and the latter has already been freed above.
 	 *
@@ -1425,7 +1306,6 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
 invalid_request:
 	return -ENOMEM;
 }
-KBASE_EXPORT_TEST_API(kbase_alloc_phy_pages_helper);
 
 static size_t free_partial_locked(struct kbase_context *kctx, struct kbase_mem_pool *pool,
 				  struct tagged_addr tp)
@@ -1441,13 +1321,13 @@ static size_t free_partial_locked(struct kbase_context *kctx, struct kbase_mem_p
 	head_page = (struct page *)p->lru.prev;
 	sa = (struct kbase_sub_alloc *)head_page->lru.next;
 	clear_bit(p - head_page, sa->sub_pages);
-	if (bitmap_empty(sa->sub_pages, NUM_PAGES_IN_2MB_LARGE_PAGE)) {
+	if (bitmap_empty(sa->sub_pages, NUM_4K_PAGES_IN_2MB_PAGE)) {
 		list_del(&sa->link);
 		kbase_mem_pool_free_locked(pool, head_page, true);
 		kfree(sa);
-		nr_pages_to_account = NUM_PAGES_IN_2MB_LARGE_PAGE;
-	} else if (bitmap_weight(sa->sub_pages, NUM_PAGES_IN_2MB_LARGE_PAGE) ==
-		   NUM_PAGES_IN_2MB_LARGE_PAGE - 1) {
+		nr_pages_to_account = NUM_4K_PAGES_IN_2MB_PAGE;
+	} else if (bitmap_weight(sa->sub_pages, NUM_4K_PAGES_IN_2MB_PAGE) ==
+		   NUM_4K_PAGES_IN_2MB_PAGE - 1) {
 		/* expose the partial again */
 		list_add(&sa->link, &kctx->mem_partials);
 	}
@@ -1482,7 +1362,7 @@ struct tagged_addr *kbase_alloc_phy_pages_helper_locked(struct kbase_mem_phy_all
 	kctx = alloc->imported.native.kctx;
 	kbdev = kctx->kbdev;
 
-	if (!kbase_is_large_pages_enabled())
+	if (!kbdev->pagesize_2mb)
 		WARN_ON(pool->order);
 
 	if (alloc->reg) {
@@ -1505,14 +1385,13 @@ struct tagged_addr *kbase_alloc_phy_pages_helper_locked(struct kbase_mem_phy_all
 	tp = alloc->pages + alloc->nents;
 	new_pages = tp;
 
-	if (kbase_is_large_pages_enabled() && pool->order) {
-		size_t nr_lp = nr_left / NUM_PAGES_IN_2MB_LARGE_PAGE;
+	if (kbdev->pagesize_2mb && pool->order) {
+		int nr_lp = nr_left / NUM_4K_PAGES_IN_2MB_PAGE;
 
-		res = kbase_mem_pool_alloc_pages_locked(pool, nr_lp * NUM_PAGES_IN_2MB_LARGE_PAGE,
-							tp);
+		res = kbase_mem_pool_alloc_pages_locked(pool, nr_lp * NUM_4K_PAGES_IN_2MB_PAGE, tp);
 
 		if (res > 0) {
-			nr_left -= (size_t)res;
+			nr_left -= res;
 			tp += res;
 			nr_pages_to_account += res;
 		}
@@ -1521,18 +1400,17 @@ struct tagged_addr *kbase_alloc_phy_pages_helper_locked(struct kbase_mem_phy_all
 			struct kbase_sub_alloc *sa, *temp_sa;
 
 			list_for_each_entry_safe(sa, temp_sa, &kctx->mem_partials, link) {
-				unsigned int pidx = 0;
+				int pidx = 0;
 
 				while (nr_left) {
-					pidx = find_next_zero_bit(
-						sa->sub_pages, NUM_PAGES_IN_2MB_LARGE_PAGE, pidx);
+					pidx = find_next_zero_bit(sa->sub_pages,
+								  NUM_4K_PAGES_IN_2MB_PAGE, pidx);
 					bitmap_set(sa->sub_pages, pidx, 1);
 					*tp++ = as_tagged_tag(page_to_phys(sa->page + pidx),
 							      FROM_PARTIAL);
 					nr_left--;
 
-					if (bitmap_full(sa->sub_pages,
-							NUM_PAGES_IN_2MB_LARGE_PAGE)) {
+					if (bitmap_full(sa->sub_pages, NUM_4K_PAGES_IN_2MB_PAGE)) {
 						/* unlink from partial list when
 						 * full
 						 */
@@ -1547,7 +1425,7 @@ struct tagged_addr *kbase_alloc_phy_pages_helper_locked(struct kbase_mem_phy_all
 		 * indicates that we couldn't allocate a 2MB above, so no point
 		 * to retry here.
 		 */
-		if (nr_left > 0 && nr_left < NUM_PAGES_IN_2MB_LARGE_PAGE) {
+		if (nr_left > 0 && nr_left < NUM_4K_PAGES_IN_2MB_PAGE) {
 			/* create a new partial and suballocate the rest from it
 			 */
 			struct page *np = NULL;
@@ -1561,10 +1439,10 @@ struct tagged_addr *kbase_alloc_phy_pages_helper_locked(struct kbase_mem_phy_all
 
 				/* store pointers back to the control struct */
 				np->lru.next = (void *)sa;
-				for (p = np; p < np + NUM_PAGES_IN_2MB_LARGE_PAGE; p++)
+				for (p = np; p < np + NUM_4K_PAGES_IN_2MB_PAGE; p++)
 					p->lru.prev = (void *)np;
 				INIT_LIST_HEAD(&sa->link);
-				bitmap_zero(sa->sub_pages, NUM_PAGES_IN_2MB_LARGE_PAGE);
+				bitmap_zero(sa->sub_pages, NUM_4K_PAGES_IN_2MB_PAGE);
 				sa->page = np;
 
 				for (i = 0; i < nr_left; i++)
@@ -1577,7 +1455,7 @@ struct tagged_addr *kbase_alloc_phy_pages_helper_locked(struct kbase_mem_phy_all
 				 * for the whole of the large page, and not just for the
 				 * sub-pages that have been used.
 				 */
-				nr_pages_to_account += NUM_PAGES_IN_2MB_LARGE_PAGE;
+				nr_pages_to_account += NUM_4K_PAGES_IN_2MB_PAGE;
 
 				/* Indicate to user that we'll free this memory
 				 * later.
@@ -1613,7 +1491,7 @@ struct tagged_addr *kbase_alloc_phy_pages_helper_locked(struct kbase_mem_phy_all
 alloc_failed:
 	/* The first step of error recovery is freeing any allocation that
 	 * might have succeeded. The function can be in this condition only
-	 * in one case: it tried to allocate a combination of 2 MB and small
+	 * in one case: it tried to allocate a combination of 2 MB and 4 kB
 	 * pages but only the former step succeeded. In this case, calculate
 	 * the number of 2 MB pages to release and free them.
 	 */
@@ -1622,15 +1500,16 @@ struct tagged_addr *kbase_alloc_phy_pages_helper_locked(struct kbase_mem_phy_all
 
 		struct tagged_addr *start_free = alloc->pages + alloc->nents;
 
-		if (kbase_is_large_pages_enabled() && pool->order) {
+		if (kbdev->pagesize_2mb && pool->order) {
 			while (nr_pages_to_free) {
 				if (is_huge_head(*start_free)) {
-					kbase_mem_pool_free_pages_locked(
-						pool, NUM_PAGES_IN_2MB_LARGE_PAGE, start_free,
-						false, /* not dirty */
-						true); /* return to pool */
-					nr_pages_to_free -= NUM_PAGES_IN_2MB_LARGE_PAGE;
-					start_free += NUM_PAGES_IN_2MB_LARGE_PAGE;
+					kbase_mem_pool_free_pages_locked(pool,
+									 NUM_4K_PAGES_IN_2MB_PAGE,
+									 start_free,
+									 false, /* not dirty */
+									 true); /* return to pool */
+					nr_pages_to_free -= NUM_4K_PAGES_IN_2MB_PAGE;
+					start_free += NUM_4K_PAGES_IN_2MB_PAGE;
 				} else if (is_partial(*start_free)) {
 					free_partial_locked(kctx, pool, *start_free);
 					nr_pages_to_free--;
@@ -1666,13 +1545,13 @@ static size_t free_partial(struct kbase_context *kctx, int group_id, struct tagg
 	sa = (struct kbase_sub_alloc *)head_page->lru.next;
 	spin_lock(&kctx->mem_partials_lock);
 	clear_bit(p - head_page, sa->sub_pages);
-	if (bitmap_empty(sa->sub_pages, NUM_PAGES_IN_2MB_LARGE_PAGE)) {
+	if (bitmap_empty(sa->sub_pages, NUM_4K_PAGES_IN_2MB_PAGE)) {
 		list_del(&sa->link);
 		kbase_mem_pool_free(&kctx->mem_pools.large[group_id], head_page, true);
 		kfree(sa);
-		nr_pages_to_account = NUM_PAGES_IN_2MB_LARGE_PAGE;
-	} else if (bitmap_weight(sa->sub_pages, NUM_PAGES_IN_2MB_LARGE_PAGE) ==
-		   NUM_PAGES_IN_2MB_LARGE_PAGE - 1) {
+		nr_pages_to_account = NUM_4K_PAGES_IN_2MB_PAGE;
+	} else if (bitmap_weight(sa->sub_pages, NUM_4K_PAGES_IN_2MB_PAGE) ==
+		   NUM_4K_PAGES_IN_2MB_PAGE - 1) {
 		/* expose the partial again */
 		list_add(&sa->link, &kctx->mem_partials);
 	}
@@ -1724,12 +1603,12 @@ int kbase_free_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pag
 			 * it points to
 			 */
 			kbase_mem_pool_free_pages(&kctx->mem_pools.large[alloc->group_id],
-						  NUM_PAGES_IN_2MB_LARGE_PAGE, start_free, syncback,
+						  NUM_4K_PAGES_IN_2MB_PAGE, start_free, syncback,
 						  reclaimed);
-			nr_pages_to_free -= NUM_PAGES_IN_2MB_LARGE_PAGE;
-			start_free += NUM_PAGES_IN_2MB_LARGE_PAGE;
-			freed += NUM_PAGES_IN_2MB_LARGE_PAGE;
-			nr_pages_to_account += NUM_PAGES_IN_2MB_LARGE_PAGE;
+			nr_pages_to_free -= NUM_4K_PAGES_IN_2MB_PAGE;
+			start_free += NUM_4K_PAGES_IN_2MB_PAGE;
+			freed += NUM_4K_PAGES_IN_2MB_PAGE;
+			nr_pages_to_account += NUM_4K_PAGES_IN_2MB_PAGE;
 		} else if (is_partial(*start_free)) {
 			nr_pages_to_account += free_partial(kctx, alloc->group_id, *start_free);
 			nr_pages_to_free--;
@@ -1745,11 +1624,11 @@ int kbase_free_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pag
 				nr_pages_to_free--;
 			}
 			kbase_mem_pool_free_pages(&kctx->mem_pools.small[alloc->group_id],
-						  (size_t)(local_end_free - start_free), start_free,
-						  syncback, reclaimed);
-			freed += (size_t)(local_end_free - start_free);
-			nr_pages_to_account += (size_t)(local_end_free - start_free);
+						  local_end_free - start_free, start_free, syncback,
+						  reclaimed);
+			freed += local_end_free - start_free;
 			start_free += local_end_free - start_free;
+			nr_pages_to_account += freed;
 		}
 	}
 
@@ -1778,7 +1657,6 @@ int kbase_free_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pag
 
 	return 0;
 }
-KBASE_EXPORT_TEST_API(kbase_free_phy_pages_helper);
 
 void kbase_free_phy_pages_helper_locked(struct kbase_mem_phy_alloc *alloc,
 					struct kbase_mem_pool *pool, struct tagged_addr *pages,
@@ -1830,12 +1708,12 @@ void kbase_free_phy_pages_helper_locked(struct kbase_mem_phy_alloc *alloc,
 			 * it points to
 			 */
 			WARN_ON(!pool->order);
-			kbase_mem_pool_free_pages_locked(pool, NUM_PAGES_IN_2MB_LARGE_PAGE,
-							 start_free, syncback, false);
-			nr_pages_to_free -= NUM_PAGES_IN_2MB_LARGE_PAGE;
-			start_free += NUM_PAGES_IN_2MB_LARGE_PAGE;
-			freed += NUM_PAGES_IN_2MB_LARGE_PAGE;
-			nr_pages_to_account += NUM_PAGES_IN_2MB_LARGE_PAGE;
+			kbase_mem_pool_free_pages_locked(pool, NUM_4K_PAGES_IN_2MB_PAGE, start_free,
+							 syncback, false);
+			nr_pages_to_free -= NUM_4K_PAGES_IN_2MB_PAGE;
+			start_free += NUM_4K_PAGES_IN_2MB_PAGE;
+			freed += NUM_4K_PAGES_IN_2MB_PAGE;
+			nr_pages_to_account += NUM_4K_PAGES_IN_2MB_PAGE;
 		} else if (is_partial(*start_free)) {
 			WARN_ON(!pool->order);
 			nr_pages_to_account += free_partial_locked(kctx, pool, *start_free);
@@ -1852,12 +1730,11 @@ void kbase_free_phy_pages_helper_locked(struct kbase_mem_phy_alloc *alloc,
 				local_end_free++;
 				nr_pages_to_free--;
 			}
-			kbase_mem_pool_free_pages_locked(pool,
-							 (size_t)(local_end_free - start_free),
+			kbase_mem_pool_free_pages_locked(pool, local_end_free - start_free,
 							 start_free, syncback, false);
-			freed += (size_t)(local_end_free - start_free);
-			nr_pages_to_account += (size_t)(local_end_free - start_free);
+			freed += local_end_free - start_free;
 			start_free += local_end_free - start_free;
+			nr_pages_to_account += freed;
 		}
 	}
 
@@ -1918,14 +1795,8 @@ void kbase_mem_kref_free(struct kref *kref)
 			WARN_ONCE(alloc->imported.umm.current_mapping_usage_count != 1,
 				  "WARNING: expected exactly 1 mapping, got %d",
 				  alloc->imported.umm.current_mapping_usage_count);
-#if (KERNEL_VERSION(6, 1, 55) <= LINUX_VERSION_CODE)
-			dma_buf_unmap_attachment_unlocked(alloc->imported.umm.dma_attachment,
-							  alloc->imported.umm.sgt,
-							  DMA_BIDIRECTIONAL);
-#else
 			dma_buf_unmap_attachment(alloc->imported.umm.dma_attachment,
 						 alloc->imported.umm.sgt, DMA_BIDIRECTIONAL);
-#endif
 			kbase_remove_dma_buf_usage(alloc->imported.umm.kctx, alloc);
 		}
 		dma_buf_detach(alloc->imported.umm.dma_buf, alloc->imported.umm.dma_attachment);
@@ -2017,18 +1888,16 @@ int kbase_alloc_phy_pages(struct kbase_va_region *reg, size_t vsize, size_t size
 }
 KBASE_EXPORT_TEST_API(kbase_alloc_phy_pages);
 
-void kbase_set_phy_alloc_page_status(struct kbase_context *kctx, struct kbase_mem_phy_alloc *alloc,
+void kbase_set_phy_alloc_page_status(struct kbase_mem_phy_alloc *alloc,
 				     enum kbase_page_status status)
 {
 	u32 i = 0;
 
-	lockdep_assert_held(&kctx->reg_lock);
-
 	for (; i < alloc->nents; i++) {
 		struct tagged_addr phys = alloc->pages[i];
 		struct kbase_page_metadata *page_md = kbase_page_private(as_page(phys));
 
-		/* Skip the small page that is part of a large page, as the large page is
+		/* Skip the 4KB page that is part of a large page, as the large page is
 		 * excluded from the migration process.
 		 */
 		if (is_huge(phys) || is_partial(phys))
@@ -2043,7 +1912,7 @@ void kbase_set_phy_alloc_page_status(struct kbase_context *kctx, struct kbase_me
 	}
 }
 
-bool kbase_check_alloc_flags(struct kbase_context *kctx, unsigned long flags)
+bool kbase_check_alloc_flags(unsigned long flags)
 {
 	/* Only known input flags should be set. */
 	if (flags & ~BASE_MEM_FLAGS_INPUT_MASK)
@@ -2119,36 +1988,6 @@ bool kbase_check_alloc_flags(struct kbase_context *kctx, unsigned long flags)
 		return false;
 #endif
 
-	/* Cannot be set only allocation, only with base_mem_set */
-	if ((flags & BASE_MEM_DONT_NEED) &&
-	    (mali_kbase_supports_reject_alloc_mem_dont_need(kctx->api_version)))
-		return false;
-
-	/* Cannot directly allocate protected memory, it is imported instead */
-	if ((flags & BASE_MEM_PROTECTED) &&
-	    (mali_kbase_supports_reject_alloc_mem_protected_in_unprotected_allocs(
-		    kctx->api_version)))
-		return false;
-
-/* No unused bits are valid for allocations */
-#if MALI_USE_CSF
-	if ((flags & BASE_MEM_UNUSED_BIT_20) &&
-	    (mali_kbase_supports_reject_alloc_mem_unused_bit_20(kctx->api_version)))
-		return false;
-
-	if ((flags & BASE_MEM_UNUSED_BIT_27) &&
-	    (mali_kbase_supports_reject_alloc_mem_unused_bit_27(kctx->api_version)))
-		return false;
-#else /* MALI_USE_CSF */
-	if ((flags & BASE_MEM_UNUSED_BIT_8) &&
-	    (mali_kbase_supports_reject_alloc_mem_unused_bit_8(kctx->api_version)))
-		return false;
-
-	if ((flags & BASE_MEM_UNUSED_BIT_19) &&
-	    (mali_kbase_supports_reject_alloc_mem_unused_bit_19(kctx->api_version)))
-		return false;
-#endif /* MALI_USE_CSF */
-
 	return true;
 }
 
@@ -2197,7 +2036,7 @@ int kbase_check_alloc_sizes(struct kbase_context *kctx, unsigned long flags, u64
 			    u64 commit_pages, u64 large_extension)
 {
 	struct device *dev = kctx->kbdev->dev;
-	u32 gpu_pc_bits = kctx->kbdev->gpu_props.log2_program_counter_size;
+	int gpu_pc_bits = kctx->kbdev->gpu_props.log2_program_counter_size;
 	u64 gpu_pc_pages_max = 1ULL << gpu_pc_bits >> PAGE_SHIFT;
 	struct kbase_va_region test_reg;
 
@@ -2308,30 +2147,16 @@ void kbase_gpu_vm_lock(struct kbase_context *kctx)
 	KBASE_DEBUG_ASSERT(kctx != NULL);
 	mutex_lock(&kctx->reg_lock);
 }
-KBASE_EXPORT_TEST_API(kbase_gpu_vm_lock);
 
-void kbase_gpu_vm_lock_with_pmode_sync(struct kbase_context *kctx)
-{
-#if MALI_USE_CSF
-	down_read(&kctx->kbdev->csf.mmu_sync_sem);
-#endif
-	kbase_gpu_vm_lock(kctx);
-}
+KBASE_EXPORT_TEST_API(kbase_gpu_vm_lock);
 
 void kbase_gpu_vm_unlock(struct kbase_context *kctx)
 {
 	KBASE_DEBUG_ASSERT(kctx != NULL);
 	mutex_unlock(&kctx->reg_lock);
 }
-KBASE_EXPORT_TEST_API(kbase_gpu_vm_unlock);
 
-void kbase_gpu_vm_unlock_with_pmode_sync(struct kbase_context *kctx)
-{
-	kbase_gpu_vm_unlock(kctx);
-#if MALI_USE_CSF
-	up_read(&kctx->kbdev->csf.mmu_sync_sem);
-#endif
-}
+KBASE_EXPORT_TEST_API(kbase_gpu_vm_unlock);
 
 #if IS_ENABLED(CONFIG_DEBUG_FS)
 struct kbase_jit_debugfs_data {
@@ -2385,8 +2210,8 @@ static ssize_t kbase_jit_debugfs_common_read(struct file *file, char __user *buf
 			goto out_unlock;
 		}
 
-		size = (size_t)scnprintf(data->buffer, sizeof(data->buffer), "%llu,%llu,%llu\n",
-					 data->active_value, data->pool_value, data->destroy_value);
+		size = scnprintf(data->buffer, sizeof(data->buffer), "%llu,%llu,%llu\n",
+				 data->active_value, data->pool_value, data->destroy_value);
 	}
 
 	ret = simple_read_from_buffer(buf, len, ppos, data->buffer, size);
@@ -2631,7 +2456,7 @@ static void kbase_jit_destroy_worker(struct work_struct *work)
 		 * by implementing "free on putting the last reference",
 		 * but only for JIT regions.
 		 */
-		WARN_ON(atomic64_read(&reg->no_user_free_count) > 1);
+		WARN_ON(atomic_read(&reg->no_user_free_count) > 1);
 		kbase_va_region_no_user_free_dec(reg);
 		kbase_mem_free_region(kctx, reg);
 		kbase_gpu_vm_unlock(kctx);
@@ -2870,15 +2695,18 @@ static int kbase_jit_grow(struct kbase_context *kctx, const struct base_jit_allo
 	if (reg->gpu_alloc->nents >= info->commit_pages)
 		goto done;
 
+	/* Grow the backing */
+	old_size = reg->gpu_alloc->nents;
+
 	/* Allocate some more pages */
 	delta = info->commit_pages - reg->gpu_alloc->nents;
 	pages_required = delta;
 
-	if (kbase_is_large_pages_enabled() && pages_required >= NUM_PAGES_IN_2MB_LARGE_PAGE) {
+	if (kctx->kbdev->pagesize_2mb && pages_required >= NUM_4K_PAGES_IN_2MB_PAGE) {
 		pool = &kctx->mem_pools.large[kctx->jit_group_id];
 		/* Round up to number of 2 MB pages required */
-		pages_required += (NUM_PAGES_IN_2MB_LARGE_PAGE - 1);
-		pages_required /= NUM_PAGES_IN_2MB_LARGE_PAGE;
+		pages_required += (NUM_4K_PAGES_IN_2MB_PAGE - 1);
+		pages_required /= NUM_4K_PAGES_IN_2MB_PAGE;
 	} else {
 		pool = &kctx->mem_pools.small[kctx->jit_group_id];
 	}
@@ -2895,7 +2723,7 @@ static int kbase_jit_grow(struct kbase_context *kctx, const struct base_jit_allo
 	 * between the grow and allocation.
 	 */
 	while (kbase_mem_pool_size(pool) < pages_required) {
-		size_t pool_delta = pages_required - kbase_mem_pool_size(pool);
+		int pool_delta = pages_required - kbase_mem_pool_size(pool);
 		int ret;
 
 		kbase_mem_pool_unlock(pool);
@@ -2912,17 +2740,6 @@ static int kbase_jit_grow(struct kbase_context *kctx, const struct base_jit_allo
 		kbase_mem_pool_lock(pool);
 	}
 
-	if (reg->gpu_alloc->nents >= info->commit_pages) {
-		kbase_mem_pool_unlock(pool);
-		spin_unlock(&kctx->mem_partials_lock);
-		dev_info(
-			kctx->kbdev->dev,
-			"JIT alloc grown beyond the required number of initially required pages, this grow no longer needed.");
-		goto done;
-	}
-
-	old_size = reg->gpu_alloc->nents;
-	delta = info->commit_pages - old_size;
 	gpu_pages =
 		kbase_alloc_phy_pages_helper_locked(reg->gpu_alloc, pool, delta, &prealloc_sas[0]);
 	if (!gpu_pages) {
@@ -3165,7 +2982,7 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 	if (!jit_allow_allocate(kctx, info, ignore_pressure_limit))
 		return NULL;
 
-	if (kbase_is_large_pages_enabled()) {
+	if (kctx->kbdev->pagesize_2mb) {
 		/* Preallocate memory for the sub-allocation structs */
 		for (i = 0; i != ARRAY_SIZE(prealloc_sas); ++i) {
 			prealloc_sas[i] = kmalloc(sizeof(*prealloc_sas[i]), GFP_KERNEL);
@@ -3174,7 +2991,7 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 		}
 	}
 
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 	mutex_lock(&kctx->jit_evict_lock);
 
 	/*
@@ -3252,7 +3069,7 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 			kbase_jit_done_phys_increase(kctx, needed_pages);
 #endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
-		kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+		kbase_gpu_vm_unlock(kctx);
 
 		if (ret) {
 			/*
@@ -3285,17 +3102,15 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 			if (kbase_is_page_migration_enabled()) {
 				kbase_gpu_vm_lock(kctx);
 				mutex_lock(&kctx->jit_evict_lock);
-				kbase_set_phy_alloc_page_status(kctx, reg->gpu_alloc,
-								ALLOCATED_MAPPED);
+				kbase_set_phy_alloc_page_status(reg->gpu_alloc, ALLOCATED_MAPPED);
 				mutex_unlock(&kctx->jit_evict_lock);
 				kbase_gpu_vm_unlock(kctx);
 			}
 		}
 	} else {
 		/* No suitable JIT allocation was found so create a new one */
-		base_mem_alloc_flags flags = BASE_MEM_PROT_CPU_RD | BASE_MEM_PROT_GPU_RD |
-					     BASE_MEM_PROT_GPU_WR | BASE_MEM_GROW_ON_GPF |
-					     BASE_MEM_COHERENT_LOCAL | BASEP_MEM_NO_USER_FREE;
+		u64 flags = BASE_MEM_PROT_CPU_RD | BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR |
+			    BASE_MEM_GROW_ON_GPF | BASE_MEM_COHERENT_LOCAL | BASEP_MEM_NO_USER_FREE;
 		u64 gpu_addr;
 
 #if !MALI_USE_CSF
@@ -3315,7 +3130,7 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 #endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 		mutex_unlock(&kctx->jit_evict_lock);
-		kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+		kbase_gpu_vm_unlock(kctx);
 
 		reg = kbase_mem_alloc(kctx, info->va_pages, info->commit_pages, info->extension,
 				      &flags, &gpu_addr, mmu_sync_info);
@@ -3355,7 +3170,7 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 	 * flags.
 	 */
 	kbase_gpu_vm_lock(kctx);
-	if (unlikely(atomic64_read(&reg->no_user_free_count) > 1)) {
+	if (unlikely(atomic_read(&reg->no_user_free_count) > 1)) {
 		kbase_gpu_vm_unlock(kctx);
 		dev_err(kctx->kbdev->dev, "JIT region has no_user_free_count > 1!\n");
 
@@ -3392,7 +3207,6 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 
 	return reg;
 }
-KBASE_EXPORT_TEST_API(kbase_jit_allocate);
 
 void kbase_jit_free(struct kbase_context *kctx, struct kbase_va_region *reg)
 {
@@ -3414,13 +3228,13 @@ void kbase_jit_free(struct kbase_context *kctx, struct kbase_va_region *reg)
 		 * commit size
 		 */
 		u64 new_size = MAX(reg->initial_commit,
-				   div_u64(old_pages * (100ULL - kctx->trim_level), 100ULL));
+				   div_u64(old_pages * (100 - kctx->trim_level), 100));
 		u64 delta = old_pages - new_size;
 
 		if (delta) {
-			kbase_gpu_vm_lock_with_pmode_sync(kctx);
+			mutex_lock(&kctx->reg_lock);
 			kbase_mem_shrink(kctx, reg, old_pages - delta);
-			kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+			mutex_unlock(&kctx->reg_lock);
 		}
 	}
 
@@ -3434,30 +3248,13 @@ void kbase_jit_free(struct kbase_context *kctx, struct kbase_va_region *reg)
 
 	trace_jit_stats(kctx, reg->jit_bin_id, UINT_MAX);
 
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
-	if (unlikely(atomic_read(&reg->cpu_alloc->kernel_mappings))) {
-		WARN_ON(atomic64_read(&reg->no_user_free_count) > 1);
-		kbase_va_region_no_user_free_dec(reg);
-		mutex_lock(&kctx->jit_evict_lock);
-		list_del(&reg->jit_node);
-		mutex_unlock(&kctx->jit_evict_lock);
-		kbase_mem_free_region(kctx, reg);
-		kbase_gpu_vm_unlock_with_pmode_sync(kctx);
-		return;
-	}
 	kbase_mem_evictable_mark_reclaim(reg->gpu_alloc);
+
+	kbase_gpu_vm_lock(kctx);
 	reg->flags |= KBASE_REG_DONT_NEED;
 	reg->flags &= ~KBASE_REG_ACTIVE_JIT_ALLOC;
 	kbase_mem_shrink_cpu_mapping(kctx, reg, 0, reg->gpu_alloc->nents);
-
-	/* Inactive JIT regions should be freed by the shrinker and not impacted
-	 * by page migration. Once freed, they will enter into the page migration
-	 * state machine via the mempools.
-	 */
-	if (kbase_is_page_migration_enabled())
-		kbase_set_phy_alloc_page_status(kctx, reg->gpu_alloc, NOT_MOVABLE);
-
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 
 	/*
 	 * Add the allocation to the eviction list and the jit pool, after this
@@ -3472,9 +3269,14 @@ void kbase_jit_free(struct kbase_context *kctx, struct kbase_va_region *reg)
 
 	list_move(&reg->jit_node, &kctx->jit_pool_head);
 
+	/* Inactive JIT regions should be freed by the shrinker and not impacted
+	 * by page migration. Once freed, they will enter into the page migration
+	 * state machine via the mempools.
+	 */
+	if (kbase_is_page_migration_enabled())
+		kbase_set_phy_alloc_page_status(reg->gpu_alloc, NOT_MOVABLE);
 	mutex_unlock(&kctx->jit_evict_lock);
 }
-KBASE_EXPORT_TEST_API(kbase_jit_free);
 
 void kbase_jit_backing_lost(struct kbase_va_region *reg)
 {
@@ -3524,7 +3326,7 @@ bool kbase_jit_evict(struct kbase_context *kctx)
 		 * by implementing "free on putting the last reference",
 		 * but only for JIT regions.
 		 */
-		WARN_ON(atomic64_read(&reg->no_user_free_count) > 1);
+		WARN_ON(atomic_read(&reg->no_user_free_count) > 1);
 		kbase_va_region_no_user_free_dec(reg);
 		kbase_mem_free_region(kctx, reg);
 	}
@@ -3537,7 +3339,8 @@ void kbase_jit_term(struct kbase_context *kctx)
 	struct kbase_va_region *walker;
 
 	/* Free all allocations for this context */
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+
+	kbase_gpu_vm_lock(kctx);
 	mutex_lock(&kctx->jit_evict_lock);
 	/* Free all allocations from the pool */
 	while (!list_empty(&kctx->jit_pool_head)) {
@@ -3551,7 +3354,7 @@ void kbase_jit_term(struct kbase_context *kctx)
 		 * by implementing "free on putting the last reference",
 		 * but only for JIT regions.
 		 */
-		WARN_ON(atomic64_read(&walker->no_user_free_count) > 1);
+		WARN_ON(atomic_read(&walker->no_user_free_count) > 1);
 		kbase_va_region_no_user_free_dec(walker);
 		kbase_mem_free_region(kctx, walker);
 		mutex_lock(&kctx->jit_evict_lock);
@@ -3569,7 +3372,7 @@ void kbase_jit_term(struct kbase_context *kctx)
 		 * by implementing "free on putting the last reference",
 		 * but only for JIT regions.
 		 */
-		WARN_ON(atomic64_read(&walker->no_user_free_count) > 1);
+		WARN_ON(atomic_read(&walker->no_user_free_count) > 1);
 		kbase_va_region_no_user_free_dec(walker);
 		kbase_mem_free_region(kctx, walker);
 		mutex_lock(&kctx->jit_evict_lock);
@@ -3578,7 +3381,7 @@ void kbase_jit_term(struct kbase_context *kctx)
 	WARN_ON(kctx->jit_phys_pages_to_be_allocated);
 #endif
 	mutex_unlock(&kctx->jit_evict_lock);
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 
 	/*
 	 * Flush the freeing of allocations whose backing has been freed
@@ -3715,7 +3518,7 @@ int kbase_user_buf_pin_pages(struct kbase_context *kctx, struct kbase_va_region
 	 */
 	for (i = 0; i < pinned_pages; i++)
 		pa[i] = as_tagged(page_to_phys(pages[i]));
-	alloc->nents = (size_t)pinned_pages;
+	alloc->nents = pinned_pages;
 
 	return 0;
 }
@@ -3744,7 +3547,7 @@ int kbase_user_buf_dma_map_pages(struct kbase_context *kctx, struct kbase_va_reg
 	struct page **pages = alloc->imported.user_buf.pages;
 	struct device *dev = kctx->kbdev->dev;
 	int write;
-	size_t i, pinned_pages, dma_mapped_pages;
+	long i, pinned_pages, dma_mapped_pages;
 	enum dma_data_direction dma_dir;
 
 	if (WARN_ON(alloc->type != KBASE_MEM_TYPE_IMPORTED_USER_BUF))
@@ -3821,12 +3624,12 @@ int kbase_user_buf_dma_map_pages(struct kbase_context *kctx, struct kbase_va_reg
  */
 static int kbase_user_buf_map(struct kbase_context *kctx, struct kbase_va_region *reg)
 {
-	size_t pinned_pages = 0;
+	long pinned_pages = 0;
 	struct kbase_mem_phy_alloc *alloc;
 	struct page **pages;
 	struct tagged_addr *pa;
-	size_t i;
-	unsigned long gwt_mask = ~0UL;
+	long i;
+	unsigned long gwt_mask = ~0;
 	int ret;
 	/* Calls to this function are inherently asynchronous, with respect to
 	 * MMU operations.
@@ -4096,6 +3899,9 @@ int kbase_map_external_resource(struct kbase_context *kctx, struct kbase_va_regi
 	case KBASE_MEM_TYPE_IMPORTED_USER_BUF: {
 		user_buf_original_state = reg->gpu_alloc->imported.user_buf.state;
 
+		if ((reg->gpu_alloc->imported.user_buf.mm != locked_mm) && (!reg->gpu_alloc->nents))
+			return -EINVAL;
+
 		/* This function is reachable through many code paths, and the imported
 		 * memory handle could be in any of the possible states: consider all
 		 * of them as a valid starting point, and progress through all stages
@@ -4105,31 +3911,19 @@ int kbase_map_external_resource(struct kbase_context *kctx, struct kbase_va_regi
 		 * Error recovery restores the original state and goes no further.
 		 */
 		switch (user_buf_original_state) {
-		case KBASE_USER_BUF_STATE_EMPTY: {
-			if (reg->gpu_alloc->imported.user_buf.mm != locked_mm)
-				return -EINVAL;
-			err = kbase_user_buf_from_empty_to_gpu_mapped(kctx, reg);
-			break;
-		}
-		case KBASE_USER_BUF_STATE_PINNED: {
-			if (!reg->gpu_alloc->nents)
-				return -EINVAL;
-			err = kbase_user_buf_from_pinned_to_gpu_mapped(kctx, reg);
-			break;
-		}
+		case KBASE_USER_BUF_STATE_EMPTY:
+		case KBASE_USER_BUF_STATE_PINNED:
 		case KBASE_USER_BUF_STATE_DMA_MAPPED: {
-			/* If the imported handle has not pinned any physical pages yet:
-			 * this function can only be called within the context of a user
-			 * process, which must be the same process as the one that
-			 * originally created the memory handle.
-			 *
-			 * In all other transitions: make sure that the imported handle
-			 * has already pinned physical pages before proceeding to mapping
-			 * operations.
-			 */
-			if (!reg->gpu_alloc->nents)
-				return -EINVAL;
-			err = kbase_user_buf_from_dma_mapped_to_gpu_mapped(kctx, reg);
+			if (user_buf_original_state == KBASE_USER_BUF_STATE_EMPTY)
+				err = kbase_user_buf_from_empty_to_gpu_mapped(kctx, reg);
+			else if (user_buf_original_state == KBASE_USER_BUF_STATE_PINNED)
+				err = kbase_user_buf_from_pinned_to_gpu_mapped(kctx, reg);
+			else
+				err = kbase_user_buf_from_dma_mapped_to_gpu_mapped(kctx, reg);
+
+			if (err)
+				goto user_buf_to_gpu_mapped_fail;
+
 			break;
 		}
 		case KBASE_USER_BUF_STATE_GPU_MAPPED: {
@@ -4143,8 +3937,6 @@ int kbase_map_external_resource(struct kbase_context *kctx, struct kbase_va_regi
 				reg->gpu_alloc->imported.user_buf.state);
 			return -EINVAL;
 		}
-		if (err)
-			return err;
 
 		/* If the state was valid and the transition is happening, then the handle
 		 * must be in GPU_MAPPED state now and the reference counter of GPU mappings
@@ -4169,6 +3961,9 @@ int kbase_map_external_resource(struct kbase_context *kctx, struct kbase_va_regi
 	kbase_va_region_alloc_get(kctx, reg);
 	kbase_mem_phy_alloc_get(alloc);
 	return 0;
+
+user_buf_to_gpu_mapped_fail:
+	return err;
 }
 
 void kbase_unmap_external_resource(struct kbase_context *kctx, struct kbase_va_region *reg)
@@ -4212,8 +4007,13 @@ void kbase_unmap_external_resource(struct kbase_context *kctx, struct kbase_va_r
 	kbase_va_region_alloc_put(kctx, reg);
 }
 
-struct kbase_ctx_ext_res_meta *
-kbase_sticky_resource_acquire(struct kbase_context *kctx, u64 gpu_addr, struct mm_struct *locked_mm)
+static inline u64 kbasep_get_va_gpu_addr(struct kbase_va_region *reg)
+{
+	return reg->start_pfn << PAGE_SHIFT;
+}
+
+struct kbase_ctx_ext_res_meta *kbase_sticky_resource_acquire(struct kbase_context *kctx,
+							     u64 gpu_addr)
 {
 	struct kbase_ctx_ext_res_meta *meta = NULL;
 	struct kbase_ctx_ext_res_meta *walker;
@@ -4252,7 +4052,7 @@ kbase_sticky_resource_acquire(struct kbase_context *kctx, u64 gpu_addr, struct m
 		/* Map the external resource to the GPU allocation of the region
 		 * and acquire the reference to the VA region
 		 */
-		if (kbase_map_external_resource(kctx, meta->reg, locked_mm))
+		if (kbase_map_external_resource(kctx, meta->reg, NULL))
 			goto fail_map;
 		meta->ref = 1;
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem.h b/drivers/gpu/arm/bifrost/mali_kbase_mem.h
index 880b8525ae37..a8e59b65a73d 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -31,6 +31,7 @@
 #endif
 
 #include <hw_access/mali_kbase_hw_access_regmap.h>
+#include <linux/kref.h>
 #include <uapi/gpu/arm/bifrost/mali_base_kernel.h>
 #include <mali_kbase_hw.h>
 #include "mali_kbase_pm.h"
@@ -39,10 +40,7 @@
 #include "mali_kbase_mem_linux.h"
 #include "mali_kbase_reg_track.h"
 #include "mali_kbase_mem_migrate.h"
-
-#include <linux/version_compat_defs.h>
-#include <linux/sched/mm.h>
-#include <linux/kref.h>
+#include "mali_kbase_refcount_defs.h"
 
 static inline void kbase_process_page_usage_inc(struct kbase_context *kctx, int pages);
 
@@ -104,54 +102,54 @@ static inline void kbase_process_page_usage_inc(struct kbase_context *kctx, int
 
 /* Index of chosen MEMATTR for this region (0..7) */
 #define KBASE_REG_MEMATTR_MASK (7ul << 16)
-#define KBASE_REG_MEMATTR_INDEX(x) (((x) & 7) << 16)
-#define KBASE_REG_MEMATTR_VALUE(x) (((x) & KBASE_REG_MEMATTR_MASK) >> 16)
+#define KBASE_REG_MEMATTR_INDEX(x) (((x)&7) << 16)
+#define KBASE_REG_MEMATTR_VALUE(x) (((x)&KBASE_REG_MEMATTR_MASK) >> 16)
 
 /* AS<n>_MEMATTR values from MMU_MEMATTR_STAGE1: */
 /* Use GPU implementation-defined caching policy. */
-#define KBASE_MEMATTR_IMPL_DEF_CACHE_POLICY                                         \
-	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                  \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_IMPL) | \
-			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(                \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_WRITE_BACK)))
+#define KBASE_MEMATTR_IMPL_DEF_CACHE_POLICY                                      \
+	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(               \
+				      0, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_IMPL) | \
+			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(             \
+				      0, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_WRITE_BACK)))
 /* The attribute set to force all resources to be cached. */
-#define KBASE_MEMATTR_FORCE_TO_CACHE_ALL                                             \
-	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_W_MASK |                   \
-			      AS_MEMATTR_ATTRIBUTE0_ALLOC_R_MASK |                   \
-			      AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                   \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_ALLOC) | \
-			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(                 \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_WRITE_BACK)))
+#define KBASE_MEMATTR_FORCE_TO_CACHE_ALL                                          \
+	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_W_MASK |                \
+			      AS_MEMATTR_ATTRIBUTE0_ALLOC_R_MASK |                \
+			      AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                \
+				      0, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_ALLOC) | \
+			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(              \
+				      0, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_WRITE_BACK)))
 /* Inner write-alloc cache setup, no outer caching */
-#define KBASE_MEMATTR_WRITE_ALLOC                                                    \
-	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_W_MASK |                   \
-			      AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                   \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_ALLOC) | \
-			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(                 \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_WRITE_BACK)))
+#define KBASE_MEMATTR_WRITE_ALLOC                                                 \
+	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_W_MASK |                \
+			      AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                \
+				      0, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_ALLOC) | \
+			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(              \
+				      0, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_WRITE_BACK)))
 /* Set to implementation defined, outer caching */
-#define KBASE_MEMATTR_AARCH64_OUTER_IMPL_DEF                                        \
-	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                  \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_IMPL) | \
-			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(                \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_WRITE_BACK)))
+#define KBASE_MEMATTR_AARCH64_OUTER_IMPL_DEF                                     \
+	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(               \
+				      0, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_IMPL) | \
+			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(             \
+				      0, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_WRITE_BACK)))
 /* Set to write back memory, outer caching */
-#define KBASE_MEMATTR_AARCH64_OUTER_WA                                               \
-	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_W_MASK |                   \
-			      AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                   \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_ALLOC) | \
-			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(                 \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_WRITE_BACK)))
+#define KBASE_MEMATTR_AARCH64_OUTER_WA                                            \
+	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_W_MASK |                \
+			      AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                \
+				      0, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_ALLOC) | \
+			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(              \
+				      0, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_WRITE_BACK)))
 /* Set to inner non-cacheable, outer-non-cacheable
  * Setting defined by the alloc bits is ignored, but set to a valid encoding:
  * - no-alloc on read
  * - no alloc on write
  */
-#define KBASE_MEMATTR_AARCH64_NON_CACHEABLE                                          \
-	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                   \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_ALLOC) | \
-			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(                 \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_NON_CACHEABLE)))
+#define KBASE_MEMATTR_AARCH64_NON_CACHEABLE                                       \
+	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                \
+				      0, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_ALLOC) | \
+			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(              \
+				      0, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_NON_CACHEABLE)))
 
 /* Symbols for default MEMATTR to use
  * Default is - HW implementation defined caching
@@ -177,11 +175,11 @@ static inline void kbase_process_page_usage_inc(struct kbase_context *kctx, int
  * shared, otherwise inner non-cacheable.
  * Outer cacheable if inner or outer shared, otherwise outer non-cacheable.
  */
-#define KBASE_MEMATTR_AARCH64_SHARED                                                \
-	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(                  \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_IMPL) | \
-			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(                \
-				      0ull, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SHARED)))
+#define KBASE_MEMATTR_AARCH64_SHARED                                             \
+	((unsigned long long)(AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_SET(               \
+				      0, AS_MEMATTR_ATTRIBUTE0_ALLOC_SEL_IMPL) | \
+			      AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SET(             \
+				      0, AS_MEMATTR_ATTRIBUTE0_MEMORY_TYPE_SHARED)))
 
 /* Normal memory, shared between MCU and Host */
 #define KBASE_MEMATTR_INDEX_SHARED 6
@@ -456,7 +454,7 @@ enum kbase_page_status {
  * @vmap_count:    Counter of kernel mappings.
  * @group_id:      Memory group ID obtained at the time of page allocation.
  *
- * Each small page will have a reference to this struct in the private field.
+ * Each 4KB page will have a reference to this struct in the private field.
  * This will be used to keep track of information required for Linux page
  * migration functionality as well as address for DMA mapping.
  */
@@ -475,33 +473,11 @@ struct kbase_page_metadata {
 		struct {
 			struct kbase_va_region *reg;
 			struct kbase_mmu_table *mmut;
-			/* GPU virtual page frame number, in GPU_PAGE_SIZE units */
 			u64 vpfn;
 		} mapped;
 		struct {
 			struct kbase_mmu_table *mmut;
-			/* GPU virtual page frame number info is in GPU_PAGE_SIZE units */
 			u64 pgd_vpfn_level;
-#if GPU_PAGES_PER_CPU_PAGE > 1
-			/**
-			 * @pgd_link: Link to the &kbase_mmu_table.pgd_pages_list
-			 */
-			struct list_head pgd_link;
-			/**
-			 * @pgd_page: Back pointer to the PGD page that the metadata is
-			 *            associated with
-			 */
-			struct page *pgd_page;
-			/**
-			 * @allocated_sub_pages: Bitmap representing the allocation status
-			 *                       of sub pages in the @pgd_page
-			 */
-			DECLARE_BITMAP(allocated_sub_pages, GPU_PAGES_PER_CPU_PAGE);
-			/**
-			 * @num_allocated_sub_pages: The number of allocated sub pages in @pgd_page
-			 */
-			s8 num_allocated_sub_pages;
-#endif
 		} pt_mapped;
 		struct {
 			struct kbase_device *kbdev;
@@ -530,7 +506,6 @@ enum kbase_jit_report_flags { KBASE_JIT_REPORT_ON_ALLOC_OR_FREE = (1u << 0) };
 /**
  * kbase_set_phy_alloc_page_status - Set the page migration status of the underlying
  *                                   physical allocation.
- * @kctx:   Pointer to Kbase context.
  * @alloc:  the physical allocation containing the pages whose metadata is going
  *          to be modified
  * @status: the status the pages should end up in
@@ -539,7 +514,7 @@ enum kbase_jit_report_flags { KBASE_JIT_REPORT_ON_ALLOC_OR_FREE = (1u << 0) };
  * proper states are set. Instead, it is only used when we change the allocation
  * to NOT_MOVABLE or from NOT_MOVABLE to ALLOCATED_MAPPED
  */
-void kbase_set_phy_alloc_page_status(struct kbase_context *kctx, struct kbase_mem_phy_alloc *alloc,
+void kbase_set_phy_alloc_page_status(struct kbase_mem_phy_alloc *alloc,
 				     enum kbase_page_status status);
 
 static inline void kbase_mem_phy_alloc_gpu_mapped(struct kbase_mem_phy_alloc *alloc)
@@ -610,11 +585,6 @@ int kbase_mem_init(struct kbase_device *kbdev);
 void kbase_mem_halt(struct kbase_device *kbdev);
 void kbase_mem_term(struct kbase_device *kbdev);
 
-static inline unsigned int kbase_mem_phy_alloc_ref_read(struct kbase_mem_phy_alloc *alloc)
-{
-	return kref_read(&alloc->kref);
-}
-
 static inline struct kbase_mem_phy_alloc *kbase_mem_phy_alloc_get(struct kbase_mem_phy_alloc *alloc)
 {
 	kref_get(&alloc->kref);
@@ -641,6 +611,9 @@ static inline struct kbase_mem_phy_alloc *kbase_mem_phy_alloc_put(struct kbase_m
  * @nr_pages:        The size of the region in pages.
  * @initial_commit:  Initial commit, for aligning the start address and
  *                   correctly growing KBASE_REG_TILER_ALIGN_TOP regions.
+ * @threshold_pages: If non-zero and the amount of memory committed to a region
+ *                   that can grow on page fault exceeds this number of pages
+ *                   then the driver switches to incremental rendering.
  * @flags:           Flags
  * @extension:    Number of pages allocated on page fault.
  * @cpu_alloc: The physical memory we mmap to the CPU when mapping this region.
@@ -677,7 +650,8 @@ struct kbase_va_region {
 	void *user_data;
 	size_t nr_pages;
 	size_t initial_commit;
-	base_mem_alloc_flags flags;
+	size_t threshold_pages;
+	unsigned long flags;
 	size_t extension;
 	struct kbase_mem_phy_alloc *cpu_alloc;
 	struct kbase_mem_phy_alloc *gpu_alloc;
@@ -713,7 +687,7 @@ struct kbase_va_region {
 #endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 	kbase_refcount_t va_refcnt;
-	atomic64_t no_user_free_count;
+	atomic_t no_user_free_count;
 };
 
 /* Special marker for failed JIT allocations that still must be marked as
@@ -811,7 +785,7 @@ static inline struct kbase_va_region *kbase_va_region_alloc_put(struct kbase_con
  */
 static inline bool kbase_va_region_is_no_user_free(struct kbase_va_region *region)
 {
-	return atomic64_read(&region->no_user_free_count) > 0;
+	return atomic_read(&region->no_user_free_count) > 0;
 }
 
 /**
@@ -827,10 +801,10 @@ static inline bool kbase_va_region_is_no_user_free(struct kbase_va_region *regio
 static inline void kbase_va_region_no_user_free_inc(struct kbase_va_region *region)
 {
 	WARN_ON(kbase_is_region_shrinkable(region));
-	WARN_ON(atomic64_read(&region->no_user_free_count) == S64_MAX);
+	WARN_ON(atomic_read(&region->no_user_free_count) == INT_MAX);
 
 	/* non-atomic as kctx->reg_lock is held */
-	atomic64_inc(&region->no_user_free_count);
+	atomic_inc(&region->no_user_free_count);
 }
 
 /**
@@ -842,7 +816,7 @@ static inline void kbase_va_region_no_user_free_dec(struct kbase_va_region *regi
 {
 	WARN_ON(!kbase_va_region_is_no_user_free(region));
 
-	atomic64_dec(&region->no_user_free_count);
+	atomic_dec(&region->no_user_free_count);
 }
 
 /* Common functions */
@@ -931,12 +905,10 @@ static inline struct kbase_mem_phy_alloc *kbase_alloc_create(struct kbase_contex
 	atomic_set(&alloc->gpu_mappings, 0);
 	atomic_set(&alloc->kernel_mappings, 0);
 	alloc->nents = 0;
-	if (type != KBASE_MEM_TYPE_ALIAS) {
-		alloc->pages = (void *)(alloc + 1);
-		/* fill pages with invalid address value */
-		for (i = 0; i < nr_pages; i++)
-			alloc->pages[i] = as_tagged(KBASE_INVALID_PHYSICAL_ADDRESS);
-	}
+	alloc->pages = (void *)(alloc + 1);
+	/* fill pages with invalid address value */
+	for (i = 0; i < nr_pages; i++)
+		alloc->pages[i] = as_tagged(KBASE_INVALID_PHYSICAL_ADDRESS);
 	INIT_LIST_HEAD(&alloc->mappings);
 	alloc->type = type;
 	alloc->group_id = group_id;
@@ -995,14 +967,14 @@ static inline int kbase_reg_prepare_native(struct kbase_va_region *reg, struct k
 #define KBASE_MEM_POOL_MAX_SIZE_KCTX (SZ_64M >> PAGE_SHIFT)
 
 /*
- * The order required for a 2MB page allocation (2^order * PAGE_SIZE = 2MB)
+ * The order required for a 2MB page allocation (2^order * 4KB = 2MB)
  */
-#define KBASE_MEM_POOL_2MB_PAGE_TABLE_ORDER (__builtin_ffs(NUM_PAGES_IN_2MB_LARGE_PAGE) - 1)
+#define KBASE_MEM_POOL_2MB_PAGE_TABLE_ORDER 9
 
 /*
- * The order required for a small page allocation
+ * The order required for a 4KB page allocation
  */
-#define KBASE_MEM_POOL_SMALL_PAGE_TABLE_ORDER 0
+#define KBASE_MEM_POOL_4KB_PAGE_TABLE_ORDER 0
 
 /**
  * kbase_mem_pool_config_set_max_size - Set maximum number of free pages in
@@ -1037,7 +1009,7 @@ kbase_mem_pool_config_get_max_size(const struct kbase_mem_pool_config *const con
  * kbase_mem_pool_init - Create a memory pool for a kbase device
  * @pool:      Memory pool to initialize
  * @config:    Initial configuration for the memory pool
- * @order:     Page order for physical page size (order=0 => small page, order != 0 => 2MB)
+ * @order:     Page order for physical page size (order=0=>4kB, order=9=>2MB)
  * @group_id:  A memory group ID to be passed to a platform-specific
  *             memory group manager, if present.
  *             Valid range is 0..(MEMORY_GROUP_MANAGER_NR_GROUPS-1).
@@ -1136,7 +1108,7 @@ void kbase_mem_pool_free_locked(struct kbase_mem_pool *pool, struct page *p, boo
 /**
  * kbase_mem_pool_alloc_pages - Allocate pages from memory pool
  * @pool:     Memory pool to allocate from
- * @nr_small_pages: Number of pages to allocate
+ * @nr_4k_pages: Number of pages to allocate
  * @pages:    Pointer to array where the physical address of the allocated
  *            pages will be stored.
  * @partial_allowed: If fewer pages allocated is allowed
@@ -1158,14 +1130,14 @@ void kbase_mem_pool_free_locked(struct kbase_mem_pool *pool, struct page *p, boo
  * the kernel OoM killer runs. If the caller must allocate pages while holding
  * this lock, it should use kbase_mem_pool_alloc_pages_locked() instead.
  */
-int kbase_mem_pool_alloc_pages(struct kbase_mem_pool *pool, size_t nr_small_pages,
+int kbase_mem_pool_alloc_pages(struct kbase_mem_pool *pool, size_t nr_4k_pages,
 			       struct tagged_addr *pages, bool partial_allowed,
 			       struct task_struct *page_owner);
 
 /**
  * kbase_mem_pool_alloc_pages_locked - Allocate pages from memory pool
  * @pool:        Memory pool to allocate from
- * @nr_small_pages: Number of pages to allocate
+ * @nr_4k_pages: Number of pages to allocate
  * @pages:       Pointer to array where the physical address of the allocated
  *               pages will be stored.
  *
@@ -1199,7 +1171,7 @@ int kbase_mem_pool_alloc_pages(struct kbase_mem_pool *pool, size_t nr_small_page
  *
  * Note : Caller must hold the pool lock.
  */
-int kbase_mem_pool_alloc_pages_locked(struct kbase_mem_pool *pool, size_t nr_small_pages,
+int kbase_mem_pool_alloc_pages_locked(struct kbase_mem_pool *pool, size_t nr_4k_pages,
 				      struct tagged_addr *pages);
 
 /**
@@ -1277,9 +1249,7 @@ void kbase_mem_pool_set_max_size(struct kbase_mem_pool *pool, size_t max_size);
  * Adds @nr_to_grow pages to the pool. Note that this may cause the pool to
  * become larger than the maximum size specified.
  *
- * Return: 0 on success, -ENOMEM if unable to allocate sufficient pages or
- * -EPERM if the allocation of pages is not permitted due to the process exit
- * or context termination.
+ * Return: 0 on success, -ENOMEM if unable to allocate sufficent pages
  */
 int kbase_mem_pool_grow(struct kbase_mem_pool *pool, size_t nr_to_grow,
 			struct task_struct *page_owner);
@@ -1326,7 +1296,7 @@ struct page *kbase_mem_alloc_page(struct kbase_mem_pool *pool, const bool alloc_
  */
 void kbase_mem_pool_free_page(struct kbase_mem_pool *pool, struct page *p);
 
-bool kbase_check_alloc_flags(struct kbase_context *kctx, unsigned long flags);
+bool kbase_check_alloc_flags(unsigned long flags);
 bool kbase_check_import_flags(unsigned long flags);
 
 static inline bool kbase_import_size_is_valid(struct kbase_device *kbdev, u64 va_pages)
@@ -1432,30 +1402,12 @@ int kbase_update_region_flags(struct kbase_context *kctx, struct kbase_va_region
  */
 void kbase_gpu_vm_lock(struct kbase_context *kctx);
 
-/**
- * kbase_gpu_vm_lock_with_pmode_sync() - Wrapper of kbase_gpu_vm_lock.
- * @kctx:  KBase context
- *
- * Same as kbase_gpu_vm_lock for JM GPU.
- * Additionally acquire P.mode read-write semaphore for CSF GPU.
- */
-void kbase_gpu_vm_lock_with_pmode_sync(struct kbase_context *kctx);
-
 /**
  * kbase_gpu_vm_unlock() - Release the per-context region list lock
  * @kctx:  KBase context
  */
 void kbase_gpu_vm_unlock(struct kbase_context *kctx);
 
-/**
- * kbase_gpu_vm_unlock_with_pmode_sync() - Wrapper of kbase_gpu_vm_unlock.
- * @kctx:  KBase context
- *
- * Same as kbase_gpu_vm_unlock for JM GPU.
- * Additionally release P.mode read-write semaphore for CSF GPU.
- */
-void kbase_gpu_vm_unlock_with_pmode_sync(struct kbase_context *kctx);
-
 int kbase_alloc_phy_pages(struct kbase_va_region *reg, size_t vsize, size_t size);
 
 /**
@@ -1548,7 +1500,7 @@ void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat);
  * Return: The address of the buffer containing the MMU dump or NULL on error
  * (including if the @c nr_pages is too small)
  */
-void *kbase_mmu_dump(struct kbase_context *kctx, size_t nr_pages);
+void *kbase_mmu_dump(struct kbase_context *kctx, int nr_pages);
 #endif
 
 /**
@@ -1693,7 +1645,7 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
  *
  * @prealloc_sa:        Information about the partial allocation if the amount of memory requested
  *                      is not a multiple of 2MB. One instance of struct kbase_sub_alloc must be
- *                      allocated by the caller if large pages are enabled.
+ *                      allocated by the caller if kbdev->pagesize_2mb is enabled.
  *
  * Allocates @nr_pages_requested and updates the alloc object. This function does not allocate new
  * pages from the kernel, and therefore will never trigger the OoM killer. Therefore, it can be
@@ -1721,9 +1673,9 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pa
  * This ensures that the pool can be grown to the required size and that the allocation can
  * complete without another thread using the newly grown pages.
  *
- * If large (2MiB) pages are enabled and the allocation is >= 2MiB, then @pool
- * must be one of the pools from alloc->imported.native.kctx->mem_pools.large[]. Otherwise it
- * must be one of the mempools from alloc->imported.native.kctx->mem_pools.small[].
+ * If kbdev->pagesize_2mb is enabled and the allocation is >= 2MB, then @pool must be one of the
+ * pools from alloc->imported.native.kctx->mem_pools.large[]. Otherwise it must be one of the
+ * mempools from alloc->imported.native.kctx->mem_pools.small[].
  *
  * @prealloc_sa is used to manage the non-2MB sub-allocation. It has to be pre-allocated because we
  * must not sleep (due to the usage of kmalloc()) whilst holding pool->pool_lock.  @prealloc_sa
@@ -1818,8 +1770,8 @@ static inline dma_addr_t kbase_dma_addr_from_tagged(struct tagged_addr tagged_pa
 	phys_addr_t pa = as_phys_addr_t(tagged_pa);
 	struct page *page = pfn_to_page(PFN_DOWN(pa));
 	dma_addr_t dma_addr = (is_huge(tagged_pa) || is_partial(tagged_pa)) ?
-				      kbase_dma_addr_as_priv(page) :
-				      kbase_dma_addr(page);
+					    kbase_dma_addr_as_priv(page) :
+					    kbase_dma_addr(page);
 
 	return dma_addr;
 }
@@ -2112,8 +2064,7 @@ bool kbase_has_exec_va_zone(struct kbase_context *kctx);
  * kbase_map_external_resource - Map an external resource to the GPU.
  * @kctx:              kbase context.
  * @reg:               External resource to map.
- * @locked_mm:         The mm_struct which has been locked for this operation,
- *                     or NULL if none is available.
+ * @locked_mm:         The mm_struct which has been locked for this operation.
  *
  * On successful mapping, the VA region and the gpu_alloc refcounts will be
  * increased, making it safe to use and store both values directly.
@@ -2378,15 +2329,12 @@ int kbase_sticky_resource_init(struct kbase_context *kctx);
  * kbase_sticky_resource_acquire - Acquire a reference on a sticky resource.
  * @kctx:     kbase context.
  * @gpu_addr: The GPU address of the external resource.
- * @locked_mm:         The mm_struct which has been locked for this operation,
- *                     or NULL if none is available.
  *
  * Return: The metadata object which represents the binding between the
  * external resource and the kbase context on success or NULL on failure.
  */
 struct kbase_ctx_ext_res_meta *kbase_sticky_resource_acquire(struct kbase_context *kctx,
-							     u64 gpu_addr,
-							     struct mm_struct *locked_mm);
+							     u64 gpu_addr);
 
 /**
  * kbase_sticky_resource_release - Release a reference on a sticky resource.
@@ -2540,19 +2488,19 @@ void kbase_mem_umm_unmap(struct kbase_context *kctx, struct kbase_va_region *reg
 			 struct kbase_mem_phy_alloc *alloc);
 
 /**
- * kbase_sync_imported_umm - Sync caches for imported UMM memory
+ * kbase_mem_do_sync_imported - Sync caches for imported memory
  * @kctx: Pointer to the kbase context
  * @reg: Pointer to the region with imported memory to sync
  * @sync_fn: The type of sync operation to perform
  *
- * Sync CPU caches for supported dma-buf (UMM) memory.
+ * Sync CPU caches for supported (currently only dma-buf (UMM)) memory.
  * Attempting to sync unsupported imported memory types will result in an error
  * code, -EINVAL.
  *
  * Return: 0 on success, or a negative error code.
  */
-int kbase_sync_imported_umm(struct kbase_context *kctx, struct kbase_va_region *reg,
-			    enum kbase_sync_type sync_fn);
+int kbase_mem_do_sync_imported(struct kbase_context *kctx, struct kbase_va_region *reg,
+			       enum kbase_sync_type sync_fn);
 
 /**
  * kbase_mem_copy_to_pinned_user_pages - Memcpy from source input page to
@@ -2641,7 +2589,4 @@ static inline base_mem_alloc_flags kbase_mem_group_id_set(int id)
 {
 	return BASE_MEM_GROUP_ID_SET(id);
 }
-
-bool kbase_is_large_pages_enabled(void);
-
 #endif /* _KBASE_MEM_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.c b/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.c
index a32da2645077..522495ecaec0 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -46,7 +46,6 @@
 #include <mali_kbase_caps.h>
 #include <mali_kbase_trace_gpu_mem.h>
 #include <mali_kbase_reset_gpu.h>
-#include <linux/version_compat_defs.h>
 
 #if ((KERNEL_VERSION(5, 3, 0) <= LINUX_VERSION_CODE) || \
      (KERNEL_VERSION(5, 0, 0) > LINUX_VERSION_CODE))
@@ -83,17 +82,7 @@
 #define KBASE_MEM_ION_SYNC_WORKAROUND
 #endif
 
-/*
- * fully_backed_gpf_memory - enable full physical backing of all grow-on-GPU-page-fault
- * allocations in the kernel.
- */
-static bool fully_backed_gpf_memory;
-module_param(fully_backed_gpf_memory, bool, 0444);
-MODULE_PARM_DESC(
-	fully_backed_gpf_memory,
-	"Enable the full physical backing of all grow-on-GPU-page-fault allocations in the kernel."
-	"Note that this should only be enabled for testing worst case memory usage "
-	"and should not be enabled in production");
+#define IR_THRESHOLD_STEPS (256u)
 
 #if MALI_USE_CSF
 static int kbase_csf_cpu_mmap_user_reg_page(struct kbase_context *kctx, struct vm_area_struct *vma);
@@ -293,7 +282,7 @@ void kbase_phy_alloc_mapping_put(struct kbase_context *kctx, struct kbase_vmap_s
 }
 
 struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages, u64 commit_pages,
-					u64 extension, base_mem_alloc_flags *flags, u64 *gpu_va,
+					u64 extension, u64 *flags, u64 *gpu_va,
 					enum kbase_caller_mmu_sync_info mmu_sync_info)
 {
 	struct kbase_va_region *reg;
@@ -318,8 +307,9 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages
 	else
 		dev_dbg(dev, "Keeping requested GPU VA of 0x%llx\n", (unsigned long long)*gpu_va);
 
-	if (!kbase_check_alloc_flags(kctx, *flags)) {
-		dev_warn(dev, "%s called with bad flags (%llx)", __func__, *flags);
+	if (!kbase_check_alloc_flags(*flags)) {
+		dev_warn(dev, "%s called with bad flags (%llx)", __func__,
+			 (unsigned long long)*flags);
 		goto bad_flags;
 	}
 
@@ -332,12 +322,6 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages
 	}
 #endif
 
-	/* Ensure GPU cached if CPU cached */
-	if ((*flags & BASE_MEM_CACHED_CPU) != 0) {
-		dev_warn_once(dev, "Clearing BASE_MEM_UNCACHED_GPU flag to avoid MMA violation\n");
-		*flags &= ~BASE_MEM_UNCACHED_GPU;
-	}
-
 	if ((*flags & BASE_MEM_UNCACHED_GPU) != 0 &&
 	    (*flags & BASE_MEM_COHERENT_SYSTEM_REQUIRED) != 0) {
 		/* Remove COHERENT_SYSTEM_REQUIRED flag if uncached GPU mapping is requested */
@@ -357,13 +341,11 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages
 	if (kbase_check_alloc_sizes(kctx, *flags, va_pages, commit_pages, extension))
 		goto bad_sizes;
 
-	/* Ensure that memory is fully physically-backed if full physical backing
-	 * has been requested for GPF allocations.
-	 */
-	if (unlikely(fully_backed_gpf_memory)) {
-		if (*flags & BASE_MEM_GROW_ON_GPF)
-			commit_pages = va_pages;
-	}
+#ifdef CONFIG_MALI_MEMORY_FULLY_BACKED
+	/* Ensure that memory is fully physically-backed. */
+	if (*flags & BASE_MEM_GROW_ON_GPF)
+		commit_pages = va_pages;
+#endif
 
 	/* find out which VA zone to use */
 	if (*flags & BASE_MEM_SAME_VA)
@@ -409,8 +391,16 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages
 		*flags &= ~BASE_MEM_CACHED_CPU;
 
 	if (*flags & BASE_MEM_GROW_ON_GPF) {
-		/* kbase_check_alloc_sizes() already checks extension is valid for assigning to
-		 * reg->extension.
+		unsigned int const ir_threshold = atomic_read(&kctx->kbdev->memdev.ir_threshold);
+
+		reg->threshold_pages =
+			((va_pages * ir_threshold) + (IR_THRESHOLD_STEPS / 2)) / IR_THRESHOLD_STEPS;
+	} else
+		reg->threshold_pages = 0;
+
+	if (*flags & BASE_MEM_GROW_ON_GPF) {
+		/* kbase_check_alloc_sizes() already checks extension is valid for
+		 * assigning to reg->extension
 		 */
 		reg->extension = extension;
 #if !MALI_USE_CSF
@@ -428,7 +418,7 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages
 	}
 	reg->initial_commit = commit_pages;
 
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	if (reg->flags & KBASE_REG_PERMANENT_KERNEL_MAPPING) {
 		/* Permanent kernel mappings must happen as soon as
@@ -438,7 +428,7 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages
 		 */
 		int err = kbase_phy_alloc_mapping_init(kctx, reg, va_pages, commit_pages);
 		if (err < 0) {
-			kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+			kbase_gpu_vm_unlock(kctx);
 			goto no_kern_mapping;
 		}
 	}
@@ -450,7 +440,7 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages
 		/* Bind to a cookie */
 		if (bitmap_empty(kctx->cookies, BITS_PER_LONG)) {
 			dev_err(dev, "No cookies available for allocation!");
-			kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+			kbase_gpu_vm_unlock(kctx);
 			goto no_cookie;
 		}
 		/* return a cookie */
@@ -467,14 +457,14 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages
 	} else /* we control the VA */ {
 		size_t align = 1;
 
-		if (kbase_is_large_pages_enabled()) {
+		if (kctx->kbdev->pagesize_2mb) {
 			/* If there's enough (> 33 bits) of GPU VA space, align to 2MB
 			* boundaries. The similar condition is used for mapping from
 			* the SAME_VA zone inside kbase_context_get_unmapped_area().
 			*/
 			if (kctx->kbdev->gpu_props.mmu.va_bits > 33) {
-				if (va_pages >= NUM_PAGES_IN_2MB_LARGE_PAGE)
-					align = NUM_PAGES_IN_2MB_LARGE_PAGE;
+				if (va_pages >= (SZ_2M / SZ_4K))
+					align = (SZ_2M / SZ_4K);
 			}
 			if (*gpu_va)
 				align = 1;
@@ -485,7 +475,7 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages
 		}
 		if (kbase_gpu_mmap(kctx, reg, *gpu_va, va_pages, align, mmu_sync_info) != 0) {
 			dev_warn(dev, "Failed to map memory on GPU");
-			kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+			kbase_gpu_vm_unlock(kctx);
 			goto no_mmap;
 		}
 		/* return real GPU VA */
@@ -503,7 +493,7 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages
 	}
 #endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 
 #if MALI_USE_CSF
 	if (*flags & BASE_MEM_FIXABLE)
@@ -591,11 +581,9 @@ int kbase_mem_query(struct kbase_context *kctx, u64 gpu_addr, u64 query, u64 *co
 			*out |= BASE_MEM_COHERENT_SYSTEM;
 		if (KBASE_REG_SHARE_IN & reg->flags)
 			*out |= BASE_MEM_COHERENT_LOCAL;
-		if (mali_kbase_supports_query_mem_dont_need(kctx->api_version)) {
-			if (KBASE_REG_DONT_NEED & reg->flags)
-				*out |= BASE_MEM_DONT_NEED;
-		}
-		if (mali_kbase_supports_query_mem_grow_on_gpf(kctx->api_version)) {
+		if (KBASE_REG_DONT_NEED & reg->flags)
+			*out |= BASE_MEM_DONT_NEED;
+		if (mali_kbase_supports_mem_grow_on_gpf(kctx->api_version)) {
 			/* Prior to this version, this was known about by
 			 * user-side but we did not return them. Returning
 			 * it caused certain clients that were not expecting
@@ -605,7 +593,7 @@ int kbase_mem_query(struct kbase_context *kctx, u64 gpu_addr, u64 query, u64 *co
 			if (KBASE_REG_PF_GROW & reg->flags)
 				*out |= BASE_MEM_GROW_ON_GPF;
 		}
-		if (mali_kbase_supports_query_mem_protected(kctx->api_version)) {
+		if (mali_kbase_supports_mem_protected(kctx->api_version)) {
 			/* Prior to this version, this was known about by
 			 * user-side but we did not return them. Returning
 			 * it caused certain clients that were not expecting
@@ -631,30 +619,9 @@ int kbase_mem_query(struct kbase_context *kctx, u64 gpu_addr, u64 query, u64 *co
 			else
 				*out |= BASE_MEM_FIXABLE;
 		}
-#endif /* MALI_USE_CSF */
+#endif
 		if (KBASE_REG_GPU_VA_SAME_4GB_PAGE & reg->flags)
 			*out |= BASE_MEM_GPU_VA_SAME_4GB_PAGE;
-		if (mali_kbase_supports_query_mem_import_sync_on_map_unmap(kctx->api_version)) {
-			if (reg->gpu_alloc->type == KBASE_MEM_TYPE_IMPORTED_UMM) {
-				if (reg->gpu_alloc->imported.umm.need_sync)
-					*out |= BASE_MEM_IMPORT_SYNC_ON_MAP_UNMAP;
-			}
-		}
-		if (mali_kbase_supports_query_mem_kernel_sync(kctx->api_version)) {
-			if (unlikely(reg->cpu_alloc != reg->gpu_alloc))
-				*out |= BASE_MEM_KERNEL_SYNC;
-		}
-		if (mali_kbase_supports_query_mem_same_va(kctx->api_version)) {
-			if (kbase_bits_to_zone(reg->flags) == SAME_VA_ZONE) {
-				/* Imported memory is an edge case, where declaring it SAME_VA
-				 * would be ambiguous.
-				 */
-				if (reg->gpu_alloc->type != KBASE_MEM_TYPE_IMPORTED_UMM &&
-				    reg->gpu_alloc->type != KBASE_MEM_TYPE_IMPORTED_USER_BUF) {
-					*out |= BASE_MEM_SAME_VA;
-				}
-			}
-		}
 
 		*out |= kbase_mem_group_id_set(reg->cpu_alloc->group_id);
 
@@ -685,9 +652,7 @@ int kbase_mem_query(struct kbase_context *kctx, u64 gpu_addr, u64 query, u64 *co
 static unsigned long kbase_mem_evictable_reclaim_count_objects(struct shrinker *s,
 							       struct shrink_control *sc)
 {
-	struct kbase_context *kctx =
-		KBASE_GET_KBASE_DATA_FROM_SHRINKER(s, struct kbase_context, reclaim);
-
+	struct kbase_context *kctx = container_of(s, struct kbase_context, reclaim);
 	int evict_nents = atomic_read(&kctx->evict_nents);
 	unsigned long nr_freeable_items;
 
@@ -699,7 +664,7 @@ static unsigned long kbase_mem_evictable_reclaim_count_objects(struct shrinker *
 		dev_err(kctx->kbdev->dev, "invalid evict_nents(%d)", evict_nents);
 		nr_freeable_items = 0;
 	} else {
-		nr_freeable_items = (unsigned long)evict_nents;
+		nr_freeable_items = evict_nents;
 	}
 
 #if KERNEL_VERSION(4, 19, 0) <= LINUX_VERSION_CODE
@@ -737,15 +702,8 @@ static unsigned long kbase_mem_evictable_reclaim_scan_objects(struct shrinker *s
 	struct kbase_mem_phy_alloc *tmp;
 	unsigned long freed = 0;
 
-	kctx = KBASE_GET_KBASE_DATA_FROM_SHRINKER(s, struct kbase_context, reclaim);
+	kctx = container_of(s, struct kbase_context, reclaim);
 
-#if MALI_USE_CSF
-	if (!down_read_trylock(&kctx->kbdev->csf.mmu_sync_sem)) {
-		dev_warn(kctx->kbdev->dev,
-			 "Can't shrink GPU memory when P.Mode entrance is in progress");
-		return 0;
-	}
-#endif
 	mutex_lock(&kctx->jit_evict_lock);
 
 	list_for_each_entry_safe(alloc, tmp, &kctx->evict_list, evict_node) {
@@ -784,36 +742,32 @@ static unsigned long kbase_mem_evictable_reclaim_scan_objects(struct shrinker *s
 	}
 
 	mutex_unlock(&kctx->jit_evict_lock);
-#if MALI_USE_CSF
-	up_read(&kctx->kbdev->csf.mmu_sync_sem);
-#endif
+
 	return freed;
 }
 
 int kbase_mem_evictable_init(struct kbase_context *kctx)
 {
-	struct shrinker *reclaim;
-
 	INIT_LIST_HEAD(&kctx->evict_list);
 	mutex_init(&kctx->jit_evict_lock);
 
-	reclaim = KBASE_INIT_RECLAIM(kctx, reclaim, "mali-mem");
-	if (!reclaim)
-		return -ENOMEM;
-	KBASE_SET_RECLAIM(kctx, reclaim, reclaim);
-
-	reclaim->count_objects = kbase_mem_evictable_reclaim_count_objects;
-	reclaim->scan_objects = kbase_mem_evictable_reclaim_scan_objects;
-	reclaim->seeks = DEFAULT_SEEKS;
-
-	KBASE_REGISTER_SHRINKER(reclaim, "mali-mem", kctx);
-
+	kctx->reclaim.count_objects = kbase_mem_evictable_reclaim_count_objects;
+	kctx->reclaim.scan_objects = kbase_mem_evictable_reclaim_scan_objects;
+	kctx->reclaim.seeks = DEFAULT_SEEKS;
+	/* Kernel versions prior to 3.1 :
+	 * struct shrinker does not define batch
+	 */
+#if KERNEL_VERSION(6, 0, 0) > LINUX_VERSION_CODE
+	register_shrinker(&kctx->reclaim);
+#else
+	register_shrinker(&kctx->reclaim, "mali-mem");
+#endif
 	return 0;
 }
 
 void kbase_mem_evictable_deinit(struct kbase_context *kctx)
 {
-	KBASE_UNREGISTER_SHRINKER(kctx->reclaim);
+	unregister_shrinker(&kctx->reclaim);
 }
 
 /**
@@ -880,7 +834,7 @@ void kbase_mem_evictable_make(struct kbase_mem_phy_alloc *gpu_alloc)
 	/* Indicate to page migration that the memory can be reclaimed by the shrinker.
 	 */
 	if (kbase_is_page_migration_enabled())
-		kbase_set_phy_alloc_page_status(kctx, gpu_alloc, NOT_MOVABLE);
+		kbase_set_phy_alloc_page_status(gpu_alloc, NOT_MOVABLE);
 
 	mutex_unlock(&kctx->jit_evict_lock);
 	kbase_mem_evictable_mark_reclaim(gpu_alloc);
@@ -938,7 +892,7 @@ bool kbase_mem_evictable_unmake(struct kbase_mem_phy_alloc *gpu_alloc)
 			 * from.
 			 */
 			if (kbase_is_page_migration_enabled())
-				kbase_set_phy_alloc_page_status(kctx, gpu_alloc, ALLOCATED_MAPPED);
+				kbase_set_phy_alloc_page_status(gpu_alloc, ALLOCATED_MAPPED);
 		}
 	}
 
@@ -956,8 +910,7 @@ bool kbase_mem_evictable_unmake(struct kbase_mem_phy_alloc *gpu_alloc)
  *
  * Return: 0 on success, error code otherwise.
  */
-static int kbase_mem_flags_change_imported_umm(struct kbase_context *kctx,
-					       base_mem_alloc_flags flags,
+static int kbase_mem_flags_change_imported_umm(struct kbase_context *kctx, unsigned int flags,
 					       struct kbase_va_region *reg)
 {
 	unsigned int real_flags = 0;
@@ -970,10 +923,6 @@ static int kbase_mem_flags_change_imported_umm(struct kbase_context *kctx,
 	if (flags & ~(BASE_MEM_COHERENT_SYSTEM | BASE_MEM_COHERENT_LOCAL))
 		return ret;
 
-	/* Don't change outer sharability if platform is not coherent */
-	if ((BASE_MEM_COHERENT_SYSTEM & flags) && !kbase_device_is_cpu_coherent(kctx->kbdev))
-		return ret;
-
 	/* shareability flags are ignored for GPU uncached memory
 	 * instead of causing an error.
 	 */
@@ -1040,7 +989,7 @@ static int kbase_mem_flags_change_imported_umm(struct kbase_context *kctx,
  *
  * Return: 0 on success, error code otherwise.
  */
-static int kbase_mem_flags_change_native(struct kbase_context *kctx, base_mem_alloc_flags flags,
+static int kbase_mem_flags_change_native(struct kbase_context *kctx, unsigned int flags,
 					 struct kbase_va_region *reg)
 {
 	bool kbase_reg_dont_need_flag = (KBASE_REG_DONT_NEED & reg->flags);
@@ -1072,8 +1021,8 @@ static int kbase_mem_flags_change_native(struct kbase_context *kctx, base_mem_al
 	return ret;
 }
 
-int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, base_mem_alloc_flags flags,
-			   base_mem_alloc_flags mask)
+int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, unsigned int flags,
+			   unsigned int mask)
 {
 	struct kbase_va_region *reg;
 	int ret = -EINVAL;
@@ -1090,7 +1039,7 @@ int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, base_mem_al
 
 	/* Lock down the context, and find the region */
 	down_write(kbase_mem_get_process_mmap_lock());
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	/* Validate the region */
 	reg = kbase_region_tracker_find_region_base_address(kctx, gpu_addr);
@@ -1142,7 +1091,7 @@ int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, base_mem_al
 	}
 
 out_unlock:
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 	up_write(kbase_mem_get_process_mmap_lock());
 
 	return ret;
@@ -1150,8 +1099,8 @@ int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, base_mem_al
 
 #define KBASE_MEM_IMPORT_HAVE_PAGES (1UL << BASE_MEM_FLAGS_NR_BITS)
 
-int kbase_sync_imported_umm(struct kbase_context *kctx, struct kbase_va_region *reg,
-			    enum kbase_sync_type sync_fn)
+int kbase_mem_do_sync_imported(struct kbase_context *kctx, struct kbase_va_region *reg,
+			       enum kbase_sync_type sync_fn)
 {
 	int ret = -EINVAL;
 	struct dma_buf __maybe_unused *dma_buf;
@@ -1193,7 +1142,7 @@ int kbase_sync_imported_umm(struct kbase_context *kctx, struct kbase_va_region *
 				reg->gpu_alloc->imported.umm.dma_attachment;
 			struct sg_table *sgt = reg->gpu_alloc->imported.umm.sgt;
 
-			dma_sync_sg_for_device(attachment->dev, sgt->sgl, (int)sgt->nents, dir);
+			dma_sync_sg_for_device(attachment->dev, sgt->sgl, sgt->nents, dir);
 			ret = 0;
 		}
 #else
@@ -1209,7 +1158,7 @@ int kbase_sync_imported_umm(struct kbase_context *kctx, struct kbase_va_region *
 				reg->gpu_alloc->imported.umm.dma_attachment;
 			struct sg_table *sgt = reg->gpu_alloc->imported.umm.sgt;
 
-			dma_sync_sg_for_cpu(attachment->dev, sgt->sgl, (int)sgt->nents, dir);
+			dma_sync_sg_for_cpu(attachment->dev, sgt->sgl, sgt->nents, dir);
 			ret = 0;
 		}
 #else
@@ -1238,13 +1187,8 @@ static void kbase_mem_umm_unmap_attachment(struct kbase_context *kctx,
 {
 	struct tagged_addr *pa = alloc->pages;
 
-#if (KERNEL_VERSION(6, 1, 55) <= LINUX_VERSION_CODE)
-	dma_buf_unmap_attachment_unlocked(alloc->imported.umm.dma_attachment,
-					  alloc->imported.umm.sgt, DMA_BIDIRECTIONAL);
-#else
 	dma_buf_unmap_attachment(alloc->imported.umm.dma_attachment, alloc->imported.umm.sgt,
 				 DMA_BIDIRECTIONAL);
-#endif
 	alloc->imported.umm.sgt = NULL;
 
 	kbase_remove_dma_buf_usage(kctx, alloc);
@@ -1279,12 +1223,7 @@ static int kbase_mem_umm_map_attachment(struct kbase_context *kctx, struct kbase
 	WARN_ON_ONCE(alloc->type != KBASE_MEM_TYPE_IMPORTED_UMM);
 	WARN_ON_ONCE(alloc->imported.umm.sgt);
 
-#if (KERNEL_VERSION(6, 1, 55) <= LINUX_VERSION_CODE)
-	sgt = dma_buf_map_attachment_unlocked(alloc->imported.umm.dma_attachment,
-					      DMA_BIDIRECTIONAL);
-#else
 	sgt = dma_buf_map_attachment(alloc->imported.umm.dma_attachment, DMA_BIDIRECTIONAL);
-#endif
 	if (IS_ERR_OR_NULL(sgt))
 		return -EINVAL;
 
@@ -1333,7 +1272,7 @@ int kbase_mem_umm_map(struct kbase_context *kctx, struct kbase_va_region *reg)
 {
 	int err;
 	struct kbase_mem_phy_alloc *alloc;
-	unsigned long gwt_mask = ~0UL;
+	unsigned long gwt_mask = ~0;
 
 	/* Calls to this function are inherently asynchronous, with respect to
 	 * MMU operations.
@@ -1349,7 +1288,7 @@ int kbase_mem_umm_map(struct kbase_context *kctx, struct kbase_va_region *reg)
 		if (IS_ENABLED(CONFIG_MALI_DMA_BUF_LEGACY_COMPAT) ||
 		    alloc->imported.umm.need_sync) {
 			if (!kbase_is_region_invalid_or_free(reg)) {
-				err = kbase_sync_imported_umm(kctx, reg, KBASE_SYNC_TO_DEVICE);
+				err = kbase_mem_do_sync_imported(kctx, reg, KBASE_SYNC_TO_DEVICE);
 				WARN_ON_ONCE(err);
 			}
 		}
@@ -1411,7 +1350,7 @@ void kbase_mem_umm_unmap(struct kbase_context *kctx, struct kbase_va_region *reg
 		if (IS_ENABLED(CONFIG_MALI_DMA_BUF_LEGACY_COMPAT) ||
 		    alloc->imported.umm.need_sync) {
 			if (!kbase_is_region_invalid_or_free(reg)) {
-				int err = kbase_sync_imported_umm(kctx, reg, KBASE_SYNC_TO_CPU);
+				int err = kbase_mem_do_sync_imported(kctx, reg, KBASE_SYNC_TO_CPU);
 				WARN_ON_ONCE(err);
 			}
 		}
@@ -1463,7 +1402,7 @@ static int get_umm_memory_group_id(struct kbase_context *kctx, struct dma_buf *d
  * object that wraps the dma-buf.
  */
 static struct kbase_va_region *kbase_mem_from_umm(struct kbase_context *kctx, int fd, u64 *va_pages,
-						  base_mem_alloc_flags *flags, u32 padding)
+						  u64 *flags, u32 padding)
 {
 	struct kbase_va_region *reg;
 	struct dma_buf *dma_buf;
@@ -1600,7 +1539,7 @@ static struct kbase_va_region *kbase_mem_from_umm(struct kbase_context *kctx, in
 
 u32 kbase_get_cache_line_alignment(struct kbase_device *kbdev)
 {
-	u32 cpu_cache_line_size = (u32)cache_line_size();
+	u32 cpu_cache_line_size = cache_line_size();
 	u32 gpu_cache_line_size = (1UL << kbdev->gpu_props.log2_line_size);
 
 	return ((cpu_cache_line_size > gpu_cache_line_size) ? cpu_cache_line_size :
@@ -1609,8 +1548,7 @@ u32 kbase_get_cache_line_alignment(struct kbase_device *kbdev)
 
 static struct kbase_va_region *kbase_mem_from_user_buffer(struct kbase_context *kctx,
 							  unsigned long address, unsigned long size,
-							  u64 *va_pages,
-							  base_mem_alloc_flags *flags)
+							  u64 *va_pages, u64 *flags)
 {
 	struct kbase_va_region *reg;
 	enum kbase_memory_zone zone = CUSTOM_VA_ZONE;
@@ -1742,7 +1680,7 @@ static struct kbase_va_region *kbase_mem_from_user_buffer(struct kbase_context *
 	return NULL;
 }
 
-u64 kbase_mem_alias(struct kbase_context *kctx, base_mem_alloc_flags *flags, u64 stride, u64 nents,
+u64 kbase_mem_alias(struct kbase_context *kctx, u64 *flags, u64 stride, u64 nents,
 		    struct base_mem_aliasing_info *ai, u64 *num_pages)
 {
 	struct kbase_va_region *reg;
@@ -1827,7 +1765,7 @@ u64 kbase_mem_alias(struct kbase_context *kctx, base_mem_alloc_flags *flags, u64
 	if (!reg->gpu_alloc->imported.alias.aliased)
 		goto no_aliased_array;
 
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	/* validate and add src handles */
 	for (i = 0; i < nents; i++) {
@@ -1937,7 +1875,7 @@ u64 kbase_mem_alias(struct kbase_context *kctx, base_mem_alloc_flags *flags, u64
 	reg->flags &= ~KBASE_REG_FREE;
 	reg->flags &= ~KBASE_REG_GROWABLE;
 
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 
 	return gpu_va;
 
@@ -1948,7 +1886,7 @@ u64 kbase_mem_alias(struct kbase_context *kctx, base_mem_alloc_flags *flags, u64
 	 * them is handled by putting reg's allocs, so no rollback of those
 	 * actions is done here.
 	 */
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 no_aliased_array:
 invalid_flags:
 	kbase_mem_phy_alloc_put(reg->cpu_alloc);
@@ -1964,8 +1902,7 @@ u64 kbase_mem_alias(struct kbase_context *kctx, base_mem_alloc_flags *flags, u64
 }
 
 int kbase_mem_import(struct kbase_context *kctx, enum base_mem_import_type type,
-		     void __user *phandle, u32 padding, u64 *gpu_va, u64 *va_pages,
-		     base_mem_alloc_flags *flags)
+		     void __user *phandle, u32 padding, u64 *gpu_va, u64 *va_pages, u64 *flags)
 {
 	struct kbase_va_region *reg;
 
@@ -2050,7 +1987,7 @@ int kbase_mem_import(struct kbase_context *kctx, enum base_mem_import_type type,
 	if (!reg)
 		goto no_reg;
 
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	/* mmap needed to setup VA? */
 	if (*flags & (BASE_MEM_SAME_VA | BASE_MEM_NEED_MMAP)) {
@@ -2084,13 +2021,13 @@ int kbase_mem_import(struct kbase_context *kctx, enum base_mem_import_type type,
 	/* clear out private flags */
 	*flags &= ((1UL << BASE_MEM_FLAGS_NR_BITS) - 1);
 
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 
 	return 0;
 
 no_gpu_va:
 no_cookie:
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 	kbase_mem_phy_alloc_put(reg->cpu_alloc);
 	kbase_mem_phy_alloc_put(reg->gpu_alloc);
 	kfree(reg);
@@ -2130,9 +2067,9 @@ void kbase_mem_shrink_cpu_mapping(struct kbase_context *kctx, struct kbase_va_re
 		/* Nothing to do */
 		return;
 
-	unmap_mapping_range(kctx->filp->f_inode->i_mapping,
-			    (loff_t)(gpu_va_start + new_pages) << PAGE_SHIFT,
-			    (loff_t)(old_pages - new_pages) << PAGE_SHIFT, 1);
+	unmap_mapping_range(kctx->kfile->filp->f_inode->i_mapping,
+			    (gpu_va_start + new_pages) << PAGE_SHIFT,
+			    (old_pages - new_pages) << PAGE_SHIFT, 1);
 }
 
 int kbase_mem_shrink_gpu_mapping(struct kbase_context *const kctx,
@@ -2176,7 +2113,7 @@ int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 	}
 
 	down_write(kbase_mem_get_process_mmap_lock());
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	/* Validate the region */
 	reg = kbase_region_tracker_find_region_base_address(kctx, gpu_addr);
@@ -2220,22 +2157,14 @@ int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 	if (kbase_va_region_is_no_user_free(reg))
 		goto out_unlock;
 
-	if (unlikely(fully_backed_gpf_memory)) {
-		if ((reg->flags & KBASE_REG_PF_GROW) && (reg->nr_pages != reg->initial_commit)) {
-			dev_err(kctx->kbdev->dev,
-				"GPF region has different physical and VA page size!");
-			res = -EINVAL;
-		} else {
-			/* The condition should already be satisfied, so no need to change
-			 * the commit size.
-			 */
-			res = 0;
-		}
-		goto out_unlock;
-	}
+#ifdef CONFIG_MALI_MEMORY_FULLY_BACKED
+	/* Reject resizing commit size */
+	if (reg->flags & KBASE_REG_PF_GROW)
+		new_pages = reg->nr_pages;
+#endif
 
-	/* no change */
 	if (new_pages == reg->gpu_alloc->nents) {
+		/* no change */
 		res = 0;
 		goto out_unlock;
 	}
@@ -2284,7 +2213,7 @@ int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 	}
 
 out_unlock:
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 	if (read_locked)
 		up_read(kbase_mem_get_process_mmap_lock());
 	else
@@ -2308,16 +2237,11 @@ int kbase_mem_shrink(struct kbase_context *const kctx, struct kbase_va_region *c
 		return -EINVAL;
 
 	old_pages = kbase_reg_current_backed_size(reg);
-	if (old_pages < new_pages) {
-		dev_warn(
-			kctx->kbdev->dev,
-			"Requested number of pages (%llu) is larger than the current number of pages (%llu)",
-			new_pages, old_pages);
+	if (WARN_ON(old_pages < new_pages))
 		return -EINVAL;
-	}
 
 	delta = old_pages - new_pages;
-	if (kbase_is_large_pages_enabled()) {
+	if (kctx->kbdev->pagesize_2mb) {
 		struct tagged_addr *start_free = reg->gpu_alloc->pages + new_pages;
 
 		/* Move the end of new commited range to a valid location.
@@ -2371,7 +2295,7 @@ static void kbase_cpu_vm_close(struct vm_area_struct *vma)
 	KBASE_DEBUG_ASSERT(map->kctx);
 	KBASE_DEBUG_ASSERT(map->alloc);
 
-	kbase_gpu_vm_lock_with_pmode_sync(map->kctx);
+	kbase_gpu_vm_lock(map->kctx);
 
 	if (map->free_on_close) {
 		KBASE_DEBUG_ASSERT(kbase_bits_to_zone(map->region->flags) == SAME_VA_ZONE);
@@ -2385,9 +2309,10 @@ static void kbase_cpu_vm_close(struct vm_area_struct *vma)
 	list_del(&map->mappings_list);
 
 	kbase_va_region_alloc_put(map->kctx, map->region);
-	kbase_gpu_vm_unlock_with_pmode_sync(map->kctx);
+	kbase_gpu_vm_unlock(map->kctx);
 
 	kbase_mem_phy_alloc_put(map->alloc);
+	kbase_file_dec_cpu_mapping_count(map->kctx->kfile);
 	kfree(map);
 }
 
@@ -2535,7 +2460,7 @@ static int kbase_cpu_mmap(struct kbase_context *kctx, struct kbase_va_region *re
 	 * See MIDBASE-1057
 	 */
 
-	vm_flags_set(vma, VM_DONTCOPY | VM_DONTDUMP | VM_DONTEXPAND | VM_IO);
+	vma->vm_flags |= VM_DONTCOPY | VM_DONTDUMP | VM_DONTEXPAND | VM_IO;
 	vma->vm_ops = &kbase_vm_ops;
 	vma->vm_private_data = map;
 
@@ -2562,11 +2487,11 @@ static int kbase_cpu_mmap(struct kbase_context *kctx, struct kbase_va_region *re
 	}
 
 	if (!kaddr)
-		vm_flags_set(vma, VM_PFNMAP);
+		vma->vm_flags |= VM_PFNMAP;
 	else {
 		WARN_ON(aligned_offset);
 		/* MIXEDMAP so we can vfree the kaddr early and not track it after map time */
-		vm_flags_set(vma, VM_MIXEDMAP);
+		vma->vm_flags |= VM_MIXEDMAP;
 		/* vmalloc remaping is easy... */
 		err = remap_vmalloc_range(vma, kaddr, 0);
 		WARN_ON(err);
@@ -2587,6 +2512,7 @@ static int kbase_cpu_mmap(struct kbase_context *kctx, struct kbase_va_region *re
 		map->alloc->properties |= KBASE_MEM_PHY_ALLOC_ACCESSED_CACHED;
 
 	list_add(&map->mappings_list, &map->alloc->mappings);
+	kbase_file_inc_cpu_mapping_count(kctx->kfile);
 
 out:
 	return err;
@@ -2614,7 +2540,7 @@ static int kbase_mmu_dump_mmap(struct kbase_context *kctx, struct vm_area_struct
 {
 	struct kbase_va_region *new_reg;
 	void *kaddr;
-	size_t nr_pages;
+	u32 nr_pages;
 	size_t size;
 	int err = 0;
 
@@ -2772,9 +2698,9 @@ int kbase_context_mmap(struct kbase_context *const kctx, struct vm_area_struct *
 	dev_dbg(dev, "kbase_mmap\n");
 
 	if (!(vma->vm_flags & VM_READ))
-		vm_flags_clear(vma, VM_MAYREAD);
+		vma->vm_flags &= ~VM_MAYREAD;
 	if (!(vma->vm_flags & VM_WRITE))
-		vm_flags_clear(vma, VM_MAYWRITE);
+		vma->vm_flags &= ~VM_MAYWRITE;
 
 	if (nr_pages == 0) {
 		err = -EINVAL;
@@ -2786,7 +2712,7 @@ int kbase_context_mmap(struct kbase_context *const kctx, struct vm_area_struct *
 		goto out;
 	}
 
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	if (vma->vm_pgoff == PFN_DOWN(BASE_MEM_MAP_TRACKING_HANDLE)) {
 		/* The non-mapped tracking helper page */
@@ -2821,11 +2747,11 @@ int kbase_context_mmap(struct kbase_context *const kctx, struct vm_area_struct *
 #endif /* defined(CONFIG_MALI_VECTOR_DUMP) */
 #if MALI_USE_CSF
 	case PFN_DOWN(BASEP_MEM_CSF_USER_REG_PAGE_HANDLE):
-		kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+		kbase_gpu_vm_unlock(kctx);
 		err = kbase_csf_cpu_mmap_user_reg_page(kctx, vma);
 		goto out;
 	case PFN_DOWN(BASEP_MEM_CSF_USER_IO_PAGES_HANDLE)... PFN_DOWN(BASE_MEM_COOKIE_BASE) - 1: {
-		kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+		kbase_gpu_vm_unlock(kctx);
 		mutex_lock(&kctx->csf.lock);
 		err = kbase_csf_cpu_mmap_user_io_pages(kctx, vma);
 		mutex_unlock(&kctx->csf.lock);
@@ -2919,7 +2845,7 @@ int kbase_context_mmap(struct kbase_context *const kctx, struct vm_area_struct *
 	}
 #endif /* defined(CONFIG_MALI_VECTOR_DUMP) */
 out_unlock:
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 out:
 	if (err)
 		dev_err(dev, "mmap failed %d\n", err);
@@ -2934,10 +2860,10 @@ void kbase_sync_mem_regions(struct kbase_context *kctx, struct kbase_vmap_struct
 {
 	size_t i;
 	off_t const offset = map->offset_in_page;
-	size_t const page_count = PFN_UP((size_t)offset + map->size);
+	size_t const page_count = PFN_UP(offset + map->size);
 
 	/* Sync first page */
-	size_t sz = MIN((PAGE_SIZE - (size_t)offset), map->size);
+	size_t sz = MIN(((size_t)PAGE_SIZE - offset), map->size);
 	struct tagged_addr cpu_pa = map->cpu_pages[0];
 	struct tagged_addr gpu_pa = map->gpu_pages[0];
 
@@ -2954,7 +2880,7 @@ void kbase_sync_mem_regions(struct kbase_context *kctx, struct kbase_vmap_struct
 	if (page_count > 1) {
 		cpu_pa = map->cpu_pages[page_count - 1];
 		gpu_pa = map->gpu_pages[page_count - 1];
-		sz = (((size_t)offset + map->size - 1) & ~PAGE_MASK) + 1;
+		sz = ((offset + map->size - 1) & ~PAGE_MASK) + 1;
 		kbase_sync_single(kctx, cpu_pa, gpu_pa, 0, sz, dest);
 	}
 }
@@ -2986,7 +2912,7 @@ static void kbase_vmap_phy_pages_migrate_count_increment(struct tagged_addr *pag
 		struct page *p = as_page(pages[i]);
 		struct kbase_page_metadata *page_md = kbase_page_private(p);
 
-		/* Skip the small page that is part of a large page, as the large page is
+		/* Skip the 4KB page that is part of a large page, as the large page is
 		 * excluded from the migration process.
 		 */
 		if (is_huge(pages[i]) || is_partial(pages[i]))
@@ -3038,7 +2964,7 @@ static void kbase_vunmap_phy_pages_migrate_count_decrement(struct tagged_addr *p
 		struct page *p = as_page(pages[i]);
 		struct kbase_page_metadata *page_md = kbase_page_private(p);
 
-		/* Skip the small page that is part of a large page, as the large page is
+		/* Skip the 4KB page that is part of a large page, as the large page is
 		 * excluded from the migration process.
 		 */
 		if (is_huge(pages[i]) || is_partial(pages[i]))
@@ -3095,7 +3021,7 @@ static int kbase_vmap_phy_pages(struct kbase_context *kctx, struct kbase_va_regi
 
 	if ((vmap_flags & KBASE_VMAP_FLAG_PERMANENT_MAP_ACCOUNTING) &&
 	    (page_count > (KBASE_PERMANENTLY_MAPPED_MEM_LIMIT_PAGES -
-			   (size_t)atomic_read(&kctx->permanent_mapped_pages)))) {
+			   atomic_read(&kctx->permanent_mapped_pages)))) {
 		dev_warn(
 			kctx->kbdev->dev,
 			"Request for %llu more pages mem needing a permanent mapping would breach limit %lu, currently at %d pages",
@@ -3104,7 +3030,7 @@ static int kbase_vmap_phy_pages(struct kbase_context *kctx, struct kbase_va_regi
 		return -ENOMEM;
 	}
 
-	if (kbase_is_region_shrinkable(reg))
+	if (reg->flags & KBASE_REG_DONT_NEED)
 		return -EINVAL;
 
 	prot = PAGE_KERNEL;
@@ -3246,7 +3172,7 @@ static void kbase_vunmap_phy_pages(struct kbase_context *kctx, struct kbase_vmap
 	 * haven't been released yet.
 	 */
 	if (kbase_is_page_migration_enabled() && !kbase_mem_is_imported(map->gpu_alloc->type)) {
-		const size_t page_count = PFN_UP((size_t)map->offset_in_page + map->size);
+		const size_t page_count = PFN_UP(map->offset_in_page + map->size);
 		struct tagged_addr *pages_array = map->cpu_pages;
 
 		kbase_vunmap_phy_pages_migrate_count_decrement(pages_array, page_count);
@@ -3255,7 +3181,7 @@ static void kbase_vunmap_phy_pages(struct kbase_context *kctx, struct kbase_vmap
 	if (map->flags & KBASE_VMAP_FLAG_SYNC_NEEDED)
 		kbase_sync_mem_regions(kctx, map, KBASE_SYNC_TO_DEVICE);
 	if (map->flags & KBASE_VMAP_FLAG_PERMANENT_MAP_ACCOUNTING) {
-		size_t page_count = PFN_UP((size_t)map->offset_in_page + map->size);
+		size_t page_count = PFN_UP(map->offset_in_page + map->size);
 
 		WARN_ON(page_count > (size_t)atomic_read(&kctx->permanent_mapped_pages));
 		atomic_sub(page_count, &kctx->permanent_mapped_pages);
@@ -3313,15 +3239,37 @@ void kbasep_os_process_page_usage_update(struct kbase_context *kctx, int pages)
 #endif
 }
 
+static void kbase_special_vm_open(struct vm_area_struct *vma)
+{
+	struct kbase_context *kctx = vma->vm_private_data;
+
+	kbase_file_inc_cpu_mapping_count(kctx->kfile);
+}
+
+static void kbase_special_vm_close(struct vm_area_struct *vma)
+{
+	struct kbase_context *kctx = vma->vm_private_data;
+
+	kbase_file_dec_cpu_mapping_count(kctx->kfile);
+}
+
+static const struct vm_operations_struct kbase_vm_special_ops = {
+	.open = kbase_special_vm_open,
+	.close = kbase_special_vm_close,
+};
+
 static int kbase_tracking_page_setup(struct kbase_context *kctx, struct vm_area_struct *vma)
 {
 	if (vma_pages(vma) != 1)
 		return -EINVAL;
 
 	/* no real access */
-	vm_flags_clear(vma, VM_READ | VM_MAYREAD | VM_WRITE | VM_MAYWRITE | VM_EXEC | VM_MAYEXEC);
-	vm_flags_set(vma, VM_DONTCOPY | VM_DONTEXPAND | VM_DONTDUMP | VM_IO);
+	vma->vm_flags &= ~(VM_READ | VM_MAYREAD | VM_WRITE | VM_MAYWRITE | VM_EXEC | VM_MAYEXEC);
+	vma->vm_flags |= VM_DONTCOPY | VM_DONTEXPAND | VM_DONTDUMP | VM_IO;
+	vma->vm_ops = &kbase_vm_special_ops;
+	vma->vm_private_data = kctx;
 
+	kbase_file_inc_cpu_mapping_count(kctx->kfile);
 	return 0;
 }
 
@@ -3382,6 +3330,7 @@ static void kbase_csf_user_io_pages_vm_close(struct vm_area_struct *vma)
 	struct kbase_device *kbdev;
 	int err;
 	bool reset_prevented = false;
+	struct kbase_file *kfile;
 
 	if (!queue) {
 		pr_debug("Close method called for the new User IO pages mapping vma\n");
@@ -3390,6 +3339,7 @@ static void kbase_csf_user_io_pages_vm_close(struct vm_area_struct *vma)
 
 	kctx = queue->kctx;
 	kbdev = kctx->kbdev;
+	kfile = kctx->kfile;
 
 	err = kbase_reset_gpu_prevent_and_wait(kbdev);
 	if (err)
@@ -3407,8 +3357,9 @@ static void kbase_csf_user_io_pages_vm_close(struct vm_area_struct *vma)
 	if (reset_prevented)
 		kbase_reset_gpu_allow(kbdev);
 
+	kbase_file_dec_cpu_mapping_count(kfile);
 	/* Now as the vma is closed, drop the reference on mali device file */
-	fput(kctx->filp);
+	fput(kfile->filp);
 }
 
 #if (KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE)
@@ -3538,13 +3489,13 @@ static int kbase_csf_cpu_mmap_user_io_pages(struct kbase_context *kctx, struct v
 	if (err)
 		goto map_failed;
 
-	vm_flags_set(vma, VM_DONTCOPY | VM_DONTDUMP | VM_DONTEXPAND | VM_IO);
+	vma->vm_flags |= VM_DONTCOPY | VM_DONTDUMP | VM_DONTEXPAND | VM_IO;
 	/* TODO use VM_MIXEDMAP, since it is more appropriate as both types of
 	 * memory with and without "struct page" backing are being inserted here.
 	 * Hw Doorbell pages comes from the device register area so kernel does
 	 * not use "struct page" for them.
 	 */
-	vm_flags_set(vma, VM_PFNMAP);
+	vma->vm_flags |= VM_PFNMAP;
 
 	vma->vm_ops = &kbase_csf_user_io_pages_vm_ops;
 	vma->vm_private_data = queue;
@@ -3558,6 +3509,7 @@ static int kbase_csf_cpu_mmap_user_io_pages(struct kbase_context *kctx, struct v
 	/* Also adjust the vm_pgoff */
 	vma->vm_pgoff = queue->db_file_offset;
 
+	kbase_file_inc_cpu_mapping_count(kctx->kfile);
 	return 0;
 
 map_failed:
@@ -3597,6 +3549,7 @@ static void kbase_csf_user_reg_vm_close(struct vm_area_struct *vma)
 {
 	struct kbase_context *kctx = vma->vm_private_data;
 	struct kbase_device *kbdev;
+	struct kbase_file *kfile;
 
 	if (unlikely(!kctx)) {
 		pr_debug("Close function called for the unexpected mapping");
@@ -3604,6 +3557,7 @@ static void kbase_csf_user_reg_vm_close(struct vm_area_struct *vma)
 	}
 
 	kbdev = kctx->kbdev;
+	kfile = kctx->kfile;
 
 	if (unlikely(!kctx->csf.user_reg.vma))
 		dev_warn(kbdev->dev, "user_reg VMA pointer unexpectedly NULL for ctx %d_%d",
@@ -3615,8 +3569,9 @@ static void kbase_csf_user_reg_vm_close(struct vm_area_struct *vma)
 
 	kctx->csf.user_reg.vma = NULL;
 
+	kbase_file_dec_cpu_mapping_count(kfile);
 	/* Now as the VMA is closed, drop the reference on mali device file */
-	fput(kctx->filp);
+	fput(kfile->filp);
 }
 
 /**
@@ -3721,12 +3676,12 @@ static int kbase_csf_cpu_mmap_user_reg_page(struct kbase_context *kctx, struct v
 	/* Map uncached */
 	vma->vm_page_prot = pgprot_device(vma->vm_page_prot);
 
-	vm_flags_set(vma, VM_DONTCOPY | VM_DONTDUMP | VM_DONTEXPAND | VM_IO);
+	vma->vm_flags |= VM_DONTCOPY | VM_DONTDUMP | VM_DONTEXPAND | VM_IO;
 
 	/* User register page comes from the device register area so
 	 * "struct page" isn't available for it.
 	 */
-	vm_flags_set(vma, VM_PFNMAP);
+	vma->vm_flags |= VM_PFNMAP;
 
 	kctx->csf.user_reg.vma = vma;
 
@@ -3746,6 +3701,7 @@ static int kbase_csf_cpu_mmap_user_reg_page(struct kbase_context *kctx, struct v
 	vma->vm_ops = &kbase_csf_user_reg_vm_ops;
 	vma->vm_private_data = kctx;
 
+	kbase_file_inc_cpu_mapping_count(kctx->kfile);
 	return 0;
 }
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.h b/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.h
index a4b3db7fdf89..28666037d8c6 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -51,7 +51,7 @@ struct kbase_hwc_dma_mapping {
  * Return: 0 on success or error code
  */
 struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx, u64 va_pages, u64 commit_pages,
-					u64 extension, base_mem_alloc_flags *flags, u64 *gpu_va,
+					u64 extension, u64 *flags, u64 *gpu_va,
 					enum kbase_caller_mmu_sync_info mmu_sync_info);
 
 /**
@@ -84,8 +84,7 @@ int kbase_mem_query(struct kbase_context *kctx, u64 gpu_addr, u64 query, u64 *co
  * Return: 0 on success or error code
  */
 int kbase_mem_import(struct kbase_context *kctx, enum base_mem_import_type type,
-		     void __user *phandle, u32 padding, u64 *gpu_va, u64 *va_pages,
-		     base_mem_alloc_flags *flags);
+		     void __user *phandle, u32 padding, u64 *gpu_va, u64 *va_pages, u64 *flags);
 
 /**
  * kbase_mem_alias - Create a new allocation for GPU, aliasing one or more
@@ -100,7 +99,7 @@ int kbase_mem_import(struct kbase_context *kctx, enum base_mem_import_type type,
  *
  * Return: 0 on failure or otherwise the GPU VA for the alias
  */
-u64 kbase_mem_alias(struct kbase_context *kctx, base_mem_alloc_flags *flags, u64 stride, u64 nents,
+u64 kbase_mem_alias(struct kbase_context *kctx, u64 *flags, u64 stride, u64 nents,
 		    struct base_mem_aliasing_info *ai, u64 *num_pages);
 
 /**
@@ -113,8 +112,8 @@ u64 kbase_mem_alias(struct kbase_context *kctx, base_mem_alloc_flags *flags, u64
  *
  * Return: 0 on success or error code
  */
-int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, base_mem_alloc_flags flags,
-			   base_mem_alloc_flags mask);
+int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, unsigned int flags,
+			   unsigned int mask);
 
 /**
  * kbase_mem_commit - Change the physical backing size of a region
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_lowlevel.h b/drivers/gpu/arm/bifrost/mali_kbase_mem_lowlevel.h
index 629ca3a2c07b..cb3b5038554c 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_lowlevel.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_lowlevel.h
@@ -28,11 +28,6 @@
 
 #include <linux/dma-mapping.h>
 
-/* Kernel/CPU page size can be 4/16/64KB whereas GPU page size is fixed as 4KB */
-#define GPU_PAGE_SIZE SZ_4K
-#define GPU_PAGE_MASK (~(SZ_4K - 1))
-#define GPU_PAGES_PER_CPU_PAGE (PAGE_SIZE / GPU_PAGE_SIZE)
-
 /* Flags for kbase_phy_allocator_pages_alloc */
 #define KBASE_PHY_PAGES_FLAG_DEFAULT (0) /** Default allocation flag */
 #define KBASE_PHY_PAGES_FLAG_CLEAR (1 << 0) /** Clear the pages after allocation */
@@ -54,7 +49,7 @@ struct tagged_addr {
 #define HUGE_HEAD (1u << 1)
 #define FROM_PARTIAL (1u << 2)
 
-#define NUM_PAGES_IN_2MB_LARGE_PAGE (SZ_2M / PAGE_SIZE)
+#define NUM_4K_PAGES_IN_2MB_PAGE (SZ_2M / SZ_4K)
 
 #define KBASE_INVALID_PHYSICAL_ADDRESS (~(phys_addr_t)0 & PAGE_MASK)
 
@@ -76,7 +71,7 @@ struct tagged_addr {
  */
 static inline phys_addr_t as_phys_addr_t(struct tagged_addr t)
 {
-	return t.tagged_addr & GPU_PAGE_MASK;
+	return t.tagged_addr & PAGE_MASK;
 }
 
 /**
@@ -95,7 +90,7 @@ static inline struct page *as_page(struct tagged_addr t)
  *             there is no tag info present, the lower order 12 bits will be 0
  * @phys: physical address to be converted to tagged type
  *
- * This is used for small physical pages allocated by the Driver or imported pages
+ * This is used for 4KB physical pages allocated by the Driver or imported pages
  * and is needed as physical pages tracking object stores the reference for
  * physical pages using tagged address type in lieu of the type generally used
  * for physical addresses.
@@ -106,7 +101,7 @@ static inline struct tagged_addr as_tagged(phys_addr_t phys)
 {
 	struct tagged_addr t;
 
-	t.tagged_addr = phys & GPU_PAGE_MASK;
+	t.tagged_addr = phys & PAGE_MASK;
 	return t;
 }
 
@@ -124,12 +119,12 @@ static inline struct tagged_addr as_tagged_tag(phys_addr_t phys, int tag)
 {
 	struct tagged_addr t;
 
-	t.tagged_addr = (phys & GPU_PAGE_MASK) | (tag & ~GPU_PAGE_MASK);
+	t.tagged_addr = (phys & PAGE_MASK) | (tag & ~PAGE_MASK);
 	return t;
 }
 
 /**
- * is_huge - Check if the physical page is one of the small pages of the
+ * is_huge - Check if the physical page is one of the 512 4KB pages of the
  *           large page which was not split to be used partially
  * @t: tagged address storing the tag in the lower order bits.
  *
@@ -141,8 +136,9 @@ static inline bool is_huge(struct tagged_addr t)
 }
 
 /**
- * is_huge_head - Check if the physical page is the first small page within
- *                a large page which was not split to be used partially
+ * is_huge_head - Check if the physical page is the first 4KB page of the
+ *                512 4KB pages within a large page which was not split
+ *                to be used partially
  * @t: tagged address storing the tag in the lower order bits.
  *
  * Return: true if page is the first page of a large page, or false
@@ -155,8 +151,8 @@ static inline bool is_huge_head(struct tagged_addr t)
 }
 
 /**
- * is_partial - Check if the physical page is one of the small pages of the
- *              large page which was split in small pages to be used
+ * is_partial - Check if the physical page is one of the 512 pages of the
+ *              large page which was split in 4KB pages to be used
  *              partially for allocations >= 2 MB in size.
  * @t: tagged address storing the tag in the lower order bits.
  *
@@ -168,19 +164,19 @@ static inline bool is_partial(struct tagged_addr t)
 }
 
 /**
- * index_in_large_page() - Get index of a small page within a 2MB page which
+ * index_in_large_page() - Get index of a 4KB page within a 2MB page which
  *                         wasn't split to be used partially.
  *
- * @t:  Tagged physical address of the physical small page that lies within
+ * @t:  Tagged physical address of the physical 4KB page that lies within
  *      the large (or 2 MB) physical page.
  *
- * Return: Index of the small page within a 2MB page
+ * Return: Index of the 4KB page within a 2MB page
  */
 static inline unsigned int index_in_large_page(struct tagged_addr t)
 {
 	WARN_ON(!is_huge(t));
 
-	return (PFN_DOWN(as_phys_addr_t(t)) & (NUM_PAGES_IN_2MB_LARGE_PAGE - 1));
+	return (PFN_DOWN(as_phys_addr_t(t)) & (NUM_4K_PAGES_IN_2MB_PAGE - 1));
 }
 
 /**
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_migrate.c b/drivers/gpu/arm/bifrost/mali_kbase_mem_migrate.c
index eecab323f59f..c59036201fbe 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_migrate.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_migrate.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2022-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -28,9 +28,6 @@
 #include <mali_kbase_mem_migrate.h>
 #include <mmu/mali_kbase_mmu.h>
 
-/* Static key used to determine if page migration is enabled or not */
-static DEFINE_STATIC_KEY_FALSE(page_migration_static_key);
-
 /* Global integer used to determine if module parameter value has been
  * provided and if page migration feature is enabled.
  * Feature is disabled on all platforms by default.
@@ -53,6 +50,15 @@ MODULE_PARM_DESC(kbase_page_migration_enabled,
 
 KBASE_EXPORT_TEST_API(kbase_page_migration_enabled);
 
+bool kbase_is_page_migration_enabled(void)
+{
+	/* Handle uninitialised int case */
+	if (kbase_page_migration_enabled < 0)
+		return false;
+	return IS_ENABLED(CONFIG_PAGE_MIGRATION_SUPPORT) && kbase_page_migration_enabled;
+}
+KBASE_EXPORT_SYMBOL(kbase_is_page_migration_enabled);
+
 #if (KERNEL_VERSION(6, 0, 0) <= LINUX_VERSION_CODE)
 static const struct movable_operations movable_ops;
 #endif
@@ -68,12 +74,6 @@ bool kbase_alloc_page_metadata(struct kbase_device *kbdev, struct page *p, dma_a
 	if (!IS_ENABLED(CONFIG_PAGE_MIGRATION_SUPPORT))
 		return false;
 
-	/* Composite large-page is excluded from migration, trigger a warn if a development
-	 * wrongly leads to it.
-	 */
-	if (is_huge_head(as_tagged(page_to_phys(p))) || is_partial(as_tagged(page_to_phys(p))))
-		dev_WARN(kbdev->dev, "%s: migration-metadata attempted on large-page.", __func__);
-
 	page_md = kzalloc(sizeof(struct kbase_page_metadata), GFP_KERNEL);
 	if (!page_md)
 		return false;
@@ -225,7 +225,7 @@ static int kbasep_migrate_page_pt_mapped(struct page *old_page, struct page *new
 	 * This blocks the CPU page fault handler from remapping pages.
 	 * Only MCU's mmut is device wide, i.e. no corresponding kctx.
 	 */
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	ret = kbase_mmu_migrate_page(
 		as_tagged(page_to_phys(old_page)), as_tagged(page_to_phys(new_page)), old_dma_addr,
@@ -254,7 +254,7 @@ static int kbasep_migrate_page_pt_mapped(struct page *old_page, struct page *new
 		dma_unmap_page(kbdev->dev, new_dma_addr, PAGE_SIZE, DMA_BIDIRECTIONAL);
 
 	/* Page fault handler for CPU mapping unblocked. */
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 
 	return ret;
 }
@@ -293,13 +293,11 @@ static int kbasep_migrate_page_allocated_mapped(struct page *old_page, struct pa
 	/* Lock context to protect access to array of pages in physical allocation.
 	 * This blocks the CPU page fault handler from remapping pages.
 	 */
-	kbase_gpu_vm_lock_with_pmode_sync(kctx);
+	kbase_gpu_vm_lock(kctx);
 
 	/* Unmap the old physical range. */
-	unmap_mapping_range(kctx->filp->f_inode->i_mapping,
-			    (loff_t)(page_md->data.mapped.vpfn / GPU_PAGES_PER_CPU_PAGE)
-				    << PAGE_SHIFT,
-			    PAGE_SIZE, 1);
+	unmap_mapping_range(kctx->kfile->filp->f_inode->i_mapping,
+			    page_md->data.mapped.vpfn << PAGE_SHIFT, PAGE_SIZE, 1);
 
 	ret = kbase_mmu_migrate_page(as_tagged(page_to_phys(old_page)),
 				     as_tagged(page_to_phys(new_page)), old_dma_addr, new_dma_addr,
@@ -332,7 +330,7 @@ static int kbasep_migrate_page_allocated_mapped(struct page *old_page, struct pa
 		dma_unmap_page(kctx->kbdev->dev, new_dma_addr, PAGE_SIZE, DMA_BIDIRECTIONAL);
 
 	/* Page fault handler for CPU mapping unblocked. */
-	kbase_gpu_vm_unlock_with_pmode_sync(kctx);
+	kbase_gpu_vm_unlock(kctx);
 
 	return ret;
 }
@@ -685,15 +683,11 @@ void kbase_mem_migrate_init(struct kbase_device *kbdev)
 	 * integer for a negative value to see if insmod parameter was
 	 * passed in at all (it will override the default negative value).
 	 */
-	if (kbase_page_migration_enabled < 0) {
-		if (kbase_is_large_pages_enabled())
-			static_branch_inc(&page_migration_static_key);
-	} else {
+	if (kbase_page_migration_enabled < 0)
+		kbase_page_migration_enabled = kbdev->pagesize_2mb ? 1 : 0;
+	else
 		dev_info(kbdev->dev, "Page migration support explicitly %s at insmod.",
 			 kbase_page_migration_enabled ? "enabled" : "disabled");
-		if (kbase_page_migration_enabled)
-			static_branch_inc(&page_migration_static_key);
-	}
 
 	spin_lock_init(&mem_migrate->free_pages_lock);
 	INIT_LIST_HEAD(&mem_migrate->free_pages_list);
@@ -718,9 +712,3 @@ void kbase_mem_migrate_term(struct kbase_device *kbdev)
 	iput(mem_migrate->inode);
 #endif
 }
-
-bool kbase_is_page_migration_enabled(void)
-{
-	return static_branch_unlikely(&page_migration_static_key);
-}
-KBASE_EXPORT_TEST_API(kbase_is_page_migration_enabled);
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_migrate.h b/drivers/gpu/arm/bifrost/mali_kbase_mem_migrate.h
index 70c3135a7829..a51a6ce19754 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_migrate.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_migrate.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2022-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -21,14 +21,6 @@
 #ifndef _KBASE_MEM_MIGRATE_H
 #define _KBASE_MEM_MIGRATE_H
 
-#include <linux/version_compat_defs.h>
-
-#include <linux/types.h>
-
-struct kbase_device;
-struct file;
-struct page;
-
 /**
  * DOC: Base kernel page migration implementation.
  */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_pool.c b/drivers/gpu/arm/bifrost/mali_kbase_mem_pool.c
index 5984730c337c..159d84042366 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_pool.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_pool.c
@@ -79,8 +79,8 @@ static inline bool can_alloc_page(struct kbase_mem_pool *pool, struct task_struc
 
 static size_t kbase_mem_pool_capacity(struct kbase_mem_pool *pool)
 {
-	ssize_t max_size = (ssize_t)kbase_mem_pool_max_size(pool);
-	ssize_t cur_size = (ssize_t)kbase_mem_pool_size(pool);
+	ssize_t max_size = kbase_mem_pool_max_size(pool);
+	ssize_t cur_size = kbase_mem_pool_size(pool);
 
 	return max(max_size - cur_size, (ssize_t)0);
 }
@@ -299,7 +299,7 @@ struct page *kbase_mem_alloc_page(struct kbase_mem_pool *pool, const bool alloc_
 		return NULL;
 	}
 
-	/* Setup page metadata for small pages when page migration is enabled */
+	/* Setup page metadata for 4KB pages when page migration is enabled */
 	if (!pool->order && kbase_is_page_migration_enabled()) {
 		INIT_LIST_HEAD(&p->lru);
 		if (!kbase_alloc_page_metadata(kbdev, p, dma_addr, pool->group_id)) {
@@ -398,16 +398,13 @@ int kbase_mem_pool_grow(struct kbase_mem_pool *pool, size_t nr_to_grow,
 			pool->dont_reclaim = false;
 			kbase_mem_pool_shrink_locked(pool, nr_to_grow);
 			kbase_mem_pool_unlock(pool);
-			if (page_owner)
-				dev_info(pool->kbdev->dev, "%s : Ctx of process %s/%d dying",
-					 __func__, page_owner->comm, task_pid_nr(page_owner));
 
-			return -EPERM;
+			return -ENOMEM;
 		}
 		kbase_mem_pool_unlock(pool);
 
 		if (unlikely(!can_alloc_page(pool, page_owner)))
-			return -EPERM;
+			return -ENOMEM;
 
 		p = kbase_mem_alloc_page(pool, alloc_from_kthread);
 		if (!p) {
@@ -480,7 +477,7 @@ static unsigned long kbase_mem_pool_reclaim_count_objects(struct shrinker *s,
 
 	CSTD_UNUSED(sc);
 
-	pool = KBASE_GET_KBASE_DATA_FROM_SHRINKER(s, struct kbase_mem_pool, reclaim);
+	pool = container_of(s, struct kbase_mem_pool, reclaim);
 
 	kbase_mem_pool_lock(pool);
 	if (pool->dont_reclaim && !pool->dying) {
@@ -502,7 +499,7 @@ static unsigned long kbase_mem_pool_reclaim_scan_objects(struct shrinker *s,
 	struct kbase_mem_pool *pool;
 	unsigned long freed;
 
-	pool = KBASE_GET_KBASE_DATA_FROM_SHRINKER(s, struct kbase_mem_pool, reclaim);
+	pool = container_of(s, struct kbase_mem_pool, reclaim);
 
 	kbase_mem_pool_lock(pool);
 	if (pool->dont_reclaim && !pool->dying) {
@@ -528,8 +525,6 @@ int kbase_mem_pool_init(struct kbase_mem_pool *pool, const struct kbase_mem_pool
 			unsigned int order, int group_id, struct kbase_device *kbdev,
 			struct kbase_mem_pool *next_pool)
 {
-	struct shrinker *reclaim;
-
 	if (WARN_ON(group_id < 0) || WARN_ON(group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS)) {
 		return -EINVAL;
 	}
@@ -546,17 +541,18 @@ int kbase_mem_pool_init(struct kbase_mem_pool *pool, const struct kbase_mem_pool
 	spin_lock_init(&pool->pool_lock);
 	INIT_LIST_HEAD(&pool->page_list);
 
-	reclaim = KBASE_INIT_RECLAIM(pool, reclaim, "mali-mem-pool");
-	if (!reclaim)
-		return -ENOMEM;
-	KBASE_SET_RECLAIM(pool, reclaim, reclaim);
-
-	reclaim->count_objects = kbase_mem_pool_reclaim_count_objects;
-	reclaim->scan_objects = kbase_mem_pool_reclaim_scan_objects;
-	reclaim->seeks = DEFAULT_SEEKS;
-	reclaim->batch = 0;
-
-	KBASE_REGISTER_SHRINKER(reclaim, "mali-mem-pool", pool);
+	pool->reclaim.count_objects = kbase_mem_pool_reclaim_count_objects;
+	pool->reclaim.scan_objects = kbase_mem_pool_reclaim_scan_objects;
+	pool->reclaim.seeks = DEFAULT_SEEKS;
+	/* Kernel versions prior to 3.1 :
+	 * struct shrinker does not define batch
+	 */
+	pool->reclaim.batch = 0;
+#if KERNEL_VERSION(6, 0, 0) > LINUX_VERSION_CODE
+	register_shrinker(&pool->reclaim);
+#else
+	register_shrinker(&pool->reclaim, "mali-mem-pool");
+#endif
 
 	pool_dbg(pool, "initialized\n");
 
@@ -582,7 +578,7 @@ void kbase_mem_pool_term(struct kbase_mem_pool *pool)
 
 	pool_dbg(pool, "terminate()\n");
 
-	KBASE_UNREGISTER_SHRINKER(pool->reclaim);
+	unregister_shrinker(&pool->reclaim);
 
 	kbase_mem_pool_lock(pool);
 	pool->max_size = 0;
@@ -706,7 +702,7 @@ void kbase_mem_pool_free_locked(struct kbase_mem_pool *pool, struct page *p, boo
 	}
 }
 
-int kbase_mem_pool_alloc_pages(struct kbase_mem_pool *pool, size_t nr_small_pages,
+int kbase_mem_pool_alloc_pages(struct kbase_mem_pool *pool, size_t nr_4k_pages,
 			       struct tagged_addr *pages, bool partial_allowed,
 			       struct task_struct *page_owner)
 {
@@ -717,12 +713,12 @@ int kbase_mem_pool_alloc_pages(struct kbase_mem_pool *pool, size_t nr_small_page
 	size_t nr_pages_internal;
 	const bool alloc_from_kthread = !!(current->flags & PF_KTHREAD);
 
-	nr_pages_internal = nr_small_pages / (1u << (pool->order));
+	nr_pages_internal = nr_4k_pages / (1u << (pool->order));
 
-	if (nr_pages_internal * (1u << pool->order) != nr_small_pages)
+	if (nr_pages_internal * (1u << pool->order) != nr_4k_pages)
 		return -EINVAL;
 
-	pool_dbg(pool, "alloc_pages(small=%zu):\n", nr_small_pages);
+	pool_dbg(pool, "alloc_pages(4k=%zu):\n", nr_4k_pages);
 	pool_dbg(pool, "alloc_pages(internal=%zu):\n", nr_pages_internal);
 
 	/* Get pages from this pool */
@@ -745,18 +741,18 @@ int kbase_mem_pool_alloc_pages(struct kbase_mem_pool *pool, size_t nr_small_page
 	}
 	kbase_mem_pool_unlock(pool);
 
-	if (i != nr_small_pages && pool->next_pool) {
+	if (i != nr_4k_pages && pool->next_pool) {
 		/* Allocate via next pool */
-		err = kbase_mem_pool_alloc_pages(pool->next_pool, nr_small_pages - i, pages + i,
+		err = kbase_mem_pool_alloc_pages(pool->next_pool, nr_4k_pages - i, pages + i,
 						 partial_allowed, page_owner);
 
 		if (err < 0)
 			goto err_rollback;
 
-		i += (size_t)err;
+		i += err;
 	} else {
 		/* Get any remaining pages from kernel */
-		while (i != nr_small_pages) {
+		while (i != nr_4k_pages) {
 			if (unlikely(!can_alloc_page(pool, page_owner)))
 				goto err_rollback;
 
@@ -793,7 +789,7 @@ int kbase_mem_pool_alloc_pages(struct kbase_mem_pool *pool, size_t nr_small_page
 	return err;
 }
 
-int kbase_mem_pool_alloc_pages_locked(struct kbase_mem_pool *pool, size_t nr_small_pages,
+int kbase_mem_pool_alloc_pages_locked(struct kbase_mem_pool *pool, size_t nr_4k_pages,
 				      struct tagged_addr *pages)
 {
 	struct page *p;
@@ -802,12 +798,12 @@ int kbase_mem_pool_alloc_pages_locked(struct kbase_mem_pool *pool, size_t nr_sma
 
 	lockdep_assert_held(&pool->pool_lock);
 
-	nr_pages_internal = nr_small_pages / (1u << (pool->order));
+	nr_pages_internal = nr_4k_pages / (1u << (pool->order));
 
-	if (nr_pages_internal * (1u << pool->order) != nr_small_pages)
+	if (nr_pages_internal * (1u << pool->order) != nr_4k_pages)
 		return -EINVAL;
 
-	pool_dbg(pool, "alloc_pages_locked(small=%zu):\n", nr_small_pages);
+	pool_dbg(pool, "alloc_pages_locked(4k=%zu):\n", nr_4k_pages);
 	pool_dbg(pool, "alloc_pages_locked(internal=%zu):\n", nr_pages_internal);
 
 	if (kbase_mem_pool_size(pool) < nr_pages_internal) {
@@ -830,7 +826,7 @@ int kbase_mem_pool_alloc_pages_locked(struct kbase_mem_pool *pool, size_t nr_sma
 		}
 	}
 
-	return nr_small_pages;
+	return nr_4k_pages;
 }
 
 static void kbase_mem_pool_add_array(struct kbase_mem_pool *pool, size_t nr_pages,
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_pool_group.c b/drivers/gpu/arm/bifrost/mali_kbase_mem_pool_group.c
index 1b224f7d0cc0..c1fcca6b47dc 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_pool_group.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_pool_group.c
@@ -29,7 +29,7 @@ void kbase_mem_pool_group_config_set_max_size(struct kbase_mem_pool_group_config
 					      size_t const max_size)
 {
 	size_t const large_max_size = max_size >> (KBASE_MEM_POOL_2MB_PAGE_TABLE_ORDER -
-						   KBASE_MEM_POOL_SMALL_PAGE_TABLE_ORDER);
+						   KBASE_MEM_POOL_4KB_PAGE_TABLE_ORDER);
 	int gid;
 
 	for (gid = 0; gid < MEMORY_GROUP_MANAGER_NR_GROUPS; ++gid) {
@@ -48,7 +48,7 @@ int kbase_mem_pool_group_init(struct kbase_mem_pool_group *const mem_pools,
 
 	for (gid = 0; gid < MEMORY_GROUP_MANAGER_NR_GROUPS; ++gid) {
 		err = kbase_mem_pool_init(&mem_pools->small[gid], &configs->small[gid],
-					  KBASE_MEM_POOL_SMALL_PAGE_TABLE_ORDER, gid, kbdev,
+					  KBASE_MEM_POOL_4KB_PAGE_TABLE_ORDER, gid, kbdev,
 					  next_pools ? &next_pools->small[gid] : NULL);
 
 		if (!err) {
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_pool_group.h b/drivers/gpu/arm/bifrost/mali_kbase_mem_pool_group.h
index cb69eab061ac..27dd935c87cf 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_pool_group.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_pool_group.h
@@ -53,7 +53,7 @@ kbase_mem_pool_group_select(struct kbase_device *kbdev, u32 mem_group_id, bool i
  * a set of memory pools
  *
  * @configs:  Initial configuration for the set of memory pools
- * @max_size: Maximum number of free small pages each pool can hold
+ * @max_size: Maximum number of free 4 KiB pages each pool can hold
  *
  * This function sets the initial configuration for every memory pool so that
  * the maximum amount of free memory that each pool can hold is identical.
@@ -74,7 +74,7 @@ void kbase_mem_pool_group_config_set_max_size(struct kbase_mem_pool_group_config
  *
  * Initializes a complete set of physical memory pools. Memory pools are used to
  * allow efficient reallocation of previously-freed physical pages. A pair of
- * memory pools is initialized for each physical memory group: one for small
+ * memory pools is initialized for each physical memory group: one for 4 KiB
  * pages and one for 2 MiB pages.
  *
  * If @next_pools is not NULL then a request to allocate memory from an
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs.h b/drivers/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs.h
index 56c7a4995ab5..d9729e3efbbb 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs.h
@@ -30,8 +30,6 @@
 #include <linux/debugfs.h>
 #include <linux/seq_file.h>
 
-struct kbase_context;
-
 /**
  * kbasep_mem_profile_debugfs_remove - Remove entry from Mali memory profile debugfs
  *
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_native_mgm.c b/drivers/gpu/arm/bifrost/mali_kbase_native_mgm.c
index f9a3788a2ecf..e836011376b1 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_native_mgm.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_native_mgm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -35,12 +35,8 @@
  * @group_id: A physical memory group ID, which must be valid but is not used.
  *            Its valid range is 0 .. MEMORY_GROUP_MANAGER_NR_GROUPS-1.
  * @gfp_mask: Bitmask of Get Free Page flags affecting allocator behavior.
- * @order:    Page order for physical page size.
- *            order = 0 refers to small pages
- *            order != 0 refers to 2 MB pages, so
- *            order = 9 (when small page size is 4KB,  2^9 *  4KB = 2 MB)
- *            order = 7 (when small page size is 16KB, 2^7 * 16KB = 2 MB)
- *            order = 5 (when small page size is 64KB, 2^5 * 64KB = 2 MB)
+ * @order:    Page order for physical page size (order=0 means 4 KiB,
+ *            order=9 means 2 MiB).
  *
  * Delegates all memory allocation requests to the kernel's alloc_pages
  * function.
@@ -48,8 +44,7 @@
  * Return: Pointer to allocated page, or NULL if allocation failed.
  */
 static struct page *kbase_native_mgm_alloc(struct memory_group_manager_device *mgm_dev,
-					   unsigned int group_id, gfp_t gfp_mask,
-					   unsigned int order)
+					   int group_id, gfp_t gfp_mask, unsigned int order)
 {
 	/*
 	 * Check that the base and the mgm defines, from separate header files,
@@ -83,8 +78,8 @@ static struct page *kbase_native_mgm_alloc(struct memory_group_manager_device *m
  *
  * Delegates all memory freeing requests to the kernel's __free_pages function.
  */
-static void kbase_native_mgm_free(struct memory_group_manager_device *mgm_dev,
-				  unsigned int group_id, struct page *page, unsigned int order)
+static void kbase_native_mgm_free(struct memory_group_manager_device *mgm_dev, int group_id,
+				  struct page *page, unsigned int order)
 {
 	CSTD_UNUSED(mgm_dev);
 	CSTD_UNUSED(group_id);
@@ -110,8 +105,7 @@ static void kbase_native_mgm_free(struct memory_group_manager_device *mgm_dev,
  *         entry was successfully installed.
  */
 static vm_fault_t kbase_native_mgm_vmf_insert_pfn_prot(struct memory_group_manager_device *mgm_dev,
-						       unsigned int group_id,
-						       struct vm_area_struct *vma,
+						       int group_id, struct vm_area_struct *vma,
 						       unsigned long addr, unsigned long pfn,
 						       pgprot_t pgprot)
 {
@@ -121,57 +115,62 @@ static vm_fault_t kbase_native_mgm_vmf_insert_pfn_prot(struct memory_group_manag
 	return vmf_insert_pfn_prot(vma, addr, pfn, pgprot);
 }
 
+/**
+ * kbase_native_mgm_update_gpu_pte - Native method to modify a GPU page table
+ *                                   entry
+ *
+ * @mgm_dev:   The memory group manager the request is being made through.
+ * @group_id:  A physical memory group ID, which must be valid but is not used.
+ *             Its valid range is 0 .. MEMORY_GROUP_MANAGER_NR_GROUPS-1.
+ * @mmu_level: The level of the MMU page table where the page is getting mapped.
+ * @pte:       The prepared page table entry.
+ *
+ * This function simply returns the @pte without modification.
+ *
+ * Return: A GPU page table entry to be stored in a page table.
+ */
 static u64 kbase_native_mgm_update_gpu_pte(struct memory_group_manager_device *mgm_dev,
-					   unsigned int group_id, unsigned int pbha_id,
-					   unsigned int pte_flags, int mmu_level, u64 pte)
+					   int group_id, int mmu_level, u64 pte)
 {
-	if (WARN_ON(group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS))
-		return pte;
-
-	if ((pte_flags & BIT(MMA_VIOLATION)) && pbha_id) {
-		pr_warn_once("MMA violation! Applying PBHA override workaround to PTE\n");
-		pte |= ((u64)pbha_id << PTE_PBHA_SHIFT) & PTE_PBHA_MASK;
-	}
-
-	/* Address could be translated into a different bus address here */
-	pte |= ((u64)1 << PTE_RES_BIT_MULTI_AS_SHIFT);
+	CSTD_UNUSED(mgm_dev);
+	CSTD_UNUSED(group_id);
+	CSTD_UNUSED(mmu_level);
 
 	return pte;
 }
 
+/**
+ * kbase_native_mgm_pte_to_original_pte - Native method to undo changes done in
+ *                                        kbase_native_mgm_update_gpu_pte()
+ *
+ * @mgm_dev:   The memory group manager the request is being made through.
+ * @group_id:  A physical memory group ID, which must be valid but is not used.
+ *             Its valid range is 0 .. MEMORY_GROUP_MANAGER_NR_GROUPS-1.
+ * @mmu_level: The level of the MMU page table where the page is getting mapped.
+ * @pte:       The prepared page table entry.
+ *
+ * This function simply returns the @pte without modification.
+ *
+ * Return: A GPU page table entry to be stored in a page table.
+ */
 static u64 kbase_native_mgm_pte_to_original_pte(struct memory_group_manager_device *mgm_dev,
-						unsigned int group_id, int mmu_level, u64 pte)
+						int group_id, int mmu_level, u64 pte)
 {
 	CSTD_UNUSED(mgm_dev);
 	CSTD_UNUSED(group_id);
 	CSTD_UNUSED(mmu_level);
 
-	/* Undo the group ID modification */
-	pte &= ~PTE_PBHA_MASK;
-	/* Undo the bit set */
-	pte &= ~((u64)1 << PTE_RES_BIT_MULTI_AS_SHIFT);
-
 	return pte;
 }
 
-static bool kbase_native_mgm_get_import_memory_cached_access_permitted(
-	struct memory_group_manager_device *mgm_dev,
-	struct memory_group_manager_import_data *import_data)
-{
-	CSTD_UNUSED(mgm_dev);
-	CSTD_UNUSED(import_data);
-
-	return true;
-}
-
 struct memory_group_manager_device kbase_native_mgm_dev = {
-	.ops = { .mgm_alloc_page = kbase_native_mgm_alloc,
-		 .mgm_free_page = kbase_native_mgm_free,
-		 .mgm_get_import_memory_id = NULL,
-		 .mgm_vmf_insert_pfn_prot = kbase_native_mgm_vmf_insert_pfn_prot,
-		 .mgm_update_gpu_pte = kbase_native_mgm_update_gpu_pte,
-		 .mgm_pte_to_original_pte = kbase_native_mgm_pte_to_original_pte,
-		 .mgm_get_import_memory_cached_access_permitted =
-			 kbase_native_mgm_get_import_memory_cached_access_permitted },
+	.ops = {
+		.mgm_alloc_page = kbase_native_mgm_alloc,
+		.mgm_free_page = kbase_native_mgm_free,
+		.mgm_get_import_memory_id = NULL,
+		.mgm_vmf_insert_pfn_prot = kbase_native_mgm_vmf_insert_pfn_prot,
+		.mgm_update_gpu_pte = kbase_native_mgm_update_gpu_pte,
+		.mgm_pte_to_original_pte = kbase_native_mgm_pte_to_original_pte,
+	},
 	.data = NULL
 };
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_pbha.c b/drivers/gpu/arm/bifrost/mali_kbase_pbha.c
index ea79811ea293..9f75c3371c15 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_pbha.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_pbha.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2021-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2021-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -155,7 +155,7 @@ int kbase_pbha_record_settings(struct kbase_device *kbdev, bool runtime, unsigne
 
 #if MALI_USE_CSF
 		if (runtime) {
-			uint i;
+			int i;
 
 			kbase_pm_context_active(kbdev);
 			/* Ensure host copy of SYSC_ALLOC is up to date */
@@ -215,29 +215,11 @@ void kbase_pbha_write_settings(struct kbase_device *kbdev)
 {
 #if MALI_USE_CSF
 	if (kbasep_pbha_supported(kbdev)) {
-		uint i;
+		int i;
 
 		for (i = 0; i < GPU_SYSC_ALLOC_COUNT; ++i)
 			kbase_reg_write32(kbdev, GPU_SYSC_ALLOC_OFFSET(i), kbdev->sysc_alloc[i]);
 	}
-
-	if (kbdev->mma_wa_id) {
-		/* PBHA OVERRIDE register index (0-3) */
-		uint reg_index = kbdev->mma_wa_id >> 2;
-		/* PBHA index within a PBHA OVERRIDE register (0-3) */
-		uint pbha_index = kbdev->mma_wa_id & 0x3;
-		/* 4 bits of read attributes + 4 bits of write attributes for each PBHA */
-		uint pbha_shift = pbha_index * 8;
-		/* Noncacheable read = noncacheable write = b0001*/
-		uint pbha_override_rw_noncacheable = 0x01 | 0x10;
-
-		u32 pbha_override_val =
-			kbase_reg_read32(kbdev, GPU_SYSC_PBHA_OVERRIDE_OFFSET(reg_index));
-		pbha_override_val &= ~((u32)0xFF << pbha_shift);
-		pbha_override_val |= ((u32)pbha_override_rw_noncacheable << pbha_shift);
-		kbase_reg_write32(kbdev, GPU_SYSC_PBHA_OVERRIDE_OFFSET(reg_index),
-				  pbha_override_val);
-	}
 #else
 	CSTD_UNUSED(kbdev);
 #endif /* MALI_USE_CSF */
@@ -267,10 +249,9 @@ static int kbase_pbha_read_int_id_override_property(struct kbase_device *kbdev,
 		dev_err(kbdev->dev, "Bad DTB format: pbha.int_id_override\n");
 		return -EINVAL;
 	}
-	if (of_property_read_u32_array(pbha_node, "int-id-override", dtb_data, (size_t)sz) != 0) {
+	if (of_property_read_u32_array(pbha_node, "int-id-override", dtb_data, sz) != 0) {
 		/* There may be no int-id-override field. Fallback to int_id_override instead */
-		if (of_property_read_u32_array(pbha_node, "int_id_override", dtb_data,
-					       (size_t)sz) != 0) {
+		if (of_property_read_u32_array(pbha_node, "int_id_override", dtb_data, sz) != 0) {
 			dev_err(kbdev->dev, "Failed to read DTB pbha.int_id_override\n");
 			return -EINVAL;
 		}
@@ -295,16 +276,16 @@ static int kbase_pbha_read_int_id_override_property(struct kbase_device *kbdev,
 static int kbase_pbha_read_propagate_bits_property(struct kbase_device *kbdev,
 						   const struct device_node *pbha_node)
 {
-	u8 bits = 0;
+	u32 bits = 0;
 	int err;
 
-	if (!kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_PBHA_HWU))
+	if (!kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_PBHA_HWU))
 		return 0;
 
-	err = of_property_read_u8(pbha_node, "propagate-bits", &bits);
+	err = of_property_read_u32(pbha_node, "propagate-bits", &bits);
 
 	if (err == -EINVAL) {
-		err = of_property_read_u8(pbha_node, "propagate_bits", &bits);
+		err = of_property_read_u32(pbha_node, "propagate_bits", &bits);
 	}
 
 	if (err < 0) {
@@ -328,43 +309,6 @@ static int kbase_pbha_read_propagate_bits_property(struct kbase_device *kbdev,
 	kbdev->pbha_propagate_bits = bits;
 	return 0;
 }
-
-static int kbase_pbha_read_mma_wa_id_property(struct kbase_device *kbdev,
-					      const struct device_node *pbha_node)
-{
-	u32 mma_wa_id = 0;
-	int err;
-
-	/* Skip if kbdev->mma_wa_id has already been set via the module parameter */
-	if ((kbdev->gpu_props.gpu_id.arch_id < GPU_ID_ARCH_MAKE(14, 8, 0)) || kbdev->mma_wa_id != 0)
-		return 0;
-
-	err = of_property_read_u32(pbha_node, "mma-wa-id", &mma_wa_id);
-
-	/* Property does not exist. This is not a mandatory property, ignore this error */
-	if (err == -EINVAL)
-		return 0;
-
-	if (err == -ENODATA) {
-		dev_err(kbdev->dev, "DTB property mma-wa-id has no value\n");
-		return err;
-	}
-
-	if (err == -EOVERFLOW) {
-		dev_err(kbdev->dev, "DTB value for mma-wa-id is out of range\n");
-		return err;
-	}
-
-	if (mma_wa_id == 0 || mma_wa_id > 15) {
-		dev_err(kbdev->dev,
-			"Invalid DTB value for mma-wa-id: %u. Valid range is between 1 and 15.\n",
-			mma_wa_id);
-		return -EINVAL;
-	}
-
-	kbdev->mma_wa_id = mma_wa_id;
-	return 0;
-}
 #endif /* MALI_USE_CSF */
 
 int kbase_pbha_read_dtb(struct kbase_device *kbdev)
@@ -386,12 +330,6 @@ int kbase_pbha_read_dtb(struct kbase_device *kbdev)
 		return err;
 
 	err = kbase_pbha_read_propagate_bits_property(kbdev, pbha_node);
-
-	if (err < 0)
-		return err;
-
-	err = kbase_pbha_read_mma_wa_id_property(kbdev, pbha_node);
-
 	return err;
 #else
 	return 0;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_pbha.h b/drivers/gpu/arm/bifrost/mali_kbase_pbha.h
index 9cecf83bf820..a8eb546a9a4d 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_pbha.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_pbha.h
@@ -22,9 +22,7 @@
 #ifndef _KBASE_PBHA_H
 #define _KBASE_PBHA_H
 
-#include <linux/types.h>
-
-struct kbase_device;
+#include <mali_kbase.h>
 
 /**
  * kbasep_pbha_supported - check whether PBHA registers are
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_pbha_debugfs.c b/drivers/gpu/arm/bifrost/mali_kbase_pbha_debugfs.c
index 81f2df5ea977..5b13a0bd8f32 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_pbha_debugfs.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_pbha_debugfs.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2021-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2021-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -50,8 +50,8 @@ static int int_id_overrides_show(struct seq_file *sfile, void *data)
 #endif /* MALI_USE_CSF */
 
 		for (j = 0; j < sizeof(u32); ++j) {
-			u8 r_val = 0;
-			u8 w_val = 0;
+			u8 r_val;
+			u8 w_val;
 
 			switch (j) {
 			case 0:
@@ -112,7 +112,7 @@ static ssize_t int_id_overrides_write(struct file *file, const char __user *ubuf
 
 	kbase_pm_context_idle(kbdev);
 
-	return (ssize_t)count;
+	return count;
 }
 
 static int int_id_overrides_open(struct inode *in, struct file *file)
@@ -196,7 +196,7 @@ static ssize_t propagate_bits_write(struct file *file, const char __user *ubuf,
 		kbase_reset_gpu_wait(kbdev);
 	}
 
-	return (ssize_t)count;
+	return count;
 }
 
 static const struct file_operations pbha_propagate_bits_fops = {
@@ -234,7 +234,7 @@ void kbase_pbha_debugfs_init(struct kbase_device *kbdev)
 		debugfs_create_file("int_id_overrides", mode, debugfs_pbha_dir, kbdev,
 				    &pbha_int_id_overrides_fops);
 #if MALI_USE_CSF
-		if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_PBHA_HWU))
+		if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_PBHA_HWU))
 			debugfs_create_file("propagate_bits", mode, debugfs_pbha_dir, kbdev,
 					    &pbha_propagate_bits_fops);
 #endif /* MALI_USE_CSF */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_pbha_debugfs.h b/drivers/gpu/arm/bifrost/mali_kbase_pbha_debugfs.h
index 10b4c63abe74..508ecdff9162 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_pbha_debugfs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_pbha_debugfs.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2021-2023 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2021-2022 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -22,7 +22,7 @@
 #ifndef _KBASE_PBHA_DEBUGFS_H
 #define _KBASE_PBHA_DEBUGFS_H
 
-struct kbase_device;
+#include <mali_kbase.h>
 
 /**
  * kbase_pbha_debugfs_init - Initialize pbha debugfs directory
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_pm.c b/drivers/gpu/arm/bifrost/mali_kbase_pm.c
index 6719a120c1f3..b636d4288511 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_pm.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_pm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -31,7 +31,9 @@
 #include <mali_kbase_pm.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 #include <arbiter/mali_kbase_arbiter_pm.h>
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 #include <backend/gpu/mali_kbase_clk_rate_trace_mgr.h>
 
@@ -50,21 +52,22 @@ void kbase_pm_context_active(struct kbase_device *kbdev)
 	(void)kbase_pm_context_active_handle_suspend(kbdev, KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE);
 }
 
-int kbase_pm_context_active_handle_suspend_locked(struct kbase_device *kbdev,
-						  enum kbase_pm_suspend_handler suspend_handler)
+int kbase_pm_context_active_handle_suspend(struct kbase_device *kbdev,
+					   enum kbase_pm_suspend_handler suspend_handler)
 {
 	int c;
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
 	dev_dbg(kbdev->dev, "%s - reason = %d, pid = %d\n", __func__, suspend_handler,
 		current->pid);
-	lockdep_assert_held(&kbdev->pm.lock);
+	kbase_pm_lock(kbdev);
 
-	/* If there is an Arbiter, wait for Arbiter to grant GPU back to KBase
-	 * so suspend request can be handled.
-	 */
-	if (kbase_arbiter_pm_ctx_active_handle_suspend(kbdev, suspend_handler))
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbase_arbiter_pm_ctx_active_handle_suspend(kbdev, suspend_handler)) {
+		kbase_pm_unlock(kbdev);
 		return 1;
+	}
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 	if (kbase_pm_is_suspending(kbdev)) {
 		switch (suspend_handler) {
@@ -73,6 +76,7 @@ int kbase_pm_context_active_handle_suspend_locked(struct kbase_device *kbdev,
 				break;
 			fallthrough;
 		case KBASE_PM_SUSPEND_HANDLER_DONT_INCREASE:
+			kbase_pm_unlock(kbdev);
 			return 1;
 
 		case KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE:
@@ -83,45 +87,37 @@ int kbase_pm_context_active_handle_suspend_locked(struct kbase_device *kbdev,
 		}
 	}
 	c = ++kbdev->pm.active_count;
-	KBASE_KTRACE_ADD(kbdev, PM_CONTEXT_ACTIVE, NULL, (u64)c);
+	KBASE_KTRACE_ADD(kbdev, PM_CONTEXT_ACTIVE, NULL, c);
 
 	if (c == 1) {
 		/* First context active: Power on the GPU and
 		 * any cores requested by the policy
 		 */
 		kbase_hwaccess_pm_gpu_active(kbdev);
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 		kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_REF_EVENT);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 		kbase_clk_rate_trace_manager_gpu_active(kbdev);
 	}
 
+	kbase_pm_unlock(kbdev);
 	dev_dbg(kbdev->dev, "%s %d\n", __func__, kbdev->pm.active_count);
 
 	return 0;
 }
 
-int kbase_pm_context_active_handle_suspend(struct kbase_device *kbdev,
-					   enum kbase_pm_suspend_handler suspend_handler)
-{
-	int ret;
-
-	kbase_pm_lock(kbdev);
-	ret = kbase_pm_context_active_handle_suspend_locked(kbdev, suspend_handler);
-	kbase_pm_unlock(kbdev);
-
-	return ret;
-}
-
 KBASE_EXPORT_TEST_API(kbase_pm_context_active);
 
-void kbase_pm_context_idle_locked(struct kbase_device *kbdev)
+void kbase_pm_context_idle(struct kbase_device *kbdev)
 {
 	int c;
 
 	KBASE_DEBUG_ASSERT(kbdev != NULL);
-	lockdep_assert_held(&kbdev->pm.lock);
+
+	kbase_pm_lock(kbdev);
 
 	c = --kbdev->pm.active_count;
-	KBASE_KTRACE_ADD(kbdev, PM_CONTEXT_IDLE, NULL, (u64)c);
+	KBASE_KTRACE_ADD(kbdev, PM_CONTEXT_IDLE, NULL, c);
 
 	KBASE_DEBUG_ASSERT(c >= 0);
 
@@ -137,14 +133,8 @@ void kbase_pm_context_idle_locked(struct kbase_device *kbdev)
 		wake_up(&kbdev->pm.zero_active_count_wait);
 	}
 
-	dev_dbg(kbdev->dev, "%s %d (pid = %d)\n", __func__, kbdev->pm.active_count, current->pid);
-}
-
-void kbase_pm_context_idle(struct kbase_device *kbdev)
-{
-	kbase_pm_lock(kbdev);
-	kbase_pm_context_idle_locked(kbdev);
 	kbase_pm_unlock(kbdev);
+	dev_dbg(kbdev->dev, "%s %d (pid = %d)\n", __func__, kbdev->pm.active_count, current->pid);
 }
 
 KBASE_EXPORT_TEST_API(kbase_pm_context_idle);
@@ -165,12 +155,7 @@ static void reenable_hwcnt_on_resume(struct kbase_device *kbdev)
 #endif
 
 	/* Resume HW counters intermediaries. */
-#if MALI_USE_CSF
-	if (kbdev->csf.firmware_inited)
-#endif
-	{
-		kbase_kinstr_prfcnt_resume(kbdev->kinstr_prfcnt_ctx);
-	}
+	kbase_kinstr_prfcnt_resume(kbdev->kinstr_prfcnt_ctx);
 }
 
 static void resume_job_scheduling(struct kbase_device *kbdev)
@@ -198,12 +183,7 @@ int kbase_pm_driver_suspend(struct kbase_device *kbdev)
 	/* Suspend HW counter intermediaries. This blocks until workers and timers
 	 * are no longer running.
 	 */
-#if MALI_USE_CSF
-	if (kbdev->csf.firmware_inited)
-#endif
-	{
-		kbase_kinstr_prfcnt_suspend(kbdev->kinstr_prfcnt_ctx);
-	}
+	kbase_kinstr_prfcnt_suspend(kbdev->kinstr_prfcnt_ctx);
 
 	/* Disable GPU hardware counters.
 	 * This call will block until counters are disabled.
@@ -219,24 +199,19 @@ int kbase_pm_driver_suspend(struct kbase_device *kbdev)
 	kbdev->pm.suspending = true;
 	mutex_unlock(&kbdev->pm.lock);
 
-	if (kbase_has_arbiter(kbdev)) {
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbdev->arb.arb_if) {
+		int i;
 		unsigned long flags;
 
-#if MALI_USE_CSF
-		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-		kbase_disjoint_state_up(kbdev);
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-#else
-		unsigned int i;
-
 		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 		kbdev->js_data.runpool_irq.submit_allowed = 0;
 		kbase_disjoint_state_up(kbdev);
 		for (i = 0; i < kbdev->gpu_props.num_job_slots; i++)
 			kbase_job_slot_softstop(kbdev, i, NULL);
 		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-#endif
 	}
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 	/* From now on, the active count will drop towards zero. Sometimes,
 	 * it'll go up briefly before going down again. However, once
@@ -282,21 +257,19 @@ int kbase_pm_driver_suspend(struct kbase_device *kbdev)
 	 */
 	if (kbase_hwaccess_pm_suspend(kbdev)) {
 		/* No early return yet */
-		if (kbase_has_arbiter(kbdev))
+		if (IS_ENABLED(CONFIG_MALI_ARBITER_SUPPORT))
 			WARN_ON_ONCE(1);
 		else
 			goto exit;
 	}
 
-	if (kbase_has_arbiter(kbdev)) {
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbdev->arb.arb_if) {
 		mutex_lock(&kbdev->pm.arb_vm_state->vm_state_lock);
 		kbase_arbiter_pm_vm_stopped(kbdev);
 		mutex_unlock(&kbdev->pm.arb_vm_state->vm_state_lock);
 	}
-
-#if MALI_USE_CSF
-	kbase_backend_invalidate_gpu_timestamp_offset(kbdev);
-#endif
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 	return 0;
 
@@ -332,13 +305,14 @@ void kbase_pm_driver_resume(struct kbase_device *kbdev, bool arb_gpu_start)
 	kbase_hwaccess_pm_resume(kbdev);
 
 	/* Initial active call, to power on the GPU/cores if needed */
-	if (kbase_has_arbiter(kbdev)) {
-		if (kbase_pm_context_active_handle_suspend(
-			    kbdev, (arb_gpu_start ? KBASE_PM_SUSPEND_HANDLER_VM_GPU_GRANTED :
-							  KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE)))
-			return;
-	} else
-		kbase_pm_context_active(kbdev);
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbase_pm_context_active_handle_suspend(
+		    kbdev, (arb_gpu_start ? KBASE_PM_SUSPEND_HANDLER_VM_GPU_GRANTED :
+						  KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE)))
+		return;
+#else
+	kbase_pm_context_active(kbdev);
+#endif
 
 	resume_job_scheduling(kbdev);
 
@@ -362,19 +336,26 @@ void kbase_pm_driver_resume(struct kbase_device *kbdev, bool arb_gpu_start)
 int kbase_pm_suspend(struct kbase_device *kbdev)
 {
 	int result = 0;
-
-	if (kbase_has_arbiter(kbdev))
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbdev->arb.arb_if)
 		kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_OS_SUSPEND_EVENT);
 	else
 		result = kbase_pm_driver_suspend(kbdev);
+#else
+	result = kbase_pm_driver_suspend(kbdev);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 	return result;
 }
 
 void kbase_pm_resume(struct kbase_device *kbdev)
 {
-	if (kbase_has_arbiter(kbdev))
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbdev->arb.arb_if)
 		kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_OS_RESUME_EVENT);
 	else
 		kbase_pm_driver_resume(kbdev, false);
+#else
+	kbase_pm_driver_resume(kbdev, false);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 }
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_pm.h b/drivers/gpu/arm/bifrost/mali_kbase_pm.h
index 25e4732a8d08..187fb9efaaf2 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_pm.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_pm.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -26,19 +26,18 @@
 #ifndef _KBASE_PM_H_
 #define _KBASE_PM_H_
 
-#include <linux/types.h>
-
-struct kbase_device;
+#include "mali_kbase_hwaccess_pm.h"
 
 #define PM_ENABLE_IRQS 0x01
 #define PM_HW_ISSUES_DETECT 0x02
 
-/* Case 1: the GPU was granted by the Arbiter, it will have
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+/* In the case that the GPU was granted by the Arbiter, it will have
  * already been reset. The following flag ensures it is not reset
  * twice.
- * Case 2: GPU already in reset state after power on, then no soft-reset is needed.
  */
 #define PM_NO_RESET 0x04
+#endif
 
 /**
  * kbase_pm_init - Initialize the power management framework.
@@ -120,10 +119,12 @@ enum kbase_pm_suspend_handler {
 	 * (e.g. guarantee it's going to be idled very soon after)
 	 */
 	KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE,
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	/** Special case when Arbiter has notified we can use GPU.
 	 * Active count should always start at 0 in this case.
 	 */
 	KBASE_PM_SUSPEND_HANDLER_VM_GPU_GRANTED,
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 };
 
 /**
@@ -145,18 +146,6 @@ enum kbase_pm_suspend_handler {
 int kbase_pm_context_active_handle_suspend(struct kbase_device *kbdev,
 					   enum kbase_pm_suspend_handler suspend_handler);
 
-/**
- * kbase_pm_context_active_handle_suspend_locked - Same as kbase_pm_context_active_handle_suspend(),
- *                                                 except that pm.lock is held by the caller.
- *
- * @kbdev:     The kbase device structure for the device (must be a valid pointer)
- * @suspend_handler: The handler code for how to handle a suspend that might occur
- *
- * Return: 0 on success, non-zero othrewise.
- */
-int kbase_pm_context_active_handle_suspend_locked(struct kbase_device *kbdev,
-						  enum kbase_pm_suspend_handler suspend_handler);
-
 /**
  * kbase_pm_context_idle - Decrement the reference count of active contexts.
  *
@@ -168,14 +157,6 @@ int kbase_pm_context_active_handle_suspend_locked(struct kbase_device *kbdev,
  */
 void kbase_pm_context_idle(struct kbase_device *kbdev);
 
-/**
- * kbase_pm_context_idle_locked - Same as kbase_pm_context_idle(), except that
- *                                pm.lock is held by the caller.
- *
- * @kbdev:     The kbase device structure for the device (must be a valid pointer)
- */
-void kbase_pm_context_idle_locked(struct kbase_device *kbdev);
-
 /* NOTE: kbase_pm_is_active() is in mali_kbase.h, because it is an inline
  * function
  */
@@ -232,7 +213,7 @@ void kbase_pm_vsync_callback(int buffer_updated, void *data);
  * kbase components to complete the suspend.
  *
  * Despite kbase_pm_suspend(), it will ignore to update Arbiter
- * status if there is one.
+ * status if MALI_ARBITER_SUPPORT is enabled.
  *
  * @note the mechanisms used here rely on all user-space threads being frozen
  * by the OS before we suspend. Otherwise, an IOCTL could occur that powers up
@@ -256,10 +237,11 @@ int kbase_pm_driver_suspend(struct kbase_device *kbdev);
  * Also called when using VM arbiter, when GPU access has been granted.
  *
  * Despite kbase_pm_resume(), it will ignore to update Arbiter
- * status if there is one.
+ * status if MALI_ARBITER_SUPPORT is enabled.
  */
 void kbase_pm_driver_resume(struct kbase_device *kbdev, bool arb_gpu_start);
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 /**
  * kbase_pm_handle_gpu_lost() - Handle GPU Lost for the VM
  * @kbdev: Device pointer
@@ -270,5 +252,6 @@ void kbase_pm_driver_resume(struct kbase_device *kbdev, bool arb_gpu_start);
  * Kill any running tasks and put the driver into a GPU powered-off state.
  */
 void kbase_pm_handle_gpu_lost(struct kbase_device *kbdev);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 #endif /* _KBASE_PM_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_refcount_defs.h b/drivers/gpu/arm/bifrost/mali_kbase_refcount_defs.h
new file mode 100644
index 000000000000..c517a2d2ab83
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_refcount_defs.h
@@ -0,0 +1,57 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ *
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU license.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ */
+
+#ifndef _KBASE_REFCOUNT_DEFS_H_
+#define _KBASE_REFCOUNT_DEFS_H_
+
+/*
+ * The Refcount API is available from 4.11 onwards
+ * This file hides the compatibility issues with this for the rest the driver
+ */
+
+#include <linux/version.h>
+#include <linux/types.h>
+
+#if (KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE)
+
+#define kbase_refcount_t atomic_t
+#define kbase_refcount_read(x) atomic_read(x)
+#define kbase_refcount_set(x, v) atomic_set(x, v)
+#define kbase_refcount_dec_and_test(x) atomic_dec_and_test(x)
+#define kbase_refcount_dec(x) atomic_dec(x)
+#define kbase_refcount_inc_not_zero(x) atomic_inc_not_zero(x)
+#define kbase_refcount_inc(x) atomic_inc(x)
+
+#else
+
+#include <linux/refcount.h>
+
+#define kbase_refcount_t refcount_t
+#define kbase_refcount_read(x) refcount_read(x)
+#define kbase_refcount_set(x, v) refcount_set(x, v)
+#define kbase_refcount_dec_and_test(x) refcount_dec_and_test(x)
+#define kbase_refcount_dec(x) refcount_dec(x)
+#define kbase_refcount_inc_not_zero(x) refcount_inc_not_zero(x)
+#define kbase_refcount_inc(x) refcount_inc(x)
+
+#endif /* (KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE) */
+
+#endif /* _KBASE_REFCOUNT_DEFS_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_reg_track.c b/drivers/gpu/arm/bifrost/mali_kbase_reg_track.c
index e490a2a3d179..e30857c7b35d 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_reg_track.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_reg_track.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -553,8 +553,8 @@ int kbase_add_va_region(struct kbase_context *kctx, struct kbase_va_region *reg,
 {
 	int err = 0;
 	struct kbase_device *kbdev = kctx->kbdev;
-	const size_t cpu_va_bits = kbase_get_num_cpu_va_bits(kctx);
-	const size_t gpu_pc_bits = kbdev->gpu_props.log2_program_counter_size;
+	const int cpu_va_bits = kbase_get_num_cpu_va_bits(kctx);
+	const int gpu_pc_bits = kbdev->gpu_props.log2_program_counter_size;
 
 	KBASE_DEBUG_ASSERT(kctx != NULL);
 	KBASE_DEBUG_ASSERT(reg != NULL);
@@ -648,7 +648,7 @@ int kbase_add_va_region_rbtree(struct kbase_device *kbdev, struct kbase_va_regio
 		} else if (!kbase_is_region_free(tmp)) {
 			dev_warn(
 				dev,
-				"!(tmp->flags & KBASE_REG_FREE): tmp->start_pfn=0x%llx tmp->flags=0x%llx tmp->nr_pages=0x%zx gpu_pfn=0x%llx nr_pages=0x%zx\n",
+				"!(tmp->flags & KBASE_REG_FREE): tmp->start_pfn=0x%llx tmp->flags=0x%lx tmp->nr_pages=0x%zx gpu_pfn=0x%llx nr_pages=0x%zx\n",
 				tmp->start_pfn, tmp->flags, tmp->nr_pages, gpu_pfn, nr_pages);
 			err = -ENOMEM;
 			goto exit;
@@ -1367,7 +1367,7 @@ struct kbase_va_region *kbase_alloc_free_region(struct kbase_reg_zone *zone, u64
 		return NULL;
 
 	kbase_refcount_set(&new_reg->va_refcnt, 1);
-	atomic64_set(&new_reg->no_user_free_count, 0);
+	atomic_set(&new_reg->no_user_free_count, 0);
 	new_reg->cpu_alloc = NULL; /* no alloc bound yet */
 	new_reg->gpu_alloc = NULL; /* no alloc bound yet */
 	new_reg->rbtree = &zone->reg_rbtree;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.c b/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.c
index 14143f0bdb30..d3c030fe6915 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.c
@@ -133,7 +133,7 @@ void kbase_io_history_dump(struct kbase_device *kbdev)
 		char const access = (io->addr & 1) ? 'w' : 'r';
 
 		dev_err(kbdev->dev, "%6zu: %c: reg 0x%16pK val %08x\n", i, access,
-			(void *)(io->addr & ~(uintptr_t)0x1), io->value);
+			(void *)(io->addr & ~0x1), io->value);
 	}
 
 	spin_unlock_irqrestore(&h->lock, flags);
@@ -191,7 +191,7 @@ static int regs_history_show(struct seq_file *sfile, void *data)
 		char const access = (io->addr & 1) ? 'w' : 'r';
 
 		seq_printf(sfile, "%6zu: %c: reg 0x%16pK val %08x\n", i, access,
-			   (void *)(io->addr & ~(uintptr_t)0x1), io->value);
+			   (void *)(io->addr & ~0x1), io->value);
 	}
 
 	spin_unlock_irqrestore(&h->lock, flags);
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_reset_gpu.h b/drivers/gpu/arm/bifrost/mali_kbase_reset_gpu.h
index b7b2cf8cb2f4..035b1b2bda3e 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_reset_gpu.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_reset_gpu.h
@@ -22,8 +22,6 @@
 #ifndef _KBASE_RESET_GPU_H_
 #define _KBASE_RESET_GPU_H_
 
-struct kbase_device;
-
 /**
  * kbase_reset_gpu_prevent_and_wait - Prevent GPU resets from starting whilst
  *                                    the current thread is accessing the GPU,
@@ -238,7 +236,7 @@ int kbase_reset_gpu_silent(struct kbase_device *kbdev);
 bool kbase_reset_gpu_is_active(struct kbase_device *kbdev);
 
 /**
- * kbase_reset_gpu_is_not_pending - Reports if the GPU reset isn't pending
+ * kbase_reset_gpu_not_pending - Reports if the GPU reset isn't pending
  *
  * @kbdev: Device pointer
  *
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_smc.h b/drivers/gpu/arm/bifrost/mali_kbase_smc.h
index 7f298799a4ba..b9f224f7ae51 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_smc.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_smc.h
@@ -24,13 +24,13 @@
 
 #if IS_ENABLED(CONFIG_ARM64)
 
-#include <linux/types.h>
+#include <mali_kbase.h>
 
-#define SMC_FAST_CALL (1U << 31)
-#define SMC_64 (1U << 30)
+#define SMC_FAST_CALL (1 << 31)
+#define SMC_64 (1 << 30)
 
 #define SMC_OEN_OFFSET 24
-#define SMC_OEN_MASK (0x3FU << SMC_OEN_OFFSET) /* 6 bits */
+#define SMC_OEN_MASK (0x3F << SMC_OEN_OFFSET) /* 6 bits */
 #define SMC_OEN_SIP (2 << SMC_OEN_OFFSET)
 #define SMC_OEN_STD (4 << SMC_OEN_OFFSET)
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_softjobs.c b/drivers/gpu/arm/bifrost/mali_kbase_softjobs.c
index bae1630c94a9..bbd756dbf840 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_softjobs.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_softjobs.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2011-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -83,8 +83,7 @@ static void kbasep_add_waiting_with_timeout(struct kbase_jd_atom *katom)
 
 	/* Schedule timeout of this atom after a period if it is not active */
 	if (!timer_pending(&kctx->soft_job_timeout)) {
-		unsigned int timeout_ms =
-			(unsigned int)atomic_read(&kctx->kbdev->js_data.soft_job_timeout_ms);
+		int timeout_ms = atomic_read(&kctx->kbdev->js_data.soft_job_timeout_ms);
 		mod_timer(&kctx->soft_job_timeout, jiffies + msecs_to_jiffies(timeout_ms));
 	}
 }
@@ -143,8 +142,9 @@ static int kbase_dump_cpu_gpu_time(struct kbase_jd_atom *katom)
 	 * delay suspend until we process the atom (which may be at the end of a
 	 * long chain of dependencies
 	 */
-	if (kbase_has_arbiter(kctx->kbdev))
-		atomic_inc(&kctx->kbdev->pm.gpu_users_waiting);
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	atomic_inc(&kctx->kbdev->pm.gpu_users_waiting);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 	pm_active_err = kbase_pm_context_active_handle_suspend(
 		kctx->kbdev, KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE);
 	if (pm_active_err) {
@@ -162,14 +162,17 @@ static int kbase_dump_cpu_gpu_time(struct kbase_jd_atom *katom)
 		kbasep_add_waiting_soft_job(katom);
 
 		return pm_active_err;
-	} else if (kbase_has_arbiter(kctx->kbdev))
+	}
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	else
 		atomic_dec(&kctx->kbdev->pm.gpu_users_waiting);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 	kbase_backend_get_gpu_time(kctx->kbdev, &cycle_counter, &system_time, &ts);
 
 	kbase_pm_context_idle(kctx->kbdev);
 
-	data.sec = (__u64)ts.tv_sec;
+	data.sec = ts.tv_sec;
 	data.usec = ts.tv_nsec / 1000;
 	data.system_time = system_time;
 	data.cycle_counter = cycle_counter;
@@ -549,7 +552,7 @@ static int kbase_debug_copy_prepare(struct kbase_jd_atom *katom)
 		goto out_cleanup;
 	}
 
-	ret = copy_from_user(user_buffers, user_structs, size_mul(sizeof(*user_buffers), nr));
+	ret = copy_from_user(user_buffers, user_structs, sizeof(*user_buffers) * nr);
 	if (ret) {
 		ret = -EFAULT;
 		goto out_cleanup;
@@ -609,7 +612,7 @@ static int kbase_debug_copy_prepare(struct kbase_jd_atom *katom)
 			/* Adjust number of pages, so that we only attempt to
 			 * release pages in the array that we know are valid.
 			 */
-			buffers[i].nr_pages = (unsigned int)pinned_pages;
+			buffers[i].nr_pages = pinned_pages;
 
 			ret = -EINVAL;
 			goto out_cleanup;
@@ -623,8 +626,7 @@ static int kbase_debug_copy_prepare(struct kbase_jd_atom *katom)
 
 		kbase_gpu_vm_lock(katom->kctx);
 		reg = kbase_region_tracker_find_region_enclosing_address(
-			katom->kctx,
-			user_extres.ext_resource & ~(__u64)BASE_EXT_RES_ACCESS_EXCLUSIVE);
+			katom->kctx, user_extres.ext_resource & ~BASE_EXT_RES_ACCESS_EXCLUSIVE);
 
 		if (kbase_is_region_invalid_or_free(reg) || reg->gpu_alloc == NULL) {
 			ret = -EINVAL;
@@ -666,7 +668,7 @@ static int kbase_debug_copy_prepare(struct kbase_jd_atom *katom)
 				if (ret < 0)
 					buffers[i].nr_extres_pages = 0;
 				else
-					buffers[i].nr_extres_pages = (unsigned int)ret;
+					buffers[i].nr_extres_pages = ret;
 
 				goto out_unlock;
 			}
@@ -1231,7 +1233,7 @@ static int kbase_jit_free_prepare(struct kbase_jd_atom *katom)
 			goto free_info;
 		}
 
-		if (copy_from_user(ids, data, size_mul(sizeof(*ids), count)) != 0) {
+		if (copy_from_user(ids, data, sizeof(*ids) * count) != 0) {
 			ret = -EINVAL;
 			goto free_info;
 		}
@@ -1402,9 +1404,9 @@ static void kbase_ext_res_process(struct kbase_jd_atom *katom, bool map)
 	for (i = 0; i < ext_res->count; i++) {
 		u64 gpu_addr;
 
-		gpu_addr = ext_res->ext_res[i].ext_resource & ~(__u64)BASE_EXT_RES_ACCESS_EXCLUSIVE;
+		gpu_addr = ext_res->ext_res[i].ext_resource & ~BASE_EXT_RES_ACCESS_EXCLUSIVE;
 		if (map) {
-			if (!kbase_sticky_resource_acquire(katom->kctx, gpu_addr, NULL))
+			if (!kbase_sticky_resource_acquire(katom->kctx, gpu_addr))
 				goto failed_loop;
 		} else {
 			if (!kbase_sticky_resource_release_force(katom->kctx, NULL, gpu_addr))
@@ -1429,7 +1431,7 @@ static void kbase_ext_res_process(struct kbase_jd_atom *katom, bool map)
 failed_loop:
 	while (i > 0) {
 		u64 const gpu_addr = ext_res->ext_res[i - 1].ext_resource &
-				     ~(__u64)BASE_EXT_RES_ACCESS_EXCLUSIVE;
+				     ~BASE_EXT_RES_ACCESS_EXCLUSIVE;
 
 		kbase_sticky_resource_release_force(katom->kctx, NULL, gpu_addr);
 
@@ -1684,8 +1686,9 @@ void kbase_resume_suspended_soft_jobs(struct kbase_device *kbdev)
 		if (kbase_process_soft_job(katom_iter) == 0) {
 			kbase_finish_soft_job(katom_iter);
 			resched |= kbase_jd_done_nolock(katom_iter, true);
-			if (kbase_has_arbiter(kctx->kbdev))
-				atomic_dec(&kbdev->pm.gpu_users_waiting);
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+			atomic_dec(&kbdev->pm.gpu_users_waiting);
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 		}
 		mutex_unlock(&kctx->jctx.lock);
 	}
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_sync.h b/drivers/gpu/arm/bifrost/mali_kbase_sync.h
index d871426f8a8a..ff5206d8d395 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_sync.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_sync.h
@@ -34,12 +34,7 @@
 #include <linux/sync_file.h>
 #endif
 
-#include <linux/version_compat_defs.h>
-
-#if !MALI_USE_CSF
-struct kbase_jd_atom;
-struct work_struct;
-#endif
+#include "mali_kbase.h"
 
 /**
  * struct kbase_sync_fence_info - Information about a fence
@@ -180,8 +175,12 @@ int kbase_sync_fence_out_info_get(struct kbase_jd_atom *katom, struct kbase_sync
 #endif /* !MALI_USE_CSF */
 
 #if IS_ENABLED(CONFIG_SYNC_FILE)
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+void kbase_sync_fence_info_get(struct fence *fence, struct kbase_sync_fence_info *info);
+#else
 void kbase_sync_fence_info_get(struct dma_fence *fence, struct kbase_sync_fence_info *info);
 #endif
+#endif
 
 /**
  * kbase_sync_status_string() - Get string matching @status
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_sync_file.c b/drivers/gpu/arm/bifrost/mali_kbase_sync_file.c
index f0337e61a17e..aa4bf980e2bb 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_sync_file.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_sync_file.c
@@ -22,9 +22,6 @@
 /*
  * Code for supporting explicit Linux fences (CONFIG_SYNC_FILE)
  */
-#include "mali_kbase_sync.h"
-#include "mali_kbase_fence.h"
-#include "mali_kbase.h"
 
 #include <linux/sched.h>
 #include <linux/fdtable.h>
@@ -36,7 +33,9 @@
 #include <linux/uaccess.h>
 #include <linux/sync_file.h>
 #include <linux/slab.h>
-#include <linux/version_compat_defs.h>
+#include "mali_kbase_sync.h"
+#include "mali_kbase_fence.h"
+#include "mali_kbase.h"
 
 static const struct file_operations stream_fops = { .owner = THIS_MODULE };
 
@@ -55,7 +54,11 @@ int kbase_sync_fence_stream_create(const char *name, int *const out_fd)
 #if !MALI_USE_CSF
 int kbase_sync_fence_out_create(struct kbase_jd_atom *katom, int stream_fd)
 {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence;
+#else
 	struct dma_fence *fence;
+#endif
 	struct sync_file *sync_file;
 	int fd;
 
@@ -91,14 +94,18 @@ int kbase_sync_fence_out_create(struct kbase_jd_atom *katom, int stream_fd)
 		return fd;
 	}
 
-	fd_install((unsigned int)fd, sync_file->file);
+	fd_install(fd, sync_file->file);
 
 	return fd;
 }
 
 int kbase_sync_fence_in_from_fd(struct kbase_jd_atom *katom, int fd)
 {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence = sync_file_get_fence(fd);
+#else
 	struct dma_fence *fence = sync_file_get_fence(fd);
+#endif
 
 	lockdep_assert_held(&katom->kctx->jctx.lock);
 
@@ -114,14 +121,18 @@ int kbase_sync_fence_in_from_fd(struct kbase_jd_atom *katom, int fd)
 
 int kbase_sync_fence_validate(int fd)
 {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence = sync_file_get_fence(fd);
+#else
 	struct dma_fence *fence = sync_file_get_fence(fd);
+#endif
 
 	if (!fence)
 		return -EINVAL;
 
 	dma_fence_put(fence);
 
-	return 0;
+	return 0; /* valid */
 }
 
 #if !MALI_USE_CSF
@@ -144,7 +155,11 @@ enum base_jd_event_code kbase_sync_fence_out_trigger(struct kbase_jd_atom *katom
 	return (result != 0) ? BASE_JD_EVENT_JOB_CANCELLED : BASE_JD_EVENT_DONE;
 }
 
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+static void kbase_fence_wait_callback(struct fence *fence, struct fence_cb *cb)
+#else
 static void kbase_fence_wait_callback(struct dma_fence *fence, struct dma_fence_cb *cb)
+#endif
 {
 	struct kbase_jd_atom *katom = container_of(cb, struct kbase_jd_atom, dma_fence.fence_cb);
 	struct kbase_context *kctx = katom->kctx;
@@ -176,7 +191,11 @@ static void kbase_fence_wait_callback(struct dma_fence *fence, struct dma_fence_
 int kbase_sync_fence_in_wait(struct kbase_jd_atom *katom)
 {
 	int err;
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence;
+#else
 	struct dma_fence *fence;
+#endif
 
 	lockdep_assert_held(&katom->kctx->jctx.lock);
 
@@ -293,9 +312,12 @@ void kbase_sync_fence_in_remove(struct kbase_jd_atom *katom)
 }
 #endif /* !MALI_USE_CSF */
 
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+void kbase_sync_fence_info_get(struct fence *fence, struct kbase_sync_fence_info *info)
+#else
 void kbase_sync_fence_info_get(struct dma_fence *fence, struct kbase_sync_fence_info *info)
+#endif
 {
-	int status;
 	info->fence = fence;
 
 	/* Translate into the following status, with support for error handling:
@@ -303,14 +325,21 @@ void kbase_sync_fence_info_get(struct dma_fence *fence, struct kbase_sync_fence_
 	 * 0 : active
 	 * 1 : signaled
 	 */
-	status = dma_fence_get_status(fence);
-
-	if (status < 0)
-		info->status = status; /* signaled with error */
-	else if (status > 0)
-		info->status = 1; /* signaled with success */
-	else
+	if (dma_fence_is_signaled(fence)) {
+#if (KERNEL_VERSION(4, 11, 0) <= LINUX_VERSION_CODE || \
+     (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE && \
+      KERNEL_VERSION(4, 9, 68) <= LINUX_VERSION_CODE))
+		int status = fence->error;
+#else
+		int status = fence->status;
+#endif
+		if (status < 0)
+			info->status = status; /* signaled with error */
+		else
+			info->status = 1; /* signaled with success */
+	} else {
 		info->status = 0; /* still active (unsignaled) */
+	}
 
 #if (KERNEL_VERSION(5, 1, 0) > LINUX_VERSION_CODE)
 	scnprintf(info->name, sizeof(info->name), "%llu#%u", fence->context, fence->seqno);
@@ -322,7 +351,11 @@ void kbase_sync_fence_info_get(struct dma_fence *fence, struct kbase_sync_fence_
 #if !MALI_USE_CSF
 int kbase_sync_fence_in_info_get(struct kbase_jd_atom *katom, struct kbase_sync_fence_info *info)
 {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence;
+#else
 	struct dma_fence *fence;
+#endif
 
 	fence = kbase_fence_in_get(katom);
 	if (!fence)
@@ -337,7 +370,11 @@ int kbase_sync_fence_in_info_get(struct kbase_jd_atom *katom, struct kbase_sync_
 
 int kbase_sync_fence_out_info_get(struct kbase_jd_atom *katom, struct kbase_sync_fence_info *info)
 {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence;
+#else
 	struct dma_fence *fence;
+#endif
 
 	fence = kbase_fence_out_get(katom);
 	if (!fence)
diff --git a/drivers/gpu/arm/bifrost/mmu/Kbuild b/drivers/gpu/arm/bifrost/mmu/Kbuild
index 3c3defdb88e9..416432397b5c 100644
--- a/drivers/gpu/arm/bifrost/mmu/Kbuild
+++ b/drivers/gpu/arm/bifrost/mmu/Kbuild
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2021-2024 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2021 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -21,15 +21,10 @@
 bifrost_kbase-y += \
     mmu/mali_kbase_mmu.o \
     mmu/mali_kbase_mmu_hw_direct.o \
-    mmu/mali_kbase_mmu_faults_decoder_luts.o \
-    mmu/mali_kbase_mmu_faults_decoder.o \
     mmu/mali_kbase_mmu_mode_aarch64.o
 
 ifeq ($(CONFIG_MALI_CSF_SUPPORT),y)
-    bifrost_kbase-y += mmu/backend/mali_kbase_mmu_csf.o \
-    mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.o
+    bifrost_kbase-y += mmu/backend/mali_kbase_mmu_csf.o
 else
-    bifrost_kbase-y += mmu/backend/mali_kbase_mmu_jm.o \
-	mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.o
-
+    bifrost_kbase-y += mmu/backend/mali_kbase_mmu_jm.o
 endif
diff --git a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_csf.c b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_csf.c
index 196d481d6827..8d6eb5fb651f 100644
--- a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_csf.c
+++ b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_csf.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -29,7 +29,6 @@
 #include <mali_kbase_reset_gpu.h>
 #include <mali_kbase_as_fault_debugfs.h>
 #include <mmu/mali_kbase_mmu_internal.h>
-#include <mmu/mali_kbase_mmu_faults_decoder.h>
 
 void kbase_mmu_get_as_setup(struct kbase_mmu_table *mmut, struct kbase_mmu_setup *const setup)
 {
@@ -47,7 +46,7 @@ void kbase_mmu_get_as_setup(struct kbase_mmu_table *mmut, struct kbase_mmu_setup
 		(KBASE_MEMATTR_AARCH64_SHARED << (KBASE_MEMATTR_INDEX_SHARED * 8));
 
 	setup->transtab = (u64)mmut->pgd & AS_TRANSTAB_BASE_MASK;
-	setup->transcfg = AS_TRANSCFG_MODE_SET(0ULL, AS_TRANSCFG_MODE_AARCH64_4K);
+	setup->transcfg = AS_TRANSCFG_MODE_SET(0, AS_TRANSCFG_MODE_AARCH64_4K);
 }
 
 /**
@@ -97,30 +96,23 @@ void kbase_mmu_report_mcu_as_fault_and_reset(struct kbase_device *kbdev, struct
 	u32 exception_type = fault->status & 0xFF;
 	u32 access_type = (fault->status >> 8) & 0x3;
 	u32 source_id = (fault->status >> 16);
-	u32 as_no;
+	int as_no;
 
 	/* terminal fault, print info about the fault */
-	if (kbdev->gpu_props.gpu_id.product_model < GPU_ID_MODEL_MAKE(14, 0)) {
-		dev_err(kbdev->dev,
-			"Unexpected Page fault in firmware address space at VA 0x%016llX\n"
-			"raw fault status: 0x%X\n"
-			"exception type 0x%X: %s\n"
-			"access type 0x%X: %s\n"
-			"source id 0x%X (core_id:utlb:IR 0x%X:0x%X:0x%X): %s, %s\n",
-			fault->addr, fault->status, exception_type,
-			kbase_gpu_exception_name(exception_type), access_type,
-			kbase_gpu_access_type_name(fault->status), source_id,
-			FAULT_SOURCE_ID_CORE_ID_GET(source_id),
-			FAULT_SOURCE_ID_UTLB_ID_GET(source_id),
-			fault_source_id_internal_requester_get(kbdev, source_id),
-			fault_source_id_core_type_description_get(kbdev, source_id),
-			fault_source_id_internal_requester_get_str(kbdev, source_id, access_type));
-	}
+	dev_err(kbdev->dev,
+		"Unexpected Page fault in firmware address space at VA 0x%016llX\n"
+		"raw fault status: 0x%X\n"
+		"exception type 0x%X: %s\n"
+		"access type 0x%X: %s\n"
+		"source id 0x%X\n",
+		fault->addr, fault->status, exception_type,
+		kbase_gpu_exception_name(exception_type), access_type,
+		kbase_gpu_access_type_name(fault->status), source_id);
 
 	kbase_debug_csf_fault_notify(kbdev, NULL, DF_GPU_PAGE_FAULT);
 
 	/* Report MMU fault for all address spaces (except MCU_AS_NR) */
-	for (as_no = 1u; as_no < (u32)kbdev->nr_hw_address_spaces; as_no++)
+	for (as_no = 1; as_no < kbdev->nr_hw_address_spaces; as_no++)
 		submit_work_pagefault(kbdev, as_no, fault);
 
 	/* GPU reset is required to recover */
@@ -135,43 +127,37 @@ void kbase_gpu_report_bus_fault_and_kill(struct kbase_context *kctx, struct kbas
 {
 	struct kbase_device *kbdev = kctx->kbdev;
 	u32 const status = fault->status;
-	unsigned int exception_type = (status & GPU_FAULTSTATUS_EXCEPTION_TYPE_MASK) >>
-				      GPU_FAULTSTATUS_EXCEPTION_TYPE_SHIFT;
-	unsigned int access_type = (status & GPU_FAULTSTATUS_ACCESS_TYPE_MASK) >>
-				   GPU_FAULTSTATUS_ACCESS_TYPE_SHIFT;
-	unsigned int source_id = (status & GPU_FAULTSTATUS_SOURCE_ID_MASK) >>
-				 GPU_FAULTSTATUS_SOURCE_ID_SHIFT;
+	int exception_type = (status & GPU_FAULTSTATUS_EXCEPTION_TYPE_MASK) >>
+			     GPU_FAULTSTATUS_EXCEPTION_TYPE_SHIFT;
+	int access_type = (status & GPU_FAULTSTATUS_ACCESS_TYPE_MASK) >>
+			  GPU_FAULTSTATUS_ACCESS_TYPE_SHIFT;
+	int source_id = (status & GPU_FAULTSTATUS_SOURCE_ID_MASK) >>
+			GPU_FAULTSTATUS_SOURCE_ID_SHIFT;
 	const char *addr_valid = (status & GPU_FAULTSTATUS_ADDRESS_VALID_MASK) ? "true" : "false";
-	unsigned int as_no = as->number;
+	int as_no = as->number;
 	unsigned long flags;
 	const uintptr_t fault_addr = fault->addr;
 
 	/* terminal fault, print info about the fault */
-	if (kbdev->gpu_props.gpu_id.product_model < GPU_ID_MODEL_MAKE(14, 0)) {
-		dev_err(kbdev->dev,
-			"GPU bus fault in AS%u at PA %pK\n"
-			"PA_VALID: %s\n"
-			"raw fault status: 0x%X\n"
-			"exception type 0x%X: %s\n"
-			"access type 0x%X: %s\n"
-			"source id 0x%X (core_id:utlb:IR 0x%X:0x%X:0x%X): %s, %s\n"
-			"pid: %d\n",
-			as_no, (void *)fault_addr, addr_valid, status, exception_type,
-			kbase_gpu_exception_name(exception_type), access_type,
-			kbase_gpu_access_type_name(access_type), source_id,
-			FAULT_SOURCE_ID_CORE_ID_GET(source_id),
-			FAULT_SOURCE_ID_UTLB_ID_GET(source_id),
-			fault_source_id_internal_requester_get(kbdev, source_id),
-			fault_source_id_core_type_description_get(kbdev, source_id),
-			fault_source_id_internal_requester_get_str(kbdev, source_id, access_type),
-			kctx->pid);
-	}
+	dev_err(kbdev->dev,
+		"GPU bus fault in AS%d at PA %pK\n"
+		"PA_VALID: %s\n"
+		"raw fault status: 0x%X\n"
+		"exception type 0x%X: %s\n"
+		"access type 0x%X: %s\n"
+		"source id 0x%X\n"
+		"pid: %d\n",
+		as_no, (void *)fault_addr, addr_valid, status, exception_type,
+		kbase_gpu_exception_name(exception_type), access_type,
+		kbase_gpu_access_type_name(access_type), source_id, kctx->pid);
 
 	/* AS transaction begin */
+	mutex_lock(&kbdev->mmu_hw_mutex);
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	kbase_mmu_disable(kctx);
 	kbase_ctx_flag_set(kctx, KCTX_AS_DISABLED_ON_FAULT);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	mutex_unlock(&kbdev->mmu_hw_mutex);
 
 	/* Switching to UNMAPPED mode above would have enabled the firmware to
 	 * recover from the fault (if the memory access was made by firmware)
@@ -196,63 +182,52 @@ void kbase_mmu_report_fault_and_kill(struct kbase_context *kctx, struct kbase_as
 				     const char *reason_str, struct kbase_fault *fault)
 {
 	unsigned long flags;
-	struct kbase_device *kbdev = kctx->kbdev;
+	unsigned int exception_type;
+	unsigned int access_type;
+	unsigned int source_id;
+	int as_no;
+	struct kbase_device *kbdev;
+	const u32 status = fault->status;
+
+	as_no = as->number;
+	kbdev = kctx->kbdev;
 
 	/* Make sure the context was active */
 	if (WARN_ON(atomic_read(&kctx->refcount) <= 0))
 		return;
 
-	if (!kbase_ctx_flag(kctx, KCTX_PAGE_FAULT_REPORT_SKIP)) {
-		const u32 status = fault->status;
-		/* decode the fault status */
-		unsigned int exception_type = AS_FAULTSTATUS_EXCEPTION_TYPE_GET(status);
-		unsigned int access_type = AS_FAULTSTATUS_ACCESS_TYPE_GET(status);
-		unsigned int source_id = AS_FAULTSTATUS_SOURCE_ID_GET(status);
-		unsigned int as_no = as->number;
-
-		/* terminal fault, print info about the fault */
-		if (kbdev->gpu_props.gpu_id.product_model < GPU_ID_MODEL_MAKE(14, 0)) {
-			dev_err(kbdev->dev,
-				"Unhandled Page fault in AS%u at VA 0x%016llX\n"
-				"Reason: %s\n"
-				"raw fault status: 0x%X\n"
-				"exception type 0x%X: %s\n"
-				"access type 0x%X: %s\n"
-				"source id 0x%X (core_id:utlb:IR 0x%X:0x%X:0x%X): %s, %s\n"
-				"pid: %d\n",
-				as_no, fault->addr, reason_str, status, exception_type,
-				kbase_gpu_exception_name(exception_type), access_type,
-				kbase_gpu_access_type_name(status), source_id,
-				FAULT_SOURCE_ID_CORE_ID_GET(source_id),
-				FAULT_SOURCE_ID_UTLB_ID_GET(source_id),
-				fault_source_id_internal_requester_get(kbdev, source_id),
-				fault_source_id_core_type_description_get(kbdev, source_id),
-				fault_source_id_internal_requester_get_str(kbdev, source_id,
-									   access_type),
-				kctx->pid);
-		}
-	}
+	/* decode the fault status */
+	exception_type = AS_FAULTSTATUS_EXCEPTION_TYPE_GET(status);
+	access_type = AS_FAULTSTATUS_ACCESS_TYPE_GET(status);
+	source_id = AS_FAULTSTATUS_SOURCE_ID_GET(status);
+
+	/* terminal fault, print info about the fault */
+	dev_err(kbdev->dev,
+		"Unhandled Page fault in AS%d at VA 0x%016llX\n"
+		"Reason: %s\n"
+		"raw fault status: 0x%X\n"
+		"exception type 0x%X: %s\n"
+		"access type 0x%X: %s\n"
+		"source id 0x%X\n"
+		"pid: %d\n",
+		as_no, fault->addr, reason_str, status, exception_type,
+		kbase_gpu_exception_name(exception_type), access_type,
+		kbase_gpu_access_type_name(status), source_id, kctx->pid);
 
 	/* AS transaction begin */
+	mutex_lock(&kbdev->mmu_hw_mutex);
 
 	/* switch to UNMAPPED mode,
 	 * will abort all jobs and stop any hw counter dumping
 	 */
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	/* Update the page fault counter value in firmware visible memory, just before disabling
-	 * the MMU which would in turn unblock the MCU firmware.
-	 */
-	if (kbdev->csf.page_fault_cnt_ptr) {
-		spin_lock(&kbdev->mmu_mask_change);
-		*kbdev->csf.page_fault_cnt_ptr = ++kbdev->csf.page_fault_cnt;
-		spin_unlock(&kbdev->mmu_mask_change);
-	}
 	kbase_mmu_disable(kctx);
 	kbase_ctx_flag_set(kctx, KCTX_AS_DISABLED_ON_FAULT);
 	kbase_debug_csf_fault_notify(kbdev, kctx, DF_GPU_PAGE_FAULT);
 	kbase_csf_ctx_report_page_fault_for_active_groups(kctx, fault);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
+	mutex_unlock(&kbdev->mmu_hw_mutex);
 	/* AS transaction end */
 
 	/* Switching to UNMAPPED mode above would have enabled the firmware to
@@ -277,9 +252,7 @@ void kbase_mmu_report_fault_and_kill(struct kbase_context *kctx, struct kbase_as
  * @as:		The address space that has the fault
  * @fault:	Data relating to the fault
  *
- * This function will process a fault on a specific address space.
- * The function must be called with the ref_count of the kctx already increased/acquired.
- * If it fails to queue the work, the ref_count will be decreased.
+ * This function will process a fault on a specific address space
  */
 static void kbase_mmu_interrupt_process(struct kbase_device *kbdev, struct kbase_context *kctx,
 					struct kbase_as *as, struct kbase_fault *fault)
@@ -318,18 +291,11 @@ static void kbase_mmu_interrupt_process(struct kbase_device *kbdev, struct kbase
 		 * We need to switch to UNMAPPED mode - but we do this in a
 		 * worker so that we can sleep
 		 */
-		if (!queue_work(as->pf_wq, &as->work_busfault)) {
-			dev_warn(kbdev->dev, "Bus fault is already pending for as %u", as->number);
-			kbase_ctx_sched_release_ctx(kctx);
-		} else {
-			atomic_inc(&kbdev->faults_pending);
-		}
+		WARN_ON(!queue_work(as->pf_wq, &as->work_busfault));
+		atomic_inc(&kbdev->faults_pending);
 	} else {
-		if (!queue_work(as->pf_wq, &as->work_pagefault)) {
-			dev_warn(kbdev->dev, "Page fault is already pending for as %u", as->number);
-			kbase_ctx_sched_release_ctx(kctx);
-		} else
-			atomic_inc(&kbdev->faults_pending);
+		WARN_ON(!queue_work(as->pf_wq, &as->work_pagefault));
+		atomic_inc(&kbdev->faults_pending);
 	}
 }
 
@@ -384,7 +350,7 @@ void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat)
 
 	while (pf_bits) {
 		struct kbase_context *kctx;
-		unsigned int as_no = (unsigned int)ffs((int)pf_bits) - 1;
+		int as_no = ffs(pf_bits) - 1;
 		struct kbase_as *as = &kbdev->as[as_no];
 		struct kbase_fault *fault = &as->pf_data;
 
@@ -440,6 +406,15 @@ void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat)
 	spin_unlock_irqrestore(&kbdev->mmu_mask_change, flags);
 }
 
+int kbase_mmu_switch_to_ir(struct kbase_context *const kctx, struct kbase_va_region *const reg)
+{
+	CSTD_UNUSED(kctx);
+	CSTD_UNUSED(reg);
+
+	/* Can't soft-stop the provoking job */
+	return -EPERM;
+}
+
 /**
  * kbase_mmu_gpu_fault_worker() - Process a GPU fault for the device.
  *
@@ -535,7 +510,7 @@ void kbase_mmu_gpu_fault_interrupt(struct kbase_device *kbdev, u32 status, u32 a
 		 * the address space is invalid or it's MCU address space.
 		 */
 		for (as = 1; as < kbdev->nr_hw_address_spaces; as++)
-			submit_work_gpufault(kbdev, status, (u32)as, address);
+			submit_work_gpufault(kbdev, status, as, address);
 	} else
 		submit_work_gpufault(kbdev, status, as_nr, address);
 }
diff --git a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.c b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.c
deleted file mode 100644
index d8eec91ba887..000000000000
--- a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.c
+++ /dev/null
@@ -1,139 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
-/*
- *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-/**
- * DOC: Base kernel MMU faults decoder for CSF GPUs.
- */
-
-#include <mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.h>
-
-#define GPU_ID_ARCH_ID_MAJOR_GET(gpu_id) ((gpu_id >> 16) & 0xFF)
-#define GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id) (gpu_id & 0xFFFF)
-#define NELEMS(s) (sizeof(s) / sizeof((s)[0]))
-
-struct decode_lut_element {
-	u16 arch_minor_rev;
-	u16 key;
-	const char *text;
-};
-
-static const char *decode_lut_element_lookup(u16 arch_minor_rev, u16 key,
-					     struct decode_lut_element *decode_element_lut,
-					     unsigned int lut_len)
-{
-	struct decode_lut_element *p;
-
-	for (p = decode_element_lut; p < decode_element_lut + lut_len; p++) {
-		if (p->key == key &&
-		    (p->arch_minor_rev == 0xffff || p->arch_minor_rev == arch_minor_rev))
-			break;
-	}
-	if (p < decode_element_lut + lut_len)
-		return p->text;
-	else
-		return "unknown";
-}
-
-/* Auto-generated code: DO NOT MODIFY! */
-
-static struct decode_lut_element lut_fault_source_csf_r_t_major_10[] = {
-	{ 0xFFFF, 0, "pref0" },
-	{ 0xFFFF, 4, "iter0" },
-	{ 0xFFFF, 12, "lsu" },
-	{ 0xFFFF, 13, "mcu" },
-};
-
-static struct decode_lut_element lut_fault_source_csf_r_t_major_11[] = {
-	{ 0xFFFF, 0, "pref0" },
-	{ 0xFFFF, 4, "iter0" },
-	{ 0xFFFF, 12, "lsu" },
-	{ 0xFFFF, 13, "mcu" },
-};
-
-static struct decode_lut_element lut_fault_source_csf_r_t_major_12[] = {
-	{ 0xFFFF, 0, "pref0" },
-	{ 0xFFFF, 4, "iter0" },
-	{ 0xFFFF, 12, "lsu" },
-	{ 0xFFFF, 13, "mcu" },
-};
-
-static struct decode_lut_element lut_fault_source_csf_w_t_major_10[] = {
-	{ 0xFFFF, 8, "pcb0" },
-	{ 0xFFFF, 12, "lsu" },
-	{ 0xFFFF, 13, "mcu" },
-};
-
-static struct decode_lut_element lut_fault_source_csf_w_t_major_11[] = {
-	{ 0xFFFF, 8, "pcb0" },
-	{ 0xFFFF, 12, "lsu" },
-	{ 0xFFFF, 13, "mcu" },
-};
-
-static struct decode_lut_element lut_fault_source_csf_w_t_major_12[] = {
-	{ 0xFFFF, 8, "pcb0" },
-	{ 0xFFFF, 12, "lsu" },
-	{ 0xFFFF, 13, "mcu" },
-};
-
-
-const char *decode_fault_source_csf_r_t(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 10:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_csf_r_t_major_10,
-						NELEMS(lut_fault_source_csf_r_t_major_10));
-		break;
-	case 11:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_csf_r_t_major_11,
-						NELEMS(lut_fault_source_csf_r_t_major_11));
-		break;
-	case 12:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_csf_r_t_major_12,
-						NELEMS(lut_fault_source_csf_r_t_major_12));
-		break;
-	}
-	return ret;
-}
-
-const char *decode_fault_source_csf_w_t(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 10:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_csf_w_t_major_10,
-						NELEMS(lut_fault_source_csf_w_t_major_10));
-		break;
-	case 11:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_csf_w_t_major_11,
-						NELEMS(lut_fault_source_csf_w_t_major_11));
-		break;
-	case 12:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_csf_w_t_major_12,
-						NELEMS(lut_fault_source_csf_w_t_major_12));
-		break;
-	}
-	return ret;
-}
diff --git a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.h b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.h
deleted file mode 100644
index 04f5c02ccc3d..000000000000
--- a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.h
+++ /dev/null
@@ -1,50 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
-/*
- *
- * (C) COPYRIGHT 2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-#ifndef _MALI_KBASE_MMU_FAULTS_DECODER_LUTS_CSF_H_
-#define _MALI_KBASE_MMU_FAULTS_DECODER_LUTS_CSF_H_
-#include <linux/types.h>
-
-/**
- * decode_fault_source_csf_r_t() - Get internal requester of a
- * fault in a human readable format.
- *
- * @idx: Internal requester part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: Internal requester of a fault in a human readable format for a read
- * operation on a CSF core.
- */
-const char *decode_fault_source_csf_r_t(u16 idx, u32 gpu_id);
-
-/**
- * decode_fault_source_csf_w_t() - Get internal requester of a
- * fault in a human readable format.
- *
- * @idx: Internal requester part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: Internal requester of a fault in a human readable format for a write
- * operation on a CSF core.
- */
-const char *decode_fault_source_csf_w_t(u16 idx, u32 gpu_id);
-
-#endif /* _MALI_KBASE_MMU_FAULTS_DECODER_LUTS_CSF_H_ */
diff --git a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.c b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.c
deleted file mode 100644
index a053a93978b5..000000000000
--- a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.c
+++ /dev/null
@@ -1,74 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
-/*
- *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-/**
- * DOC: Base kernel MMU faults decoder for Job Manager GPUs.
- */
-
-#include <mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.h>
-
-#define GPU_ID_ARCH_ID_MAJOR_GET(gpu_id) ((gpu_id >> 16) & 0xFF)
-#define GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id) (gpu_id & 0xFFFF)
-#define NELEMS(s) (sizeof(s) / sizeof((s)[0]))
-
-struct decode_lut_element {
-	u16 arch_minor_rev;
-	u16 key;
-	const char *text;
-};
-
-static const char *decode_lut_element_lookup(u16 arch_minor_rev, u16 key,
-					     struct decode_lut_element *decode_element_lut,
-					     unsigned int lut_len)
-{
-	struct decode_lut_element *p;
-
-	for (p = decode_element_lut; p < decode_element_lut + lut_len; p++) {
-		if (p->key == key &&
-		    (p->arch_minor_rev == 0xffff || p->arch_minor_rev == arch_minor_rev))
-			break;
-	}
-	if (p < decode_element_lut + lut_len)
-		return p->text;
-	else
-		return "unknown";
-}
-
-/* Auto-generated code: DO NOT MODIFY! */
-
-static struct decode_lut_element lut_fault_source_jm_t_major_9[] = {
-	{ 0xFFFF, 0, "js" },
-	{ 0xFFFF, 1, "pcm" },
-};
-
-const char *decode_fault_source_jm_t(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 9:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_jm_t_major_9,
-						NELEMS(lut_fault_source_jm_t_major_9));
-		break;
-	}
-	return ret;
-}
diff --git a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.h b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.h
deleted file mode 100644
index f686e555d86a..000000000000
--- a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.h
+++ /dev/null
@@ -1,37 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
-/*
- *
- * (C) COPYRIGHT 2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-#ifndef _MALI_KBASE_MMU_FAULTS_DECODER_LUTS_JM_H_
-#define _MALI_KBASE_MMU_FAULTS_DECODER_LUTS_JM_H_
-#include <linux/types.h>
-
-/**
- * decode_fault_source_jm_t() - Get internal requester of a
- * fault in a human readable format.
- *
- * @idx: Internal requester part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: Internal requester of a fault in a human readable format for a JM core.
- */
-const char *decode_fault_source_jm_t(u16 idx, u32 gpu_id);
-
-#endif /* _MALI_KBASE_MMU_FAULTS_DECODER_LUTS_JM_H_ */
diff --git a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_jm.c b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_jm.c
index a7f3f40ef325..7cf0ed292fb0 100644
--- a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_jm.c
+++ b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_jm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -29,7 +29,6 @@
 #include <device/mali_kbase_device.h>
 #include <mali_kbase_as_fault_debugfs.h>
 #include <mmu/mali_kbase_mmu_internal.h>
-#include <mmu/mali_kbase_mmu_faults_decoder.h>
 
 void kbase_mmu_get_as_setup(struct kbase_mmu_table *mmut, struct kbase_mmu_setup *const setup)
 {
@@ -46,37 +45,29 @@ void kbase_mmu_get_as_setup(struct kbase_mmu_table *mmut, struct kbase_mmu_setup
 		(KBASE_MEMATTR_AARCH64_NON_CACHEABLE << (KBASE_MEMATTR_INDEX_NON_CACHEABLE * 8));
 
 	setup->transtab = (u64)mmut->pgd & AS_TRANSTAB_BASE_MASK;
-	setup->transcfg = AS_TRANSCFG_MODE_SET(0ULL, AS_TRANSCFG_MODE_AARCH64_4K);
+	setup->transcfg = AS_TRANSCFG_MODE_SET(0, AS_TRANSCFG_MODE_AARCH64_4K);
 }
 
 void kbase_gpu_report_bus_fault_and_kill(struct kbase_context *kctx, struct kbase_as *as,
 					 struct kbase_fault *fault)
 {
 	struct kbase_device *const kbdev = kctx->kbdev;
-	const u32 status = fault->status;
-	const u32 exception_type = AS_FAULTSTATUS_EXCEPTION_TYPE_GET(status);
-	const u32 access_type = AS_FAULTSTATUS_ACCESS_TYPE_GET(status);
-	const u32 source_id = AS_FAULTSTATUS_SOURCE_ID_GET(status);
-	unsigned int const as_no = as->number;
+	u32 const status = fault->status;
+	u32 const exception_type = (status & 0xFF);
+	u32 const exception_data = (status >> 8) & 0xFFFFFF;
+	int const as_no = as->number;
 	unsigned long flags;
 	const uintptr_t fault_addr = fault->addr;
 
 	/* terminal fault, print info about the fault */
 	dev_err(kbdev->dev,
-		"GPU bus fault in AS%u at PA %pK\n"
+		"GPU bus fault in AS%d at PA %pK\n"
 		"raw fault status: 0x%X\n"
 		"exception type 0x%X: %s\n"
-		"access type 0x%X: %s\n"
-		"source id 0x%X (core_id:utlb:IR 0x%X:0x%X:0x%X): %s, %s\n"
+		"exception data 0x%X\n"
 		"pid: %d\n",
 		as_no, (void *)fault_addr, status, exception_type,
-		kbase_gpu_exception_name(exception_type), access_type,
-		kbase_gpu_access_type_name(access_type), source_id,
-		FAULT_SOURCE_ID_CORE_ID_GET(source_id), FAULT_SOURCE_ID_UTLB_ID_GET(source_id),
-		fault_source_id_internal_requester_get(kbdev, source_id),
-		fault_source_id_core_type_description_get(kbdev, source_id),
-		fault_source_id_internal_requester_get_str(kbdev, source_id, access_type),
-		kctx->pid);
+		kbase_gpu_exception_name(exception_type), exception_data, kctx->pid);
 
 	/* switch to UNMAPPED mode, will abort all jobs and stop any hw counter
 	 * dumping AS transaction begin
@@ -104,53 +95,38 @@ void kbase_mmu_report_fault_and_kill(struct kbase_context *kctx, struct kbase_as
 				     const char *reason_str, struct kbase_fault *fault)
 {
 	unsigned long flags;
-	struct kbase_device *kbdev = kctx->kbdev;
-	struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
-	unsigned int as_no = as->number;
+	u32 exception_type;
+	u32 access_type;
+	u32 source_id;
+	int as_no;
+	struct kbase_device *kbdev;
+	struct kbasep_js_device_data *js_devdata;
+
+	as_no = as->number;
+	kbdev = kctx->kbdev;
+	js_devdata = &kbdev->js_data;
 
 	/* Make sure the context was active */
 	if (WARN_ON(atomic_read(&kctx->refcount) <= 0))
 		return;
 
-	if (!kbase_ctx_flag(kctx, KCTX_PAGE_FAULT_REPORT_SKIP)) {
-		/* decode the fault status */
-		const u32 status = fault->status;
-		const u32 exception_type = AS_FAULTSTATUS_EXCEPTION_TYPE_GET(status);
-		const u32 access_type = AS_FAULTSTATUS_ACCESS_TYPE_GET(status);
-		const u32 source_id = AS_FAULTSTATUS_SOURCE_ID_GET(status);
-		/* terminal fault, print info about the fault */
-		if (kbdev->gpu_props.gpu_id.product_model < GPU_ID_MODEL_MAKE(9, 0)) {
-			dev_err(kbdev->dev,
-				"Unhandled Page fault in AS%u at VA 0x%016llX\n"
-				"Reason: %s\n"
-				"raw fault status: 0x%X\n"
-				"exception type 0x%X: %s\n"
-				"access type 0x%X: %s\n"
-				"pid: %d\n",
-				as_no, fault->addr, reason_str, status, exception_type,
-				kbase_gpu_exception_name(exception_type), access_type,
-				kbase_gpu_access_type_name(status), kctx->pid);
-		} else {
-			dev_err(kbdev->dev,
-				"Unhandled Page fault in AS%u at VA 0x%016llX\n"
-				"Reason: %s\n"
-				"raw fault status: 0x%X\n"
-				"exception type 0x%X: %s\n"
-				"access type 0x%X: %s\n"
-				"source id 0x%X (core_id:utlb:IR 0x%X:0x%X:0x%X): %s, %s\n"
-				"pid: %d\n",
-				as_no, fault->addr, reason_str, status, exception_type,
-				kbase_gpu_exception_name(exception_type), access_type,
-				kbase_gpu_access_type_name(status), source_id,
-				FAULT_SOURCE_ID_CORE_ID_GET(source_id),
-				FAULT_SOURCE_ID_UTLB_ID_GET(source_id),
-				fault_source_id_internal_requester_get(kbdev, source_id),
-				fault_source_id_core_type_description_get(kbdev, source_id),
-				fault_source_id_internal_requester_get_str(kbdev, source_id,
-									   access_type),
-				kctx->pid);
-		}
-	}
+	/* decode the fault status */
+	exception_type = fault->status & 0xFF;
+	access_type = (fault->status >> 8) & 0x3;
+	source_id = (fault->status >> 16);
+
+	/* terminal fault, print info about the fault */
+	dev_err(kbdev->dev,
+		"Unhandled Page fault in AS%d at VA 0x%016llX\n"
+		"Reason: %s\n"
+		"raw fault status: 0x%X\n"
+		"exception type 0x%X: %s\n"
+		"access type 0x%X: %s\n"
+		"source id 0x%X\n"
+		"pid: %d\n",
+		as_no, fault->addr, reason_str, fault->status, exception_type,
+		kbase_gpu_exception_name(exception_type), access_type,
+		kbase_gpu_access_type_name(fault->status), source_id, kctx->pid);
 
 	/* hardware counters dump fault handling */
 	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
@@ -217,12 +193,12 @@ static void kbase_mmu_interrupt_process(struct kbase_device *kbdev, struct kbase
 		if (kbase_as_has_bus_fault(as, fault)) {
 			dev_warn(
 				kbdev->dev,
-				"Bus error in AS%u at PA 0x%pK with no context present! Spurious IRQ or SW Design Error?\n",
+				"Bus error in AS%d at PA 0x%pK with no context present! Spurious IRQ or SW Design Error?\n",
 				as->number, (void *)(uintptr_t)fault->addr);
 		} else {
 			dev_warn(
 				kbdev->dev,
-				"Page fault in AS%u at VA 0x%016llx with no context present! Spurious IRQ or SW Design Error?\n",
+				"Page fault in AS%d at VA 0x%016llx with no context present! Spurious IRQ or SW Design Error?\n",
 				as->number, fault->addr);
 		}
 		/* Since no ctx was found, the MMU must be disabled. */
@@ -260,7 +236,7 @@ static void kbase_mmu_interrupt_process(struct kbase_device *kbdev, struct kbase
 		 */
 		kbasep_js_clear_submit_allowed(js_devdata, kctx);
 
-		dev_warn(kbdev->dev, "Bus error in AS%u at PA=0x%pK, IPA=0x%pK\n", as->number,
+		dev_warn(kbdev->dev, "Bus error in AS%d at PA=0x%pK, IPA=0x%pK\n", as->number,
 			 (void *)(uintptr_t)fault->addr, (void *)(uintptr_t)fault->extra_addr);
 
 		/*
@@ -285,7 +261,7 @@ static void validate_protected_page_fault(struct kbase_device *kbdev)
 	 */
 	u32 protected_debug_mode = 0;
 
-	if (kbase_hw_has_feature(kbdev, KBASE_HW_FEATURE_PROTECTED_DEBUG_MODE)) {
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_PROTECTED_DEBUG_MODE)) {
 		protected_debug_mode = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_STATUS)) &
 				       GPU_STATUS_GPU_DBG_ENABLED;
 	}
@@ -334,11 +310,11 @@ void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat)
 		 * the while logic ensures we have a bit set, no need to check
 		 * for not-found here
 		 */
-		as_no = (unsigned int)ffs((int)(bf_bits | pf_bits)) - 1;
+		as_no = ffs(bf_bits | pf_bits) - 1;
 		as = &kbdev->as[as_no];
 
 		/* find the fault type */
-		if (bf_bits & (1UL << as_no))
+		if (bf_bits & (1 << as_no))
 			fault = &as->bf_data;
 		else
 			fault = &as->pf_data;
@@ -401,6 +377,13 @@ void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat)
 	dev_dbg(kbdev->dev, "Leaving %s irq_stat %u\n", __func__, irq_stat);
 }
 
+int kbase_mmu_switch_to_ir(struct kbase_context *const kctx, struct kbase_va_region *const reg)
+{
+	dev_dbg(kctx->kbdev->dev, "Switching to incremental rendering for region %pK\n",
+		(void *)reg);
+	return kbase_job_slot_softstop_start_rp(kctx, reg);
+}
+
 int kbase_mmu_as_init(struct kbase_device *kbdev, unsigned int i)
 {
 	kbdev->as[i].number = i;
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.c b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.c
index 4963d990054f..f3095f3b1f2a 100644
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.c
+++ b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -40,6 +40,7 @@
 #include <mali_kbase_reset_gpu.h>
 #include <mmu/mali_kbase_mmu.h>
 #include <mmu/mali_kbase_mmu_internal.h>
+#include <mali_kbase_cs_experimental.h>
 #include <device/mali_kbase_device.h>
 #include <uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h>
 #if !MALI_USE_CSF
@@ -57,271 +58,68 @@
 /* Macro to convert updated PDGs to flags indicating levels skip in flush */
 #define pgd_level_to_skip_flush(dirty_pgds) (~(dirty_pgds)&0xF)
 
-/**
- * kmap_pgd() - Map a PGD page and return the address of it
- *
- * @p:           Pointer to the PGD page to be mapped.
- * @pgd:         The physical address of the PGD. May not be PAGE_SIZE aligned but shall be
- *               GPU_PAGE_SIZE aligned.
- *
- * Return: The mapped address of the @pgd, adjusted by the offset of @pgd from the start of page.
- */
-static inline void *kmap_pgd(struct page *p, phys_addr_t pgd)
-{
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	return kbase_kmap(p) + (pgd & ~PAGE_MASK);
-#else
-	CSTD_UNUSED(pgd);
-	return kbase_kmap(p);
-#endif
-}
-
-/**
- * kmap_atomic_pgd() - Variant of kmap_pgd for atomic mapping
- *
- * @p:           Pointer to the PGD page to be mapped.
- * @pgd:         The physical address of the PGD. May not be PAGE_SIZE aligned but shall be
- *               GPU_PAGE_SIZE aligned.
- *
- * Return: The mapped address of the @pgd.
- */
-static inline void *kmap_atomic_pgd(struct page *p, phys_addr_t pgd)
-{
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	return kbase_kmap_atomic(p) + (pgd & ~PAGE_MASK);
-#else
-	CSTD_UNUSED(pgd);
-	return kbase_kmap_atomic(p);
-#endif
-}
-
-/**
- * kunmap_pgd() - Unmap a PGD page
- *
- * @p:           Pointer to the PGD page to be unmapped.
- * @pgd_address: The address of the PGD. May not be PAGE_SIZE aligned but shall be
- *               GPU_PAGE_SIZE aligned.
- */
-static inline void kunmap_pgd(struct page *p, void *pgd_address)
-{
-	/* It is okay to not align pgd_address to PAGE_SIZE boundary */
-	kbase_kunmap(p, pgd_address);
-}
-
-/**
- * kunmap_atomic_pgd() - Variant of kunmap_pgd for atomic unmapping
- *
- * @pgd_address: The address of the PGD. May not be PAGE_SIZE aligned but shall be
- *               GPU_PAGE_SIZE aligned.
- */
-static inline void kunmap_atomic_pgd(void *pgd_address)
-{
-	/* It is okay to not align pgd_address to PAGE_SIZE boundary */
-	kbase_kunmap_atomic(pgd_address);
-}
-
-/**
- * pgd_dma_addr() - Return dma addr of a PGD
- *
- * @p:       Pointer to the PGD page.
- * @pgd:     The physical address of the PGD.
- *
- * Return:   DMA address of the PGD
- */
-static inline dma_addr_t pgd_dma_addr(struct page *p, phys_addr_t pgd)
-{
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	return kbase_page_private(p)->dma_addr + (pgd & ~PAGE_MASK);
-#else
-	CSTD_UNUSED(pgd);
-	return kbase_dma_addr(p);
-#endif
-}
+static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mmu_table *mmut,
+				     const u64 start_vpfn, struct tagged_addr *phys, size_t nr,
+				     unsigned long flags, int const group_id, u64 *dirty_pgds,
+				     struct kbase_va_region *reg, bool ignore_page_migration);
 
-/**
- * get_pgd_sub_page_index() - Return the index of a sub PGD page in the PGD page.
- *
- * @pgd:         The physical address of the PGD.
- *
- * Return:       The index value ranging from 0 to (GPU_PAGES_PER_CPU_PAGE - 1)
- */
-static inline u32 get_pgd_sub_page_index(phys_addr_t pgd)
+/* Small wrapper function to factor out GPU-dependent context releasing */
+static void release_ctx(struct kbase_device *kbdev, struct kbase_context *kctx)
 {
-	return (pgd & ~PAGE_MASK) / GPU_PAGE_SIZE;
+#if MALI_USE_CSF
+	CSTD_UNUSED(kbdev);
+	kbase_ctx_sched_release_ctx_lock(kctx);
+#else /* MALI_USE_CSF */
+	kbasep_js_runpool_release_ctx(kbdev, kctx);
+#endif /* MALI_USE_CSF */
 }
 
-#if GPU_PAGES_PER_CPU_PAGE > 1
-/**
- * alloc_pgd_page_metadata() - Allocate page metadata for a PGD.
- *
- * @kbdev:      Pointer to the instance of a kbase device.
- * @mmut:       Structure holding details of the MMU table for a kcontext.
- * @p:          PGD page.
- *
- * The PGD page, @p is linked to &kbase_mmu_table.pgd_pages_list for allocating
- * sub PGD pages from the list.
- *
- * Return:      True on success.
- */
-static bool alloc_pgd_page_metadata(struct kbase_device *kbdev, struct kbase_mmu_table *mmut,
-				    struct page *p)
+static void mmu_hw_operation_begin(struct kbase_device *kbdev)
 {
-	struct kbase_page_metadata *page_md;
+#if !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
+#if MALI_USE_CSF
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_GPU2019_3878)) {
+		unsigned long flags;
 
-	if (!kbase_is_page_migration_enabled()) {
-		page_md = kmem_cache_zalloc(kbdev->page_metadata_slab, GFP_KERNEL);
-		if (!page_md)
-			return false;
+		lockdep_assert_held(&kbdev->mmu_hw_mutex);
 
-		page_md->dma_addr = kbase_dma_addr_as_priv(p);
-		set_page_private(p, (unsigned long)page_md);
-	} else {
-		page_md = kbase_page_private(p);
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		WARN_ON_ONCE(kbdev->mmu_hw_operation_in_progress);
+		kbdev->mmu_hw_operation_in_progress = true;
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 	}
-
-	page_md->data.pt_mapped.num_allocated_sub_pages = 1;
-	set_bit(0, page_md->data.pt_mapped.allocated_sub_pages);
-	page_md->data.pt_mapped.pgd_page = p;
-	list_add(&page_md->data.pt_mapped.pgd_link, &mmut->pgd_pages_list);
-
-	return true;
-}
-
-/**
- * free_pgd_page_metadata() - Free page metadata for a PGD.
- *
- * @kbdev:      Pointer to the instance of a kbase device.
- * @p:          PGD page where the metadata belongs to.
- *
- * The PGD page, @p is removed from &kbase_mmu_table.pgd_pages_list.
- */
-static void free_pgd_page_metadata(struct kbase_device *kbdev, struct page *p)
-{
-	struct kbase_page_metadata *page_md = kbase_page_private(p);
-
-	WARN_ON_ONCE(page_md->data.pt_mapped.num_allocated_sub_pages);
-	page_md->data.pt_mapped.pgd_page = NULL;
-	list_del_init(&page_md->data.pt_mapped.pgd_link);
-
-	if (kbase_is_page_migration_enabled())
-		return;
-
-	set_page_private(p, (unsigned long)page_md->dma_addr);
-	kmem_cache_free(kbdev->page_metadata_slab, page_md);
-}
-
-/**
- * allocate_pgd_sub_page() - Allocate a PGD sub page
- *
- * @page_md:  Page metadata of a PGD page where a sub page is allocated from.
- *
- * Return:    Physical address of allocated PGD sub page on success.
- *            KBASE_INVALID_PHYSICAL_ADDRESS on failure.
- */
-static inline phys_addr_t allocate_pgd_sub_page(struct kbase_page_metadata *page_md)
-{
-	unsigned long sub_page_index;
-
-	if (page_md->data.pt_mapped.num_allocated_sub_pages == GPU_PAGES_PER_CPU_PAGE)
-		return KBASE_INVALID_PHYSICAL_ADDRESS;
-	sub_page_index = find_first_zero_bit(page_md->data.pt_mapped.allocated_sub_pages,
-					     GPU_PAGES_PER_CPU_PAGE);
-
-#ifdef CONFIG_MALI_BIFROST_DEBUG
-	if (WARN_ON_ONCE(sub_page_index >= GPU_PAGES_PER_CPU_PAGE))
-		return KBASE_INVALID_PHYSICAL_ADDRESS;
-	if (WARN_ON_ONCE(page_md->data.pt_mapped.num_allocated_sub_pages > GPU_PAGES_PER_CPU_PAGE))
-		return KBASE_INVALID_PHYSICAL_ADDRESS;
-#endif
-	set_bit(sub_page_index, page_md->data.pt_mapped.allocated_sub_pages);
-	page_md->data.pt_mapped.num_allocated_sub_pages++;
-
-	return (page_to_phys(page_md->data.pt_mapped.pgd_page) + (sub_page_index * GPU_PAGE_SIZE));
-}
-
-/**
- * free_pgd_sub_page() - Free a PGD sub page
- *
- * @pgd:      Sub PGD to be freed.
- *
- * Return:    The number of remaining allocated sub pages in the PGD.
- */
-static int free_pgd_sub_page(phys_addr_t pgd)
-{
-	struct page *p = pfn_to_page(PFN_DOWN(pgd));
-	struct kbase_page_metadata *page_md = kbase_page_private(p);
-	const u32 sub_page_index = get_pgd_sub_page_index(pgd);
-
-#ifdef CONFIG_MALI_BIFROST_DEBUG
-	if (WARN_ON_ONCE(!test_bit(sub_page_index, page_md->data.pt_mapped.allocated_sub_pages)))
-		return page_md->data.pt_mapped.num_allocated_sub_pages;
-#endif
-	clear_bit(sub_page_index, page_md->data.pt_mapped.allocated_sub_pages);
-	if (!WARN_ON_ONCE(page_md->data.pt_mapped.num_allocated_sub_pages <= 0))
-		page_md->data.pt_mapped.num_allocated_sub_pages--;
-
-	return page_md->data.pt_mapped.num_allocated_sub_pages;
+#else
+	CSTD_UNUSED(kbdev);
+#endif /* MALI_USE_CSF */
+#else
+	CSTD_UNUSED(kbdev);
+#endif /* !CONFIG_MALI_BIFROST_NO_MALI */
 }
 
-/**
- * allocate_from_pgd_pages_list() - Allocate a PGD from the PGD pages list
- *
- * @mmut:     Structure holding details of the MMU table for a kcontext.
- *
- * Return:    Physical address of the allocated PGD.
- */
-static inline phys_addr_t allocate_from_pgd_pages_list(struct kbase_mmu_table *mmut)
+static void mmu_hw_operation_end(struct kbase_device *kbdev)
 {
-	struct list_head *entry;
-	phys_addr_t pgd;
-
-	lockdep_assert_held(&mmut->mmu_lock);
-
-	if (unlikely(!mmut->num_free_pgd_sub_pages))
-		return KBASE_INVALID_PHYSICAL_ADDRESS;
-
-	if (mmut->last_allocated_pgd_page) {
-		pgd = allocate_pgd_sub_page(kbase_page_private(mmut->last_allocated_pgd_page));
-		if (pgd != KBASE_INVALID_PHYSICAL_ADDRESS)
-			goto success;
-	}
-
-	if (mmut->last_freed_pgd_page) {
-		pgd = allocate_pgd_sub_page(kbase_page_private(mmut->last_freed_pgd_page));
-		if (pgd != KBASE_INVALID_PHYSICAL_ADDRESS)
-			goto success;
-	}
+#if !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
+#if MALI_USE_CSF
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_GPU2019_3878)) {
+		unsigned long flags;
 
-	list_for_each(entry, &mmut->pgd_pages_list) {
-		struct kbase_page_metadata *page_md =
-			list_entry(entry, struct kbase_page_metadata, data.pt_mapped.pgd_link);
+		lockdep_assert_held(&kbdev->mmu_hw_mutex);
 
-		pgd = allocate_pgd_sub_page(page_md);
-		if (pgd != KBASE_INVALID_PHYSICAL_ADDRESS)
-			goto success;
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		WARN_ON_ONCE(!kbdev->mmu_hw_operation_in_progress);
+		kbdev->mmu_hw_operation_in_progress = false;
+		/* Invoke the PM state machine, the L2 power off may have been
+		 * skipped due to the MMU command.
+		 */
+		kbase_pm_update_state(kbdev);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 	}
-
-	return KBASE_INVALID_PHYSICAL_ADDRESS;
-success:
-	mmut->num_free_pgd_sub_pages--;
-	return pgd;
-}
-#endif
-
-static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mmu_table *mmut,
-				     const u64 start_vpfn, struct tagged_addr *phys, size_t nr,
-				     unsigned long flags, int const group_id, u64 *dirty_pgds,
-				     struct kbase_va_region *reg, bool ignore_page_migration);
-
-/* Small wrapper function to factor out GPU-dependent context releasing */
-static void release_ctx(struct kbase_device *kbdev, struct kbase_context *kctx)
-{
-#if MALI_USE_CSF
+#else
 	CSTD_UNUSED(kbdev);
-	kbase_ctx_sched_release_ctx_lock(kctx);
-#else /* MALI_USE_CSF */
-	kbasep_js_runpool_release_ctx(kbdev, kctx);
 #endif /* MALI_USE_CSF */
+#else
+	CSTD_UNUSED(kbdev);
+#endif /* !CONFIG_MALI_BIFROST_NO_MALI */
 }
 
 /**
@@ -401,44 +199,6 @@ static void mmu_invalidate(struct kbase_device *kbdev, struct kbase_context *kct
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 }
 
-/**
- * mmu_invalidate_on_teardown() - Perform an invalidate operation on MMU caches on page
- *                                table teardown.
- * @kbdev:      The Kbase device.
- * @kctx:       The Kbase context.
- * @vpfn:       The virtual page frame number at which teardown is done.
- * @num_pages:  The number of entries that were invalidated in top most level PGD, that
- *              was affected by the teardown operation.
- * @level:      The top most PGD level that was touched on teardown.
- * @as_nr:      GPU address space number for which invalidate is required.
- *
- * Perform an MMU invalidate operation after the teardown of top most level PGD on a
- * particular address space by issuing a UNLOCK command.
- */
-static inline void mmu_invalidate_on_teardown(struct kbase_device *kbdev,
-					      struct kbase_context *kctx, u64 vpfn,
-					      size_t num_pages, int level, int as_nr)
-{
-	u32 invalidate_range_num_pages = num_pages;
-	u64 invalidate_range_start_vpfn = vpfn;
-	struct kbase_mmu_hw_op_param op_param;
-
-	if (level != MIDGARD_MMU_BOTTOMLEVEL) {
-		invalidate_range_num_pages = 1 << ((3 - level) * 9);
-		invalidate_range_start_vpfn = vpfn - (vpfn & (invalidate_range_num_pages - 1));
-	}
-
-	op_param = (struct kbase_mmu_hw_op_param){
-		.vpfn = invalidate_range_start_vpfn,
-		.nr = invalidate_range_num_pages,
-		.mmu_sync_info = CALLER_MMU_ASYNC,
-		.kctx_id = kctx ? kctx->id : 0xFFFFFFFF,
-		.flush_skip_levels = (1ULL << level) - 1,
-	};
-
-	mmu_invalidate(kbdev, kctx, as_nr, &op_param);
-}
-
 /* Perform a flush/invalidate on a particular address space
  */
 static void mmu_flush_invalidate_as(struct kbase_device *kbdev, struct kbase_as *as,
@@ -450,7 +210,7 @@ static void mmu_flush_invalidate_as(struct kbase_device *kbdev, struct kbase_as
 	mutex_lock(&kbdev->mmu_hw_mutex);
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 
-	if (kbdev->pm.backend.gpu_ready && kbase_mmu_hw_do_flush(kbdev, as, op_param))
+	if (kbdev->pm.backend.gpu_ready && (kbase_mmu_hw_do_flush_locked(kbdev, as, op_param)))
 		dev_err(kbdev->dev, "Flush for GPU page table update did not complete");
 
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
@@ -606,16 +366,14 @@ static int kbase_mmu_update_pages_no_flush(struct kbase_device *kbdev, struct kb
  * @mmut:     GPU MMU page table.
  * @pgds:     Physical addresses of page directories to be freed.
  * @vpfn:     The virtual page frame number.
- * @level:    The level of MMU page table that needs to be updated.
+ * @level:    The level of MMU page table.
  * @flush_op: The type of MMU flush operation to perform.
  * @dirty_pgds: Flags to track every level where a PGD has been updated.
- * @as_nr:     GPU address space number for which invalidate is required.
  */
 static void kbase_mmu_update_and_free_parent_pgds(struct kbase_device *kbdev,
 						  struct kbase_mmu_table *mmut, phys_addr_t *pgds,
 						  u64 vpfn, int level,
-						  enum kbase_mmu_op_type flush_op, u64 *dirty_pgds,
-						  int as_nr);
+						  enum kbase_mmu_op_type flush_op, u64 *dirty_pgds);
 
 static void kbase_mmu_account_freed_pgd(struct kbase_device *kbdev, struct kbase_mmu_table *mmut)
 {
@@ -667,7 +425,7 @@ static bool kbase_mmu_handle_isolated_pgd_page(struct kbase_device *kbdev,
 		 * PGD page, which is done inside kbase_mmu_free_pgd() for the
 		 * PGD page that did not get isolated.
 		 */
-		dma_sync_single_for_device(kbdev->dev, pgd_dma_addr(p, page_to_phys(p)), PAGE_SIZE,
+		dma_sync_single_for_device(kbdev->dev, kbase_dma_addr(p), PAGE_SIZE,
 					   DMA_BIDIRECTIONAL);
 		kbase_mmu_account_freed_pgd(kbdev, mmut);
 	}
@@ -694,20 +452,6 @@ static void kbase_mmu_free_pgd(struct kbase_device *kbdev, struct kbase_mmu_tabl
 	lockdep_assert_held(&mmut->mmu_lock);
 
 	p = pfn_to_page(PFN_DOWN(pgd));
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	if (free_pgd_sub_page(pgd)) {
-		mmut->num_free_pgd_sub_pages++;
-		mmut->last_freed_pgd_page = p;
-		return;
-	}
-
-	mmut->num_free_pgd_sub_pages -= (GPU_PAGES_PER_CPU_PAGE - 1);
-	if (p == mmut->last_freed_pgd_page)
-		mmut->last_freed_pgd_page = NULL;
-	if (p == mmut->last_allocated_pgd_page)
-		mmut->last_allocated_pgd_page = NULL;
-	free_pgd_page_metadata(kbdev, p);
-#endif
 	page_is_isolated = kbase_mmu_handle_isolated_pgd_page(kbdev, mmut, p);
 
 	if (likely(!page_is_isolated)) {
@@ -737,19 +481,19 @@ static void kbase_mmu_free_pgds_list(struct kbase_device *kbdev, struct kbase_mm
 	lockdep_assert_held(&mmut->mmu_lock);
 
 	for (i = 0; i < mmut->scratch_mem.free_pgds.head_index; i++)
-		kbase_mmu_free_pgd(kbdev, mmut, mmut->scratch_mem.free_pgds.pgds[i]);
+		kbase_mmu_free_pgd(kbdev, mmut, page_to_phys(mmut->scratch_mem.free_pgds.pgds[i]));
 
 	mmut->scratch_mem.free_pgds.head_index = 0;
 }
 
-static void kbase_mmu_add_to_free_pgds_list(struct kbase_mmu_table *mmut, phys_addr_t pgd)
+static void kbase_mmu_add_to_free_pgds_list(struct kbase_mmu_table *mmut, struct page *p)
 {
 	lockdep_assert_held(&mmut->mmu_lock);
 
 	if (WARN_ON_ONCE(mmut->scratch_mem.free_pgds.head_index > (MAX_FREE_PGDS - 1)))
 		return;
 
-	mmut->scratch_mem.free_pgds.pgds[mmut->scratch_mem.free_pgds.head_index++] = pgd;
+	mmut->scratch_mem.free_pgds.pgds[mmut->scratch_mem.free_pgds.head_index++] = p;
 }
 
 static inline void kbase_mmu_reset_free_pgds_list(struct kbase_mmu_table *mmut)
@@ -848,9 +592,10 @@ static void kbase_gpu_mmu_handle_write_faulting_as(struct kbase_device *kbdev,
 	 */
 	const enum kbase_caller_mmu_sync_info mmu_sync_info = CALLER_MMU_SYNC;
 	struct kbase_mmu_hw_op_param op_param;
-	unsigned long irq_flags;
 	int ret = 0;
 
+	mutex_lock(&kbdev->mmu_hw_mutex);
+
 	kbase_mmu_hw_clear_fault(kbdev, faulting_as, KBASE_MMU_FAULT_TYPE_PAGE);
 
 	/* flush L2 and unlock the VA (resumes the MMU) */
@@ -859,14 +604,20 @@ static void kbase_gpu_mmu_handle_write_faulting_as(struct kbase_device *kbdev,
 	op_param.op = KBASE_MMU_OP_FLUSH_PT;
 	op_param.kctx_id = kctx_id;
 	op_param.mmu_sync_info = mmu_sync_info;
-	spin_lock_irqsave(&kbdev->hwaccess_lock, irq_flags);
 	if (mmu_flush_cache_on_gpu_ctrl(kbdev)) {
+		unsigned long irq_flags;
+
+		spin_lock_irqsave(&kbdev->hwaccess_lock, irq_flags);
 		op_param.flush_skip_levels = pgd_level_to_skip_flush(dirty_pgds);
 		ret = kbase_mmu_hw_do_flush_on_gpu_ctrl(kbdev, faulting_as, &op_param);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, irq_flags);
 	} else {
+		mmu_hw_operation_begin(kbdev);
 		ret = kbase_mmu_hw_do_flush(kbdev, faulting_as, &op_param);
+		mmu_hw_operation_end(kbdev);
 	}
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, irq_flags);
+
+	mutex_unlock(&kbdev->mmu_hw_mutex);
 
 	if (ret)
 		dev_err(kbdev->dev,
@@ -879,14 +630,14 @@ static void set_gwt_element_page_addr_and_size(struct kbasep_gwt_list_element *e
 					       u64 fault_page_addr, struct tagged_addr fault_phys)
 {
 	u64 fault_pfn = fault_page_addr >> PAGE_SHIFT;
-	unsigned int vindex = fault_pfn & (NUM_PAGES_IN_2MB_LARGE_PAGE - 1);
+	unsigned int vindex = fault_pfn & (NUM_4K_PAGES_IN_2MB_PAGE - 1);
 
 	/* If the fault address lies within a 2MB page, then consider
 	 * the whole 2MB page for dumping to avoid incomplete dumps.
 	 */
 	if (is_huge(fault_phys) && (vindex == index_in_large_page(fault_phys))) {
-		element->page_addr = fault_page_addr & ~(SZ_2M - 1UL);
-		element->num_pages = NUM_PAGES_IN_2MB_LARGE_PAGE;
+		element->page_addr = fault_page_addr & ~(SZ_2M - 1);
+		element->num_pages = NUM_4K_PAGES_IN_2MB_PAGE;
 	} else {
 		element->page_addr = fault_page_addr;
 		element->num_pages = 1;
@@ -902,7 +653,7 @@ static void kbase_gpu_mmu_handle_write_fault(struct kbase_context *kctx,
 	struct tagged_addr *fault_phys_addr;
 	struct kbase_fault *fault;
 	u64 fault_pfn, pfn_offset;
-	unsigned int as_no;
+	int as_no;
 	u64 dirty_pgds = 0;
 
 	as_no = faulting_as->number;
@@ -930,14 +681,6 @@ static void kbase_gpu_mmu_handle_write_fault(struct kbase_context *kctx,
 		return;
 	}
 
-	if (unlikely(region->gpu_alloc->type == KBASE_MEM_TYPE_ALIAS)) {
-		kbase_gpu_vm_unlock(kctx);
-		kbase_mmu_report_fault_and_kill(
-			kctx, faulting_as, "Unexpected write permission fault on an alias region",
-			&faulting_as->pf_data);
-		return;
-	}
-
 	pfn_offset = fault_pfn - region->start_pfn;
 	fault_phys_addr = &kbase_get_gpu_phy_pages(region)[pfn_offset];
 
@@ -1007,7 +750,7 @@ static void kbase_gpu_mmu_handle_permission_fault(struct kbase_context *kctx,
  * estimate_pool_space_required - Determine how much a pool should be grown by to support a future
  * allocation
  * @pool:           The memory pool to check, including its linked pools
- * @pages_required: Number of small pages require for the pool to support a future allocation
+ * @pages_required: Number of 4KiB pages require for the pool to support a future allocation
  *
  * The value returned is accounting for the size of @pool and the size of each memory pool linked to
  * @pool. Hence, the caller should use @pool and (if not already satisfied) all its linked pools to
@@ -1019,7 +762,7 @@ static void kbase_gpu_mmu_handle_permission_fault(struct kbase_context *kctx,
  * should keep attempting an allocation and then re-growing with a new value queried form this
  * function until the allocation succeeds.
  *
- * Return: an estimate of the amount of extra small pages in @pool that are required to satisfy an
+ * Return: an estimate of the amount of extra 4KiB pages in @pool that are required to satisfy an
  * allocation, or 0 if @pool (including its linked pools) is likely to already satisfy the
  * allocation.
  */
@@ -1029,15 +772,15 @@ static size_t estimate_pool_space_required(struct kbase_mem_pool *pool, const si
 
 	for (pages_still_required = pages_required; pool != NULL && pages_still_required;
 	     pool = pool->next_pool) {
-		size_t pool_size_small;
+		size_t pool_size_4k;
 
 		kbase_mem_pool_lock(pool);
 
-		pool_size_small = kbase_mem_pool_size(pool) << pool->order;
-		if (pool_size_small >= pages_still_required)
+		pool_size_4k = kbase_mem_pool_size(pool) << pool->order;
+		if (pool_size_4k >= pages_still_required)
 			pages_still_required = 0;
 		else
-			pages_still_required -= pool_size_small;
+			pages_still_required -= pool_size_4k;
 
 		kbase_mem_pool_unlock(pool);
 	}
@@ -1048,12 +791,11 @@ static size_t estimate_pool_space_required(struct kbase_mem_pool *pool, const si
  * page_fault_try_alloc - Try to allocate memory from a context pool
  * @kctx:          Context pointer
  * @region:        Region to grow
- * @new_pages:     Number of small pages to allocate
+ * @new_pages:     Number of 4 KiB pages to allocate
  * @pages_to_grow: Pointer to variable to store number of outstanding pages on failure. This can be
- *                 either small or 2 MiB pages, depending on the number of pages requested.
+ *                 either 4 KiB or 2 MiB pages, depending on the number of pages requested.
  * @grow_2mb_pool: Pointer to variable to store which pool needs to grow - true for 2 MiB, false for
- *                 pool of small pages.
- * @fallback_to_small:  Whether fallback to small pages or not
+ *                 4 KiB.
  * @prealloc_sas:  Pointer to kbase_sub_alloc structures
  *
  * This function will try to allocate as many pages as possible from the context pool, then if
@@ -1065,20 +807,20 @@ static size_t estimate_pool_space_required(struct kbase_mem_pool *pool, const si
  * held could invoke the OoM killer and cause an effective deadlock with kbase_cpu_vm_close().
  *
  * If 2 MiB pages are enabled and new_pages is >= 2 MiB then pages_to_grow will be a count of 2 MiB
- * pages, otherwise it will be a count of small pages.
+ * pages, otherwise it will be a count of 4 KiB pages.
  *
  * Return: true if successful, false on failure
  */
 static bool page_fault_try_alloc(struct kbase_context *kctx, struct kbase_va_region *region,
-				 size_t new_pages, size_t *pages_to_grow, bool *grow_2mb_pool,
-				 bool fallback_to_small, struct kbase_sub_alloc **prealloc_sas)
+				 size_t new_pages, int *pages_to_grow, bool *grow_2mb_pool,
+				 struct kbase_sub_alloc **prealloc_sas)
 {
 	size_t total_gpu_pages_alloced = 0;
 	size_t total_cpu_pages_alloced = 0;
 	struct kbase_mem_pool *pool, *root_pool;
 	bool alloc_failed = false;
 	size_t pages_still_required;
-	size_t total_mempools_free_small = 0;
+	size_t total_mempools_free_4k = 0;
 
 	lockdep_assert_held(&kctx->reg_lock);
 	lockdep_assert_held(&kctx->mem_partials_lock);
@@ -1089,8 +831,7 @@ static bool page_fault_try_alloc(struct kbase_context *kctx, struct kbase_va_reg
 		return false;
 	}
 
-	if (kbase_is_large_pages_enabled() && new_pages >= NUM_PAGES_IN_2MB_LARGE_PAGE &&
-	    !fallback_to_small) {
+	if (kctx->kbdev->pagesize_2mb && new_pages >= (SZ_2M / SZ_4K)) {
 		root_pool = &kctx->mem_pools.large[region->gpu_alloc->group_id];
 		*grow_2mb_pool = true;
 	} else {
@@ -1120,40 +861,40 @@ static bool page_fault_try_alloc(struct kbase_context *kctx, struct kbase_va_reg
 	 */
 	pages_still_required = new_pages;
 	for (pool = root_pool; pool != NULL && pages_still_required; pool = pool->next_pool) {
-		size_t pool_size_small;
-		size_t pages_to_alloc_small;
-		size_t pages_to_alloc_small_per_alloc;
+		size_t pool_size_4k;
+		size_t pages_to_alloc_4k;
+		size_t pages_to_alloc_4k_per_alloc;
 
 		kbase_mem_pool_lock(pool);
 
 		/* Allocate as much as possible from this pool*/
-		pool_size_small = kbase_mem_pool_size(pool) << pool->order;
-		total_mempools_free_small += pool_size_small;
-		pages_to_alloc_small = MIN(pages_still_required, pool_size_small);
+		pool_size_4k = kbase_mem_pool_size(pool) << pool->order;
+		total_mempools_free_4k += pool_size_4k;
+		pages_to_alloc_4k = MIN(pages_still_required, pool_size_4k);
 		if (region->gpu_alloc == region->cpu_alloc)
-			pages_to_alloc_small_per_alloc = pages_to_alloc_small;
+			pages_to_alloc_4k_per_alloc = pages_to_alloc_4k;
 		else
-			pages_to_alloc_small_per_alloc = pages_to_alloc_small >> 1;
+			pages_to_alloc_4k_per_alloc = pages_to_alloc_4k >> 1;
 
-		if (pages_to_alloc_small) {
+		if (pages_to_alloc_4k) {
 			struct tagged_addr *gpu_pages = kbase_alloc_phy_pages_helper_locked(
-				region->gpu_alloc, pool, pages_to_alloc_small_per_alloc,
+				region->gpu_alloc, pool, pages_to_alloc_4k_per_alloc,
 				&prealloc_sas[0]);
 
 			if (!gpu_pages)
 				alloc_failed = true;
 			else
-				total_gpu_pages_alloced += pages_to_alloc_small_per_alloc;
+				total_gpu_pages_alloced += pages_to_alloc_4k_per_alloc;
 
 			if (!alloc_failed && region->gpu_alloc != region->cpu_alloc) {
 				struct tagged_addr *cpu_pages = kbase_alloc_phy_pages_helper_locked(
-					region->cpu_alloc, pool, pages_to_alloc_small_per_alloc,
+					region->cpu_alloc, pool, pages_to_alloc_4k_per_alloc,
 					&prealloc_sas[1]);
 
 				if (!cpu_pages)
 					alloc_failed = true;
 				else
-					total_cpu_pages_alloced += pages_to_alloc_small_per_alloc;
+					total_cpu_pages_alloced += pages_to_alloc_4k_per_alloc;
 			}
 		}
 
@@ -1161,12 +902,12 @@ static bool page_fault_try_alloc(struct kbase_context *kctx, struct kbase_va_reg
 
 		if (alloc_failed) {
 			WARN_ON(!pages_still_required);
-			WARN_ON(pages_to_alloc_small >= pages_still_required);
-			WARN_ON(pages_to_alloc_small_per_alloc >= pages_still_required);
+			WARN_ON(pages_to_alloc_4k >= pages_still_required);
+			WARN_ON(pages_to_alloc_4k_per_alloc >= pages_still_required);
 			break;
 		}
 
-		pages_still_required -= pages_to_alloc_small;
+		pages_still_required -= pages_to_alloc_4k;
 	}
 
 	if (pages_still_required) {
@@ -1190,7 +931,7 @@ static bool page_fault_try_alloc(struct kbase_context *kctx, struct kbase_va_reg
 				kctx->kbdev->dev,
 				"Page allocation failure of %zu pages: managed %zu pages, mempool (inc linked pools) had %zu pages available",
 				new_pages, total_gpu_pages_alloced + total_cpu_pages_alloced,
-				total_mempools_free_small);
+				total_mempools_free_4k);
 			*pages_to_grow = 0;
 		} else {
 			/* Tell the caller to try to grow the memory pool
@@ -1229,23 +970,21 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 	size_t new_pages;
 	size_t fault_rel_pfn;
 	struct kbase_as *faulting_as;
-	unsigned int as_no;
+	int as_no;
 	struct kbase_context *kctx;
 	struct kbase_device *kbdev;
 	struct kbase_va_region *region;
 	struct kbase_fault *fault;
 	int err;
 	bool grown = false;
-	size_t pages_to_grow;
-	bool grow_2mb_pool = false;
-	bool fallback_to_small = false;
+	int pages_to_grow;
+	bool grow_2mb_pool;
 	struct kbase_sub_alloc *prealloc_sas[2] = { NULL, NULL };
 	int i;
 	size_t current_backed_size;
 #if MALI_JIT_PRESSURE_LIMIT_BASE
 	size_t pages_trimmed = 0;
 #endif
-	unsigned long hwaccess_flags;
 
 	/* Calls to this function are inherently synchronous, with respect to
 	 * MMU operations.
@@ -1258,7 +997,7 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 	as_no = faulting_as->number;
 
 	kbdev = container_of(faulting_as, struct kbase_device, as[as_no]);
-	dev_dbg(kbdev->dev, "Entering %s %pK, fault_pfn %lld, as_no %u", __func__, (void *)data,
+	dev_dbg(kbdev->dev, "Entering %s %pK, fault_pfn %lld, as_no %d", __func__, (void *)data,
 		fault_pfn, as_no);
 
 	/* Grab the context that was already refcounted in kbase_mmu_interrupt()
@@ -1279,11 +1018,13 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 #endif
 #endif
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	/* check if we still have GPU */
 	if (unlikely(kbase_is_gpu_removed(kbdev))) {
 		dev_dbg(kbdev->dev, "%s: GPU has been removed", __func__);
 		goto fault_done;
 	}
+#endif
 
 	if (unlikely(fault->protected_mode)) {
 		kbase_mmu_report_fault_and_kill(kctx, faulting_as, "Protected mode fault", fault);
@@ -1406,7 +1147,7 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 	}
 
 page_fault_retry:
-	if (kbase_is_large_pages_enabled() && !fallback_to_small) {
+	if (kbdev->pagesize_2mb) {
 		/* Preallocate (or re-allocate) memory for the sub-allocation structs if necessary */
 		for (i = 0; i != ARRAY_SIZE(prealloc_sas); ++i) {
 			if (!prealloc_sas[i]) {
@@ -1443,14 +1184,6 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 		goto fault_done;
 	}
 
-	if (unlikely(region->gpu_alloc->type == KBASE_MEM_TYPE_ALIAS)) {
-		kbase_gpu_vm_unlock(kctx);
-		kbase_mmu_report_fault_and_kill(kctx, faulting_as,
-						"Unexpected page fault on an alias region",
-						&faulting_as->pf_data);
-		goto fault_done;
-	}
-
 	if (region->gpu_alloc->group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS) {
 		kbase_gpu_vm_unlock(kctx);
 		kbase_mmu_report_fault_and_kill(kctx, faulting_as, "Bad physical memory group ID",
@@ -1490,6 +1223,8 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 			"Page fault @ VA 0x%llx in allocated region 0x%llx-0x%llx of growable TMEM: Ignoring",
 			fault->addr, region->start_pfn, region->start_pfn + current_backed_size);
 
+		mutex_lock(&kbdev->mmu_hw_mutex);
+
 		kbase_mmu_hw_clear_fault(kbdev, faulting_as, KBASE_MMU_FAULT_TYPE_PAGE);
 		/* [1] in case another page fault occurred while we were
 		 * handling the (duplicate) page fault we need to ensure we
@@ -1501,19 +1236,19 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 		 */
 		op_param.mmu_sync_info = mmu_sync_info;
 		op_param.kctx_id = kctx->id;
-		/* Usually it is safe to skip the MMU cache invalidate for all levels
-		 * in case of duplicate page faults. But for the pathological scenario
-		 * where the faulty VA gets mapped by the time page fault worker runs it
-		 * becomes imperative to invalidate MMU cache for all levels, otherwise
-		 * there is a possibility of repeated page faults on GPUs which supports
-		 * fine grained MMU cache invalidation.
-		 */
-		op_param.flush_skip_levels = 0x0;
-		op_param.vpfn = fault_pfn;
-		op_param.nr = 1;
-		spin_lock_irqsave(&kbdev->hwaccess_lock, hwaccess_flags);
-		err = kbase_mmu_hw_do_unlock(kbdev, faulting_as, &op_param);
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, hwaccess_flags);
+		if (!mmu_flush_cache_on_gpu_ctrl(kbdev)) {
+			mmu_hw_operation_begin(kbdev);
+			err = kbase_mmu_hw_do_unlock_no_addr(kbdev, faulting_as, &op_param);
+			mmu_hw_operation_end(kbdev);
+		} else {
+			/* Can safely skip the invalidate for all levels in case
+			 * of duplicate page faults.
+			 */
+			op_param.flush_skip_levels = 0xF;
+			op_param.vpfn = fault_pfn;
+			op_param.nr = 1;
+			err = kbase_mmu_hw_do_unlock(kbdev, faulting_as, &op_param);
+		}
 
 		if (err) {
 			dev_err(kbdev->dev,
@@ -1521,6 +1256,8 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 				fault->addr);
 		}
 
+		mutex_unlock(&kbdev->mmu_hw_mutex);
+
 		kbase_mmu_hw_enable_fault(kbdev, faulting_as, KBASE_MMU_FAULT_TYPE_PAGE);
 		kbase_gpu_vm_unlock(kctx);
 
@@ -1536,25 +1273,27 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 	if (new_pages == 0) {
 		struct kbase_mmu_hw_op_param op_param;
 
+		mutex_lock(&kbdev->mmu_hw_mutex);
+
 		/* Duplicate of a fault we've already handled, nothing to do */
 		kbase_mmu_hw_clear_fault(kbdev, faulting_as, KBASE_MMU_FAULT_TYPE_PAGE);
 
 		/* See comment [1] about UNLOCK usage */
 		op_param.mmu_sync_info = mmu_sync_info;
 		op_param.kctx_id = kctx->id;
-		/* Usually it is safe to skip the MMU cache invalidate for all levels
-		 * in case of duplicate page faults. But for the pathological scenario
-		 * where the faulty VA gets mapped by the time page fault worker runs it
-		 * becomes imperative to invalidate MMU cache for all levels, otherwise
-		 * there is a possibility of repeated page faults on GPUs which supports
-		 * fine grained MMU cache invalidation.
-		 */
-		op_param.flush_skip_levels = 0x0;
-		op_param.vpfn = fault_pfn;
-		op_param.nr = 1;
-		spin_lock_irqsave(&kbdev->hwaccess_lock, hwaccess_flags);
-		err = kbase_mmu_hw_do_unlock(kbdev, faulting_as, &op_param);
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, hwaccess_flags);
+		if (!mmu_flush_cache_on_gpu_ctrl(kbdev)) {
+			mmu_hw_operation_begin(kbdev);
+			err = kbase_mmu_hw_do_unlock_no_addr(kbdev, faulting_as, &op_param);
+			mmu_hw_operation_end(kbdev);
+		} else {
+			/* Can safely skip the invalidate for all levels in case
+			 * of duplicate page faults.
+			 */
+			op_param.flush_skip_levels = 0xF;
+			op_param.vpfn = fault_pfn;
+			op_param.nr = 1;
+			err = kbase_mmu_hw_do_unlock(kbdev, faulting_as, &op_param);
+		}
 
 		if (err) {
 			dev_err(kbdev->dev,
@@ -1562,6 +1301,8 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 				fault->addr);
 		}
 
+		mutex_unlock(&kbdev->mmu_hw_mutex);
+
 		kbase_mmu_hw_enable_fault(kbdev, faulting_as, KBASE_MMU_FAULT_TYPE_PAGE);
 		kbase_gpu_vm_unlock(kctx);
 		goto fault_done;
@@ -1578,7 +1319,7 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 
 	spin_lock(&kctx->mem_partials_lock);
 	grown = page_fault_try_alloc(kctx, region, new_pages, &pages_to_grow, &grow_2mb_pool,
-				     fallback_to_small, prealloc_sas);
+				     prealloc_sas);
 	spin_unlock(&kctx->mem_partials_lock);
 
 	if (grown) {
@@ -1622,7 +1363,24 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 		else
 			trace_mali_mmu_page_fault_grow(region, fault, new_pages);
 
+#if MALI_INCREMENTAL_RENDERING_JM
+		/* Switch to incremental rendering if we have nearly run out of
+		 * memory in a JIT memory allocation.
+		 */
+		if (region->threshold_pages &&
+		    kbase_reg_current_backed_size(region) > region->threshold_pages) {
+			dev_dbg(kctx->kbdev->dev, "%zu pages exceeded IR threshold %zu",
+				new_pages + current_backed_size, region->threshold_pages);
+
+			if (kbase_mmu_switch_to_ir(kctx, region) >= 0) {
+				dev_dbg(kctx->kbdev->dev, "Get region %pK for IR", (void *)region);
+				kbase_va_region_alloc_get(kctx, region);
+			}
+		}
+#endif
+
 		/* AS transaction begin */
+		mutex_lock(&kbdev->mmu_hw_mutex);
 
 		/* clear MMU interrupt - this needs to be done after updating
 		 * the page tables but before issuing a FLUSH command. The
@@ -1639,16 +1397,16 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 		op_param.op = KBASE_MMU_OP_FLUSH_PT;
 		op_param.kctx_id = kctx->id;
 		op_param.mmu_sync_info = mmu_sync_info;
-		spin_lock_irqsave(&kbdev->hwaccess_lock, hwaccess_flags);
 		if (mmu_flush_cache_on_gpu_ctrl(kbdev)) {
 			/* Unlock to invalidate the TLB (and resume the MMU) */
 			op_param.flush_skip_levels = pgd_level_to_skip_flush(dirty_pgds);
 			err = kbase_mmu_hw_do_unlock(kbdev, faulting_as, &op_param);
 		} else {
 			/* flush L2 and unlock the VA (resumes the MMU) */
+			mmu_hw_operation_begin(kbdev);
 			err = kbase_mmu_hw_do_flush(kbdev, faulting_as, &op_param);
+			mmu_hw_operation_end(kbdev);
 		}
-		spin_unlock_irqrestore(&kbdev->hwaccess_lock, hwaccess_flags);
 
 		if (err) {
 			dev_err(kbdev->dev,
@@ -1656,6 +1414,7 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 				fault->addr);
 		}
 
+		mutex_unlock(&kbdev->mmu_hw_mutex);
 		/* AS transaction end */
 
 		/* reenable this in the mask */
@@ -1695,25 +1454,15 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 		 * Otherwise fail the allocation.
 		 */
 		if (pages_to_grow > 0) {
-			if (kbase_is_large_pages_enabled() && grow_2mb_pool) {
+			if (kbdev->pagesize_2mb && grow_2mb_pool) {
 				/* Round page requirement up to nearest 2 MB */
 				struct kbase_mem_pool *const lp_mem_pool =
 					&kctx->mem_pools.large[group_id];
 
-				pages_to_grow =
-					(pages_to_grow + ((1u << lp_mem_pool->order) - 1u)) >>
-					lp_mem_pool->order;
+				pages_to_grow = (pages_to_grow + ((1 << lp_mem_pool->order) - 1)) >>
+						lp_mem_pool->order;
 
 				ret = kbase_mem_pool_grow(lp_mem_pool, pages_to_grow, kctx->task);
-				/* Retry handling the fault with small pages if required
-				 * number of 2MB pages couldn't be allocated.
-				 */
-				if (ret < 0) {
-					fallback_to_small = true;
-					dev_dbg(kbdev->dev,
-						"No room for 2MB pages, fallback to small pages");
-					goto page_fault_retry;
-				}
 			} else {
 				struct kbase_mem_pool *const mem_pool =
 					&kctx->mem_pools.small[group_id];
@@ -1723,8 +1472,6 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 		}
 		if (ret < 0) {
 			/* failed to extend, handle as a normal PF */
-			if (unlikely(ret == -EPERM))
-				kbase_ctx_flag_set(kctx, KCTX_PAGE_FAULT_REPORT_SKIP);
 			kbase_mmu_report_fault_and_kill(kctx, faulting_as,
 							"Page allocation failure", fault);
 		} else {
@@ -1758,32 +1505,12 @@ void kbase_mmu_page_fault_worker(struct work_struct *data)
 	dev_dbg(kbdev->dev, "Leaving page_fault_worker %pK", (void *)data);
 }
 
-/**
- * kbase_mmu_alloc_pgd() - Allocate a PGD
- *
- * @kbdev:    Pointer to the instance of a kbase device.
- * @mmut:     Structure holding details of the MMU table for a kcontext.
- *
- * A 4KB sized PGD page is allocated for the PGD from the memory pool if PAGE_SIZE is 4KB.
- * Otherwise PGD is sub-allocated from a page that is allocated from the memory pool or
- * from one of the pages earlier allocated for the PGD of @mmut.
- *
- * Return:    Physical address of the allocated PGD.
- */
 static phys_addr_t kbase_mmu_alloc_pgd(struct kbase_device *kbdev, struct kbase_mmu_table *mmut)
 {
 	u64 *page;
 	struct page *p;
 	phys_addr_t pgd;
 
-	lockdep_assert_held(&mmut->mmu_lock);
-
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	pgd = allocate_from_pgd_pages_list(mmut);
-	if (pgd != KBASE_INVALID_PHYSICAL_ADDRESS)
-		return pgd;
-#endif
-
 	p = kbase_mem_pool_alloc(&kbdev->mem_pools.small[mmut->group_id]);
 	if (!p)
 		return KBASE_INVALID_PHYSICAL_ADDRESS;
@@ -1793,15 +1520,6 @@ static phys_addr_t kbase_mmu_alloc_pgd(struct kbase_device *kbdev, struct kbase_
 	if (page == NULL)
 		goto alloc_free;
 
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	if (!alloc_pgd_page_metadata(kbdev, mmut, p)) {
-		kbase_kunmap(p, page);
-		goto alloc_free;
-	}
-	mmut->num_free_pgd_sub_pages += (GPU_PAGES_PER_CPU_PAGE - 1);
-	mmut->last_allocated_pgd_page = p;
-#endif
-
 	pgd = page_to_phys(p);
 
 	/* If the MMU tables belong to a context then account the memory usage
@@ -1820,12 +1538,12 @@ static phys_addr_t kbase_mmu_alloc_pgd(struct kbase_device *kbdev, struct kbase_
 
 	kbase_trace_gpu_mem_usage_inc(kbdev, mmut->kctx, 1);
 
-	kbdev->mmu_mode->entries_invalidate(page, KBASE_MMU_PAGE_ENTRIES * GPU_PAGES_PER_CPU_PAGE);
+	kbdev->mmu_mode->entries_invalidate(page, KBASE_MMU_PAGE_ENTRIES);
 
 	/* As this page is newly created, therefore there is no content to
 	 * clean or invalidate in the GPU caches.
 	 */
-	kbase_mmu_sync_pgd_cpu(kbdev, pgd_dma_addr(p, pgd), PAGE_SIZE);
+	kbase_mmu_sync_pgd_cpu(kbdev, kbase_dma_addr(p), PAGE_SIZE);
 
 	kbase_kunmap(p, page);
 	return pgd;
@@ -1842,7 +1560,7 @@ static phys_addr_t kbase_mmu_alloc_pgd(struct kbase_device *kbdev, struct kbase_
  * @kbdev:    Device pointer.
  * @mmut:     GPU MMU page table.
  * @pgd:      Physical addresse of level N page directory.
- * @vpfn:     The virtual page frame number, in GPU_PAGE_SIZE units.
+ * @vpfn:     The virtual page frame number.
  * @level:    The level of MMU page table (N).
  *
  * Return:
@@ -1867,7 +1585,7 @@ static int mmu_get_next_pgd(struct kbase_device *kbdev, struct kbase_mmu_table *
 	vpfn &= 0x1FF;
 
 	p = pfn_to_page(PFN_DOWN(*pgd));
-	page = kmap_pgd(p, *pgd);
+	page = kbase_kmap(p);
 	if (page == NULL) {
 		dev_err(kbdev->dev, "%s: kmap failure", __func__);
 		return -EINVAL;
@@ -1876,7 +1594,7 @@ static int mmu_get_next_pgd(struct kbase_device *kbdev, struct kbase_mmu_table *
 	if (!kbdev->mmu_mode->pte_is_valid(page[vpfn], level)) {
 		dev_dbg(kbdev->dev, "%s: invalid PTE at level %d vpfn 0x%llx", __func__, level,
 			vpfn);
-		kunmap_pgd(p, page);
+		kbase_kunmap(p, page);
 		return -EFAULT;
 	} else {
 		target_pgd = kbdev->mmu_mode->pte_to_phy_addr(
@@ -1884,7 +1602,7 @@ static int mmu_get_next_pgd(struct kbase_device *kbdev, struct kbase_mmu_table *
 				kbdev->mgm_dev, MGM_DEFAULT_PTE_GROUP, level, page[vpfn]));
 	}
 
-	kunmap_pgd(p, page);
+	kbase_kunmap(p, page);
 	*pgd = target_pgd;
 
 	return 0;
@@ -1895,7 +1613,7 @@ static int mmu_get_next_pgd(struct kbase_device *kbdev, struct kbase_mmu_table *
  *
  * @kbdev:    Device pointer.
  * @mmut:     GPU MMU page table.
- * @vpfn:     The virtual page frame number, in GPU_PAGE_SIZE units.
+ * @vpfn:     The virtual page frame number.
  * @in_level:     The level of MMU page table (N).
  * @out_level:    Set to the level of the lowest valid PGD found on success.
  *                Invalid on error.
@@ -1946,7 +1664,6 @@ static int mmu_get_lowest_valid_pgd(struct kbase_device *kbdev, struct kbase_mmu
 
 	return err;
 }
-KBASE_ALLOW_ERROR_INJECTION_TEST_API(mmu_get_lowest_valid_pgd, ERRNO);
 
 /*
  * On success, sets out_pgd to the PGD for the specified level of translation
@@ -1985,10 +1702,8 @@ static void mmu_insert_pages_failure_recovery(struct kbase_device *kbdev,
 	u64 vpfn = from_vpfn;
 	struct kbase_mmu_mode const *mmu_mode;
 
-	/* Both from_vpfn and to_vpfn are in GPU_PAGE_SIZE units */
-
 	/* 64-bit address range is the max */
-	KBASE_DEBUG_ASSERT(vpfn <= (U64_MAX / GPU_PAGE_SIZE));
+	KBASE_DEBUG_ASSERT(vpfn <= (U64_MAX / PAGE_SIZE));
 	KBASE_DEBUG_ASSERT(from_vpfn <= to_vpfn);
 
 	lockdep_assert_held(&mmut->mmu_lock);
@@ -2012,14 +1727,14 @@ static void mmu_insert_pages_failure_recovery(struct kbase_device *kbdev,
 		if (count > left)
 			count = left;
 
-		/* need to check if this is a 2MB page or a small page */
+		/* need to check if this is a 2MB page or a 4kB */
 		for (level = MIDGARD_MMU_TOPLEVEL; level <= MIDGARD_MMU_BOTTOMLEVEL; level++) {
 			idx = (vpfn >> ((3 - level) * 9)) & 0x1FF;
 			pgds[level] = pgd;
-			page = kmap_pgd(p, pgd);
+			page = kbase_kmap(p);
 			if (mmu_mode->ate_is_valid(page[idx], level))
 				break; /* keep the mapping */
-			kunmap_pgd(p, page);
+			kbase_kunmap(p, page);
 			pgd = mmu_mode->pte_to_phy_addr(kbdev->mgm_dev->ops.mgm_pte_to_original_pte(
 				kbdev->mgm_dev, MGM_DEFAULT_PTE_GROUP, level, page[idx]));
 			p = phys_to_page(pgd);
@@ -2052,21 +1767,12 @@ static void mmu_insert_pages_failure_recovery(struct kbase_device *kbdev,
 		mmu_mode->entries_invalidate(&page[idx], pcount);
 
 		if (!num_of_valid_entries) {
-			mmu_mode->set_num_valid_entries(page, 0);
-
-			kunmap_pgd(p, page);
+			kbase_kunmap(p, page);
 
-			kbase_mmu_update_and_free_parent_pgds(kbdev, mmut, pgds, vpfn, level - 1,
-							      KBASE_MMU_OP_NONE, dirty_pgds, 0);
-
-			/* No CPU and GPU cache maintenance is done here as caller would do the
-			 * complete flush of GPU cache and invalidation of TLB before the PGD
-			 * page is freed. CPU cache flush would be done when the PGD page is
-			 * returned to the memory pool.
-			 */
-
-			kbase_mmu_add_to_free_pgds_list(mmut, pgd);
+			kbase_mmu_add_to_free_pgds_list(mmut, p);
 
+			kbase_mmu_update_and_free_parent_pgds(kbdev, mmut, pgds, vpfn, level,
+							      KBASE_MMU_OP_NONE, dirty_pgds);
 			vpfn += count;
 			continue;
 		}
@@ -2077,9 +1783,9 @@ static void mmu_insert_pages_failure_recovery(struct kbase_device *kbdev,
 		 * going to be done by the caller
 		 */
 		kbase_mmu_sync_pgd(kbdev, mmut->kctx, pgd + (idx * sizeof(u64)),
-				   pgd_dma_addr(p, pgd) + sizeof(u64) * idx, sizeof(u64) * pcount,
+				   kbase_dma_addr(p) + sizeof(u64) * idx, sizeof(u64) * pcount,
 				   KBASE_MMU_OP_NONE);
-		kunmap_pgd(p, page);
+		kbase_kunmap(p, page);
 next:
 		vpfn += count;
 	}
@@ -2089,9 +1795,8 @@ static void mmu_insert_pages_failure_recovery(struct kbase_device *kbdev,
 	 * going to happen to these pages at this stage. They might return
 	 * movable once they are returned to a memory pool.
 	 */
-	if (kbase_is_page_migration_enabled() && !ignore_page_migration && phys &&
-	    !is_huge(*phys) && !is_partial(*phys)) {
-		const u64 num_pages = (to_vpfn - from_vpfn) / GPU_PAGES_PER_CPU_PAGE;
+	if (kbase_is_page_migration_enabled() && !ignore_page_migration && phys) {
+		const u64 num_pages = to_vpfn - from_vpfn;
 		u64 i;
 
 		for (i = 0; i < num_pages; i++) {
@@ -2157,7 +1862,7 @@ static void mmu_flush_invalidate_insert_pages(struct kbase_device *kbdev,
  *                The bottom PGD level.
  * @insert_level: The level of MMU page table where the chain of newly allocated
  *                PGDs needs to be linked-in/inserted.
- * @insert_vpfn:  The virtual page frame number, in GPU_PAGE_SIZE units, for the ATE.
+ * @insert_vpfn:  The virtual page frame number for the ATE.
  * @pgds_to_insert: Ptr to an array (size MIDGARD_MMU_BOTTOMLEVEL+1) that contains
  *                  the physical addresses of newly allocated PGDs from index
  *                  insert_level+1 to cur_level, and an existing PGD at index
@@ -2198,7 +1903,7 @@ static int update_parent_pgds(struct kbase_device *kbdev, struct kbase_mmu_table
 			goto failure_recovery;
 		}
 
-		parent_page_va = kmap_pgd(parent_page, parent_pgd);
+		parent_page_va = kbase_kmap(parent_page);
 
 		if (unlikely(parent_page_va == NULL)) {
 			dev_err(kbdev->dev, "%s: kmap failure", __func__);
@@ -2210,17 +1915,15 @@ static int update_parent_pgds(struct kbase_device *kbdev, struct kbase_mmu_table
 
 		kbdev->mmu_mode->entry_set_pte(&pte, target_pgd);
 		parent_page_va[parent_vpfn] = kbdev->mgm_dev->ops.mgm_update_gpu_pte(
-			kbdev->mgm_dev, MGM_DEFAULT_PTE_GROUP, PBHA_ID_DEFAULT, PTE_FLAGS_NONE,
-			parent_index, pte);
+			kbdev->mgm_dev, MGM_DEFAULT_PTE_GROUP, parent_index, pte);
 		kbdev->mmu_mode->set_num_valid_entries(parent_page_va, current_valid_entries + 1);
-		kunmap_pgd(parent_page, parent_page_va);
+		kbase_kunmap(parent_page, parent_page_va);
 
 		if (parent_index != insert_level) {
 			/* Newly allocated PGDs */
-			kbase_mmu_sync_pgd_cpu(kbdev,
-					       pgd_dma_addr(parent_page, parent_pgd) +
-						       (parent_vpfn * sizeof(u64)),
-					       sizeof(u64));
+			kbase_mmu_sync_pgd_cpu(
+				kbdev, kbase_dma_addr(parent_page) + (parent_vpfn * sizeof(u64)),
+				sizeof(u64));
 		} else {
 			/* A new valid entry is added to an existing PGD. Perform the
 			 * invalidate operation for GPU cache as it could be having a
@@ -2228,7 +1931,7 @@ static int update_parent_pgds(struct kbase_device *kbdev, struct kbase_mmu_table
 			 */
 			kbase_mmu_sync_pgd(
 				kbdev, mmut->kctx, parent_pgd + (parent_vpfn * sizeof(u64)),
-				pgd_dma_addr(parent_page, parent_pgd) + (parent_vpfn * sizeof(u64)),
+				kbase_dma_addr(parent_page) + (parent_vpfn * sizeof(u64)),
 				sizeof(u64), KBASE_MMU_OP_FLUSH_PT);
 		}
 
@@ -2239,9 +1942,6 @@ static int update_parent_pgds(struct kbase_device *kbdev, struct kbase_mmu_table
 
 			spin_lock(&page_md->migrate_lock);
 
-#if GPU_PAGES_PER_CPU_PAGE > 1
-			page_md->status = PAGE_STATUS_SET(page_md->status, NOT_MOVABLE);
-#else
 			WARN_ON_ONCE(PAGE_STATUS_GET(page_md->status) != ALLOCATE_IN_PROGRESS ||
 				     IS_PAGE_ISOLATED(page_md->status));
 
@@ -2253,7 +1953,6 @@ static int update_parent_pgds(struct kbase_device *kbdev, struct kbase_mmu_table
 			} else {
 				page_md->status = PAGE_STATUS_SET(page_md->status, NOT_MOVABLE);
 			}
-#endif
 
 			spin_unlock(&page_md->migrate_lock);
 		}
@@ -2266,11 +1965,11 @@ static int update_parent_pgds(struct kbase_device *kbdev, struct kbase_mmu_table
 	for (; pgd_index < cur_level; pgd_index++) {
 		phys_addr_t pgd = pgds_to_insert[pgd_index];
 		struct page *pgd_page = pfn_to_page(PFN_DOWN(pgd));
-		u64 *pgd_page_va = kmap_pgd(pgd_page, pgd);
+		u64 *pgd_page_va = kbase_kmap(pgd_page);
 		u64 vpfn = (insert_vpfn >> ((3 - pgd_index) * 9)) & 0x1FF;
 
 		kbdev->mmu_mode->entries_invalidate(&pgd_page_va[vpfn], 1);
-		kunmap_pgd(pgd_page, pgd_page_va);
+		kbase_kunmap(pgd_page, pgd_page_va);
 	}
 
 	return err;
@@ -2286,8 +1985,6 @@ static int update_parent_pgds(struct kbase_device *kbdev, struct kbase_mmu_table
  * @level_high: The higher bound for the levels for which the PGD allocs are required
  * @new_pgds:   Ptr to an array (size MIDGARD_MMU_BOTTOMLEVEL+1) to write the
  *              newly allocated PGD addresses to.
- * @pool_grown: True if new PGDs required the memory pool to grow to allocate more pages,
- *              or false otherwise
  *
  * Numerically, level_low < level_high, not to be confused with top level and
  * bottom level concepts for MMU PGDs. They are only used as low and high bounds
@@ -2298,32 +1995,37 @@ static int update_parent_pgds(struct kbase_device *kbdev, struct kbase_mmu_table
  * * -ENOMEM - allocation failed for a PGD.
  */
 static int mmu_insert_alloc_pgds(struct kbase_device *kbdev, struct kbase_mmu_table *mmut,
-				 phys_addr_t *new_pgds, int level_low, int level_high,
-				 bool *pool_grown)
+				 phys_addr_t *new_pgds, int level_low, int level_high)
 {
 	int err = 0;
 	int i;
 
 	lockdep_assert_held(&mmut->mmu_lock);
 
-	*pool_grown = false;
 	for (i = level_low; i <= level_high; i++) {
-		if (new_pgds[i] != KBASE_INVALID_PHYSICAL_ADDRESS)
-			continue;
 		do {
 			new_pgds[i] = kbase_mmu_alloc_pgd(kbdev, mmut);
 			if (new_pgds[i] != KBASE_INVALID_PHYSICAL_ADDRESS)
 				break;
+
 			mutex_unlock(&mmut->mmu_lock);
 			err = kbase_mem_pool_grow(&kbdev->mem_pools.small[mmut->group_id],
-						  (size_t)level_high, NULL);
+						  level_high, NULL);
 			mutex_lock(&mmut->mmu_lock);
 			if (err) {
 				dev_err(kbdev->dev, "%s: kbase_mem_pool_grow() returned error %d",
 					__func__, err);
+
+				/* Free all PGDs allocated in previous successful iterations
+				 * from (i-1) to level_low
+				 */
+				for (i = (i - 1); i >= level_low; i--) {
+					if (new_pgds[i] != KBASE_INVALID_PHYSICAL_ADDRESS)
+						kbase_mmu_free_pgd(kbdev, mmut, new_pgds[i]);
+				}
+
 				return err;
 			}
-			*pool_grown = true;
 		} while (1);
 	}
 
@@ -2348,13 +2050,10 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 	enum kbase_mmu_op_type flush_op;
 	struct kbase_mmu_table *mmut = &kctx->mmu;
 	int l, cur_level, insert_level;
-	const phys_addr_t base_phys_address = as_phys_addr_t(phys);
 
 	if (WARN_ON(kctx == NULL))
 		return -EINVAL;
 
-	lockdep_assert_held(&kctx->reg_lock);
-
 	/* 64-bit address range is the max */
 	KBASE_DEBUG_ASSERT(start_vpfn <= (U64_MAX / PAGE_SIZE));
 
@@ -2364,10 +2063,6 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 	if (nr == 0)
 		return 0;
 
-	/* Convert to GPU_PAGE_SIZE units. */
-	insert_vpfn *= GPU_PAGES_PER_CPU_PAGE;
-	remain *= GPU_PAGES_PER_CPU_PAGE;
-
 	/* If page migration is enabled, pages involved in multiple GPU mappings
 	 * are always treated as not movable.
 	 */
@@ -2390,7 +2085,6 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 		struct page *p;
 		register unsigned int num_of_valid_entries;
 		bool newly_created_pgd = false;
-		bool pool_grown;
 
 		if (count > remain)
 			count = remain;
@@ -2398,10 +2092,6 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 		cur_level = MIDGARD_MMU_BOTTOMLEVEL;
 		insert_level = cur_level;
 
-		for (l = MIDGARD_MMU_TOPLEVEL + 1; l <= cur_level; l++)
-			new_pgds[l] = KBASE_INVALID_PHYSICAL_ADDRESS;
-
-repeat_page_table_walk:
 		/*
 		 * Repeatedly calling mmu_get_lowest_valid_pgd() is clearly
 		 * suboptimal. We don't have to re-parse the whole tree
@@ -2416,7 +2106,7 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 		if (err) {
 			dev_err(kbdev->dev, "%s: mmu_get_lowest_valid_pgd() returned error %d",
 				__func__, err);
-			goto fail_unlock_free_pgds;
+			goto fail_unlock;
 		}
 
 		/* No valid pgd at cur_level */
@@ -2425,12 +2115,9 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 			 * down to the lowest valid pgd at insert_level
 			 */
 			err = mmu_insert_alloc_pgds(kbdev, mmut, new_pgds, (insert_level + 1),
-						    cur_level, &pool_grown);
+						    cur_level);
 			if (err)
-				goto fail_unlock_free_pgds;
-
-			if (pool_grown)
-				goto repeat_page_table_walk;
+				goto fail_unlock;
 
 			newly_created_pgd = true;
 
@@ -2445,7 +2132,7 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 
 		p = pfn_to_page(PFN_DOWN(pgd));
 
-		pgd_page = kmap_pgd(p, pgd);
+		pgd_page = kbase_kmap(p);
 		if (!pgd_page) {
 			dev_err(kbdev->dev, "%s: kmap failure", __func__);
 			err = -ENOMEM;
@@ -2455,19 +2142,14 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 
 		num_of_valid_entries = kbdev->mmu_mode->get_num_valid_entries(pgd_page);
 
-		for (i = 0; i < count; i += GPU_PAGES_PER_CPU_PAGE) {
-			unsigned int j;
+		for (i = 0; i < count; i++) {
+			unsigned int ofs = vindex + i;
 
-			for (j = 0; j < GPU_PAGES_PER_CPU_PAGE; j++) {
-				unsigned int ofs = vindex + i + j;
-				phys_addr_t page_address = base_phys_address + (j * GPU_PAGE_SIZE);
+			/* Fail if the current page is a valid ATE entry */
+			KBASE_DEBUG_ASSERT(0 == (pgd_page[ofs] & 1UL));
 
-				/* Fail if the current page is a valid ATE entry */
-				WARN_ON_ONCE((pgd_page[ofs] & 1UL));
-				pgd_page[ofs] = kbase_mmu_create_ate(kbdev, as_tagged(page_address),
-								     flags, MIDGARD_MMU_BOTTOMLEVEL,
-								     group_id);
-			}
+			pgd_page[ofs] = kbase_mmu_create_ate(kbdev, phys, flags,
+							     MIDGARD_MMU_BOTTOMLEVEL, group_id);
 		}
 
 		kbdev->mmu_mode->set_num_valid_entries(pgd_page, num_of_valid_entries + count);
@@ -2484,8 +2166,8 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 		flush_op = newly_created_pgd ? KBASE_MMU_OP_NONE : KBASE_MMU_OP_FLUSH_PT;
 
 		kbase_mmu_sync_pgd(kbdev, kctx, pgd + (vindex * sizeof(u64)),
-				   pgd_dma_addr(p, pgd) + (vindex * sizeof(u64)),
-				   count * sizeof(u64), flush_op);
+				   kbase_dma_addr(p) + (vindex * sizeof(u64)), count * sizeof(u64),
+				   flush_op);
 
 		if (newly_created_pgd) {
 			err = update_parent_pgds(kbdev, mmut, cur_level, insert_level, insert_vpfn,
@@ -2496,14 +2178,14 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 
 				kbdev->mmu_mode->entries_invalidate(&pgd_page[vindex], count);
 
-				kunmap_pgd(p, pgd_page);
+				kbase_kunmap(p, pgd_page);
 				goto fail_unlock_free_pgds;
 			}
 		}
 
 		insert_vpfn += count;
 		remain -= count;
-		kunmap_pgd(p, pgd_page);
+		kbase_kunmap(p, pgd_page);
 	}
 
 	mutex_unlock(&mmut->mmu_lock);
@@ -2516,13 +2198,13 @@ static int kbase_mmu_insert_single_page(struct kbase_context *kctx, u64 start_vp
 fail_unlock_free_pgds:
 	/* Free the pgds allocated by us from insert_level+1 to bottom level */
 	for (l = cur_level; l > insert_level; l--)
-		if (new_pgds[l] != KBASE_INVALID_PHYSICAL_ADDRESS)
-			kbase_mmu_free_pgd(kbdev, mmut, new_pgds[l]);
+		kbase_mmu_free_pgd(kbdev, mmut, new_pgds[l]);
 
-	if (insert_vpfn != (start_vpfn * GPU_PAGES_PER_CPU_PAGE)) {
+fail_unlock:
+	if (insert_vpfn != start_vpfn) {
 		/* Invalidate the pages we have partially completed */
-		mmu_insert_pages_failure_recovery(kbdev, mmut, start_vpfn * GPU_PAGES_PER_CPU_PAGE,
-						  insert_vpfn, &dirty_pgds, NULL, true);
+		mmu_insert_pages_failure_recovery(kbdev, mmut, start_vpfn, insert_vpfn, &dirty_pgds,
+						  NULL, true);
 	}
 
 	mmu_flush_invalidate_insert_pages(kbdev, mmut, start_vpfn, nr, dirty_pgds, mmu_sync_info,
@@ -2604,7 +2286,7 @@ static void kbase_mmu_progress_migration_on_teardown(struct kbase_device *kbdev,
 		struct page *phys_page = as_page(phys[i]);
 		struct kbase_page_metadata *page_md = kbase_page_private(phys_page);
 
-		/* Skip the small page that is part of a large page, as the large page is
+		/* Skip the 4KB page that is part of a large page, as the large page is
 		 * excluded from the migration process.
 		 */
 		if (is_huge(phys[i]) || is_partial(phys[i]))
@@ -2642,19 +2324,13 @@ u64 kbase_mmu_create_ate(struct kbase_device *const kbdev, struct tagged_addr co
 			 unsigned long const flags, int const level, int const group_id)
 {
 	u64 entry;
-	unsigned int pte_flags = 0;
 
 	kbdev->mmu_mode->entry_set_ate(&entry, phy, flags, level);
-
-	if ((flags & KBASE_REG_GPU_CACHED) && !(flags & KBASE_REG_CPU_CACHED))
-		pte_flags |= BIT(MMA_VIOLATION);
-
-	return kbdev->mgm_dev->ops.mgm_update_gpu_pte(kbdev->mgm_dev, (unsigned int)group_id,
-						      kbdev->mma_wa_id, pte_flags, level, entry);
+	return kbdev->mgm_dev->ops.mgm_update_gpu_pte(kbdev->mgm_dev, group_id, level, entry);
 }
 
 static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mmu_table *mmut,
-				     u64 start_vpfn, struct tagged_addr *phys, size_t nr,
+				     const u64 start_vpfn, struct tagged_addr *phys, size_t nr,
 				     unsigned long flags, int const group_id, u64 *dirty_pgds,
 				     struct kbase_va_region *reg, bool ignore_page_migration)
 {
@@ -2669,9 +2345,6 @@ static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mm
 	int l, cur_level, insert_level;
 	struct tagged_addr *start_phys = phys;
 
-	if (mmut->kctx)
-		lockdep_assert_held(&mmut->kctx->reg_lock);
-
 	/* Note that 0 is a valid start_vpfn */
 	/* 64-bit address range is the max */
 	KBASE_DEBUG_ASSERT(start_vpfn <= (U64_MAX / PAGE_SIZE));
@@ -2682,9 +2355,6 @@ static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mm
 	if (nr == 0)
 		return 0;
 
-	/* Convert to GPU_PAGE_SIZE units. */
-	insert_vpfn *= GPU_PAGES_PER_CPU_PAGE;
-	remain *= GPU_PAGES_PER_CPU_PAGE;
 	mutex_lock(&mmut->mmu_lock);
 
 	while (remain) {
@@ -2694,30 +2364,17 @@ static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mm
 		register unsigned int num_of_valid_entries;
 		bool newly_created_pgd = false;
 		enum kbase_mmu_op_type flush_op;
-		bool pool_grown;
 
 		if (count > remain)
 			count = remain;
 
-		/* There are 3 conditions to satisfy in order to create a level 2 ATE:
-		 *
-		 * - The GPU VA is aligned to 2 MB.
-		 * - The physical address is tagged as the head of a 2 MB region,
-		 *   which guarantees a contiguous physical address range.
-		 * - There are actually 2 MB of virtual and physical pages to map,
-		 *   i.e. 512 entries for the MMU page table.
-		 */
-		if (!vindex && is_huge_head(*phys) && (count == KBASE_MMU_PAGE_ENTRIES))
+		if (!vindex && is_huge_head(*phys))
 			cur_level = MIDGARD_MMU_LEVEL(2);
 		else
 			cur_level = MIDGARD_MMU_BOTTOMLEVEL;
 
 		insert_level = cur_level;
 
-		for (l = MIDGARD_MMU_TOPLEVEL + 1; l <= cur_level; l++)
-			new_pgds[l] = KBASE_INVALID_PHYSICAL_ADDRESS;
-
-repeat_page_table_walk:
 		/*
 		 * Repeatedly calling mmu_get_lowest_valid_pgd() is clearly
 		 * suboptimal. We don't have to re-parse the whole tree
@@ -2732,7 +2389,7 @@ static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mm
 		if (err) {
 			dev_err(kbdev->dev, "%s: mmu_get_lowest_valid_pgd() returned error %d",
 				__func__, err);
-			goto fail_unlock_free_pgds;
+			goto fail_unlock;
 		}
 
 		/* No valid pgd at cur_level */
@@ -2741,12 +2398,9 @@ static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mm
 			 * down to the lowest valid pgd at insert_level
 			 */
 			err = mmu_insert_alloc_pgds(kbdev, mmut, new_pgds, (insert_level + 1),
-						    cur_level, &pool_grown);
+						    cur_level);
 			if (err)
-				goto fail_unlock_free_pgds;
-
-			if (pool_grown)
-				goto repeat_page_table_walk;
+				goto fail_unlock;
 
 			newly_created_pgd = true;
 
@@ -2760,7 +2414,7 @@ static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mm
 		}
 
 		p = pfn_to_page(PFN_DOWN(pgd));
-		pgd_page = kmap_pgd(p, pgd);
+		pgd_page = kbase_kmap(p);
 
 		if (!pgd_page) {
 			dev_err(kbdev->dev, "%s: kmap failure", __func__);
@@ -2778,39 +2432,29 @@ static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mm
 
 			num_of_valid_entries++;
 		} else {
-			for (i = 0; i < count; i += GPU_PAGES_PER_CPU_PAGE) {
-				struct tagged_addr base_tagged_addr =
-					phys[i / GPU_PAGES_PER_CPU_PAGE];
-				phys_addr_t base_phys_address = as_phys_addr_t(base_tagged_addr);
-				unsigned int j;
-
-				for (j = 0; j < GPU_PAGES_PER_CPU_PAGE; j++) {
-					unsigned int ofs = vindex + i + j;
-					u64 *target = &pgd_page[ofs];
-					phys_addr_t page_address =
-						base_phys_address + (j * GPU_PAGE_SIZE);
-
-					/* Warn if the current page is a valid ATE
-					 * entry. The page table shouldn't have anything
-					 * in the place where we are trying to put a
-					 * new entry. Modification to page table entries
-					 * should be performed with
-					 * kbase_mmu_update_pages()
-					 */
-					WARN_ON_ONCE((*target & 1UL) != 0);
+			for (i = 0; i < count; i++) {
+				unsigned int ofs = vindex + i;
+				u64 *target = &pgd_page[ofs];
+
+				/* Warn if the current page is a valid ATE
+				 * entry. The page table shouldn't have anything
+				 * in the place where we are trying to put a
+				 * new entry. Modification to page table entries
+				 * should be performed with
+				 * kbase_mmu_update_pages()
+				 */
+				WARN_ON((*target & 1UL) != 0);
 
-					*target = kbase_mmu_create_ate(kbdev,
-								       as_tagged(page_address),
-								       flags, cur_level, group_id);
-				}
+				*target = kbase_mmu_create_ate(kbdev, phys[i], flags, cur_level,
+							       group_id);
 
 				/* If page migration is enabled, this is the right time
 				 * to update the status of the page.
 				 */
 				if (kbase_is_page_migration_enabled() && !ignore_page_migration &&
-				    !is_huge(base_tagged_addr) && !is_partial(base_tagged_addr))
-					kbase_mmu_progress_migration_on_insert(
-						base_tagged_addr, reg, mmut, insert_vpfn + i);
+				    !is_huge(phys[i]) && !is_partial(phys[i]))
+					kbase_mmu_progress_migration_on_insert(phys[i], reg, mmut,
+									       insert_vpfn + i);
 			}
 			num_of_valid_entries += count;
 		}
@@ -2830,8 +2474,8 @@ static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mm
 		flush_op = newly_created_pgd ? KBASE_MMU_OP_NONE : KBASE_MMU_OP_FLUSH_PT;
 
 		kbase_mmu_sync_pgd(kbdev, mmut->kctx, pgd + (vindex * sizeof(u64)),
-				   pgd_dma_addr(p, pgd) + (vindex * sizeof(u64)),
-				   count * sizeof(u64), flush_op);
+				   kbase_dma_addr(p) + (vindex * sizeof(u64)), count * sizeof(u64),
+				   flush_op);
 
 		if (newly_created_pgd) {
 			err = update_parent_pgds(kbdev, mmut, cur_level, insert_level, insert_vpfn,
@@ -2842,15 +2486,15 @@ static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mm
 
 				kbdev->mmu_mode->entries_invalidate(&pgd_page[vindex], count);
 
-				kunmap_pgd(p, pgd_page);
+				kbase_kunmap(p, pgd_page);
 				goto fail_unlock_free_pgds;
 			}
 		}
 
-		phys += (count / GPU_PAGES_PER_CPU_PAGE);
+		phys += count;
 		insert_vpfn += count;
 		remain -= count;
-		kunmap_pgd(p, pgd_page);
+		kbase_kunmap(p, pgd_page);
 	}
 
 	mutex_unlock(&mmut->mmu_lock);
@@ -2860,14 +2504,13 @@ static int mmu_insert_pages_no_flush(struct kbase_device *kbdev, struct kbase_mm
 fail_unlock_free_pgds:
 	/* Free the pgds allocated by us from insert_level+1 to bottom level */
 	for (l = cur_level; l > insert_level; l--)
-		if (new_pgds[l] != KBASE_INVALID_PHYSICAL_ADDRESS)
-			kbase_mmu_free_pgd(kbdev, mmut, new_pgds[l]);
+		kbase_mmu_free_pgd(kbdev, mmut, new_pgds[l]);
 
-	if (insert_vpfn != (start_vpfn * GPU_PAGES_PER_CPU_PAGE)) {
+fail_unlock:
+	if (insert_vpfn != start_vpfn) {
 		/* Invalidate the pages we have partially completed */
-		mmu_insert_pages_failure_recovery(kbdev, mmut, start_vpfn * GPU_PAGES_PER_CPU_PAGE,
-						  insert_vpfn, dirty_pgds, start_phys,
-						  ignore_page_migration);
+		mmu_insert_pages_failure_recovery(kbdev, mmut, start_vpfn, insert_vpfn, dirty_pgds,
+						  start_phys, ignore_page_migration);
 	}
 
 	mmu_flush_invalidate_insert_pages(kbdev, mmut, start_vpfn, nr,
@@ -2924,7 +2567,6 @@ int kbase_mmu_insert_pages(struct kbase_device *kbdev, struct kbase_mmu_table *m
 }
 
 KBASE_EXPORT_TEST_API(kbase_mmu_insert_pages);
-KBASE_ALLOW_ERROR_INJECTION_TEST_API(kbase_mmu_insert_pages, ERRNO);
 
 int kbase_mmu_insert_pages_skip_status_update(struct kbase_device *kbdev,
 					      struct kbase_mmu_table *mmut, u64 vpfn,
@@ -2982,7 +2624,6 @@ int kbase_mmu_insert_aliased_pages(struct kbase_device *kbdev, struct kbase_mmu_
 
 	return 0;
 }
-KBASE_ALLOW_ERROR_INJECTION_TEST_API(kbase_mmu_insert_aliased_pages, ERRNO);
 
 void kbase_mmu_update(struct kbase_device *kbdev, struct kbase_mmu_table *mmut, int as_nr)
 {
@@ -2997,9 +2638,7 @@ KBASE_EXPORT_TEST_API(kbase_mmu_update);
 void kbase_mmu_disable_as(struct kbase_device *kbdev, int as_nr)
 {
 	lockdep_assert_held(&kbdev->hwaccess_lock);
-#if !MALI_USE_CSF
 	lockdep_assert_held(&kbdev->mmu_hw_mutex);
-#endif
 
 	kbdev->mmu_mode->disable_as(kbdev, as_nr);
 }
@@ -3022,9 +2661,10 @@ void kbase_mmu_disable(struct kbase_context *kctx)
 	KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
 
 	lockdep_assert_held(&kctx->kbdev->hwaccess_lock);
+	lockdep_assert_held(&kctx->kbdev->mmu_hw_mutex);
 
 	op_param.vpfn = 0;
-	op_param.nr = ~0U;
+	op_param.nr = ~0;
 	op_param.op = KBASE_MMU_OP_FLUSH_MEM;
 	op_param.kctx_id = kctx->id;
 	op_param.mmu_sync_info = mmu_sync_info;
@@ -3065,8 +2705,6 @@ void kbase_mmu_disable(struct kbase_context *kctx)
 				kctx->tgid, kctx->id);
 	}
 #else
-	lockdep_assert_held(&kctx->kbdev->mmu_hw_mutex);
-
 	CSTD_UNUSED(lock_err);
 
 	/*
@@ -3076,7 +2714,7 @@ void kbase_mmu_disable(struct kbase_context *kctx)
 	 * The job scheduler code will already be holding the locks and context
 	 * so just do the flush.
 	 */
-	flush_err = kbase_mmu_hw_do_flush(kbdev, &kbdev->as[kctx->as_nr], &op_param);
+	flush_err = kbase_mmu_hw_do_flush_locked(kbdev, &kbdev->as[kctx->as_nr], &op_param);
 	if (flush_err) {
 		dev_err(kbdev->dev,
 			"Flush for GPU page table update did not complete to disable AS %d for ctx %d_%d",
@@ -3100,66 +2738,50 @@ KBASE_EXPORT_TEST_API(kbase_mmu_disable);
 static void kbase_mmu_update_and_free_parent_pgds(struct kbase_device *kbdev,
 						  struct kbase_mmu_table *mmut, phys_addr_t *pgds,
 						  u64 vpfn, int level,
-						  enum kbase_mmu_op_type flush_op, u64 *dirty_pgds,
-						  int as_nr)
+						  enum kbase_mmu_op_type flush_op, u64 *dirty_pgds)
 {
-	phys_addr_t current_pgd = pgds[level];
-	struct page *p = phys_to_page(current_pgd);
-	u64 *current_page = kmap_pgd(p, current_pgd);
-	unsigned int current_valid_entries = kbdev->mmu_mode->get_num_valid_entries(current_page);
-	unsigned int index = (vpfn >> ((3 - level) * 9)) & 0x1FFU;
+	int current_level;
 
 	lockdep_assert_held(&mmut->mmu_lock);
 
-	/* We need to track every level that needs updating */
-	if (dirty_pgds)
-		*dirty_pgds |= 1ULL << level;
-
-	kbdev->mmu_mode->entries_invalidate(&current_page[index], 1);
-	if (current_valid_entries == 1 && level != MIDGARD_MMU_LEVEL(0)) {
-		kbdev->mmu_mode->set_num_valid_entries(current_page, 0);
-
-		kunmap_pgd(p, current_page);
-
-		kbase_mmu_update_and_free_parent_pgds(kbdev, mmut, pgds, vpfn, level - 1, flush_op,
-						      dirty_pgds, as_nr);
-
-		/* Check if fine grained GPU cache maintenance is being used */
-		if (flush_op == KBASE_MMU_OP_FLUSH_PT) {
-			/* Ensure the invalidated PTE is visible in memory right away */
-			kbase_mmu_sync_pgd_cpu(kbdev,
-					       pgd_dma_addr(p, current_pgd) + (index * sizeof(u64)),
-					       sizeof(u64));
-			/* Invalidate the GPU cache for the whole PGD page and not just for
-			 * the cacheline containing the invalidated PTE, as the PGD page is
-			 * going to be freed. There is an extremely remote possibility that
-			 * other cachelines (containing all invalid PTEs) of PGD page are
-			 * also present in the GPU cache.
-			 */
-			kbase_mmu_sync_pgd_gpu(kbdev, mmut->kctx, current_pgd, 512 * sizeof(u64),
-					       KBASE_MMU_OP_FLUSH_PT);
-		}
+	for (current_level = level - 1; current_level >= MIDGARD_MMU_LEVEL(0); current_level--) {
+		phys_addr_t current_pgd = pgds[current_level];
+		struct page *p = phys_to_page(current_pgd);
 
-		kbase_mmu_add_to_free_pgds_list(mmut, current_pgd);
-	} else {
-		current_valid_entries--;
+		u64 *current_page = kbase_kmap(p);
+		unsigned int current_valid_entries =
+			kbdev->mmu_mode->get_num_valid_entries(current_page);
+		int index = (vpfn >> ((3 - current_level) * 9)) & 0x1FF;
 
-		kbdev->mmu_mode->set_num_valid_entries(current_page, current_valid_entries);
+		/* We need to track every level that needs updating */
+		if (dirty_pgds)
+			*dirty_pgds |= 1ULL << current_level;
 
-		kunmap_pgd(p, current_page);
+		kbdev->mmu_mode->entries_invalidate(&current_page[index], 1);
+		if (current_valid_entries == 1 && current_level != MIDGARD_MMU_LEVEL(0)) {
+			kbase_kunmap(p, current_page);
 
-		kbase_mmu_sync_pgd(kbdev, mmut->kctx, current_pgd + (index * sizeof(u64)),
-				   pgd_dma_addr(p, current_pgd) + (index * sizeof(u64)),
-				   sizeof(u64), flush_op);
+			/* Ensure the cacheline containing the last valid entry
+			 * of PGD is invalidated from the GPU cache, before the
+			 * PGD page is freed.
+			 */
+			kbase_mmu_sync_pgd_gpu(kbdev, mmut->kctx,
+					       current_pgd + (index * sizeof(u64)), sizeof(u64),
+					       flush_op);
 
-		/* When fine grained GPU cache maintenance is used then invalidate the MMU caches
-		 * now as the top most level PGD entry, affected by the teardown operation, has
-		 * been invalidated (both in memory as well as in GPU L2 cache). This is to avoid
-		 * the possibility of invalid ATEs being reloaded into the GPU L2 cache whilst the
-		 * teardown is happening.
-		 */
-		if (flush_op == KBASE_MMU_OP_FLUSH_PT)
-			mmu_invalidate_on_teardown(kbdev, mmut->kctx, vpfn, 1, level, as_nr);
+			kbase_mmu_add_to_free_pgds_list(mmut, p);
+		} else {
+			current_valid_entries--;
+
+			kbdev->mmu_mode->set_num_valid_entries(current_page, current_valid_entries);
+
+			kbase_kunmap(p, current_page);
+
+			kbase_mmu_sync_pgd(kbdev, mmut->kctx, current_pgd + (index * sizeof(u64)),
+					   kbase_dma_addr(p) + (index * sizeof(u64)), sizeof(u64),
+					   flush_op);
+			break;
+		}
 	}
 }
 
@@ -3200,11 +2822,13 @@ static void mmu_flush_invalidate_teardown_pages(struct kbase_device *kbdev,
 	}
 #if MALI_USE_CSF
 	else {
-		/* Partial GPU cache flush of the pages that were unmapped */
+		/* Partial GPU cache flush with MMU cache invalidation */
 		unsigned long irq_flags;
 		unsigned int i;
 		bool flush_done = false;
 
+		mmu_invalidate(kbdev, kctx, as_nr, op_param);
+
 		for (i = 0; !flush_done && i < phys_page_nr; i++) {
 			spin_lock_irqsave(&kbdev->hwaccess_lock, irq_flags);
 			if (kbdev->pm.backend.gpu_ready && (!kctx || kctx->as_nr >= 0))
@@ -3224,7 +2848,7 @@ static void mmu_flush_invalidate_teardown_pages(struct kbase_device *kbdev,
 static int kbase_mmu_teardown_pgd_pages(struct kbase_device *kbdev, struct kbase_mmu_table *mmut,
 					u64 vpfn, size_t nr, u64 *dirty_pgds,
 					struct list_head *free_pgds_list,
-					enum kbase_mmu_op_type flush_op, int as_nr)
+					enum kbase_mmu_op_type flush_op)
 {
 	struct kbase_mmu_mode const *mmu_mode = kbdev->mmu_mode;
 
@@ -3232,9 +2856,6 @@ static int kbase_mmu_teardown_pgd_pages(struct kbase_device *kbdev, struct kbase
 
 	lockdep_assert_held(&mmut->mmu_lock);
 	kbase_mmu_reset_free_pgds_list(mmut);
-	/* Convert to GPU_PAGE_SIZE units. */
-	vpfn *= GPU_PAGES_PER_CPU_PAGE;
-	nr *= GPU_PAGES_PER_CPU_PAGE;
 
 	while (nr) {
 		unsigned int index = vpfn & 0x1FF;
@@ -3247,29 +2868,41 @@ static int kbase_mmu_teardown_pgd_pages(struct kbase_device *kbdev, struct kbase
 		phys_addr_t pgd = mmut->pgd;
 		struct page *p = phys_to_page(pgd);
 
-		count = MIN(nr, count);
+		if (count > nr)
+			count = nr;
 
-		/* need to check if this is a 2MB page or a small page */
+		/* need to check if this is a 2MB page or a 4kB */
 		for (level = MIDGARD_MMU_TOPLEVEL; level <= MIDGARD_MMU_BOTTOMLEVEL; level++) {
 			phys_addr_t next_pgd;
 
 			index = (vpfn >> ((3 - level) * 9)) & 0x1FF;
-			page = kmap_pgd(p, pgd);
+			page = kbase_kmap(p);
 			if (mmu_mode->ate_is_valid(page[index], level))
 				break; /* keep the mapping */
 			else if (!mmu_mode->pte_is_valid(page[index], level)) {
-				dev_warn(kbdev->dev, "Invalid PTE found @ level %d for VA %llx",
-					 level, vpfn << PAGE_SHIFT);
-				/* nothing here, advance to the next PTE of the current level */
-				count = (1 << ((3 - level) * 9));
-				count -= (vpfn & (count - 1));
-				count = MIN(nr, count);
+				/* nothing here, advance */
+				switch (level) {
+				case MIDGARD_MMU_LEVEL(0):
+					count = 134217728;
+					break;
+				case MIDGARD_MMU_LEVEL(1):
+					count = 262144;
+					break;
+				case MIDGARD_MMU_LEVEL(2):
+					count = 512;
+					break;
+				case MIDGARD_MMU_LEVEL(3):
+					count = 1;
+					break;
+				}
+				if (count > nr)
+					count = nr;
 				goto next;
 			}
 			next_pgd = mmu_mode->pte_to_phy_addr(
 				kbdev->mgm_dev->ops.mgm_pte_to_original_pte(
 					kbdev->mgm_dev, MGM_DEFAULT_PTE_GROUP, level, page[index]));
-			kunmap_pgd(p, page);
+			kbase_kunmap(p, page);
 			pgds[level] = pgd;
 			pgd = next_pgd;
 			p = phys_to_page(pgd);
@@ -3280,7 +2913,7 @@ static int kbase_mmu_teardown_pgd_pages(struct kbase_device *kbdev, struct kbase
 		case MIDGARD_MMU_LEVEL(1):
 			dev_warn(kbdev->dev, "%s: No support for ATEs at level %d", __func__,
 				 level);
-			kunmap_pgd(p, page);
+			kbase_kunmap(p, page);
 			goto out;
 		case MIDGARD_MMU_LEVEL(2):
 			/* can only teardown if count >= 512 */
@@ -3318,36 +2951,19 @@ static int kbase_mmu_teardown_pgd_pages(struct kbase_device *kbdev, struct kbase
 		mmu_mode->entries_invalidate(&page[index], pcount);
 
 		if (!num_of_valid_entries) {
-			mmu_mode->set_num_valid_entries(page, 0);
-
-			kunmap_pgd(p, page);
+			kbase_kunmap(p, page);
 
-			/* To avoid the invalid ATEs from the PGD page (that is going to be freed)
-			 * from getting reloaded into the GPU L2 cache whilst the teardown is
-			 * happening, the fine grained GPU L2 cache maintenance is done in the top
-			 * to bottom level PGD order. MMU cache invalidation is done after
-			 * invalidating the entry of top most level PGD, affected by the teardown.
+			/* Ensure the cacheline(s) containing the last valid entries
+			 * of PGD is invalidated from the GPU cache, before the
+			 * PGD page is freed.
 			 */
-			kbase_mmu_update_and_free_parent_pgds(kbdev, mmut, pgds, vpfn, level - 1,
-							      flush_op, dirty_pgds, as_nr);
-
-			/* Check if fine grained GPU cache maintenance is being used */
-			if (flush_op == KBASE_MMU_OP_FLUSH_PT) {
-				/* Ensure the invalidated ATEs are visible in memory right away */
-				kbase_mmu_sync_pgd_cpu(kbdev,
-						       pgd_dma_addr(p, pgd) + (index * sizeof(u64)),
-						       pcount * sizeof(u64));
-				/* Invalidate the GPU cache for the whole PGD page and not just for
-				 * the cachelines containing the invalidated ATEs, as the PGD page
-				 * is going to be freed. There is an extremely remote possibility
-				 * that other cachelines (containing all invalid ATEs) of PGD page
-				 * are also present in the GPU cache.
-				 */
-				kbase_mmu_sync_pgd_gpu(kbdev, mmut->kctx, pgd, 512 * sizeof(u64),
-						       KBASE_MMU_OP_FLUSH_PT);
-			}
+			kbase_mmu_sync_pgd_gpu(kbdev, mmut->kctx, pgd + (index * sizeof(u64)),
+					       pcount * sizeof(u64), flush_op);
+
+			kbase_mmu_add_to_free_pgds_list(mmut, p);
 
-			kbase_mmu_add_to_free_pgds_list(mmut, pgd);
+			kbase_mmu_update_and_free_parent_pgds(kbdev, mmut, pgds, vpfn, level,
+							      flush_op, dirty_pgds);
 
 			vpfn += count;
 			nr -= count;
@@ -3357,16 +2973,10 @@ static int kbase_mmu_teardown_pgd_pages(struct kbase_device *kbdev, struct kbase
 		mmu_mode->set_num_valid_entries(page, num_of_valid_entries);
 
 		kbase_mmu_sync_pgd(kbdev, mmut->kctx, pgd + (index * sizeof(u64)),
-				   pgd_dma_addr(p, pgd) + (index * sizeof(u64)),
-				   pcount * sizeof(u64), flush_op);
-
-		/* When fine grained GPU cache maintenance is used then invalidation of MMU cache
-		 * is done inline for every bottom level PGD touched in the teardown.
-		 */
-		if (flush_op == KBASE_MMU_OP_FLUSH_PT)
-			mmu_invalidate_on_teardown(kbdev, mmut->kctx, vpfn, pcount, level, as_nr);
+				   kbase_dma_addr(p) + (index * sizeof(u64)), pcount * sizeof(u64),
+				   flush_op);
 next:
-		kunmap_pgd(p, page);
+		kbase_kunmap(p, page);
 		vpfn += count;
 		nr -= count;
 	}
@@ -3379,12 +2989,12 @@ static int kbase_mmu_teardown_pgd_pages(struct kbase_device *kbdev, struct kbase
  *
  * @kbdev:    Pointer to kbase device.
  * @mmut:     Pointer to GPU MMU page table.
- * @vpfn:     Start page frame number (in PAGE_SIZE units) of the GPU virtual pages to unmap.
+ * @vpfn:     Start page frame number of the GPU virtual pages to unmap.
  * @phys:     Array of physical pages currently mapped to the virtual
  *            pages to unmap, or NULL. This is used for GPU cache maintenance
  *            and page migration support.
- * @nr_phys_pages: Number of physical pages (in PAGE_SIZE units) to flush.
- * @nr_virt_pages: Number of virtual pages (in PAGE_SIZE units) whose PTEs should be destroyed.
+ * @nr_phys_pages: Number of physical pages to flush.
+ * @nr_virt_pages: Number of virtual pages whose PTEs should be destroyed.
  * @as_nr:    Address space number, for GPU cache maintenance operations
  *            that happen outside a specific kbase context.
  * @ignore_page_migration: Whether page migration metadata should be ignored.
@@ -3458,7 +3068,7 @@ static int mmu_teardown_pages(struct kbase_device *kbdev, struct kbase_mmu_table
 	mutex_lock(&mmut->mmu_lock);
 
 	err = kbase_mmu_teardown_pgd_pages(kbdev, mmut, vpfn, nr_virt_pages, &dirty_pgds,
-					   &free_pgds_list, flush_op, as_nr);
+					   &free_pgds_list, flush_op);
 
 	/* Set up MMU operation parameters. See above about MMU cache flush strategy. */
 	op_param = (struct kbase_mmu_hw_op_param){
@@ -3495,7 +3105,6 @@ int kbase_mmu_teardown_pages(struct kbase_device *kbdev, struct kbase_mmu_table
 	return mmu_teardown_pages(kbdev, mmut, vpfn, phys, nr_phys_pages, nr_virt_pages, as_nr,
 				  false);
 }
-KBASE_EXPORT_TEST_API(kbase_mmu_teardown_pages);
 
 int kbase_mmu_teardown_imported_pages(struct kbase_device *kbdev, struct kbase_mmu_table *mmut,
 				      u64 vpfn, struct tagged_addr *phys, size_t nr_phys_pages,
@@ -3511,12 +3120,12 @@ int kbase_mmu_teardown_imported_pages(struct kbase_device *kbdev, struct kbase_m
  *
  * @kbdev: Pointer to kbase device.
  * @mmut:  The involved MMU table
- * @vpfn:  Virtual PFN (Page Frame Number), in PAGE_SIZE units, of the first page to update
+ * @vpfn:  Virtual PFN (Page Frame Number) of the first page to update
  * @phys:  Pointer to the array of tagged physical addresses of the physical
  *         pages that are pointed to by the page table entries (that need to
  *         be updated). The pointer should be within the reg->gpu_alloc->pages
  *         array.
- * @nr:    Number of pages (in PAGE_SIZE units) to update
+ * @nr:    Number of pages to update
  * @flags: Flags
  * @group_id: The physical memory group in which the page was allocated.
  *            Valid range is 0..(MEMORY_GROUP_MANAGER_NR_GROUPS-1).
@@ -3546,9 +3155,6 @@ static int kbase_mmu_update_pages_no_flush(struct kbase_device *kbdev, struct kb
 	if (nr == 0)
 		return 0;
 
-	/* Convert to GPU_PAGE_SIZE units. */
-	vpfn *= GPU_PAGES_PER_CPU_PAGE;
-	nr *= GPU_PAGES_PER_CPU_PAGE;
 	mutex_lock(&mmut->mmu_lock);
 
 	while (nr) {
@@ -3562,8 +3168,7 @@ static int kbase_mmu_update_pages_no_flush(struct kbase_device *kbdev, struct kb
 		if (count > nr)
 			count = nr;
 
-		if (is_huge(*phys) &&
-		    (index == (index_in_large_page(*phys) * GPU_PAGES_PER_CPU_PAGE)))
+		if (is_huge(*phys) && (index == index_in_large_page(*phys)))
 			cur_level = MIDGARD_MMU_LEVEL(2);
 
 		err = mmu_get_pgd_at_level(kbdev, mmut, vpfn, cur_level, &pgd);
@@ -3571,7 +3176,7 @@ static int kbase_mmu_update_pages_no_flush(struct kbase_device *kbdev, struct kb
 			goto fail_unlock;
 
 		p = pfn_to_page(PFN_DOWN(pgd));
-		pgd_page = kmap_pgd(p, pgd);
+		pgd_page = kbase_kmap(p);
 		if (!pgd_page) {
 			dev_warn(kbdev->dev, "kmap failure on update_pages");
 			err = -ENOMEM;
@@ -3581,7 +3186,7 @@ static int kbase_mmu_update_pages_no_flush(struct kbase_device *kbdev, struct kb
 		num_of_valid_entries = kbdev->mmu_mode->get_num_valid_entries(pgd_page);
 
 		if (cur_level == MIDGARD_MMU_LEVEL(2)) {
-			unsigned int level_index = (vpfn >> 9) & 0x1FFU;
+			int level_index = (vpfn >> 9) & 0x1FF;
 			struct tagged_addr *target_phys = phys - index_in_large_page(*phys);
 
 #ifdef CONFIG_MALI_BIFROST_DEBUG
@@ -3591,32 +3196,23 @@ static int kbase_mmu_update_pages_no_flush(struct kbase_device *kbdev, struct kb
 			pgd_page[level_index] = kbase_mmu_create_ate(
 				kbdev, *target_phys, flags, MIDGARD_MMU_LEVEL(2), group_id);
 			kbase_mmu_sync_pgd(kbdev, mmut->kctx, pgd + (level_index * sizeof(u64)),
-					   pgd_dma_addr(p, pgd) + (level_index * sizeof(u64)),
+					   kbase_dma_addr(p) + (level_index * sizeof(u64)),
 					   sizeof(u64), KBASE_MMU_OP_NONE);
 		} else {
-			for (i = 0; i < count; i += GPU_PAGES_PER_CPU_PAGE) {
-				phys_addr_t base_phys_address =
-					as_phys_addr_t(phys[i / GPU_PAGES_PER_CPU_PAGE]);
-				unsigned int j;
-
-				for (j = 0; j < GPU_PAGES_PER_CPU_PAGE; j++) {
-					phys_addr_t page_address =
-						base_phys_address + (j * GPU_PAGE_SIZE);
+			for (i = 0; i < count; i++) {
 #ifdef CONFIG_MALI_BIFROST_DEBUG
-					WARN_ON_ONCE(!kbdev->mmu_mode->ate_is_valid(
-						pgd_page[index + i + j], MIDGARD_MMU_BOTTOMLEVEL));
+				WARN_ON_ONCE(!kbdev->mmu_mode->ate_is_valid(
+					pgd_page[index + i], MIDGARD_MMU_BOTTOMLEVEL));
 #endif
-					pgd_page[index + i + j] = kbase_mmu_create_ate(
-						kbdev, as_tagged(page_address), flags,
-						MIDGARD_MMU_BOTTOMLEVEL, group_id);
-				}
+				pgd_page[index + i] = kbase_mmu_create_ate(
+					kbdev, phys[i], flags, MIDGARD_MMU_BOTTOMLEVEL, group_id);
 			}
 
 			/* MMU cache flush strategy is NONE because GPU cache maintenance
 			 * will be done by the caller.
 			 */
 			kbase_mmu_sync_pgd(kbdev, mmut->kctx, pgd + (index * sizeof(u64)),
-					   pgd_dma_addr(p, pgd) + (index * sizeof(u64)),
+					   kbase_dma_addr(p) + (index * sizeof(u64)),
 					   count * sizeof(u64), KBASE_MMU_OP_NONE);
 		}
 
@@ -3625,11 +3221,11 @@ static int kbase_mmu_update_pages_no_flush(struct kbase_device *kbdev, struct kb
 		if (dirty_pgds && count > 0)
 			*dirty_pgds |= 1ULL << cur_level;
 
-		phys += (count / GPU_PAGES_PER_CPU_PAGE);
+		phys += count;
 		vpfn += count;
 		nr -= count;
 
-		kunmap_pgd(p, pgd_page);
+		kbase_kunmap(p, pgd_page);
 	}
 
 	mutex_unlock(&mmut->mmu_lock);
@@ -3737,12 +3333,10 @@ int kbase_mmu_migrate_page(struct tagged_addr old_phys, struct tagged_addr new_p
 	struct kbase_device *kbdev;
 	phys_addr_t pgd;
 	u64 *old_page, *new_page, *pgd_page, *target, vpfn;
-	unsigned int index;
-	int check_state, ret = 0;
+	int index, check_state, ret = 0;
 	unsigned long hwaccess_flags = 0;
 	unsigned int num_of_valid_entries;
 	u8 vmap_count = 0;
-	u8 pgd_entries_to_sync = (level == MIDGARD_MMU_BOTTOMLEVEL) ? GPU_PAGES_PER_CPU_PAGE : 1;
 
 	/* If page migration support is not compiled in, return with fault */
 	if (!IS_ENABLED(CONFIG_PAGE_MIGRATION_SUPPORT))
@@ -3761,7 +3355,7 @@ int kbase_mmu_migrate_page(struct tagged_addr old_phys, struct tagged_addr new_p
 		vpfn = PGD_VPFN_LEVEL_GET_VPFN(page_md->data.pt_mapped.pgd_vpfn_level);
 
 	kbdev = mmut->kctx->kbdev;
-	index = (vpfn >> ((3 - level) * 9)) & 0x1FFU;
+	index = (vpfn >> ((3 - level) * 9)) & 0x1FF;
 
 	/* Create all mappings before copying content.
 	 * This is done as early as possible because it is the only operation that may
@@ -3811,8 +3405,8 @@ int kbase_mmu_migrate_page(struct tagged_addr old_phys, struct tagged_addr new_p
 #define PGD_VPFN_MASK(level) (~((((u64)1) << ((3 - level) * 9)) - 1))
 	op_param.mmu_sync_info = CALLER_MMU_ASYNC;
 	op_param.kctx_id = mmut->kctx->id;
-	op_param.vpfn = (vpfn / GPU_PAGES_PER_CPU_PAGE) & PGD_VPFN_MASK(level);
-	op_param.nr = 1U << ((3 - level) * 9);
+	op_param.vpfn = vpfn & PGD_VPFN_MASK(level);
+	op_param.nr = 1 << ((3 - level) * 9);
 	op_param.op = KBASE_MMU_OP_FLUSH_PT;
 	/* When level is not MIDGARD_MMU_BOTTOMLEVEL, it is assumed PGD page migration */
 	op_param.flush_skip_levels = (level == MIDGARD_MMU_BOTTOMLEVEL) ?
@@ -3865,7 +3459,7 @@ int kbase_mmu_migrate_page(struct tagged_addr old_phys, struct tagged_addr new_p
 		goto get_pgd_at_level_error;
 	}
 
-	pgd_page = kmap_pgd(phys_to_page(pgd), pgd);
+	pgd_page = kbase_kmap(phys_to_page(pgd));
 	if (!pgd_page) {
 		dev_warn(kbdev->dev, "%s: kmap failure for PGD page.", __func__);
 		ret = -EINVAL;
@@ -3931,7 +3525,7 @@ int kbase_mmu_migrate_page(struct tagged_addr old_phys, struct tagged_addr new_p
 	/* Remap GPU virtual page.
 	 *
 	 * This code rests on the assumption that page migration is only enabled
-	 * for small pages, that necessarily live in the bottom level of the MMU
+	 * for 4 kB pages, that necessarily live in the bottom level of the MMU
 	 * page table. For this reason, the PGD level tells us inequivocably
 	 * whether the page being migrated is a "content page" or another PGD
 	 * of the page table:
@@ -3951,18 +3545,10 @@ int kbase_mmu_migrate_page(struct tagged_addr old_phys, struct tagged_addr new_p
 	num_of_valid_entries = kbdev->mmu_mode->get_num_valid_entries(pgd_page);
 
 	if (level == MIDGARD_MMU_BOTTOMLEVEL) {
-		phys_addr_t base_phys_address = as_phys_addr_t(new_phys);
-		unsigned int i;
-
-		for (i = 0; i < GPU_PAGES_PER_CPU_PAGE; i++) {
-			phys_addr_t page_address = base_phys_address + (i * GPU_PAGE_SIZE);
-
-			WARN_ON_ONCE((*target & 1UL) == 0);
-			*target = kbase_mmu_create_ate(
-				kbdev, as_tagged(page_address), page_md->data.mapped.reg->flags,
-				level, page_md->data.mapped.reg->gpu_alloc->group_id);
-			target++;
-		}
+		WARN_ON_ONCE((*target & 1UL) == 0);
+		*target = kbase_mmu_create_ate(kbdev, new_phys, page_md->data.mapped.reg->flags,
+					       level,
+					       page_md->data.mapped.reg->gpu_alloc->group_id);
 	} else {
 		u64 managed_pte;
 
@@ -3974,78 +3560,65 @@ int kbase_mmu_migrate_page(struct tagged_addr old_phys, struct tagged_addr new_p
 				kbdev->mgm_dev, MGM_DEFAULT_PTE_GROUP, level, pgd_page[index])));
 #endif
 		kbdev->mmu_mode->entry_set_pte(&managed_pte, as_phys_addr_t(new_phys));
-		*target = kbdev->mgm_dev->ops.mgm_update_gpu_pte(kbdev->mgm_dev,
-								 MGM_DEFAULT_PTE_GROUP,
-								 PBHA_ID_DEFAULT, PTE_FLAGS_NONE,
-								 level, managed_pte);
+		*target = kbdev->mgm_dev->ops.mgm_update_gpu_pte(
+			kbdev->mgm_dev, MGM_DEFAULT_PTE_GROUP, level, managed_pte);
 	}
 
 	kbdev->mmu_mode->set_num_valid_entries(pgd_page, num_of_valid_entries);
 
-	/* This function always updates a single entry inside an existing PGD when
-	 * level != MIDGARD_MMU_BOTTOMLEVEL, and would update more than one entry for
-	 * MIDGARD_MMU_BOTTOMLEVEL PGD when PAGE_SIZE is not 4K, therefore cache
-	 * maintenance is necessary.
+	/* This function always updates a single entry inside an existing PGD,
+	 * therefore cache maintenance is necessary and affects a single entry.
 	 */
 	kbase_mmu_sync_pgd(kbdev, mmut->kctx, pgd + (index * sizeof(u64)),
-			   pgd_dma_addr(phys_to_page(pgd), pgd) + (index * sizeof(u64)),
-			   pgd_entries_to_sync * sizeof(u64), KBASE_MMU_OP_FLUSH_PT);
+			   kbase_dma_addr(phys_to_page(pgd)) + (index * sizeof(u64)), sizeof(u64),
+			   KBASE_MMU_OP_FLUSH_PT);
 
 	/* Unlock MMU region.
 	 *
-	 * For GPUs without FLUSH_PA_RANGE support, the GPU caches were completely
-	 * cleaned and invalidated after locking the virtual address range affected
-	 * by the migration. As long as the lock is in place, GPU access to the
-	 * locked range would remain blocked. So there is no need to clean and
-	 * invalidate the GPU caches again after the copying the page contents
-	 * of old page and updating the page table entry to point to new page.
-	 *
-	 * For GPUs with FLUSH_PA_RANGE support, the contents of old page would
-	 * have been evicted from the GPU caches after locking the virtual address
-	 * range. The page table entry contents also would have been invalidated
-	 * from the GPU's L2 cache by kbase_mmu_sync_pgd() after the page table
-	 * update.
-	 *
-	 * If kbase_mmu_hw_do_unlock_no_addr() fails, GPU reset will be triggered which
-	 * would remove the MMU lock and so there is no need to rollback page migration
-	 * and the failure can be ignored.
+	 * Notice that GPUs which don't issue flush commands via GPU control
+	 * still need an additional GPU cache flush here, this time only
+	 * for the page table, because the function call above to sync PGDs
+	 * won't have any effect on them.
 	 */
 	spin_lock_irqsave(&kbdev->hwaccess_lock, hwaccess_flags);
 	if (kbdev->pm.backend.gpu_ready && mmut->kctx->as_nr >= 0) {
 		int as_nr = mmut->kctx->as_nr;
 		struct kbase_as *as = &kbdev->as[as_nr];
-		int local_ret = kbase_mmu_hw_do_unlock_no_addr(kbdev, as, &op_param);
 
-		CSTD_UNUSED(local_ret);
+		if (mmu_flush_cache_on_gpu_ctrl(kbdev)) {
+			ret = kbase_mmu_hw_do_unlock(kbdev, as, &op_param);
+		} else {
+			ret = kbase_gpu_cache_flush_and_busy_wait(kbdev,
+								  GPU_COMMAND_CACHE_CLN_INV_L2);
+			if (!ret)
+				ret = kbase_mmu_hw_do_unlock_no_addr(kbdev, as, &op_param);
+		}
 	}
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, hwaccess_flags);
+	/* Releasing locks before checking the migration transaction error state */
+	mutex_unlock(&kbdev->mmu_hw_mutex);
 
+	spin_lock_irqsave(&kbdev->hwaccess_lock, hwaccess_flags);
 	/* Release the transition prevention in L2 by ending the transaction */
 	mmu_page_migration_transaction_end(kbdev);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, hwaccess_flags);
-	/* Releasing locks before checking the migration transaction error state */
-	mutex_unlock(&kbdev->mmu_hw_mutex);
+
+	/* Checking the final migration transaction error state */
+	if (ret < 0) {
+		dev_err(kbdev->dev, "%s: failed to unlock MMU region.", __func__);
+		goto undo_mappings;
+	}
 
 	/* Undertaking metadata transfer, while we are holding the mmu_lock */
 	spin_lock(&page_md->migrate_lock);
 	if (level == MIDGARD_MMU_BOTTOMLEVEL) {
-		enum kbase_page_status page_status = PAGE_STATUS_GET(page_md->status);
+		size_t page_array_index =
+			page_md->data.mapped.vpfn - page_md->data.mapped.reg->start_pfn;
 
-		if (page_status == ALLOCATED_MAPPED) {
-			/* Replace page in array of pages of the physical allocation. */
-			size_t page_array_index =
-				div_u64(page_md->data.mapped.vpfn, GPU_PAGES_PER_CPU_PAGE) -
-				page_md->data.mapped.reg->start_pfn;
+		WARN_ON(PAGE_STATUS_GET(page_md->status) != ALLOCATED_MAPPED);
 
-			page_md->data.mapped.reg->gpu_alloc->pages[page_array_index] = new_phys;
-		} else if (page_status == NOT_MOVABLE) {
-			dev_dbg(kbdev->dev,
-				"%s: migration completed and page has become NOT_MOVABLE.",
-				__func__);
-		} else {
-			dev_WARN(kbdev->dev,
-				 "%s: migration completed but page has moved to status %d.",
-				 __func__, page_status);
-		}
+		/* Replace page in array of pages of the physical allocation. */
+		page_md->data.mapped.reg->gpu_alloc->pages[page_array_index] = new_phys;
 	}
 	/* Update the new page dma_addr with the transferred metadata from the old_page */
 	page_md->dma_addr = new_dma_addr;
@@ -4056,7 +3629,7 @@ int kbase_mmu_migrate_page(struct tagged_addr old_phys, struct tagged_addr new_p
 	set_page_private(as_page(old_phys), 0);
 
 l2_state_defer_out:
-	kunmap_pgd(phys_to_page(pgd), pgd_page);
+	kbase_kunmap(phys_to_page(pgd), pgd_page);
 pgd_page_map_error:
 get_pgd_at_level_error:
 page_state_change_out:
@@ -4071,7 +3644,7 @@ int kbase_mmu_migrate_page(struct tagged_addr old_phys, struct tagged_addr new_p
 undo_mappings:
 	/* Unlock the MMU table and undo mappings. */
 	mutex_unlock(&mmut->mmu_lock);
-	kunmap_pgd(phys_to_page(pgd), pgd_page);
+	kbase_kunmap(phys_to_page(pgd), pgd_page);
 	kbase_kunmap(as_page(new_phys), new_page);
 	kbase_kunmap(as_page(old_phys), old_page);
 
@@ -4079,7 +3652,7 @@ int kbase_mmu_migrate_page(struct tagged_addr old_phys, struct tagged_addr new_p
 }
 
 static void mmu_teardown_level(struct kbase_device *kbdev, struct kbase_mmu_table *mmut,
-			       phys_addr_t pgd, int level)
+			       phys_addr_t pgd, unsigned int level)
 {
 	u64 *pgd_page;
 	int i;
@@ -4090,7 +3663,7 @@ static void mmu_teardown_level(struct kbase_device *kbdev, struct kbase_mmu_tabl
 
 	lockdep_assert_held(&mmut->mmu_lock);
 
-	pgd_page = kmap_atomic_pgd(p, pgd);
+	pgd_page = kbase_kmap_atomic(p);
 	/* kmap_atomic should NEVER fail. */
 	if (WARN_ON_ONCE(pgd_page == NULL))
 		return;
@@ -4099,7 +3672,7 @@ static void mmu_teardown_level(struct kbase_device *kbdev, struct kbase_mmu_tabl
 		 * kmap_atomic usage
 		 */
 		pgd_page_buffer = mmut->scratch_mem.teardown_pages.levels[level];
-		memcpy(pgd_page_buffer, pgd_page, GPU_PAGE_SIZE);
+		memcpy(pgd_page_buffer, pgd_page, PAGE_SIZE);
 	}
 
 	/* When page migration is enabled, kbase_region_tracker_term() would ensure
@@ -4110,7 +3683,7 @@ static void mmu_teardown_level(struct kbase_device *kbdev, struct kbase_mmu_tabl
 		WARN_ON_ONCE(kbdev->mmu_mode->get_num_valid_entries(pgd_page));
 	/* Invalidate page after copying */
 	mmu_mode->entries_invalidate(pgd_page, KBASE_MMU_PAGE_ENTRIES);
-	kunmap_atomic_pgd(pgd_page);
+	kbase_kunmap_atomic(pgd_page);
 	pgd_page = pgd_page_buffer;
 
 	if (level < MIDGARD_MMU_BOTTOMLEVEL) {
@@ -4129,20 +3702,13 @@ static void mmu_teardown_level(struct kbase_device *kbdev, struct kbase_mmu_tabl
 	kbase_mmu_free_pgd(kbdev, mmut, pgd);
 }
 
-static void kbase_mmu_mark_non_movable(struct kbase_device *const kbdev, struct page *page)
+static void kbase_mmu_mark_non_movable(struct page *page)
 {
 	struct kbase_page_metadata *page_md;
 
 	if (!kbase_is_page_migration_enabled())
 		return;
 
-	/* Composite large-page is excluded from migration, trigger a warn if a development
-	 * wrongly leads to it.
-	 */
-	if (is_huge_head(as_tagged(page_to_phys(page))) ||
-	    is_partial(as_tagged(page_to_phys(page))))
-		dev_WARN(kbdev->dev, "%s: migration on large-page attempted.", __func__);
-
 	page_md = kbase_page_private(page);
 
 	spin_lock(&page_md->migrate_lock);
@@ -4170,10 +3736,6 @@ int kbase_mmu_init(struct kbase_device *const kbdev, struct kbase_mmu_table *con
 	mmut->kctx = kctx;
 	mmut->pgd = KBASE_INVALID_PHYSICAL_ADDRESS;
 
-#if GPU_PAGES_PER_CPU_PAGE > 1
-	INIT_LIST_HEAD(&mmut->pgd_pages_list);
-#endif
-
 	/* We allocate pages into the kbdev memory pool, then
 	 * kbase_mmu_alloc_pgd will allocate out of that pool. This is done to
 	 * avoid allocations from the kernel happening with the lock held.
@@ -4188,12 +3750,10 @@ int kbase_mmu_init(struct kbase_device *const kbdev, struct kbase_mmu_table *con
 			return -ENOMEM;
 		}
 
-		mutex_lock(&mmut->mmu_lock);
 		mmut->pgd = kbase_mmu_alloc_pgd(kbdev, mmut);
-		mutex_unlock(&mmut->mmu_lock);
 	}
 
-	kbase_mmu_mark_non_movable(kbdev, pfn_to_page(PFN_DOWN(mmut->pgd)));
+	kbase_mmu_mark_non_movable(pfn_to_page(PFN_DOWN(mmut->pgd)));
 	return 0;
 }
 
@@ -4251,7 +3811,6 @@ static size_t kbasep_mmu_dump_level(struct kbase_context *kctx, phys_addr_t pgd,
 	size_t dump_size;
 	struct kbase_device *kbdev;
 	struct kbase_mmu_mode const *mmu_mode;
-	struct page *p;
 
 	if (WARN_ON(kctx == NULL))
 		return 0;
@@ -4260,8 +3819,7 @@ static size_t kbasep_mmu_dump_level(struct kbase_context *kctx, phys_addr_t pgd,
 	kbdev = kctx->kbdev;
 	mmu_mode = kbdev->mmu_mode;
 
-	p = pfn_to_page(PFN_DOWN(pgd));
-	pgd_page = kmap_pgd(p, pgd);
+	pgd_page = kbase_kmap(pfn_to_page(PFN_DOWN(pgd)));
 	if (!pgd_page) {
 		dev_warn(kbdev->dev, "%s: kmap failure", __func__);
 		return 0;
@@ -4271,7 +3829,7 @@ static size_t kbasep_mmu_dump_level(struct kbase_context *kctx, phys_addr_t pgd,
 		/* A modified physical address that contains
 		 * the page table level
 		 */
-		u64 m_pgd = pgd | (u64)level;
+		u64 m_pgd = pgd | level;
 
 		/* Put the modified physical address in the output buffer */
 		memcpy(*buffer, &m_pgd, sizeof(m_pgd));
@@ -4295,7 +3853,7 @@ static size_t kbasep_mmu_dump_level(struct kbase_context *kctx, phys_addr_t pgd,
 				dump_size = kbasep_mmu_dump_level(kctx, target_pgd, level + 1,
 								  buffer, size_left);
 				if (!dump_size) {
-					kunmap_pgd(p, pgd_page);
+					kbase_kunmap(pfn_to_page(PFN_DOWN(pgd)), pgd_page);
 					return 0;
 				}
 				size += dump_size;
@@ -4303,12 +3861,12 @@ static size_t kbasep_mmu_dump_level(struct kbase_context *kctx, phys_addr_t pgd,
 		}
 	}
 
-	kunmap_pgd(p, pgd_page);
+	kbase_kunmap(pfn_to_page(PFN_DOWN(pgd)), pgd_page);
 
 	return size;
 }
 
-void *kbase_mmu_dump(struct kbase_context *kctx, size_t nr_pages)
+void *kbase_mmu_dump(struct kbase_context *kctx, int nr_pages)
 {
 	void *kaddr;
 	size_t size_left;
@@ -4384,7 +3942,7 @@ KBASE_EXPORT_TEST_API(kbase_mmu_dump);
 void kbase_mmu_bus_fault_worker(struct work_struct *data)
 {
 	struct kbase_as *faulting_as;
-	unsigned int as_no;
+	int as_no;
 	struct kbase_context *kctx;
 	struct kbase_device *kbdev;
 	struct kbase_fault *fault;
@@ -4409,6 +3967,7 @@ void kbase_mmu_bus_fault_worker(struct work_struct *data)
 		return;
 	}
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
 	/* check if we still have GPU */
 	if (unlikely(kbase_is_gpu_removed(kbdev))) {
 		dev_dbg(kbdev->dev, "%s: GPU has been removed", __func__);
@@ -4416,6 +3975,7 @@ void kbase_mmu_bus_fault_worker(struct work_struct *data)
 		atomic_dec(&kbdev->faults_pending);
 		return;
 	}
+#endif
 
 	if (unlikely(fault->protected_mode)) {
 		kbase_mmu_report_fault_and_kill(kctx, faulting_as, "Permission failure", fault);
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.h b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.h
index 03bc066f897f..73f41b4d94cf 100644
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.h
+++ b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.h
@@ -23,15 +23,12 @@
 #define _KBASE_MMU_H_
 
 #include <uapi/gpu/arm/bifrost/mali_base_kernel.h>
-#include <mali_kbase_debug.h>
 
 #define KBASE_MMU_PAGE_ENTRIES 512
 
 struct kbase_context;
-struct kbase_device;
 struct kbase_mmu_table;
 struct kbase_va_region;
-struct tagged_addr;
 
 /**
  * enum kbase_caller_mmu_sync_info - MMU-synchronous caller info.
@@ -168,9 +165,9 @@ int kbase_mmu_insert_pages(struct kbase_device *kbdev, struct kbase_mmu_table *m
  *
  * @kbdev:         Instance of GPU platform device, allocated from the probe method.
  * @mmut:          GPU page tables.
- * @vpfn:          Start page frame number (in PAGE_SIZE units) of the GPU virtual pages to map.
+ * @vpfn:          Start page frame number of the GPU virtual pages to map.
  * @phys:          Physical address of the page to be mapped.
- * @nr:            The number of pages (in PAGE_SIZE units) to map.
+ * @nr:            The number of pages to map.
  * @flags:         Bitmask of attributes of the GPU memory region being mapped.
  * @as_nr:         The GPU address space number.
  * @group_id:      The physical memory group in which the page was allocated.
@@ -220,11 +217,11 @@ int kbase_mmu_update_pages(struct kbase_context *kctx, u64 vpfn, struct tagged_a
  * kbase_mmu_update_csf_mcu_pages - Update MCU mappings with changes of phys and flags
  *
  * @kbdev:    Pointer to kbase device.
- * @vpfn:     GPU Virtual PFN (Page Frame Number), in PAGE_SIZE units, of the first page to update
+ * @vpfn:     Virtual PFN (Page Frame Number) of the first page to update
  * @phys:     Pointer to the array of tagged physical addresses of the physical
  *            pages that are pointed to by the page table entries (that need to
  *            be updated).
- * @nr:       Number of pages (in PAGE_SIZE units) to update
+ * @nr:       Number of pages to update
  * @flags:    Flags
  * @group_id: The physical memory group in which the page was allocated.
  *            Valid range is 0..(MEMORY_GROUP_MANAGER_NR_GROUPS-1).
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder.c b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder.c
deleted file mode 100644
index 548d88cf216e..000000000000
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder.c
+++ /dev/null
@@ -1,124 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
-/*
- *
- * (C) COPYRIGHT 2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-/**
- * DOC: Base kernel MMU faults decoder.
- */
-
-#include <mmu/mali_kbase_mmu_faults_decoder.h>
-#include <mmu/mali_kbase_mmu_faults_decoder_luts.h>
-#if MALI_USE_CSF
-#include <mmu/backend/mali_kbase_mmu_faults_decoder_luts_csf.h>
-#else
-#include <mmu/backend/mali_kbase_mmu_faults_decoder_luts_jm.h>
-#endif
-
-#include <hw_access/mali_kbase_hw_access_regmap.h>
-#include <mali_kbase.h>
-
-unsigned int fault_source_id_internal_requester_get(struct kbase_device *kbdev,
-						    unsigned int source_id)
-{
-	if (kbdev->gpu_props.gpu_id.product_model < GPU_ID_MODEL_MAKE(14, 0))
-		return ((source_id >> 4) & 0xF);
-	else
-		return (source_id & 0x3F);
-}
-
-static inline const char *source_id_enc_core_type_get_str(struct kbase_device *kbdev,
-							  unsigned int source_id)
-{
-	if (kbdev->gpu_props.gpu_id.product_model < GPU_ID_MODEL_MAKE(14, 0))
-		return decode_fault_source_core_id_t_core_type(
-			FAULT_SOURCE_ID_CORE_ID_GET(source_id), kbdev->gpu_props.gpu_id.arch_id);
-	else
-		return decode_fault_source_core_type_t_name(
-			FAULT_SOURCE_ID_CORE_TYPE_GET(source_id), kbdev->gpu_props.gpu_id.arch_id);
-}
-const char *fault_source_id_internal_requester_get_str(struct kbase_device *kbdev,
-						       unsigned int source_id,
-						       unsigned int access_type)
-{
-	unsigned int ir = fault_source_id_internal_requester_get(kbdev, source_id);
-	bool older_source_id_fmt =
-		(kbdev->gpu_props.gpu_id.product_model < GPU_ID_MODEL_MAKE(14, 0));
-	unsigned int utlb_id = 0;
-
-	if (older_source_id_fmt)
-		utlb_id = FAULT_SOURCE_ID_UTLB_ID_GET(source_id);
-
-	if (!strcmp(source_id_enc_core_type_get_str(kbdev, source_id), "shader")) {
-		if (utlb_id == 0) {
-			if (access_type == AS_FAULTSTATUS_ACCESS_TYPE_READ)
-				return decode_fault_source_shader_r_t(
-					ir, kbdev->gpu_props.gpu_id.arch_id);
-			else
-				return decode_fault_source_shader_w_t(
-					ir, kbdev->gpu_props.gpu_id.arch_id);
-		} else
-			return "Load/store cache";
-	} else if (!strcmp(source_id_enc_core_type_get_str(kbdev, source_id), "tiler")) {
-#if MALI_USE_CSF
-		if (utlb_id == 0) {
-			if (access_type == AS_FAULTSTATUS_ACCESS_TYPE_READ)
-				return decode_fault_source_tiler_r_t(
-					ir, kbdev->gpu_props.gpu_id.arch_id);
-			else
-				return decode_fault_source_tiler_w_t(
-					ir, kbdev->gpu_props.gpu_id.arch_id);
-		} else
-			return "The polygon list writer. No further details.";
-#else
-		return (utlb_id == 0) ? "Anything other than the polygon list writer" :
-					      "The polygon list writer";
-#endif
-	}
-#if MALI_USE_CSF
-	else if (!strcmp(source_id_enc_core_type_get_str(kbdev, source_id), "csf")) {
-		if (access_type == AS_FAULTSTATUS_ACCESS_TYPE_READ)
-			return decode_fault_source_csf_r_t(ir, kbdev->gpu_props.gpu_id.arch_id);
-		else
-			return decode_fault_source_csf_w_t(ir, kbdev->gpu_props.gpu_id.arch_id);
-	}
-#else
-	else if (!strcmp(source_id_enc_core_type_get_str(kbdev, source_id), "jm"))
-		return decode_fault_source_jm_t(ir, kbdev->gpu_props.gpu_id.arch_id);
-#endif
-	else if (!strcmp(source_id_enc_core_type_get_str(kbdev, source_id), "I2c") ||
-		 !strcmp(source_id_enc_core_type_get_str(kbdev, source_id), "memsys") ||
-		 !strcmp(source_id_enc_core_type_get_str(kbdev, source_id), "mmu")) {
-		return "Not used";
-	}
-
-	return "unknown";
-}
-
-const char *fault_source_id_core_type_description_get(struct kbase_device *kbdev,
-						      unsigned int source_id)
-{
-	if (kbdev->gpu_props.gpu_id.product_model < GPU_ID_MODEL_MAKE(14, 0)) {
-		return decode_fault_source_core_id_t_desc(FAULT_SOURCE_ID_CORE_ID_GET(source_id),
-							  kbdev->gpu_props.gpu_id.arch_id);
-	} else {
-		return decode_fault_source_core_type_t_desc(
-			FAULT_SOURCE_ID_CORE_TYPE_GET(source_id), kbdev->gpu_props.gpu_id.arch_id);
-	}
-}
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder.h b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder.h
deleted file mode 100644
index da5610ec94b0..000000000000
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder.h
+++ /dev/null
@@ -1,143 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
-/*
- *
- * (C) COPYRIGHT 2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-#ifndef _MALI_KBASE_MMU_FAULTS_DECODER_H_
-#define _MALI_KBASE_MMU_FAULTS_DECODER_H_
-
-#include <linux/types.h>
-#include <mali_kbase.h>
-
-/* FAULTSTATUS.SOURCE_ID encoding */
-#define SOURCE_ID_CORE_ID_SHIFT (9)
-#define SOURCE_ID_CORE_ID_MASK (0x7F << SOURCE_ID_CORE_ID_SHIFT)
-#define SOURCE_ID_UTLB_ID_SHIFT (8)
-#define SOURCE_ID_UTLB_ID_MASK (0x01 << SOURCE_ID_UTLB_ID_SHIFT)
-#define SOURCE_ID_CORE_TYPE_SHIFT (12)
-#define SOURCE_ID_CORE_TYPE_MASK (0x0F << SOURCE_ID_CORE_TYPE_SHIFT)
-#define SOURCE_ID_CORE_INDEX_SHIFT (6)
-#define SOURCE_ID_CORE_INDEX_MASK (0x3F << SOURCE_ID_CORE_INDEX_SHIFT)
-
-/**
- * FAULT_SOURCE_ID_CORE_ID_GET() - Get core ID of a fault.
- *
- * @source_id: SOURCE_ID field of FAULTSTATUS (MMU) or GPU_FAULTSTATUS (GPU)
- *			   registers.
- *
- * Get core ID part of SOURCE_ID field of FAULTSTATUS (MMU) or
- * GPU_FAULTSTATUS (GPU) registers.
- *
- * Return: core ID of the fault.
- */
-#define FAULT_SOURCE_ID_CORE_ID_GET(source_id) \
-	((source_id & SOURCE_ID_CORE_ID_MASK) >> SOURCE_ID_CORE_ID_SHIFT)
-
-/**
- * FAULT_SOURCE_ID_UTLB_ID_GET() - Get UTLB ID of a fault.
- *
- * @source_id: SOURCE_ID field of FAULTSTATUS (MMU) or GPU_FAULTSTATUS (GPU)
- *			   registers.
- *
- * Get UTLB(micro-TLB) ID part of SOURCE_ID field of FAULTSTATUS (MMU) or
- * GPU_FAULTSTATUS (GPU) registers.
- *
- * Return: UTLB ID of the fault.
- */
-#define FAULT_SOURCE_ID_UTLB_ID_GET(source_id) \
-	((source_id & SOURCE_ID_UTLB_ID_MASK) >> SOURCE_ID_UTLB_ID_SHIFT)
-
-/**
- * FAULT_SOURCE_ID_CORE_TYPE_GET() - Get core type of a fault.
- *
- * @source_id: SOURCE_ID field of FAULTSTATUS (MMU) or GPU_FAULTSTATUS (GPU)
- *			   registers.
- *
- * Get core type part of SOURCE_ID field of FAULTSTATUS (MMU) or
- * GPU_FAULTSTATUS (GPU) registers.
- *
- * Return: core type code of the fault.
- */
-#define FAULT_SOURCE_ID_CORE_TYPE_GET(source_id) \
-	((source_id & SOURCE_ID_CORE_TYPE_MASK) >> SOURCE_ID_CORE_TYPE_SHIFT)
-
-/**
- * FAULT_SOURCE_ID_CORE_INDEX_GET() - Get core index of a fault.
- *
- * @source_id: SOURCE_ID field of FAULTSTATUS (MMU) or GPU_FAULTSTATUS (GPU)
- *			   registers.
- *
- * Get core index part of SOURCE_ID field of FAULTSTATUS (MMU) or
- * GPU_FAULTSTATUS (GPU) registers.
- *
- * Return: core index of the fault.
- */
-#define FAULT_SOURCE_ID_CORE_INDEX_GET(source_id) \
-	((source_id & SOURCE_ID_CORE_INDEX_MASK) >> SOURCE_ID_CORE_INDEX_SHIFT)
-
-/**
- * fault_source_id_internal_requester_get() - Get internal_requester of a fault.
- *
- * @kbdev: The kbase device structure for the device (must be a valid pointer).
- * @source_id: SOURCE_ID field of FAULTSTATUS (MMU) or GPU_FAULTSTATUS (GPU)
- *			   registers.
- *
- * Get internal_requester part of SOURCE_ID field of FAULTSTATUS (MMU) or
- * GPU_FAULTSTATUS (GPU) registers.
- *
- * Return: Internal requester code of the fault.
- */
-unsigned int fault_source_id_internal_requester_get(struct kbase_device *kbdev,
-						    unsigned int source_id);
-
-/**
- * fault_source_id_internal_requester_get_str() - Get internal_requester of a
- * fault in a human readable format.
- *
- * @kbdev: The kbase device structure for the device (must be a valid pointer).
- * @source_id: SOURCE_ID field of FAULTSTATUS (MMU) or GPU_FAULTSTATUS (GPU)
- *			   registers.
- * @access_type: the direction of data transfer that caused the fault (atomic,
- *				 execute, read, write)
- *
- * Get the human readable decoding of internal_requester part of SOURCE_ID field
- * of FAULTSTATUS (MMU) or GPU_FAULTSTATUS (GPU) registers.
- *
- * Return: Internal requester of the fault in human readable format.
- */
-const char *fault_source_id_internal_requester_get_str(struct kbase_device *kbdev,
-						       unsigned int source_id,
-						       unsigned int access_type);
-
-/**
- * fault_source_id_core_type_description_get() - Get the core type of
- * a fault in a human readable format.
- *
- * @kbdev: The kbase device structure for the device (must be a valid pointer).
- * @source_id: SOURCE_ID field of FAULTSTATUS (MMU) or GPU_FAULTSTATUS (GPU)
- *			   registers.
- *
- * Get the human readable decoding of core type part of SOURCE_ID field
- * of FAULTSTATUS (MMU) or GPU_FAULTSTATUS (GPU) registers.
- *
- * Return: core type of the fault in human readable format.
- */
-const char *fault_source_id_core_type_description_get(struct kbase_device *kbdev,
-						      unsigned int source_id);
-
-#endif /* _MALI_KBASE_MMU_FAULTS_DECODER_H_ */
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder_luts.c b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder_luts.c
deleted file mode 100644
index 8e90cacb4efa..000000000000
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder_luts.c
+++ /dev/null
@@ -1,660 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
-/*
- *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-/**
- * DOC: Base kernel MMU faults decoder.
- */
-
-#include <mmu/mali_kbase_mmu_faults_decoder_luts.h>
-
-#define GPU_ID_ARCH_ID_MAJOR_GET(gpu_id) ((gpu_id >> 16) & 0xFF)
-#define GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id) (gpu_id & 0xFFFF)
-#define NELEMS(s) (sizeof(s) / sizeof((s)[0]))
-
-struct decode_lut_element {
-	u16 arch_minor_rev;
-	u16 key;
-	const char *text;
-};
-
-static const char *decode_lut_element_lookup(u16 arch_minor_rev, u16 key,
-					     struct decode_lut_element *decode_element_lut,
-					     unsigned int lut_len)
-{
-	struct decode_lut_element *p;
-
-	for (p = decode_element_lut; p < decode_element_lut + lut_len; p++) {
-		if (p->key == key &&
-		    (p->arch_minor_rev == 0xffff || p->arch_minor_rev == arch_minor_rev))
-			break;
-	}
-	if (p < decode_element_lut + lut_len)
-		return p->text;
-	else
-		return "unknown";
-}
-
-/* Auto-generated code: DO NOT MODIFY! */
-
-static struct decode_lut_element lut_fault_source_core_type_t_name_major_9[] = {
-	{ 0xFFFF, 0, "shader" }, { 0xFFFF, 1, "l2c" }, { 0xFFFF, 2, "tiler" },
-	{ 0xFFFF, 3, "mmu" },	 { 0xFFFF, 4, "jm" },  { 0xFFFF, 5, "pmb" },
-};
-
-static struct decode_lut_element lut_fault_source_core_type_t_desc_major_9[] = {
-	{ 0xFFFF, 0, "Shader core" }, { 0xFFFF, 1, "Level 2 cache" },
-	{ 0xFFFF, 2, "Tiler" },	      { 0xFFFF, 3, "MMU" },
-	{ 0xFFFF, 4, "Job Manager" }, { 0xFFFF, 5, "Performance Monitor Block" },
-};
-
-static struct decode_lut_element lut_fault_source_core_type_t_name_major_10[] = {
-	{ 0xFFFF, 0, "shader" }, { 0xFFFF, 1, "l2c" }, { 0xFFFF, 2, "tiler" },
-	{ 0xFFFF, 3, "mmu" },	 { 0xFFFF, 4, "csf" }, { 0xFFFF, 5, "memsys" },
-};
-
-static struct decode_lut_element lut_fault_source_core_type_t_desc_major_10[] = {
-	{ 0xFFFF, 0, "Shader core" }, { 0xFFFF, 1, "Level 2 cache" },
-	{ 0xFFFF, 2, "Tiler" },	      { 0xFFFF, 3, "MMU" },
-	{ 0xFFFF, 4, "CSF" },	      { 0xFFFF, 5, "Memory system" },
-};
-
-static struct decode_lut_element lut_fault_source_core_type_t_name_major_11[] = {
-	{ 0xFFFF, 0, "shader" }, { 0xFFFF, 1, "l2c" }, { 0xFFFF, 2, "tiler" },
-	{ 0xFFFF, 3, "mmu" },	 { 0xFFFF, 4, "csf" }, { 0xFFFF, 5, "memsys" },
-};
-
-static struct decode_lut_element lut_fault_source_core_type_t_desc_major_11[] = {
-	{ 0xFFFF, 0, "Shader core" }, { 0xFFFF, 1, "Level 2 cache" },
-	{ 0xFFFF, 2, "Tiler" },	      { 0xFFFF, 3, "MMU" },
-	{ 0xFFFF, 4, "CSF" },	      { 0xFFFF, 5, "Memory system" },
-};
-
-static struct decode_lut_element lut_fault_source_core_type_t_name_major_12[] = {
-	{ 0xFFFF, 0, "shader" }, { 0xFFFF, 1, "l2c" }, { 0xFFFF, 2, "tiler" },
-	{ 0xFFFF, 3, "mmu" },	 { 0xFFFF, 4, "csf" }, { 0xFFFF, 5, "memsys" },
-};
-
-static struct decode_lut_element lut_fault_source_core_type_t_desc_major_12[] = {
-	{ 0xFFFF, 0, "Shader core" }, { 0xFFFF, 1, "Level 2 cache" },
-	{ 0xFFFF, 2, "Tiler" },	      { 0xFFFF, 3, "MMU" },
-	{ 0xFFFF, 4, "CSF" },	      { 0xFFFF, 5, "Memory system" },
-};
-
-static struct decode_lut_element lut_fault_source_core_id_t_desc_major_9[] = {
-	{ 0xFFFF, 0, "Shader core 0" },
-	{ 0xFFFF, 1, "Shader core 1" },
-	{ 0xFFFF, 2, "Shader core 2" },
-	{ 0xFFFF, 3, "Shader core 3" },
-	{ 0xFFFF, 4, "Shader core 4" },
-	{ 0xFFFF, 5, "Shader core 5" },
-	{ 0xFFFF, 6, "Shader core 6" },
-	{ 0xFFFF, 7, "Shader core 7" },
-	{ 0xFFFF, 8, "Shader core 8" },
-	{ 0xFFFF, 9, "Shader core 9" },
-	{ 0xFFFF, 10, "Shader core 10" },
-	{ 0xFFFF, 11, "Shader core 11" },
-	{ 0xFFFF, 12, "Shader core 12" },
-	{ 0xFFFF, 13, "Shader core 13" },
-	{ 0xFFFF, 14, "Shader core 14" },
-	{ 0xFFFF, 15, "Shader core 15" },
-	{ 0xFFFF, 16, "Shader core 16" },
-	{ 0xFFFF, 17, "Shader core 17" },
-	{ 0xFFFF, 18, "Shader core 18" },
-	{ 0xFFFF, 19, "Shader core 19" },
-	{ 0xFFFF, 20, "Shader core 20" },
-	{ 0xFFFF, 21, "Shader core 21" },
-	{ 0xFFFF, 22, "Shader core 22" },
-	{ 0xFFFF, 23, "Shader core 23" },
-	{ 0xFFFF, 24, "Shader core 24" },
-	{ 0xFFFF, 25, "Shader core 25" },
-	{ 0xFFFF, 26, "Shader core 26" },
-	{ 0xFFFF, 27, "Shader core 27" },
-	{ 0xFFFF, 28, "Shader core 28" },
-	{ 0xFFFF, 29, "Shader core 29" },
-	{ 0xFFFF, 30, "Shader core 30" },
-	{ 0xFFFF, 31, "Shader core 31" },
-	{ 0xFFFF, 41, "L2 Slice 3" },
-	{ 0xFFFF, 43, "L2 Slice 2" },
-	{ 0xFFFF, 45, "L2 Slice 1" },
-	{ 0xFFFF, 46, "PMB" },
-	{ 0xFFFF, 47, "L2 Slice 0" },
-	{ 0xFFFF, 51, "Tiler" },
-	{ 0xFFFF, 55, "MMU" },
-	{ 0xFFFF, 62, "Job Manager" },
-};
-
-static struct decode_lut_element lut_fault_source_core_id_t_core_type_major_9[] = {
-	{ 0xFFFF, 0, "shader" },  { 0xFFFF, 1, "shader" },  { 0xFFFF, 2, "shader" },
-	{ 0xFFFF, 3, "shader" },  { 0xFFFF, 4, "shader" },  { 0xFFFF, 5, "shader" },
-	{ 0xFFFF, 6, "shader" },  { 0xFFFF, 7, "shader" },  { 0xFFFF, 8, "shader" },
-	{ 0xFFFF, 9, "shader" },  { 0xFFFF, 10, "shader" }, { 0xFFFF, 11, "shader" },
-	{ 0xFFFF, 12, "shader" }, { 0xFFFF, 13, "shader" }, { 0xFFFF, 14, "shader" },
-	{ 0xFFFF, 15, "shader" }, { 0xFFFF, 16, "shader" }, { 0xFFFF, 17, "shader" },
-	{ 0xFFFF, 18, "shader" }, { 0xFFFF, 19, "shader" }, { 0xFFFF, 20, "shader" },
-	{ 0xFFFF, 21, "shader" }, { 0xFFFF, 22, "shader" }, { 0xFFFF, 23, "shader" },
-	{ 0xFFFF, 24, "shader" }, { 0xFFFF, 25, "shader" }, { 0xFFFF, 26, "shader" },
-	{ 0xFFFF, 27, "shader" }, { 0xFFFF, 28, "shader" }, { 0xFFFF, 29, "shader" },
-	{ 0xFFFF, 30, "shader" }, { 0xFFFF, 31, "shader" }, { 0xFFFF, 41, "l2c" },
-	{ 0xFFFF, 43, "l2c" },	  { 0xFFFF, 45, "l2c" },    { 0xFFFF, 46, "pmb" },
-	{ 0xFFFF, 47, "l2c" },	  { 0xFFFF, 51, "tiler" },  { 0xFFFF, 55, "mmu" },
-	{ 0xFFFF, 62, "jm" },
-};
-
-static struct decode_lut_element lut_fault_source_core_id_t_desc_major_10[] = {
-	{ 0xFFFF, 0, "Shader core 0" },
-	{ 0xFFFF, 1, "Shader core 1" },
-	{ 0xFFFF, 2, "Shader core 2" },
-	{ 0xFFFF, 3, "Shader core 3" },
-	{ 0xFFFF, 4, "Shader core 4" },
-	{ 0xFFFF, 5, "Shader core 5" },
-	{ 0xFFFF, 6, "Shader core 6" },
-	{ 0xFFFF, 7, "Shader core 7" },
-	{ 0xFFFF, 8, "Shader core 8" },
-	{ 0xFFFF, 9, "Shader core 9" },
-	{ 0xFFFF, 10, "Shader core 10" },
-	{ 0xFFFF, 11, "Shader core 11" },
-	{ 0xFFFF, 12, "Shader core 12" },
-	{ 0xFFFF, 13, "Shader core 13" },
-	{ 0xFFFF, 14, "Shader core 14" },
-	{ 0xFFFF, 15, "Shader core 15" },
-	{ 0xFFFF, 16, "Shader core 16" },
-	{ 0xFFFF, 17, "Shader core 17" },
-	{ 0xFFFF, 18, "Shader core 18" },
-	{ 0xFFFF, 19, "Shader core 19" },
-	{ 0xFFFF, 20, "Shader core 20" },
-	{ 0xFFFF, 21, "Shader core 21" },
-	{ 0xFFFF, 22, "Shader core 22" },
-	{ 0xFFFF, 23, "Shader core 23" },
-	{ 0xFFFF, 24, "Shader core 24" },
-	{ 0xFFFF, 25, "Shader core 25" },
-	{ 0xFFFF, 26, "Shader core 26" },
-	{ 0xFFFF, 27, "Shader core 27" },
-	{ 0xFFFF, 28, "Shader core 28" },
-	{ 0xFFFF, 29, "Shader core 29" },
-	{ 0xFFFF, 30, "Shader core 30" },
-	{ 0xFFFF, 31, "Shader core 31" },
-	{ 0xFFFF, 41, "L2 Slice 3" },
-	{ 0xFFFF, 43, "L2 Slice 2" },
-	{ 0xFFFF, 45, "L2 Slice 1" },
-	{ 0xFFFF, 47, "L2 Slice 0" },
-	{ 0xFFFF, 51, "Tiler" },
-	{ 0xFFFF, 55, "MMU" },
-	{ 0xFFFF, 33, "L2 Slice 7" },
-	{ 0xFFFF, 35, "L2 Slice 6" },
-	{ 0xFFFF, 37, "L2 Slice 5" },
-	{ 0xFFFF, 39, "L2 Slice 4" },
-	{ 0xFFFF, 48, "Memory system, undefined" },
-	{ 0xFFFF, 62, "Command Stream Frontend" },
-};
-
-static struct decode_lut_element lut_fault_source_core_id_t_core_type_major_10[] = {
-	{ 0xFFFF, 0, "shader" },  { 0xFFFF, 1, "shader" },  { 0xFFFF, 2, "shader" },
-	{ 0xFFFF, 3, "shader" },  { 0xFFFF, 4, "shader" },  { 0xFFFF, 5, "shader" },
-	{ 0xFFFF, 6, "shader" },  { 0xFFFF, 7, "shader" },  { 0xFFFF, 8, "shader" },
-	{ 0xFFFF, 9, "shader" },  { 0xFFFF, 10, "shader" }, { 0xFFFF, 11, "shader" },
-	{ 0xFFFF, 12, "shader" }, { 0xFFFF, 13, "shader" }, { 0xFFFF, 14, "shader" },
-	{ 0xFFFF, 15, "shader" }, { 0xFFFF, 16, "shader" }, { 0xFFFF, 17, "shader" },
-	{ 0xFFFF, 18, "shader" }, { 0xFFFF, 19, "shader" }, { 0xFFFF, 20, "shader" },
-	{ 0xFFFF, 21, "shader" }, { 0xFFFF, 22, "shader" }, { 0xFFFF, 23, "shader" },
-	{ 0xFFFF, 24, "shader" }, { 0xFFFF, 25, "shader" }, { 0xFFFF, 26, "shader" },
-	{ 0xFFFF, 27, "shader" }, { 0xFFFF, 28, "shader" }, { 0xFFFF, 29, "shader" },
-	{ 0xFFFF, 30, "shader" }, { 0xFFFF, 31, "shader" }, { 0xFFFF, 41, "l2c" },
-	{ 0xFFFF, 43, "l2c" },	  { 0xFFFF, 45, "l2c" },    { 0xFFFF, 47, "l2c" },
-	{ 0xFFFF, 51, "tiler" },  { 0xFFFF, 55, "mmu" },    { 0xFFFF, 33, "l2c" },
-	{ 0xFFFF, 35, "l2c" },	  { 0xFFFF, 37, "l2c" },    { 0xFFFF, 39, "l2c" },
-	{ 0xFFFF, 48, "memsys" }, { 0xFFFF, 62, "csf" },
-};
-
-static struct decode_lut_element lut_fault_source_core_id_t_desc_major_11[] = {
-	{ 0xFFFF, 0, "Shader core 0" },
-	{ 0xFFFF, 1, "Shader core 1" },
-	{ 0xFFFF, 2, "Shader core 2" },
-	{ 0xFFFF, 3, "Shader core 3" },
-	{ 0xFFFF, 4, "Shader core 4" },
-	{ 0xFFFF, 5, "Shader core 5" },
-	{ 0xFFFF, 6, "Shader core 6" },
-	{ 0xFFFF, 7, "Shader core 7" },
-	{ 0xFFFF, 8, "Shader core 8" },
-	{ 0xFFFF, 9, "Shader core 9" },
-	{ 0xFFFF, 10, "Shader core 10" },
-	{ 0xFFFF, 11, "Shader core 11" },
-	{ 0xFFFF, 12, "Shader core 12" },
-	{ 0xFFFF, 13, "Shader core 13" },
-	{ 0xFFFF, 14, "Shader core 14" },
-	{ 0xFFFF, 15, "Shader core 15" },
-	{ 0xFFFF, 16, "Shader core 16" },
-	{ 0xFFFF, 17, "Shader core 17" },
-	{ 0xFFFF, 18, "Shader core 18" },
-	{ 0xFFFF, 19, "Shader core 19" },
-	{ 0xFFFF, 20, "Shader core 20" },
-	{ 0xFFFF, 21, "Shader core 21" },
-	{ 0xFFFF, 22, "Shader core 22" },
-	{ 0xFFFF, 23, "Shader core 23" },
-	{ 0xFFFF, 24, "Shader core 24" },
-	{ 0xFFFF, 25, "Shader core 25" },
-	{ 0xFFFF, 26, "Shader core 26" },
-	{ 0xFFFF, 27, "Shader core 27" },
-	{ 0xFFFF, 28, "Shader core 28" },
-	{ 0xFFFF, 29, "Shader core 29" },
-	{ 0xFFFF, 30, "Shader core 30" },
-	{ 0xFFFF, 31, "Shader core 31" },
-	{ 0xFFFF, 41, "L2 Slice 3" },
-	{ 0xFFFF, 43, "L2 Slice 2" },
-	{ 0xFFFF, 45, "L2 Slice 1" },
-	{ 0xFFFF, 47, "L2 Slice 0" },
-	{ 0xFFFF, 51, "Tiler" },
-	{ 0xFFFF, 55, "MMU" },
-	{ 0xFFFF, 33, "L2 Slice 7" },
-	{ 0xFFFF, 35, "L2 Slice 6" },
-	{ 0xFFFF, 37, "L2 Slice 5" },
-	{ 0xFFFF, 39, "L2 Slice 4" },
-	{ 0xFFFF, 48, "Memory system, undefined" },
-	{ 0xFFFF, 62, "Command Stream Frontend" },
-};
-
-static struct decode_lut_element lut_fault_source_core_id_t_core_type_major_11[] = {
-	{ 0xFFFF, 0, "shader" },  { 0xFFFF, 1, "shader" },  { 0xFFFF, 2, "shader" },
-	{ 0xFFFF, 3, "shader" },  { 0xFFFF, 4, "shader" },  { 0xFFFF, 5, "shader" },
-	{ 0xFFFF, 6, "shader" },  { 0xFFFF, 7, "shader" },  { 0xFFFF, 8, "shader" },
-	{ 0xFFFF, 9, "shader" },  { 0xFFFF, 10, "shader" }, { 0xFFFF, 11, "shader" },
-	{ 0xFFFF, 12, "shader" }, { 0xFFFF, 13, "shader" }, { 0xFFFF, 14, "shader" },
-	{ 0xFFFF, 15, "shader" }, { 0xFFFF, 16, "shader" }, { 0xFFFF, 17, "shader" },
-	{ 0xFFFF, 18, "shader" }, { 0xFFFF, 19, "shader" }, { 0xFFFF, 20, "shader" },
-	{ 0xFFFF, 21, "shader" }, { 0xFFFF, 22, "shader" }, { 0xFFFF, 23, "shader" },
-	{ 0xFFFF, 24, "shader" }, { 0xFFFF, 25, "shader" }, { 0xFFFF, 26, "shader" },
-	{ 0xFFFF, 27, "shader" }, { 0xFFFF, 28, "shader" }, { 0xFFFF, 29, "shader" },
-	{ 0xFFFF, 30, "shader" }, { 0xFFFF, 31, "shader" }, { 0xFFFF, 41, "l2c" },
-	{ 0xFFFF, 43, "l2c" },	  { 0xFFFF, 45, "l2c" },    { 0xFFFF, 47, "l2c" },
-	{ 0xFFFF, 51, "tiler" },  { 0xFFFF, 55, "mmu" },    { 0xFFFF, 33, "l2c" },
-	{ 0xFFFF, 35, "l2c" },	  { 0xFFFF, 37, "l2c" },    { 0xFFFF, 39, "l2c" },
-	{ 0xFFFF, 48, "memsys" }, { 0xFFFF, 62, "csf" },
-};
-
-static struct decode_lut_element lut_fault_source_core_id_t_desc_major_12[] = {
-	{ 0xFFFF, 0, "Shader core 0" },
-	{ 0xFFFF, 1, "Shader core 1" },
-	{ 0xFFFF, 2, "Shader core 2" },
-	{ 0xFFFF, 3, "Shader core 3" },
-	{ 0xFFFF, 4, "Shader core 4" },
-	{ 0xFFFF, 5, "Shader core 5" },
-	{ 0xFFFF, 6, "Shader core 6" },
-	{ 0xFFFF, 7, "Shader core 7" },
-	{ 0xFFFF, 8, "Shader core 8" },
-	{ 0xFFFF, 9, "Shader core 9" },
-	{ 0xFFFF, 10, "Shader core 10" },
-	{ 0xFFFF, 11, "Shader core 11" },
-	{ 0xFFFF, 12, "Shader core 12" },
-	{ 0xFFFF, 13, "Shader core 13" },
-	{ 0xFFFF, 14, "Shader core 14" },
-	{ 0xFFFF, 15, "Shader core 15" },
-	{ 0xFFFF, 16, "Shader core 16" },
-	{ 0xFFFF, 17, "Shader core 17" },
-	{ 0xFFFF, 18, "Shader core 18" },
-	{ 0xFFFF, 19, "Shader core 19" },
-	{ 0xFFFF, 20, "Shader core 20" },
-	{ 0xFFFF, 21, "Shader core 21" },
-	{ 0xFFFF, 22, "Shader core 22" },
-	{ 0xFFFF, 23, "Shader core 23" },
-	{ 0xFFFF, 24, "Shader core 24" },
-	{ 0xFFFF, 25, "Shader core 25" },
-	{ 0xFFFF, 26, "Shader core 26" },
-	{ 0xFFFF, 27, "Shader core 27" },
-	{ 0xFFFF, 28, "Shader core 28" },
-	{ 0xFFFF, 29, "Shader core 29" },
-	{ 0xFFFF, 30, "Shader core 30" },
-	{ 0xFFFF, 31, "Shader core 31" },
-	{ 0xFFFF, 41, "L2 Slice 3" },
-	{ 0xFFFF, 43, "L2 Slice 2" },
-	{ 0xFFFF, 45, "L2 Slice 1" },
-	{ 0xFFFF, 47, "L2 Slice 0" },
-	{ 0xFFFF, 51, "Tiler" },
-	{ 0xFFFF, 55, "MMU" },
-	{ 0xFFFF, 33, "L2 Slice 7" },
-	{ 0xFFFF, 35, "L2 Slice 6" },
-	{ 0xFFFF, 37, "L2 Slice 5" },
-	{ 0xFFFF, 39, "L2 Slice 4" },
-	{ 0xFFFF, 48, "Memory system, undefined" },
-	{ 0xFFFF, 62, "Command Stream Frontend" },
-};
-
-static struct decode_lut_element lut_fault_source_core_id_t_core_type_major_12[] = {
-	{ 0xFFFF, 0, "shader" },  { 0xFFFF, 1, "shader" },  { 0xFFFF, 2, "shader" },
-	{ 0xFFFF, 3, "shader" },  { 0xFFFF, 4, "shader" },  { 0xFFFF, 5, "shader" },
-	{ 0xFFFF, 6, "shader" },  { 0xFFFF, 7, "shader" },  { 0xFFFF, 8, "shader" },
-	{ 0xFFFF, 9, "shader" },  { 0xFFFF, 10, "shader" }, { 0xFFFF, 11, "shader" },
-	{ 0xFFFF, 12, "shader" }, { 0xFFFF, 13, "shader" }, { 0xFFFF, 14, "shader" },
-	{ 0xFFFF, 15, "shader" }, { 0xFFFF, 16, "shader" }, { 0xFFFF, 17, "shader" },
-	{ 0xFFFF, 18, "shader" }, { 0xFFFF, 19, "shader" }, { 0xFFFF, 20, "shader" },
-	{ 0xFFFF, 21, "shader" }, { 0xFFFF, 22, "shader" }, { 0xFFFF, 23, "shader" },
-	{ 0xFFFF, 24, "shader" }, { 0xFFFF, 25, "shader" }, { 0xFFFF, 26, "shader" },
-	{ 0xFFFF, 27, "shader" }, { 0xFFFF, 28, "shader" }, { 0xFFFF, 29, "shader" },
-	{ 0xFFFF, 30, "shader" }, { 0xFFFF, 31, "shader" }, { 0xFFFF, 41, "l2c" },
-	{ 0xFFFF, 43, "l2c" },	  { 0xFFFF, 45, "l2c" },    { 0xFFFF, 47, "l2c" },
-	{ 0xFFFF, 51, "tiler" },  { 0xFFFF, 55, "mmu" },    { 0xFFFF, 33, "l2c" },
-	{ 0xFFFF, 35, "l2c" },	  { 0xFFFF, 37, "l2c" },    { 0xFFFF, 39, "l2c" },
-	{ 0xFFFF, 48, "memsys" }, { 0xFFFF, 62, "csf" },
-};
-
-static struct decode_lut_element lut_fault_source_shader_r_t_major_9[] = {
-	{ 0xFFFF, 0, "ic" },	{ 0xFFFF, 1, "adc" },	{ 0xFFFF, 4, "scm" },
-	{ 0xFFFF, 5, "vl" },	{ 0xFFFF, 6, "plr" },	{ 0xFFFF, 7, "fsdc" },
-	{ 0xFFFF, 8, "lsc" },	{ 0xFFFF, 9, "cse" },	{ 0xFFFF, 10, "tb" },
-	{ 0xFFFF, 11, "tmdi" }, { 0xFFFF, 12, "tmu0" }, { 0xFFFF, 13, "tmu1" },
-	{ 0xFFFF, 14, "tma0" }, { 0xFFFF, 15, "tma1" },
-};
-
-static struct decode_lut_element lut_fault_source_shader_r_t_major_10[] = {
-	{ 0xFFFF, 4, "scm" },	{ 0xFFFF, 5, "vl" },	{ 0xFFFF, 6, "plr" },
-	{ 0xFFFF, 7, "fsdc" },	{ 0xFFFF, 8, "lsc" },	{ 0xFFFF, 9, "cse" },
-	{ 0xFFFF, 10, "tb" },	{ 0xFFFF, 11, "tmdi" }, { 0xFFFF, 12, "tmu0" },
-	{ 0xFFFF, 13, "tmu1" }, { 0xFFFF, 14, "tma0" }, { 0xFFFF, 15, "tma1" },
-	{ 0xFFFF, 0, "ic0" },	{ 0xFFFF, 1, "ic1" },	{ 0xFFFF, 2, "adc" },
-};
-
-static struct decode_lut_element lut_fault_source_shader_r_t_major_11[] = {
-	{ 0xFFFF, 4, "scm" },	{ 0xFFFF, 5, "vl" },	{ 0xFFFF, 6, "plr" },
-	{ 0xFFFF, 7, "fsdc" },	{ 0xFFFF, 8, "lsc" },	{ 0xFFFF, 9, "cse" },
-	{ 0xFFFF, 10, "tb" },	{ 0xFFFF, 11, "tmdi" }, { 0xFFFF, 12, "tmu0" },
-	{ 0xFFFF, 13, "tmu1" }, { 0xFFFF, 14, "tma0" }, { 0xFFFF, 15, "tma1" },
-	{ 0xFFFF, 0, "ic0" },	{ 0xFFFF, 1, "ic1" },	{ 0xFFFF, 2, "adc" },
-};
-
-static struct decode_lut_element lut_fault_source_shader_r_t_major_12[] = {
-	{ 0xFFFF, 4, "scm" },	{ 0xFFFF, 6, "plr" },	{ 0xFFFF, 7, "fsdc" },
-	{ 0xFFFF, 8, "lsc" },	{ 0xFFFF, 9, "cse" },	{ 0xFFFF, 10, "tb" },
-	{ 0xFFFF, 11, "tmdi" }, { 0xFFFF, 12, "tmu0" }, { 0xFFFF, 13, "tmu1" },
-	{ 0xFFFF, 14, "tma0" }, { 0xFFFF, 15, "tma1" }, { 0xFFFF, 0, "ic0" },
-	{ 0xFFFF, 1, "ic1" },	{ 0xFFFF, 2, "adc" },	{ 0xFFFF, 3, "rtas" },
-};
-
-static struct decode_lut_element lut_fault_source_shader_w_t_major_9[] = {
-	{ 0xFFFF, 0, "pcb" },
-	{ 0xFFFF, 8, "lsc" },
-	{ 0xFFFF, 10, "tb" },
-};
-
-static struct decode_lut_element lut_fault_source_shader_w_t_major_10[] = {
-	{ 0xFFFF, 0, "pcb" },  { 0xFFFF, 8, "lsc" },  { 0xFFFF, 12, "tb0" },
-	{ 0xFFFF, 13, "tb1" }, { 0xFFFF, 14, "tb2" }, { 0xFFFF, 15, "tb3" },
-};
-
-static struct decode_lut_element lut_fault_source_shader_w_t_major_11[] = {
-	{ 0xFFFF, 0, "pcb" },  { 0xFFFF, 8, "lsc" },  { 0xFFFF, 12, "tb0" },
-	{ 0xFFFF, 13, "tb1" }, { 0xFFFF, 14, "tb2" }, { 0xFFFF, 15, "tb3" },
-};
-
-static struct decode_lut_element lut_fault_source_shader_w_t_major_12[] = {
-	{ 0xFFFF, 0, "pcb" },  { 0xFFFF, 8, "lsc" },  { 0xFFFF, 12, "tb0" },
-	{ 0xFFFF, 13, "tb1" }, { 0xFFFF, 14, "tb2" }, { 0xFFFF, 15, "tb3" },
-};
-
-static struct decode_lut_element lut_fault_source_tiler_r_t_major_10[] = {
-	{ 0xFFFF, 0, "pf" },
-	{ 0xFFFF, 1, "pcache" },
-	{ 0xFFFF, 2, "tcu" },
-	{ 0xFFFF, 3, "idx" },
-};
-
-static struct decode_lut_element lut_fault_source_tiler_r_t_major_11[] = {
-	{ 0xFFFF, 0, "pf" },
-	{ 0xFFFF, 1, "pcache" },
-	{ 0xFFFF, 2, "tcu" },
-	{ 0xFFFF, 3, "idx" },
-};
-
-static struct decode_lut_element lut_fault_source_tiler_r_t_major_12[] = {
-	{ 0xFFFF, 0, "pf" },
-	{ 0xFFFF, 1, "pcache" },
-	{ 0xFFFF, 2, "tcu" },
-	{ 0xFFFF, 3, "idx" },
-};
-
-static struct decode_lut_element lut_fault_source_tiler_w_t_major_10[] = {
-	{ 0xFFFF, 1, "pcache_wb" },
-	{ 0xFFFF, 2, "tcu_pcb" },
-};
-
-static struct decode_lut_element lut_fault_source_tiler_w_t_major_11[] = {
-	{ 0xFFFF, 1, "pcache_wb" },
-	{ 0xFFFF, 2, "tcu_pcb" },
-};
-
-static struct decode_lut_element lut_fault_source_tiler_w_t_major_12[] = {
-	{ 0xFFFF, 1, "pcache_wb" },
-	{ 0xFFFF, 2, "tcu_pcb" },
-};
-
-
-const char *decode_fault_source_core_type_t_name(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 9:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_type_t_name_major_9,
-						NELEMS(lut_fault_source_core_type_t_name_major_9));
-		break;
-	case 10:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_type_t_name_major_10,
-						NELEMS(lut_fault_source_core_type_t_name_major_10));
-		break;
-	case 11:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_type_t_name_major_11,
-						NELEMS(lut_fault_source_core_type_t_name_major_11));
-		break;
-	case 12:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_type_t_name_major_12,
-						NELEMS(lut_fault_source_core_type_t_name_major_12));
-		break;
-	}
-	return ret;
-}
-
-const char *decode_fault_source_core_type_t_desc(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 9:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_type_t_desc_major_9,
-						NELEMS(lut_fault_source_core_type_t_desc_major_9));
-		break;
-	case 10:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_type_t_desc_major_10,
-						NELEMS(lut_fault_source_core_type_t_desc_major_10));
-		break;
-	case 11:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_type_t_desc_major_11,
-						NELEMS(lut_fault_source_core_type_t_desc_major_11));
-		break;
-	case 12:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_type_t_desc_major_12,
-						NELEMS(lut_fault_source_core_type_t_desc_major_12));
-		break;
-	}
-	return ret;
-}
-
-const char *decode_fault_source_core_id_t_desc(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 9:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_id_t_desc_major_9,
-						NELEMS(lut_fault_source_core_id_t_desc_major_9));
-		break;
-	case 10:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_id_t_desc_major_10,
-						NELEMS(lut_fault_source_core_id_t_desc_major_10));
-		break;
-	case 11:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_id_t_desc_major_11,
-						NELEMS(lut_fault_source_core_id_t_desc_major_11));
-		break;
-	case 12:
-		ret = decode_lut_element_lookup(min_rev, idx,
-						lut_fault_source_core_id_t_desc_major_12,
-						NELEMS(lut_fault_source_core_id_t_desc_major_12));
-		break;
-	}
-	return ret;
-}
-
-const char *decode_fault_source_core_id_t_core_type(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 9:
-		ret = decode_lut_element_lookup(
-			min_rev, idx, lut_fault_source_core_id_t_core_type_major_9,
-			NELEMS(lut_fault_source_core_id_t_core_type_major_9));
-		break;
-	case 10:
-		ret = decode_lut_element_lookup(
-			min_rev, idx, lut_fault_source_core_id_t_core_type_major_10,
-			NELEMS(lut_fault_source_core_id_t_core_type_major_10));
-		break;
-	case 11:
-		ret = decode_lut_element_lookup(
-			min_rev, idx, lut_fault_source_core_id_t_core_type_major_11,
-			NELEMS(lut_fault_source_core_id_t_core_type_major_11));
-		break;
-	case 12:
-		ret = decode_lut_element_lookup(
-			min_rev, idx, lut_fault_source_core_id_t_core_type_major_12,
-			NELEMS(lut_fault_source_core_id_t_core_type_major_12));
-		break;
-	}
-	return ret;
-}
-
-const char *decode_fault_source_shader_r_t(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 9:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_shader_r_t_major_9,
-						NELEMS(lut_fault_source_shader_r_t_major_9));
-		break;
-	case 10:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_shader_r_t_major_10,
-						NELEMS(lut_fault_source_shader_r_t_major_10));
-		break;
-	case 11:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_shader_r_t_major_11,
-						NELEMS(lut_fault_source_shader_r_t_major_11));
-		break;
-	case 12:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_shader_r_t_major_12,
-						NELEMS(lut_fault_source_shader_r_t_major_12));
-		break;
-	}
-	return ret;
-}
-
-const char *decode_fault_source_shader_w_t(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 9:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_shader_w_t_major_9,
-						NELEMS(lut_fault_source_shader_w_t_major_9));
-		break;
-	case 10:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_shader_w_t_major_10,
-						NELEMS(lut_fault_source_shader_w_t_major_10));
-		break;
-	case 11:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_shader_w_t_major_11,
-						NELEMS(lut_fault_source_shader_w_t_major_11));
-		break;
-	case 12:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_shader_w_t_major_12,
-						NELEMS(lut_fault_source_shader_w_t_major_12));
-		break;
-	}
-	return ret;
-}
-
-const char *decode_fault_source_tiler_r_t(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 10:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_tiler_r_t_major_10,
-						NELEMS(lut_fault_source_tiler_r_t_major_10));
-		break;
-	case 11:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_tiler_r_t_major_11,
-						NELEMS(lut_fault_source_tiler_r_t_major_11));
-		break;
-	case 12:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_tiler_r_t_major_12,
-						NELEMS(lut_fault_source_tiler_r_t_major_12));
-		break;
-	}
-	return ret;
-}
-
-const char *decode_fault_source_tiler_w_t(u16 idx, u32 gpu_id)
-{
-	u16 min_rev = GPU_ID_ARCH_ID_MINOR_AND_REV_GET(gpu_id);
-	const char *ret = "unknown";
-
-	switch (GPU_ID_ARCH_ID_MAJOR_GET(gpu_id)) {
-	case 10:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_tiler_w_t_major_10,
-						NELEMS(lut_fault_source_tiler_w_t_major_10));
-		break;
-	case 11:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_tiler_w_t_major_11,
-						NELEMS(lut_fault_source_tiler_w_t_major_11));
-		break;
-	case 12:
-		ret = decode_lut_element_lookup(min_rev, idx, lut_fault_source_tiler_w_t_major_12,
-						NELEMS(lut_fault_source_tiler_w_t_major_12));
-		break;
-	}
-	return ret;
-}
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder_luts.h b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder_luts.h
deleted file mode 100644
index 2b0ca5659a6c..000000000000
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_faults_decoder_luts.h
+++ /dev/null
@@ -1,119 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
-/*
- *
- * (C) COPYRIGHT 2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-#ifndef _MALI_KBASE_MMU_FAULTS_DECODER_LUTS_H_
-#define _MALI_KBASE_MMU_FAULTS_DECODER_LUTS_H_
-
-#include <linux/types.h>
-
-/**
- * decode_fault_source_core_id_t_desc() - Get core description of a
- * fault in a human readable format.
- *
- * @idx: Core ID part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: core ID of the fault in human readable format.
- */
-const char *decode_fault_source_core_id_t_desc(u16 idx, u32 gpu_id);
-
-/**
- * decode_fault_source_core_id_t_core_type() - Get core type of a
- * fault in a human readable format.
- *
- * @idx: Core ID part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: core type of the fault in human readable format.
- */
-const char *decode_fault_source_core_id_t_core_type(u16 idx, u32 gpu_id);
-
-/**
- * decode_fault_source_core_type_t_name() - Get core type name of a
- * fault.
- *
- * @idx: Core type part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: core type short name of the fault.
- */
-const char *decode_fault_source_core_type_t_name(u16 idx, u32 gpu_id);
-
-/**
- * decode_fault_source_core_type_t_desc() - Get core type description of a
- * fault.
- *
- * @idx: Core type part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: core type description of the fault.
- */
-const char *decode_fault_source_core_type_t_desc(u16 idx, u32 gpu_id);
-
-/**
- * decode_fault_source_shader_r_t() - Get internal requester of a
- * fault in a human readable format.
- *
- * @idx: Internal requester part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: Internal requester of a fault in a human readable format for read
- * operations on a shader core.
- */
-const char *decode_fault_source_shader_r_t(u16 idx, u32 gpu_id);
-
-/**
- * decode_fault_source_shader_w_t() - Get internal requester of a
- * fault in a human readable format.
- *
- * @idx: Internal requester part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: Internal requester of a fault in a human readable format for write
- * operations on a shader core.
- */
-const char *decode_fault_source_shader_w_t(u16 idx, u32 gpu_id);
-
-/**
- * decode_fault_source_tiler_r_t() - Get internal requester of a
- * fault in a human readable format.
- *
- * @idx: Internal requester part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: Internal requester of a fault in a human readable format for read
- * operations on a tiler core.
- */
-const char *decode_fault_source_tiler_r_t(u16 idx, u32 gpu_id);
-
-/**
- * decode_fault_source_tiler_w_t() - Get internal requester of a
- * fault in a human readable format.
- *
- * @idx: Internal requester part of SOURCE_ID field of the fault.
- * @gpu_id: GPU id composed of arch_major << 16 | arch_minor << 8 | arch_rev.
- *
- * Return: Internal requester of a fault in a human readable format for write
- * operations on a tiler core.
- */
-const char *decode_fault_source_tiler_w_t(u16 idx, u32 gpu_id);
-
-#endif /* _MALI_KBASE_MMU_FAULTS_DECODER_LUTS_H_ */
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw.h b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw.h
index 560baceafe8a..b0b1837ae18b 100644
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw.h
+++ b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -42,7 +42,7 @@ struct kbase_context;
  * enum kbase_mmu_fault_type - MMU fault type descriptor.
  * @KBASE_MMU_FAULT_TYPE_UNKNOWN:         unknown fault
  * @KBASE_MMU_FAULT_TYPE_PAGE:            page fault
- * @KBASE_MMU_FAULT_TYPE_BUS:             bus fault
+ * @KBASE_MMU_FAULT_TYPE_BUS:             nus fault
  * @KBASE_MMU_FAULT_TYPE_PAGE_UNEXPECTED: page_unexpected fault
  * @KBASE_MMU_FAULT_TYPE_BUS_UNEXPECTED:  bus_unexpected fault
  */
@@ -56,8 +56,8 @@ enum kbase_mmu_fault_type {
 
 /**
  * struct kbase_mmu_hw_op_param  - parameters for kbase_mmu_hw_do_* functions
- * @vpfn:           MMU Virtual Page Frame Number (in PAGE_SIZE units) to start the operation on.
- * @nr:             Number of pages (in PAGE_SIZE units) to work on.
+ * @vpfn:           MMU Virtual Page Frame Number to start the operation on.
+ * @nr:             Number of pages to work on.
  * @op:             Operation type (written to AS_COMMAND).
  * @kctx_id:        Kernel context ID for MMU command tracepoint.
  * @mmu_sync_info:  Indicates whether this call is synchronous wrt MMU ops.
@@ -141,13 +141,32 @@ int kbase_mmu_hw_do_unlock(struct kbase_device *kbdev, struct kbase_as *as,
  * Issue a flush operation on the address space as per the information
  * specified inside @op_param. This function should not be called for
  * GPUs where MMU command to flush the cache(s) is deprecated.
- * hwaccess_lock needs to be held when calling this function.
+ * mmu_hw_mutex needs to be held when calling this function.
  *
  * Return: 0 if the operation was successful, non-zero otherwise.
  */
 int kbase_mmu_hw_do_flush(struct kbase_device *kbdev, struct kbase_as *as,
 			  const struct kbase_mmu_hw_op_param *op_param);
 
+/**
+ * kbase_mmu_hw_do_flush_locked - Issue a flush operation to the MMU.
+ *
+ * @kbdev:      Kbase device to issue the MMU operation on.
+ * @as:         Address space to issue the MMU operation on.
+ * @op_param:   Pointer to struct containing information about the MMU
+ *              operation to perform.
+ *
+ * Issue a flush operation on the address space as per the information
+ * specified inside @op_param. This function should not be called for
+ * GPUs where MMU command to flush the cache(s) is deprecated.
+ * Both mmu_hw_mutex and hwaccess_lock need to be held when calling this
+ * function.
+ *
+ * Return: 0 if the operation was successful, non-zero otherwise.
+ */
+int kbase_mmu_hw_do_flush_locked(struct kbase_device *kbdev, struct kbase_as *as,
+				 const struct kbase_mmu_hw_op_param *op_param);
+
 /**
  * kbase_mmu_hw_do_flush_on_gpu_ctrl - Issue a flush operation to the MMU.
  *
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw_direct.c b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw_direct.c
index 46c04f2b1fc1..a2f55b847f71 100644
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw_direct.c
+++ b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw_direct.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -123,7 +123,7 @@ static int lock_region(struct kbase_gpu_props const *gpu_props, u64 *lockaddr,
 	 * therefore the highest bit that differs is bit #16
 	 * and the region size (as a logarithm) is 16 + 1 = 17, i.e. 128 kB.
 	 */
-	lockaddr_size_log2 = (u64)fls64(lockaddr_base ^ lockaddr_end);
+	lockaddr_size_log2 = fls64(lockaddr_base ^ lockaddr_end);
 
 	/* Cap the size against minimum and maximum values allowed. */
 	if (lockaddr_size_log2 > KBASE_LOCK_REGION_MAX_SIZE_LOG2)
@@ -166,18 +166,25 @@ static int lock_region(struct kbase_gpu_props const *gpu_props, u64 *lockaddr,
  */
 static int wait_ready(struct kbase_device *kbdev, unsigned int as_nr)
 {
-	u32 val;
-	int err;
-	const u32 timeout_us =
-		kbase_get_timeout_ms(kbdev, MMU_AS_INACTIVE_WAIT_TIMEOUT) * USEC_PER_MSEC;
+	const ktime_t wait_loop_start = ktime_get_raw();
+	const u32 mmu_as_inactive_wait_time_ms = kbdev->mmu_or_gpu_cache_op_wait_time_ms;
+	s64 diff;
 
 	if (unlikely(kbdev->mmu_unresponsive))
 		return -EBUSY;
 
-	err = kbase_reg_poll32_timeout(kbdev, MMU_AS_OFFSET(as_nr, STATUS), val,
-				       !(val & AS_STATUS_AS_ACTIVE_EXT_MASK), 0, timeout_us, false);
-	if (!err)
-		return 0;
+	do {
+		unsigned int i;
+
+		for (i = 0; i < 1000; i++) {
+			/* Wait for the MMU status to indicate there is no active command */
+			if (!(kbase_reg_read32(kbdev, MMU_AS_OFFSET(as_nr, STATUS)) &
+			      AS_STATUS_AS_ACTIVE_EXT_MASK))
+				return 0;
+		}
+
+		diff = ktime_to_ms(ktime_sub(ktime_get_raw(), wait_loop_start));
+	} while (diff < mmu_as_inactive_wait_time_ms);
 
 	dev_err(kbdev->dev,
 		"AS_ACTIVE bit stuck for as %u. Might be caused by unstable GPU clk/pwr or faulty system",
@@ -189,14 +196,14 @@ static int wait_ready(struct kbase_device *kbdev, unsigned int as_nr)
 	return -ETIMEDOUT;
 }
 
-static int write_cmd(struct kbase_device *kbdev, unsigned int as_nr, u32 cmd)
+static int write_cmd(struct kbase_device *kbdev, int as_nr, u32 cmd)
 {
 	/* write AS_COMMAND when MMU is ready to accept another command */
 	const int status = wait_ready(kbdev, as_nr);
 
 	if (likely(status == 0))
 		kbase_reg_write32(kbdev, MMU_AS_OFFSET(as_nr, COMMAND),
-				  AS_COMMAND_COMMAND_SET(0U, cmd));
+				  AS_COMMAND_COMMAND_SET(0, cmd));
 	else if (status == -EBUSY) {
 		dev_dbg(kbdev->dev,
 			"Skipped the wait for AS_ACTIVE bit for as %u, before sending MMU command %u",
@@ -210,28 +217,7 @@ static int write_cmd(struct kbase_device *kbdev, unsigned int as_nr, u32 cmd)
 	return status;
 }
 
-#if MALI_USE_CSF
-static int wait_l2_power_trans_complete(struct kbase_device *kbdev)
-{
-	u32 val;
-	const u32 timeout_us =
-		kbase_get_timeout_ms(kbdev, MMU_AS_INACTIVE_WAIT_TIMEOUT) * USEC_PER_MSEC;
-	const int err = kbase_reg_poll64_timeout(kbdev, GPU_CONTROL_ENUM(L2_PWRTRANS), val,
-						 val == 0, 1, timeout_us, false);
-
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	if (err) {
-		dev_warn(kbdev->dev, "L2_PWRTRANS %016llx set for too long",
-			 kbase_reg_read64(kbdev, GPU_CONTROL_ENUM(L2_PWRTRANS)));
-		if (kbase_prepare_to_reset_gpu_locked(kbdev, RESET_FLAGS_NONE))
-			kbase_reset_gpu_locked(kbdev);
-	}
-
-	return err;
-}
-
-#if !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
+#if MALI_USE_CSF && !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
 static int wait_cores_power_trans_complete(struct kbase_device *kbdev)
 {
 #define WAIT_TIMEOUT 50000 /* 50ms timeout */
@@ -315,8 +301,7 @@ static int apply_hw_issue_GPU2019_3901_wa(struct kbase_device *kbdev, u32 *mmu_c
 
 	return ret;
 }
-#endif /* !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI) */
-#endif /* MALI_USE_CSF */
+#endif
 
 void kbase_mmu_hw_configure(struct kbase_device *kbdev, struct kbase_as *as)
 {
@@ -324,9 +309,7 @@ void kbase_mmu_hw_configure(struct kbase_device *kbdev, struct kbase_as *as)
 	u64 transcfg = 0;
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
-#if !MALI_USE_CSF
 	lockdep_assert_held(&kbdev->mmu_hw_mutex);
-#endif
 
 	transcfg = current_setup->transcfg;
 
@@ -381,7 +364,7 @@ static void mmu_command_instr(struct kbase_device *kbdev, u32 kctx_id, u32 cmd,
 /* Helper function to program the LOCKADDR register before LOCK/UNLOCK command
  * is issued.
  */
-static int mmu_hw_set_lock_addr(struct kbase_device *kbdev, unsigned int as_nr, u64 *lock_addr,
+static int mmu_hw_set_lock_addr(struct kbase_device *kbdev, int as_nr, u64 *lock_addr,
 				const struct kbase_mmu_hw_op_param *op_param)
 {
 	int ret;
@@ -499,15 +482,22 @@ int kbase_mmu_hw_do_unlock(struct kbase_device *kbdev, struct kbase_as *as,
 	return ret;
 }
 
-int kbase_mmu_hw_do_flush(struct kbase_device *kbdev, struct kbase_as *as,
-			  const struct kbase_mmu_hw_op_param *op_param)
+/**
+ * mmu_hw_do_flush - Flush MMU and wait for its completion.
+ *
+ * @kbdev:           Kbase device to issue the MMU operation on.
+ * @as:              Address space to issue the MMU operation on.
+ * @op_param:        Pointer to a struct containing information about the MMU operation.
+ * @hwaccess_locked: Flag to indicate if the lock has been held.
+ *
+ * Return: 0 if flushing MMU was successful, otherwise an error code.
+ */
+static int mmu_hw_do_flush(struct kbase_device *kbdev, struct kbase_as *as,
+			   const struct kbase_mmu_hw_op_param *op_param, bool hwaccess_locked)
 {
 	int ret;
 	u64 lock_addr = 0x0;
 	u32 mmu_cmd = AS_COMMAND_COMMAND_FLUSH_MEM;
-	const enum kbase_mmu_op_type flush_op = op_param->op;
-
-	lockdep_assert_held(&kbdev->hwaccess_lock);
 
 	if (WARN_ON(kbdev == NULL) || WARN_ON(as == NULL))
 		return -EINVAL;
@@ -515,12 +505,14 @@ int kbase_mmu_hw_do_flush(struct kbase_device *kbdev, struct kbase_as *as,
 	/* MMU operations can be either FLUSH_PT or FLUSH_MEM, anything else at
 	 * this point would be unexpected.
 	 */
-	if (flush_op != KBASE_MMU_OP_FLUSH_PT && flush_op != KBASE_MMU_OP_FLUSH_MEM) {
+	if (op_param->op != KBASE_MMU_OP_FLUSH_PT && op_param->op != KBASE_MMU_OP_FLUSH_MEM) {
 		dev_err(kbdev->dev, "Unexpected flush operation received");
 		return -EINVAL;
 	}
 
-	if (flush_op == KBASE_MMU_OP_FLUSH_PT)
+	lockdep_assert_held(&kbdev->mmu_hw_mutex);
+
+	if (op_param->op == KBASE_MMU_OP_FLUSH_PT)
 		mmu_cmd = AS_COMMAND_COMMAND_FLUSH_PT;
 
 	/* Lock the region that needs to be updated */
@@ -529,10 +521,19 @@ int kbase_mmu_hw_do_flush(struct kbase_device *kbdev, struct kbase_as *as,
 		return ret;
 
 #if MALI_USE_CSF && !IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI)
-	/* WA for the KBASE_HW_ISSUE_GPU2019_3901. */
-	if (kbase_hw_has_issue(kbdev, KBASE_HW_ISSUE_GPU2019_3901) &&
+	/* WA for the BASE_HW_ISSUE_GPU2019_3901. */
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_GPU2019_3901) &&
 	    mmu_cmd == AS_COMMAND_COMMAND_FLUSH_MEM) {
-		ret = apply_hw_issue_GPU2019_3901_wa(kbdev, &mmu_cmd, as->number);
+		if (!hwaccess_locked) {
+			unsigned long flags = 0;
+
+			spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+			ret = apply_hw_issue_GPU2019_3901_wa(kbdev, &mmu_cmd, as->number);
+			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+		} else {
+			ret = apply_hw_issue_GPU2019_3901_wa(kbdev, &mmu_cmd, as->number);
+		}
+
 		if (ret) {
 			dev_warn(
 				kbdev->dev,
@@ -542,6 +543,8 @@ int kbase_mmu_hw_do_flush(struct kbase_device *kbdev, struct kbase_as *as,
 			/* Continue with the MMU flush operation */
 		}
 	}
+#else
+	CSTD_UNUSED(hwaccess_locked);
 #endif
 
 	ret = write_cmd(kbdev, as->number, mmu_cmd);
@@ -550,26 +553,32 @@ int kbase_mmu_hw_do_flush(struct kbase_device *kbdev, struct kbase_as *as,
 	if (likely(!ret))
 		ret = wait_ready(kbdev, as->number);
 
-	if (likely(!ret)) {
+	if (likely(!ret))
 		mmu_command_instr(kbdev, op_param->kctx_id, mmu_cmd, lock_addr,
 				  op_param->mmu_sync_info);
-#if MALI_USE_CSF
-		if (flush_op == KBASE_MMU_OP_FLUSH_MEM &&
-		    kbdev->pm.backend.apply_hw_issue_TITANHW_2938_wa &&
-		    kbdev->pm.backend.l2_state == KBASE_L2_PEND_OFF)
-			ret = wait_l2_power_trans_complete(kbdev);
-#endif
-	}
 
 	return ret;
 }
 
+int kbase_mmu_hw_do_flush_locked(struct kbase_device *kbdev, struct kbase_as *as,
+				 const struct kbase_mmu_hw_op_param *op_param)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	return mmu_hw_do_flush(kbdev, as, op_param, true);
+}
+
+int kbase_mmu_hw_do_flush(struct kbase_device *kbdev, struct kbase_as *as,
+			  const struct kbase_mmu_hw_op_param *op_param)
+{
+	return mmu_hw_do_flush(kbdev, as, op_param, false);
+}
+
 int kbase_mmu_hw_do_flush_on_gpu_ctrl(struct kbase_device *kbdev, struct kbase_as *as,
 				      const struct kbase_mmu_hw_op_param *op_param)
 {
 	int ret, ret2;
 	u32 gpu_cmd = GPU_COMMAND_CACHE_CLN_INV_L2_LSC;
-	const enum kbase_mmu_op_type flush_op = op_param->op;
 
 	if (WARN_ON(kbdev == NULL) || WARN_ON(as == NULL))
 		return -EINVAL;
@@ -577,14 +586,15 @@ int kbase_mmu_hw_do_flush_on_gpu_ctrl(struct kbase_device *kbdev, struct kbase_a
 	/* MMU operations can be either FLUSH_PT or FLUSH_MEM, anything else at
 	 * this point would be unexpected.
 	 */
-	if (flush_op != KBASE_MMU_OP_FLUSH_PT && flush_op != KBASE_MMU_OP_FLUSH_MEM) {
+	if (op_param->op != KBASE_MMU_OP_FLUSH_PT && op_param->op != KBASE_MMU_OP_FLUSH_MEM) {
 		dev_err(kbdev->dev, "Unexpected flush operation received");
 		return -EINVAL;
 	}
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
+	lockdep_assert_held(&kbdev->mmu_hw_mutex);
 
-	if (flush_op == KBASE_MMU_OP_FLUSH_PT)
+	if (op_param->op == KBASE_MMU_OP_FLUSH_PT)
 		gpu_cmd = GPU_COMMAND_CACHE_CLN_INV_L2;
 
 	/* 1. Issue MMU_AS_CONTROL.COMMAND.LOCK operation. */
@@ -598,15 +608,6 @@ int kbase_mmu_hw_do_flush_on_gpu_ctrl(struct kbase_device *kbdev, struct kbase_a
 	/* 3. Issue MMU_AS_CONTROL.COMMAND.UNLOCK operation. */
 	ret2 = kbase_mmu_hw_do_unlock_no_addr(kbdev, as, op_param);
 
-#if MALI_USE_CSF
-	if (!ret && !ret2) {
-		if (flush_op == KBASE_MMU_OP_FLUSH_MEM &&
-		    kbdev->pm.backend.apply_hw_issue_TITANHW_2938_wa &&
-		    kbdev->pm.backend.l2_state == KBASE_L2_PEND_OFF)
-			ret = wait_l2_power_trans_complete(kbdev);
-	}
-#endif
-
 	return ret ?: ret2;
 }
 
@@ -635,15 +636,6 @@ void kbase_mmu_hw_clear_fault(struct kbase_device *kbdev, struct kbase_as *as,
 #endif
 	kbase_reg_write32(kbdev, MMU_CONTROL_ENUM(IRQ_CLEAR), pf_bf_mask);
 
-#if MALI_USE_CSF
-	/* For valid page faults, this function is called just before unblocking the MMU (which
-	 * would in turn unblock the MCU firmware) and so this is an opportune location to
-	 * update the page fault counter value in firmware visible memory.
-	 */
-	if (likely(type == KBASE_MMU_FAULT_TYPE_PAGE) && kbdev->csf.page_fault_cnt_ptr)
-		*kbdev->csf.page_fault_cnt_ptr = ++kbdev->csf.page_fault_cnt;
-#endif
-
 unlock:
 	spin_unlock_irqrestore(&kbdev->mmu_mask_change, flags);
 }
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_internal.h b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_internal.h
index 8b68791e4c77..4c2c1a64ca41 100644
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_internal.h
+++ b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_internal.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -38,6 +38,19 @@ void kbase_gpu_report_bus_fault_and_kill(struct kbase_context *kctx, struct kbas
 void kbase_mmu_report_fault_and_kill(struct kbase_context *kctx, struct kbase_as *as,
 				     const char *reason_str, struct kbase_fault *fault);
 
+/**
+ * kbase_mmu_switch_to_ir() - Switch to incremental rendering if possible
+ * @kctx:	kbase_context for the faulting address space.
+ * @reg:	of a growable GPU memory region in the same context.
+ *		Takes ownership of the reference if successful.
+ *
+ * Used to switch to incremental rendering if we have nearly run out of
+ * virtual address space in a growable memory region.
+ *
+ * Return: 0 if successful, otherwise a negative error code.
+ */
+int kbase_mmu_switch_to_ir(struct kbase_context *kctx, struct kbase_va_region *reg);
+
 /**
  * kbase_mmu_page_fault_worker() - Process a page fault.
  *
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_mode_aarch64.c b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_mode_aarch64.c
index 7aace473011f..3e0fab6e64f8 100644
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_mode_aarch64.c
+++ b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_mode_aarch64.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -32,7 +32,7 @@
  */
 #define ENTRY_IS_ATE_L3 3ULL
 #define ENTRY_IS_ATE_L02 1ULL
-#define ENTRY_IS_INVAL 0ULL
+#define ENTRY_IS_INVAL 2ULL
 #define ENTRY_IS_PTE 3ULL
 
 #define ENTRY_ACCESS_RW (1ULL << 6) /* bits 6:7 */
@@ -74,7 +74,7 @@ static void mmu_disable_as(struct kbase_device *kbdev, int as_nr)
 	struct kbase_mmu_setup *const current_setup = &as->current_setup;
 
 	current_setup->transtab = 0ULL;
-	current_setup->transcfg = AS_TRANSCFG_MODE_SET(0ULL, AS_TRANSCFG_MODE_UNMAPPED);
+	current_setup->transcfg = AS_TRANSCFG_MODE_SET(0, AS_TRANSCFG_MODE_UNMAPPED);
 
 	/* Apply the address space setting */
 	kbase_mmu_hw_configure(kbdev, as);
@@ -86,7 +86,7 @@ static phys_addr_t pte_to_phy_addr(u64 entry)
 		return 0;
 
 	entry &= ~VALID_ENTRY_MASK;
-	return entry & ~0xFFFULL;
+	return entry & ~0xFFF;
 }
 
 static int ate_is_valid(u64 ate, int const level)
@@ -179,7 +179,7 @@ static void set_num_valid_entries(u64 *pgd, unsigned int num_of_valid_entries)
 
 static void entry_set_pte(u64 *entry, phys_addr_t phy)
 {
-	page_table_entry_set(entry, (phy & GPU_PAGE_MASK) | ENTRY_ACCESS_BIT | ENTRY_IS_PTE);
+	page_table_entry_set(entry, (phy & PAGE_MASK) | ENTRY_ACCESS_BIT | ENTRY_IS_PTE);
 }
 
 static void entries_invalidate(u64 *entry, u32 count)
diff --git a/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_runtime_pm.c b/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_runtime_pm.c
index d0342af60fb3..2a5030745586 100644
--- a/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_runtime_pm.c
+++ b/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_runtime_pm.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2015-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2015-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/gpu/arm/bifrost/platform/meson/mali_kbase_runtime_pm.c b/drivers/gpu/arm/bifrost/platform/meson/mali_kbase_runtime_pm.c
index bd3b4b5e2aa3..45f7638ad904 100644
--- a/drivers/gpu/arm/bifrost/platform/meson/mali_kbase_runtime_pm.c
+++ b/drivers/gpu/arm/bifrost/platform/meson/mali_kbase_runtime_pm.c
@@ -49,7 +49,7 @@ static int resets_init(struct kbase_device *kbdev)
 		return nr_resets;
 	}
 
-	resets = devm_kcalloc(kbdev->dev, (size_t)nr_resets, sizeof(*resets), GFP_KERNEL);
+	resets = devm_kcalloc(kbdev->dev, nr_resets, sizeof(*resets), GFP_KERNEL);
 	if (!resets)
 		return -ENOMEM;
 
diff --git a/drivers/gpu/arm/bifrost/platform/rk/custom_log.h b/drivers/gpu/arm/bifrost/platform/rk/custom_log.h
index fedfd41bb511..5de70ee13d25 100755
--- a/drivers/gpu/arm/bifrost/platform/rk/custom_log.h
+++ b/drivers/gpu/arm/bifrost/platform/rk/custom_log.h
@@ -1,5 +1,5 @@
 /*
- * (C) COPYRIGHT Rockchip Electronics Co., Ltd. All rights reserved.
+ * (C) COPYRIGHT RockChip Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/gpu/arm/bifrost/platform/rk/mali_kbase_config_rk.c b/drivers/gpu/arm/bifrost/platform/rk/mali_kbase_config_rk.c
index d20de78ce478..3ac4aef79719 100755
--- a/drivers/gpu/arm/bifrost/platform/rk/mali_kbase_config_rk.c
+++ b/drivers/gpu/arm/bifrost/platform/rk/mali_kbase_config_rk.c
@@ -1,5 +1,5 @@
 /*
- * (C) COPYRIGHT Rockchip Electronics Co., Ltd. All rights reserved.
+ * (C) COPYRIGHT RockChip Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -201,7 +201,7 @@ struct kbase_platform_funcs_conf platform_funcs = {
 
 /*---------------------------------------------------------------------------*/
 
-static __maybe_unused int rk_pm_callback_runtime_on(struct kbase_device *kbdev)
+static int rk_pm_callback_runtime_on(struct kbase_device *kbdev)
 {
 	struct rockchip_opp_info *opp_info = &kbdev->opp_info;
 	int ret = 0;
@@ -226,7 +226,7 @@ static __maybe_unused int rk_pm_callback_runtime_on(struct kbase_device *kbdev)
 	return 0;
 }
 
-static __maybe_unused void rk_pm_callback_runtime_off(struct kbase_device *kbdev)
+static void rk_pm_callback_runtime_off(struct kbase_device *kbdev)
 {
 	struct rockchip_opp_info *opp_info = &kbdev->opp_info;
 
@@ -310,22 +310,29 @@ static void rk_pm_callback_power_off(struct kbase_device *kbdev)
 			   msecs_to_jiffies(platform->delay_ms));
 }
 
-static __maybe_unused int rk_kbase_device_runtime_init(struct kbase_device *kbdev)
+static int rk_kbase_device_runtime_init(struct kbase_device *kbdev)
 {
 	return 0;
 }
 
-static __maybe_unused void rk_kbase_device_runtime_disable(struct kbase_device *kbdev)
+static void rk_kbase_device_runtime_disable(struct kbase_device *kbdev)
 {
 }
 
 struct kbase_pm_callback_conf pm_callbacks = {
 	.power_on_callback = rk_pm_callback_power_on,
 	.power_off_callback = rk_pm_callback_power_off,
-	.power_runtime_init_callback = pm_ptr(rk_kbase_device_runtime_init),
-	.power_runtime_term_callback = pm_ptr(rk_kbase_device_runtime_disable),
-	.power_runtime_on_callback = pm_ptr(rk_pm_callback_runtime_on),
-	.power_runtime_off_callback = pm_ptr(rk_pm_callback_runtime_off),
+#ifdef CONFIG_PM
+	.power_runtime_init_callback = rk_kbase_device_runtime_init,
+	.power_runtime_term_callback = rk_kbase_device_runtime_disable,
+	.power_runtime_on_callback = rk_pm_callback_runtime_on,
+	.power_runtime_off_callback = rk_pm_callback_runtime_off,
+#else				/* CONFIG_PM */
+	.power_runtime_init_callback = NULL,
+	.power_runtime_term_callback = NULL,
+	.power_runtime_on_callback = NULL,
+	.power_runtime_off_callback = NULL,
+#endif				/* CONFIG_PM */
 };
 
 /*---------------------------------------------------------------------------*/
@@ -508,25 +515,6 @@ static void kbase_platform_rk_remove_sysfs_files(struct device *dev)
 	device_remove_file(dev, &dev_attr_utilisation);
 }
 
-static int rk3576_gpu_set_read_margin(struct device *dev,
-				      struct rockchip_opp_info *opp_info,
-				      u32 rm)
-{
-	if (!opp_info->grf || !opp_info->volt_rm_tbl)
-		return 0;
-	if (rm == opp_info->current_rm || rm == UINT_MAX)
-		return 0;
-
-	dev_dbg(dev, "set rm to %d\n", rm);
-	regmap_write(opp_info->grf, 0x3c, 0x001c0000 | (rm << 2));
-	regmap_write(opp_info->grf, 0x40, 0x001c0000 | (rm << 2));
-	regmap_write(opp_info->grf, 0x48, 0x001c0000 | (rm << 2));
-
-	opp_info->current_rm = rm;
-
-	return 0;
-}
-
 static int rk3588_gpu_get_soc_info(struct device *dev, struct device_node *np,
 			       int *bin, int *process)
 {
@@ -638,13 +626,6 @@ static int gpu_opp_config_clks(struct device *dev, struct opp_table *opp_table,
 					&kbdev->opp_info);
 }
 
-static const struct rockchip_opp_data rk3576_gpu_opp_data = {
-	.set_read_margin = rk3576_gpu_set_read_margin,
-	.set_soc_info = rockchip_opp_set_low_length,
-	.config_regulators = gpu_opp_config_regulators,
-	.config_clks = gpu_opp_config_clks,
-};
-
 static const struct rockchip_opp_data rk3588_gpu_opp_data = {
 	.get_soc_info = rk3588_gpu_get_soc_info,
 	.set_soc_info = rk3588_gpu_set_soc_info,
@@ -658,10 +639,6 @@ static const struct rockchip_opp_data rockchip_gpu_opp_data = {
 };
 
 static const struct of_device_id rockchip_mali_of_match[] = {
-	{
-		.compatible = "rockchip,rk3576",
-		.data = (void *)&rk3576_gpu_opp_data,
-	},
 	{
 		.compatible = "rockchip,rk3588",
 		.data = (void *)&rk3588_gpu_opp_data,
diff --git a/drivers/gpu/arm/bifrost/tests/Kbuild b/drivers/gpu/arm/bifrost/tests/Kbuild
index 479b91532ed7..72ca70ac8779 100644
--- a/drivers/gpu/arm/bifrost/tests/Kbuild
+++ b/drivers/gpu/arm/bifrost/tests/Kbuild
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2017-2024 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2017-2023 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/gpu/arm/bifrost/tests/Kconfig b/drivers/gpu/arm/bifrost/tests/Kconfig
index 88a4194c5cd7..aa011bac8990 100644
--- a/drivers/gpu/arm/bifrost/tests/Kconfig
+++ b/drivers/gpu/arm/bifrost/tests/Kconfig
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2017, 2020-2024 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2017, 2020-2023 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/gpu/arm/bifrost/tests/kutf/Kbuild b/drivers/gpu/arm/bifrost/tests/kutf/Kbuild
index aef44e05d0ea..c4790bc66c23 100644
--- a/drivers/gpu/arm/bifrost/tests/kutf/Kbuild
+++ b/drivers/gpu/arm/bifrost/tests/kutf/Kbuild
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 #
-# (C) COPYRIGHT 2017, 2020-2023 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2017, 2020-2021 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -27,6 +27,5 @@ kutf-y := \
     kutf_suite.o \
     kutf_utils.o \
     kutf_helpers.o \
-    kutf_helpers_user.o \
-    kutf_kprobe.o
+    kutf_helpers_user.o
 endif
diff --git a/drivers/gpu/arm/bifrost/tests/kutf/build.bp b/drivers/gpu/arm/bifrost/tests/kutf/build.bp
index 267c14bb3a8b..89edae9c5e6f 100644
--- a/drivers/gpu/arm/bifrost/tests/kutf/build.bp
+++ b/drivers/gpu/arm/bifrost/tests/kutf/build.bp
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2018-2023 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2021 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -34,7 +34,6 @@ bob_kernel_module {
         "kutf_resultset.c",
         "kutf_suite.c",
         "kutf_utils.c",
-        "kutf_kprobe.c",
     ],
     enabled: false,
     mali_kutf: {
diff --git a/drivers/gpu/arm/bifrost/tests/kutf/kutf_helpers_user.c b/drivers/gpu/arm/bifrost/tests/kutf/kutf_helpers_user.c
index a69c791d97b0..8654fd503960 100644
--- a/drivers/gpu/arm/bifrost/tests/kutf/kutf_helpers_user.c
+++ b/drivers/gpu/arm/bifrost/tests/kutf/kutf_helpers_user.c
@@ -56,7 +56,7 @@ static const char *get_val_type_name(enum kutf_helper_valtype valtype)
  * - Has between 1 and KUTF_HELPER_MAX_VAL_NAME_LEN characters before the \0 terminator
  * - And, each char is in the character set [A-Z0-9_]
  */
-static int validate_val_name(const char *val_str, size_t str_len)
+static int validate_val_name(const char *val_str, int str_len)
 {
 	int i = 0;
 
@@ -92,16 +92,16 @@ static int validate_val_name(const char *val_str, size_t str_len)
  * That is, before any '\\', '\n' or '"' characters. This is so we don't have
  * to escape the string
  */
-static size_t find_quoted_string_valid_len(const char *str)
+static int find_quoted_string_valid_len(const char *str)
 {
 	char *ptr;
 	const char *check_chars = "\\\n\"";
 
 	ptr = strpbrk(str, check_chars);
 	if (ptr)
-		return (size_t)(ptr - str);
+		return (int)(ptr - str);
 
-	return strlen(str);
+	return (int)strlen(str);
 }
 
 static int kutf_helper_userdata_enqueue(struct kutf_context *context, const char *str)
@@ -116,7 +116,7 @@ static int kutf_helper_userdata_enqueue(struct kutf_context *context, const char
 	if (!str_copy)
 		return -ENOMEM;
 
-	strscpy(str_copy, str, len);
+	strcpy(str_copy, str);
 
 	err = kutf_add_result(context, KUTF_RESULT_USERDATA, str_copy);
 
@@ -185,14 +185,14 @@ EXPORT_SYMBOL(kutf_helper_max_str_len_for_kern);
 int kutf_helper_send_named_str(struct kutf_context *context, const char *val_name,
 			       const char *val_str)
 {
-	size_t val_str_len;
-	size_t str_buf_sz;
+	int val_str_len;
+	int str_buf_sz;
 	char *str_buf = NULL;
 	int ret = 1;
 	char *copy_ptr;
-	size_t val_name_len;
-	size_t start_delim_len = strlen(NAMED_STR_START_DELIM);
-	size_t end_delim_len = strlen(NAMED_STR_END_DELIM);
+	int val_name_len;
+	int start_delim_len = strlen(NAMED_STR_START_DELIM);
+	int end_delim_len = strlen(NAMED_STR_END_DELIM);
 	const char *errmsg = NULL;
 
 	if (validate_val_name(val_name, KUTF_HELPER_MAX_VAL_NAME_LEN + 1)) {
@@ -215,7 +215,7 @@ int kutf_helper_send_named_str(struct kutf_context *context, const char *val_nam
 	if (!str_buf) {
 		errmsg = kutf_dsprintf(
 			&context->fixture_pool,
-			"Failed to send str value named '%s': kmalloc failed, str_buf_sz=%zu",
+			"Failed to send str value named '%s': kmalloc failed, str_buf_sz=%d",
 			val_name, str_buf_sz);
 		goto out_err;
 	}
@@ -270,8 +270,8 @@ int kutf_helper_receive_named_val(struct kutf_context *context,
 	char *recv_str;
 	char *search_ptr;
 	char *name_str = NULL;
-	size_t name_len;
-	size_t strval_len;
+	int name_len;
+	int strval_len;
 	enum kutf_helper_valtype type = KUTF_HELPER_VALTYPE_INVALID;
 	char *strval = NULL;
 	u64 u64val = 0;
@@ -286,7 +286,7 @@ int kutf_helper_receive_named_val(struct kutf_context *context,
 	/* Find the '=', grab the name and validate it */
 	search_ptr = strnchr(recv_str, recv_sz, NAMED_VALUE_SEP[0]);
 	if (search_ptr) {
-		name_len = (size_t)(search_ptr - recv_str);
+		name_len = search_ptr - recv_str;
 		if (!validate_val_name(recv_str, name_len)) {
 			/* no need to reallocate - just modify string in place */
 			name_str = recv_str;
@@ -311,7 +311,7 @@ int kutf_helper_receive_named_val(struct kutf_context *context,
 		/* Find end of string */
 		search_ptr = strnchr(recv_str, recv_sz, NAMED_STR_END_DELIM[0]);
 		if (search_ptr) {
-			strval_len = (size_t)(search_ptr - recv_str);
+			strval_len = search_ptr - recv_str;
 			/* Validate the string to ensure it contains no quotes */
 			if (strval_len == find_quoted_string_valid_len(recv_str)) {
 				/* no need to reallocate - just modify string in place */
@@ -339,7 +339,7 @@ int kutf_helper_receive_named_val(struct kutf_context *context,
 		 * reads characters after the number it'll report -EINVAL
 		 */
 		if (!err) {
-			size_t len_remain = strnlen(recv_str, recv_sz);
+			int len_remain = strnlen(recv_str, recv_sz);
 
 			type = KUTF_HELPER_VALTYPE_U64;
 			recv_str += len_remain;
diff --git a/drivers/gpu/arm/bifrost/tests/kutf/kutf_kprobe.c b/drivers/gpu/arm/bifrost/tests/kutf/kutf_kprobe.c
deleted file mode 100644
index 232809e1ed58..000000000000
--- a/drivers/gpu/arm/bifrost/tests/kutf/kutf_kprobe.c
+++ /dev/null
@@ -1,354 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
-/*
- *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-/* Kernel probe functionality.
- */
-
-#ifdef CONFIG_KPROBES
-
-#include <linux/list.h>
-#include <linux/slab.h>
-#include <linux/debugfs.h>
-#include <linux/kprobes.h>
-#include <linux/version.h>
-#include <linux/delay.h>
-#include <kutf/kutf_kprobe.h>
-
-#define KUTF_KP_REG_MIN_ARGS 3
-#define KUTF_KP_UNREG_MIN_ARGS 2
-#define KUTF_KP_FUNC_NAME_ARG 0
-#define KUTF_KP_FUNC_ENTRY_EXIT_ARG 1
-#define KUTF_KP_FUNC_HANDLER_ARG 2
-
-#define KUTF_KP_WRITE_BUFSIZE 4096
-
-/* Stores address to kernel function 'kallsyms_lookup_name'
- * as 'kallsyms_lookup_name' is no longer exported from 5.7 kernel.
- */
-typedef unsigned long (*kallsyms_lookup_name_t)(const char *name);
-kallsyms_lookup_name_t kutf_ksym_lookup_name;
-
-static ssize_t kutf_kp_reg_debugfs_write(struct file *file, const char __user *user_buf,
-					 size_t count, loff_t *ppos);
-
-static ssize_t kutf_kp_unreg_debugfs_write(struct file *file, const char __user *user_buf,
-					   size_t count, loff_t *ppos);
-
-static LIST_HEAD(kp_list);
-static DEFINE_MUTEX(kp_list_lock);
-
-/**
- * struct kutf_kp_data - Structure which holds data per kprobe instance
- * @kretp:      reference to kernel ret probe
- * @entry:      true if this probe is for function entry.Otherwise it is exit.
- * @argc:       Number of arguments to be passed to probe handler
- * @argv:       arguments passed to probe handler
- * @kp_handler:	Actual handler which is called when probe is triggered
- * @list:       node for adding to kp_list
- */
-struct kutf_kp_data {
-	struct kretprobe kretp;
-	bool entry;
-	int argc;
-	char **argv;
-	kutf_kp_handler kp_handler;
-	struct list_head list;
-};
-
-const struct file_operations kutf_kp_reg_debugfs_fops = {
-	.owner = THIS_MODULE,
-	.open = simple_open,
-	.write = kutf_kp_reg_debugfs_write,
-};
-
-const struct file_operations kutf_kp_unreg_debugfs_fops = {
-	.owner = THIS_MODULE,
-	.open = simple_open,
-	.write = kutf_kp_unreg_debugfs_write,
-};
-
-struct kprobe kutf_kallsym_kp = { .symbol_name = "kallsyms_lookup_name" };
-
-void kutf_kp_delay_handler(int argc, char **argv)
-{
-	long delay;
-
-	if ((!argv) || (!argv[0]))
-		return;
-
-	if (kstrtol(argv[0], 0, &delay))
-		return;
-
-	mdelay(delay);
-}
-
-void kutf_kp_sample_kernel_function(void)
-{
-	pr_debug("%s called\n", __func__);
-}
-EXPORT_SYMBOL(kutf_kp_sample_kernel_function);
-
-void kutf_kp_sample_handler(int argc, char **argv)
-{
-	int i = 0;
-
-	for (; i < argc; i++)
-		pr_info("%s %s\n", __func__, argv[i]);
-}
-EXPORT_SYMBOL(kutf_kp_sample_handler);
-
-static int kutf_call_kp_handler(struct kretprobe *p)
-{
-	struct kutf_kp_data *kp_p;
-	kutf_kp_handler kp_handler;
-
-	kp_p = (struct kutf_kp_data *)p;
-	kp_handler = kp_p->kp_handler;
-
-	if (kp_handler) {
-		/* Arguments to registered handler starts after
-		 * KUTF_KP_REG_MIN_ARGS.
-		 */
-		(*kp_handler)((kp_p->argc) - KUTF_KP_REG_MIN_ARGS,
-			      &(kp_p->argv[KUTF_KP_REG_MIN_ARGS]));
-	}
-	return 0;
-}
-
-static int kutf_kretp_handler(struct kretprobe_instance *ri, struct pt_regs *regs)
-{
-#if (KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE)
-	return kutf_call_kp_handler(ri->rph->rp);
-#else
-	return kutf_call_kp_handler(ri->rp);
-#endif
-}
-
-static kutf_kp_handler kutf_get_kp_handler(char **argv)
-{
-	if ((NULL == argv) || (NULL == kutf_ksym_lookup_name))
-		return NULL;
-
-	return (kutf_kp_handler)((*kutf_ksym_lookup_name)(argv[KUTF_KP_FUNC_HANDLER_ARG]));
-}
-
-static ssize_t kutf_kp_reg_debugfs_write(struct file *file, const char __user *user_buf,
-					 size_t count, loff_t *ppos)
-{
-	int argc = 0;
-	int ret = count;
-	char **argv;
-	char *kbuf;
-	char *func_name;
-	struct kutf_kp_data *kp_p;
-	struct kutf_kp_data *kp_iter;
-
-	if (count >= KUTF_KP_WRITE_BUFSIZE)
-		return -EINVAL;
-
-	kbuf = memdup_user_nul(user_buf, count);
-	if (IS_ERR(kbuf))
-		return -ENOMEM;
-
-	argv = argv_split(GFP_KERNEL, kbuf, &argc);
-	if (!argv) {
-		ret = -ENOMEM;
-		goto out;
-	}
-	if (argc < KUTF_KP_REG_MIN_ARGS) {
-		pr_debug("Insufficient args in reg kprobe:%d\n", argc);
-		ret = -EINVAL;
-		goto argv_out;
-	}
-
-	kp_p = kzalloc(sizeof(struct kutf_kp_data), GFP_KERNEL);
-	if (!kp_p)
-		goto argv_out;
-
-	if (!(strcmp(argv[KUTF_KP_FUNC_ENTRY_EXIT_ARG], "entry"))) {
-		kp_p->kretp.entry_handler = kutf_kretp_handler;
-		kp_p->entry = 1;
-	} else if (!(strcmp(argv[KUTF_KP_FUNC_ENTRY_EXIT_ARG], "exit"))) {
-		kp_p->kretp.handler = kutf_kretp_handler;
-	} else {
-		pr_err("Invalid arg:%s passed in reg kprobe\n", argv[KUTF_KP_FUNC_ENTRY_EXIT_ARG]);
-		ret = -EINVAL;
-		kfree(kp_p);
-		goto argv_out;
-	}
-
-	func_name = argv[KUTF_KP_FUNC_NAME_ARG];
-
-	mutex_lock(&kp_list_lock);
-	list_for_each_entry(kp_iter, &kp_list, list) {
-		if ((kp_iter->entry == kp_p->entry) &&
-		    (!(strcmp(kp_iter->kretp.kp.symbol_name, func_name)))) {
-			ret = -EEXIST;
-			kfree(kp_p);
-			mutex_unlock(&kp_list_lock);
-			goto argv_out;
-		}
-	}
-
-	kp_p->kretp.kp.symbol_name = func_name;
-	kp_p->kp_handler = kutf_get_kp_handler(argv);
-	if (!(kp_p->kp_handler)) {
-		pr_debug("cannot find addr for handler:%s\n", argv[KUTF_KP_FUNC_HANDLER_ARG]);
-		ret = -EINVAL;
-		kfree(kp_p);
-		mutex_unlock(&kp_list_lock);
-		goto argv_out;
-	}
-	kp_p->argc = argc;
-	kp_p->argv = argv;
-	ret = register_kretprobe(&kp_p->kretp);
-	if (ret) {
-		ret = -EINVAL;
-		kfree(kp_p);
-		mutex_unlock(&kp_list_lock);
-		goto argv_out;
-	}
-	INIT_LIST_HEAD(&kp_p->list);
-	list_add(&kp_p->list, &kp_list);
-
-	mutex_unlock(&kp_list_lock);
-
-	ret = count;
-	goto out;
-
-argv_out:
-	argv_free(argv);
-
-out:
-	kfree(kbuf);
-
-	return ret;
-}
-
-static ssize_t kutf_kp_unreg_debugfs_write(struct file *file, const char __user *user_buf,
-					   size_t count, loff_t *ppos)
-{
-	int argc = 0;
-	int ret = -EINVAL;
-	char **argv = NULL;
-	char *kbuf;
-	char *func_name;
-	struct kutf_kp_data *kp_iter;
-	bool entry;
-
-	if (count >= KUTF_KP_WRITE_BUFSIZE)
-		return -EINVAL;
-
-	kbuf = memdup_user_nul(user_buf, count);
-	if (IS_ERR(kbuf))
-		return -ENOMEM;
-
-	argv = argv_split(GFP_KERNEL, kbuf, &argc);
-	if (!argv) {
-		ret = -ENOMEM;
-		goto out;
-	}
-	if (argc < KUTF_KP_UNREG_MIN_ARGS) {
-		pr_debug("Insufficient args in unreg kprobe:%d\n", argc);
-		ret = -EINVAL;
-		goto out;
-	}
-	if (!(strcmp(argv[KUTF_KP_FUNC_ENTRY_EXIT_ARG], "entry")))
-		entry = 1;
-	else if (!(strcmp(argv[KUTF_KP_FUNC_ENTRY_EXIT_ARG], "exit")))
-		entry = 0;
-	else {
-		pr_err("Invalid arg:%s passed in unreg kprobe\n",
-		       argv[KUTF_KP_FUNC_ENTRY_EXIT_ARG]);
-		ret = -EINVAL;
-		goto out;
-	}
-	func_name = argv[KUTF_KP_FUNC_NAME_ARG];
-
-	mutex_lock(&kp_list_lock);
-	list_for_each_entry(kp_iter, &kp_list, list) {
-		if ((kp_iter->entry == entry) &&
-		    (!(strcmp(func_name, kp_iter->kretp.kp.symbol_name)))) {
-			unregister_kretprobe(&kp_iter->kretp);
-			argv_free(kp_iter->argv);
-			list_del(&kp_iter->list);
-			kfree(kp_iter);
-			ret = count;
-			break;
-		}
-	}
-	mutex_unlock(&kp_list_lock);
-out:
-	argv_free(argv);
-	kfree(kbuf);
-
-	return ret;
-}
-
-int __init kutf_kprobe_init(struct dentry *base_dir)
-{
-	struct dentry *kutf_kp_reg_debugfs_file;
-	struct dentry *kutf_kp_unreg_debugfs_file;
-
-	if (!(register_kprobe(&kutf_kallsym_kp))) {
-		/* After kernel 5.7, 'kallsyms_lookup_name' is no longer
-		 * exported. So we need this workaround to get the
-		 * addr of 'kallsyms_lookup_name'. This will be used later
-		 * in kprobe handler function to call the registered
-		 * handler for a probe from the name passed from userspace.
-		 */
-		kutf_ksym_lookup_name = (kallsyms_lookup_name_t)kutf_kallsym_kp.addr;
-		unregister_kprobe(&kutf_kallsym_kp);
-		kutf_kp_reg_debugfs_file = debugfs_create_file("register_kprobe", 0200, base_dir,
-							       NULL, &kutf_kp_reg_debugfs_fops);
-		if (IS_ERR_OR_NULL(kutf_kp_reg_debugfs_file))
-			pr_err("Failed to create kprobe reg debugfs file");
-
-		kutf_kp_unreg_debugfs_file = debugfs_create_file(
-			"unregister_kprobe", 0200, base_dir, NULL, &kutf_kp_unreg_debugfs_fops);
-		if (IS_ERR_OR_NULL(kutf_kp_unreg_debugfs_file)) {
-			pr_err("Failed to create kprobe unreg debugfs file");
-			debugfs_remove(kutf_kp_reg_debugfs_file);
-		}
-	} else
-		pr_info("kallsyms_lookup_name addr not available\n");
-
-	return 0;
-}
-
-void kutf_kprobe_exit(void)
-{
-	struct kutf_kp_data *kp_iter;
-	struct kutf_kp_data *n;
-
-	mutex_lock(&kp_list_lock);
-
-	list_for_each_entry_safe(kp_iter, n, &kp_list, list) {
-		unregister_kretprobe(&kp_iter->kretp);
-		argv_free(kp_iter->argv);
-		list_del(&kp_iter->list);
-		kfree(kp_iter);
-	}
-
-	mutex_unlock(&kp_list_lock);
-}
-
-#endif
diff --git a/drivers/gpu/arm/bifrost/tests/kutf/kutf_suite.c b/drivers/gpu/arm/bifrost/tests/kutf/kutf_suite.c
index dcd4cb3f36fc..9e57c10befdf 100644
--- a/drivers/gpu/arm/bifrost/tests/kutf/kutf_suite.c
+++ b/drivers/gpu/arm/bifrost/tests/kutf/kutf_suite.c
@@ -33,15 +33,13 @@
 #include <linux/version.h>
 #include <linux/atomic.h>
 #include <linux/sched.h>
+
 #include <generated/autoconf.h>
 
 #include <kutf/kutf_suite.h>
 #include <kutf/kutf_resultset.h>
 #include <kutf/kutf_utils.h>
 #include <kutf/kutf_helpers.h>
-#ifdef CONFIG_KPROBES
-#include <kutf/kutf_kprobe.h>
-#endif
 
 /**
  * struct kutf_application - Structure which represents kutf application
@@ -403,7 +401,7 @@ static ssize_t kutf_debugfs_run_read(struct file *file, char __user *buf, size_t
 		if (bytes_not_copied != 0)
 			return -EFAULT;
 		test_context->userdata.flags |= KUTF_USERDATA_WARNING_OUTPUT;
-		return (ssize_t)message_len;
+		return message_len;
 	case KUTF_RESULT_USERDATA:
 		message_len = strlen(res->message);
 		if (message_len > len - 1) {
@@ -421,7 +419,7 @@ static ssize_t kutf_debugfs_run_read(struct file *file, char __user *buf, size_t
 			pr_warn("Failed to copy data to user space buffer\n");
 			return -EFAULT;
 		}
-		return (ssize_t)message_len + 1;
+		return message_len + 1;
 	default:
 		/* Fall through - this is a test result */
 		break;
@@ -443,28 +441,28 @@ static ssize_t kutf_debugfs_run_read(struct file *file, char __user *buf, size_t
 	/* First copy the result string */
 	if (kutf_str_ptr) {
 		bytes_not_copied = copy_to_user(&buf[0], kutf_str_ptr, kutf_str_len);
-		bytes_copied += (ssize_t)(kutf_str_len - bytes_not_copied);
+		bytes_copied += kutf_str_len - bytes_not_copied;
 		if (bytes_not_copied)
 			goto exit;
 	}
 
 	/* Then the separator */
 	bytes_not_copied = copy_to_user(&buf[bytes_copied], &separator, 1);
-	bytes_copied += (ssize_t)(1 - bytes_not_copied);
+	bytes_copied += 1 - bytes_not_copied;
 	if (bytes_not_copied)
 		goto exit;
 
 	/* Finally Next copy the result string */
 	if (res->message) {
 		bytes_not_copied = copy_to_user(&buf[bytes_copied], res->message, message_len);
-		bytes_copied += (ssize_t)(message_len - bytes_not_copied);
+		bytes_copied += message_len - bytes_not_copied;
 		if (bytes_not_copied)
 			goto exit;
 	}
 
 	/* Finally the terminator */
 	bytes_not_copied = copy_to_user(&buf[bytes_copied], &terminator, 1);
-	bytes_copied += (ssize_t)(1 - bytes_not_copied);
+	bytes_copied += 1 - bytes_not_copied;
 
 exit:
 	return bytes_copied;
@@ -497,7 +495,7 @@ static ssize_t kutf_debugfs_run_write(struct file *file, const char __user *buf,
 	if (ret < 0)
 		return ret;
 
-	return (ssize_t)len;
+	return len;
 }
 
 /**
@@ -1138,10 +1136,6 @@ static int __init init_kutf_core(void)
 		return -ENOMEM;
 	}
 
-#ifdef CONFIG_KPROBES
-	kutf_kprobe_init(base_dir);
-#endif
-
 	return 0;
 }
 
@@ -1152,9 +1146,6 @@ static int __init init_kutf_core(void)
  */
 static void __exit exit_kutf_core(void)
 {
-#ifdef CONFIG_KPROBES
-	kutf_kprobe_exit();
-#endif
 	debugfs_remove_recursive(base_dir);
 
 	if (kutf_workq)
diff --git a/drivers/gpu/arm/bifrost/tests/kutf/kutf_utils.c b/drivers/gpu/arm/bifrost/tests/kutf/kutf_utils.c
index 183ab368ecbe..6454a20b8ae9 100644
--- a/drivers/gpu/arm/bifrost/tests/kutf/kutf_utils.c
+++ b/drivers/gpu/arm/bifrost/tests/kutf/kutf_utils.c
@@ -37,7 +37,7 @@ const char *kutf_dsprintf(struct kutf_mempool *pool, const char *fmt, ...)
 {
 	va_list args;
 	int len;
-	size_t size;
+	int size;
 	void *buffer;
 
 	mutex_lock(&buffer_lock);
@@ -54,7 +54,7 @@ const char *kutf_dsprintf(struct kutf_mempool *pool, const char *fmt, ...)
 		pr_warn("%s: Truncated dsprintf message %s\n", __func__, fmt);
 		size = sizeof(tmp_buffer);
 	} else {
-		size = (size_t)(len + 1);
+		size = len + 1;
 	}
 
 	buffer = kutf_mempool_alloc(pool, size);
diff --git a/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/mali_kutf_clk_rate_trace_test.c b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/mali_kutf_clk_rate_trace_test.c
index 0598d4397e2a..a221aa75a191 100644
--- a/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/mali_kutf_clk_rate_trace_test.c
+++ b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/mali_kutf_clk_rate_trace_test.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -255,11 +255,11 @@ static const char *kutf_clk_trace_do_get_rate(struct kutf_context *context,
 		spin_unlock(&kbdev->pm.clk_rtm.lock);
 
 		if ((i + 1) == data->nclks)
-			ret += snprintf(portal_msg_buf + ret, PORTAL_MSG_LEN - (size_t)ret,
+			ret += snprintf(portal_msg_buf + ret, PORTAL_MSG_LEN - ret,
 					"0x%lx], GPU_IDLE:%d}", rate, idle);
 		else
-			ret += snprintf(portal_msg_buf + ret, PORTAL_MSG_LEN - (size_t)ret,
-					"0x%lx, ", rate);
+			ret += snprintf(portal_msg_buf + ret, PORTAL_MSG_LEN - ret, "0x%lx, ",
+					rate);
 
 		if (ret >= PORTAL_MSG_LEN) {
 			pr_warn("Message buf overflow with rate array data\n");
@@ -319,7 +319,7 @@ static const char *kutf_clk_trace_do_get_snapshot(struct kutf_context *context,
 			fmt = "(0x%lx, 0x%lx, %u, %u)]}";
 		else
 			fmt = "(0x%lx, 0x%lx, %u, %u), ";
-		ret += snprintf(portal_msg_buf + ret, PORTAL_MSG_LEN - (size_t)ret, fmt,
+		ret += snprintf(portal_msg_buf + ret, PORTAL_MSG_LEN - ret, fmt,
 				snapshot.previous_rate, snapshot.current_rate, snapshot.rate_up_cnt,
 				snapshot.rate_down_cnt);
 		if (ret >= PORTAL_MSG_LEN) {
@@ -425,7 +425,7 @@ static const char *kutf_clk_trace_do_get_platform(struct kutf_context *context,
 	const void *arbiter_if_node = NULL;
 	const void *power_node = NULL;
 	const char *platform = "GPU";
-#if defined(CONFIG_OF)
+#if defined(CONFIG_MALI_ARBITER_SUPPORT) && defined(CONFIG_OF)
 	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
 
 	arbiter_if_node = of_get_property(data->kbdev->dev->of_node, "arbiter-if", NULL);
diff --git a/drivers/gpu/arm/bifrost/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c b/drivers/gpu/arm/bifrost/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c
index c0d46719bc71..112aef433759 100644
--- a/drivers/gpu/arm/bifrost/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c
+++ b/drivers/gpu/arm/bifrost/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c
@@ -51,58 +51,56 @@ struct kutf_irq_fixture_data {
 	struct kbase_device *kbdev;
 };
 
-/* Tag for GPU IRQ */
-#define GPU_IRQ_TAG 2
+/* ID for the GPU IRQ */
+#define GPU_IRQ_HANDLER 2
 
 #define NR_TEST_IRQS ((u32)1000000)
 
+/* IRQ for the test to trigger. Currently POWER_CHANGED_SINGLE as it is
+ * otherwise unused in the DDK
+ */
+#define TEST_IRQ POWER_CHANGED_SINGLE
+
 #define IRQ_TIMEOUT HZ
 
-static void *kbase_untag(void *ptr)
-{
-	return (void *)(((uintptr_t)ptr) & ~(uintptr_t)3);
-}
+/* Kernel API for setting irq throttle hook callback and irq time in us*/
+extern int kbase_set_custom_irq_handler(struct kbase_device *kbdev, irq_handler_t custom_handler,
+					int irq_type);
+extern irqreturn_t kbase_gpu_irq_test_handler(int irq, void *data, u32 val);
 
 static DECLARE_WAIT_QUEUE_HEAD(wait);
 static bool triggered;
 static u64 irq_time;
 
+static void *kbase_untag(void *ptr)
+{
+	return (void *)(((uintptr_t)ptr) & ~3);
+}
+
 /**
  * kbase_gpu_irq_custom_handler - Custom IRQ throttle handler
  * @irq:  IRQ number
  * @data: Data associated with this IRQ
  *
- * Return: IRQ_HANDLED if any interrupt has been handled. IRQ_NONE otherwise.
+ * Return: state of the IRQ
  */
 static irqreturn_t kbase_gpu_irq_custom_handler(int irq, void *data)
 {
 	struct kbase_device *kbdev = kbase_untag(data);
-	u32 status_reg_enum = GPU_CONTROL_ENUM(GPU_IRQ_STATUS);
-	u32 clear_reg_enum = GPU_CONTROL_ENUM(GPU_IRQ_CLEAR);
-	u32 test_irq = POWER_CHANGED_SINGLE;
-	u32 val = kbase_reg_read32(kbdev, status_reg_enum);
+	u32 val = kbase_reg_read32(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_STATUS));
 	irqreturn_t result;
 	u64 tval;
-	bool has_test_irq = val & test_irq;
-
+	bool has_test_irq = val & TEST_IRQ;
 
 	if (has_test_irq) {
 		tval = ktime_get_real_ns();
 		/* Clear the test source only here */
-		kbase_reg_write32(kbdev, clear_reg_enum, test_irq);
+		kbase_reg_write32(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_CLEAR), TEST_IRQ);
 		/* Remove the test IRQ status bit */
-		val = val ^ test_irq;
+		val = val ^ TEST_IRQ;
 	}
 
-	if (!val)
-		result = IRQ_NONE;
-	else {
-#if IS_ENABLED(CONFIG_MALI_REAL_HW)
-		dev_dbg(kbdev->dev, "%s: irq %d irqstatus 0x%x\n", __func__, irq, val);
-		kbase_gpu_interrupt(kbdev, val);
-#endif
-		result = IRQ_HANDLED;
-	}
+	result = kbase_gpu_irq_test_handler(irq, data, val);
 
 	if (has_test_irq) {
 		irq_time = tval;
@@ -182,17 +180,15 @@ static void mali_kutf_irq_latency(struct kutf_context *context)
 	kbase_pm_context_active(kbdev);
 	kbase_pm_wait_for_desired_state(kbdev);
 
-	kbase_set_custom_irq_handler(kbdev, kbase_gpu_irq_custom_handler, GPU_IRQ_TAG);
+	kbase_set_custom_irq_handler(kbdev, kbase_gpu_irq_custom_handler, GPU_IRQ_HANDLER);
 
 	for (i = 1; i <= NR_TEST_IRQS; i++) {
 		u64 start_time = ktime_get_real_ns();
-		u32 reg_enum = GPU_CONTROL_ENUM(GPU_IRQ_RAWSTAT);
-		u32 test_irq = POWER_CHANGED_SINGLE;
 
 		triggered = false;
 
 		/* Trigger fake IRQ */
-		kbase_reg_write32(kbdev, reg_enum, test_irq);
+		kbase_reg_write32(kbdev, GPU_CONTROL_ENUM(GPU_IRQ_RAWSTAT), TEST_IRQ);
 
 		if (wait_event_timeout(wait, triggered, IRQ_TIMEOUT) == 0) {
 			/* Wait extra time to see if it would come */
@@ -215,7 +211,7 @@ static void mali_kutf_irq_latency(struct kutf_context *context)
 	}
 
 	/* Go back to default handler */
-	kbase_set_custom_irq_handler(kbdev, NULL, GPU_IRQ_TAG);
+	kbase_set_custom_irq_handler(kbdev, NULL, GPU_IRQ_HANDLER);
 
 	kbase_pm_context_idle(kbdev);
 
diff --git a/drivers/gpu/arm/bifrost/tests/mali_kutf_mgm_integration_test/mali_kutf_mgm_integration_test_main.c b/drivers/gpu/arm/bifrost/tests/mali_kutf_mgm_integration_test/mali_kutf_mgm_integration_test_main.c
index f341a411324e..6b0c0ffbe6c4 100644
--- a/drivers/gpu/arm/bifrost/tests/mali_kutf_mgm_integration_test/mali_kutf_mgm_integration_test_main.c
+++ b/drivers/gpu/arm/bifrost/tests/mali_kutf_mgm_integration_test/mali_kutf_mgm_integration_test_main.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2022-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -50,7 +50,7 @@ struct kutf_application *mgm_app;
  */
 struct kutf_mgm_fixture_data {
 	struct kbase_device *kbdev;
-	unsigned int group_id;
+	int group_id;
 };
 
 /**
@@ -95,9 +95,7 @@ static void mali_kutf_mgm_pte_translation_test(struct kutf_context *context)
 				data->group_id, mmu_level, original_pte);
 
 			translated_pte = mgm_dev->ops.mgm_update_gpu_pte(mgm_dev, data->group_id,
-									 PBHA_ID_DEFAULT,
-									 PTE_FLAGS_NONE, mmu_level,
-									 original_pte);
+									 mmu_level, original_pte);
 			if (translated_pte == original_pte) {
 				snprintf(
 					msg_buf, sizeof(msg_buf),
diff --git a/drivers/gpu/arm/bifrost/thirdparty/mali_kbase_mmap.c b/drivers/gpu/arm/bifrost/thirdparty/mali_kbase_mmap.c
index 97df69ede4b5..1abcb8fd98a0 100644
--- a/drivers/gpu/arm/bifrost/thirdparty/mali_kbase_mmap.c
+++ b/drivers/gpu/arm/bifrost/thirdparty/mali_kbase_mmap.c
@@ -20,102 +20,18 @@
  * kbase_context_get_unmapped_area() interface.
  */
 
-/**
- * shader_code_align_and_check() - Align the specified pointer according to shader code
- *                     requirement.
- *
- * @gap_end:           Highest possible start address for alignment. The caller must ensure
- *                     the input has already been properly aligned with info contained fields.
- * @info:              vm_unmapped_area_info structure passed, containing alignment, length
- *                     and limits for the allocation
- * The function only undertakes the shader code alignment adjustment. It's the caller's
- * responsibility that the input value provided via gap_end has already been properly aligned
- * in compliance to the fields specified in the info structure. Irrespective the return result,
- * the value of the variable pointed by the pointer gap_end may have been decreased in
- * reaching the required alignment, but will not drop below info->low_limit.
- *
- * Return: true if gap_end is now aligned correctly, false otherwise
- */
-static bool shader_code_align_and_check(unsigned long *gap_end, struct vm_unmapped_area_info *info)
-{
-	unsigned long align_adjust = (info->align_offset ? info->align_offset : info->length);
-	unsigned long align_floor = info->low_limit + align_adjust;
-
-	/* Check for 4GB address inner high-bit pattern, make adjustment if all zeros */
-	if (0 == (*gap_end & BASE_MEM_MASK_4GB) && *gap_end >= align_floor)
-		(*gap_end) -= align_adjust;
-	if (0 == ((*gap_end + info->length) & BASE_MEM_MASK_4GB) && *gap_end >= align_floor)
-		(*gap_end) -= align_adjust;
-
-	return ((*gap_end & BASE_MEM_MASK_4GB) && ((*gap_end + info->length) & BASE_MEM_MASK_4GB));
-}
-
-/**
- * align_4gb_no_straddle() - Align the specified pointer not to straddle over a 4_GB boundary.
- *
- * @gap_end:           Highest possible start address for alignment. The caller must ensure
- *                     the input has already been properly aligned with info contained fields.
- * @info:              vm_unmapped_area_info structure passed, containing alignment, length
- *                     and limits for the allocation
- *
- * The function only undertakes the 4GB boundary alignment adjustment. It's the caller's
- * responsibility that the input value provided via gap_end has already been properly aligned
- * in compliance to the fields specified in the info structure.
- *
- * Return: true is always expected and the gap_end is aligned correctly, false can only
- *         be possible when the code has been wrongly modified.
- */
-static bool align_4gb_no_straddle(unsigned long *gap_end, struct vm_unmapped_area_info *info)
-{
-	unsigned long start = *gap_end;
-	unsigned long end = *gap_end + info->length;
-	unsigned long mask = ~((unsigned long)U32_MAX);
-
-	/* Check if 4GB boundary is straddled */
-	if ((start & mask) != ((end - 1) & mask)) {
-		unsigned long offset = end - (end & mask);
-		/* This is to ensure that alignment doesn't get
-		 * disturbed in an attempt to prevent straddling at
-		 * 4GB boundary. The GPU VA is aligned to 2MB when the
-		 * allocation size is > 2MB and there is enough CPU &
-		 * GPU virtual space.
-		 */
-		unsigned long rounded_offset = ALIGN(offset, info->align_mask + 1);
-
-		start -= rounded_offset;
-		end -= rounded_offset;
-
-		/* Patch gap_end to use new starting address for VA region */
-		*gap_end = start;
-
-		/* The preceding 4GB boundary shall not get straddled,
-		 * even after accounting for the alignment, as the
-		 * size of allocation is limited to 4GB and the initial
-		 * start location was already aligned.
-		 */
-		if (WARN_ONCE((start & mask) != ((end - 1) & mask),
-			      "Alignment unexpected straddles over 4GB boundary!"))
-			return false;
-	}
-
-	return true;
-}
-
-#if (KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE) || !defined(__ANDROID_COMMON_KERNEL__)
 /**
  * align_and_check() - Align the specified pointer to the provided alignment and
- *                     check that it is still in range. For Kernel versions below
- *                     6.1, it requires that the length of the alignment is already
- *                     extended by a worst-case alignment mask.
- * @gap_end:           Highest possible start address for allocation (end of gap in
- *                     address space)
- * @gap_start:         Start address of current memory area / gap in address space
- * @info:              vm_unmapped_area_info structure passed to caller, containing
- *                     alignment, length and limits for the allocation
- * @is_shader_code:    True if the allocation is for shader code (which has
- *                     additional alignment requirements)
- * @is_same_4gb_page:  True if the allocation needs to reside completely within
- *                     a 4GB chunk
+ *                     check that it is still in range.
+ * @gap_end:        Highest possible start address for allocation (end of gap in
+ *                  address space)
+ * @gap_start:      Start address of current memory area / gap in address space
+ * @info:           vm_unmapped_area_info structure passed to caller, containing
+ *                  alignment, length and limits for the allocation
+ * @is_shader_code: True if the allocation is for shader code (which has
+ *                  additional alignment requirements)
+ * @is_same_4gb_page: True if the allocation needs to reside completely within
+ *                    a 4GB chunk
  *
  * Return: true if gap_end is now aligned correctly and is still in range,
  *         false otherwise
@@ -125,22 +41,54 @@ static bool align_and_check(unsigned long *gap_end, unsigned long gap_start,
 			    bool is_same_4gb_page)
 {
 	/* Compute highest gap address at the desired alignment */
-	*gap_end -= info->length;
-	*gap_end -= (*gap_end - info->align_offset) & info->align_mask;
+	(*gap_end) -= info->length;
+	(*gap_end) -= (*gap_end - info->align_offset) & info->align_mask;
 
 	if (is_shader_code) {
-		if (!shader_code_align_and_check(gap_end, info))
-			return false;
-	} else if (is_same_4gb_page)
-		if (!align_4gb_no_straddle(gap_end, info))
+		/* Check for 4GB boundary */
+		if (0 == (*gap_end & BASE_MEM_MASK_4GB))
+			(*gap_end) -= (info->align_offset ? info->align_offset : info->length);
+		if (0 == ((*gap_end + info->length) & BASE_MEM_MASK_4GB))
+			(*gap_end) -= (info->align_offset ? info->align_offset : info->length);
+
+		if (!(*gap_end & BASE_MEM_MASK_4GB) ||
+		    !((*gap_end + info->length) & BASE_MEM_MASK_4GB))
 			return false;
+	} else if (is_same_4gb_page) {
+		unsigned long start = *gap_end;
+		unsigned long end = *gap_end + info->length;
+		unsigned long mask = ~((unsigned long)U32_MAX);
+
+		/* Check if 4GB boundary is straddled */
+		if ((start & mask) != ((end - 1) & mask)) {
+			unsigned long offset = end - (end & mask);
+			/* This is to ensure that alignment doesn't get
+			 * disturbed in an attempt to prevent straddling at
+			 * 4GB boundary. The GPU VA is aligned to 2MB when the
+			 * allocation size is > 2MB and there is enough CPU &
+			 * GPU virtual space.
+			 */
+			unsigned long rounded_offset = ALIGN(offset, info->align_mask + 1);
+
+			start -= rounded_offset;
+			end -= rounded_offset;
+
+			*gap_end = start;
+
+			/* The preceding 4GB boundary shall not get straddled,
+			 * even after accounting for the alignment, as the
+			 * size of allocation is limited to 4GB and the initial
+			 * start location was already aligned.
+			 */
+			WARN_ON((start & mask) != ((end - 1) & mask));
+		}
+	}
 
 	if ((*gap_end < info->low_limit) || (*gap_end < gap_start))
 		return false;
 
 	return true;
 }
-#endif
 
 /**
  * kbase_unmapped_area_topdown() - allocates new areas top-down from
@@ -269,124 +217,37 @@ static unsigned long kbase_unmapped_area_topdown(struct vm_unmapped_area_info *i
 			}
 		}
 	}
-#else /* KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE */
-#ifdef __ANDROID_COMMON_KERNEL__
-	struct vm_unmapped_area_info tmp_info = *info;
-	unsigned long length;
-
-	tmp_info.flags |= VM_UNMAPPED_AREA_TOPDOWN;
-	if (!(is_shader_code || is_same_4gb_page))
-		return vm_unmapped_area(&tmp_info);
-
-	length = info->length + info->align_mask;
-
-	/* Due to additional alignment requirement, shader_code or same_4gb_page
-	 * needs iterations for alignment search and confirmation check.
-	 */
-	while (true) {
-		unsigned long saved_high_lmt = tmp_info.high_limit;
-		unsigned long gap_end, start, rev_high_limit;
-
-		gap_end = vm_unmapped_area(&tmp_info);
-		if (IS_ERR_VALUE(gap_end))
-			return gap_end;
-
-		start = gap_end;
-		if (is_shader_code) {
-			bool shader_code_aligned;
-			unsigned long align_cmp_ref;
-
-			while (true) {
-				/* Save the start value for progress check. the loop needs
-				 * to end if the alignment can't progress any further.
-				 * In summary, the loop ends condition here is either:
-				 *  1. shader_code_aligned is true; or
-				 *  2. align_cmp_ref == gap_end.
-				 */
-				align_cmp_ref = gap_end;
-
-				shader_code_aligned =
-					shader_code_align_and_check(&gap_end, &tmp_info);
-				if (shader_code_aligned || (align_cmp_ref == gap_end))
-					break;
-			}
-
-			if (shader_code_aligned) {
-				if (start == gap_end)
-					return gap_end;
-
-				rev_high_limit = gap_end + length;
-			} else
-				break;
-		} else {
-			/* must be same_4gb_page case */
-			if (likely(align_4gb_no_straddle(&gap_end, &tmp_info))) {
-				if (start == gap_end)
-					return gap_end;
-
-				rev_high_limit = gap_end + length;
-			} else
-				break;
-		}
-
-		if (rev_high_limit < info->low_limit)
-			break;
-
-		if (WARN_ONCE(rev_high_limit >= saved_high_lmt,
-			      "Unexpected recurring high_limit in search, %lx => %lx\n"
-			      "\tinfo-input: limit=[%lx, %lx], mask=%lx, len=%lx\n",
-			      saved_high_lmt, rev_high_limit, info->low_limit, info->high_limit,
-			      info->align_mask, info->length))
-			rev_high_limit = saved_high_lmt -
-					 (info->align_offset ? info->align_offset : info->length);
-
-		/* Repeat the search with a decreasing rev_high_limit */
-		tmp_info.high_limit = rev_high_limit;
-	}
-#else /* __ANDROID_COMMON_KERNEL__ */
-	unsigned long length, high_limit;
+#else
+	unsigned long length, high_limit, gap_start, gap_end;
 
 	MA_STATE(mas, &current->mm->mm_mt, 0, 0);
-
 	/* Adjust search length to account for worst case alignment overhead */
 	length = info->length + info->align_mask;
 	if (length < info->length)
 		return -ENOMEM;
 
-	high_limit = info->high_limit;
-	if ((high_limit - info->low_limit) < length)
+	/*
+	 * Adjust search limits by the desired length.
+	 * See implementation comment at top of unmapped_area().
+	 */
+	gap_end = info->high_limit;
+	if (gap_end < length)
 		return -ENOMEM;
+	high_limit = gap_end - length;
 
-	while (true) {
-		unsigned long gap_start, gap_end;
-		unsigned long saved_high_lmt = high_limit;
+	if (info->low_limit > high_limit)
+		return -ENOMEM;
 
-		if (mas_empty_area_rev(&mas, info->low_limit, high_limit - 1, length))
+	while (true) {
+		if (mas_empty_area_rev(&mas, info->low_limit, info->high_limit - 1, length))
 			return -ENOMEM;
-
 		gap_end = mas.last + 1;
-		gap_start = mas.index;
+		gap_start = mas.min;
 
 		if (align_and_check(&gap_end, gap_start, info, is_shader_code, is_same_4gb_page))
 			return gap_end;
-
-		if (gap_end < info->low_limit)
-			return -ENOMEM;
-
-		/* Adjust next search high limit */
-		high_limit = gap_end + length;
-
-		if (WARN_ONCE(high_limit >= saved_high_lmt,
-			      "Unexpected recurring high_limit in search, %lx => %lx\n"
-			      "\tinfo-input: limit=[%lx, %lx], mask=%lx, len=%lx\n",
-			      saved_high_lmt, high_limit, info->low_limit, info->high_limit,
-			      info->align_mask, info->length))
-			high_limit = saved_high_lmt -
-				     (info->align_offset ? info->align_offset : info->length);
-		mas_reset(&mas);
 	}
-#endif /* __ANDROID_COMMON_KERNEL__ */
-#endif /* KERNEL_VERSION(6, 1, 0) > LINUX_VERSION_CODE */
+#endif
 	return -ENOMEM;
 }
 
@@ -409,8 +270,8 @@ unsigned long kbase_context_get_unmapped_area(struct kbase_context *const kctx,
 	unsigned long high_limit = mm->mmap_base;
 	unsigned long low_limit = PAGE_SIZE;
 #endif
-	unsigned int cpu_va_bits = BITS_PER_LONG;
-	unsigned int gpu_pc_bits = kctx->kbdev->gpu_props.log2_program_counter_size;
+	int cpu_va_bits = BITS_PER_LONG;
+	int gpu_pc_bits = kctx->kbdev->gpu_props.log2_program_counter_size;
 	bool is_shader_code = false;
 	bool is_same_4gb_page = false;
 	unsigned long ret;
@@ -507,7 +368,7 @@ unsigned long kbase_context_get_unmapped_area(struct kbase_context *const kctx,
 		kbase_gpu_vm_unlock(kctx);
 #ifndef CONFIG_64BIT
 	} else {
-		return current->mm->get_unmapped_area(kctx->filp, addr, len, pgoff, flags);
+		return current->mm->get_unmapped_area(kctx->kfile->filp, addr, len, pgoff, flags);
 #endif
 	}
 
diff --git a/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_csf.c b/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_csf.c
index f254aa84dc20..77ebcb05e2b4 100644
--- a/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_csf.c
+++ b/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_csf.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -35,14 +35,14 @@ void kbase_create_timeline_objects(struct kbase_device *kbdev)
 	u32 const num_sb_entries = kbdev->gpu_props.gpu_id.arch_major >= 11 ? 16 : 8;
 	u32 const supports_gpu_sleep =
 #ifdef KBASE_PM_RUNTIME
-		test_bit(KBASE_GPU_SUPPORTS_GPU_SLEEP, &kbdev->pm.backend.gpu_sleep_allowed);
+		kbdev->pm.backend.gpu_sleep_supported;
 #else
 		false;
 #endif /* KBASE_PM_RUNTIME */
 
 	/* Summarize the Address Space objects. */
 	for (as_nr = 0; as_nr < kbdev->nr_hw_address_spaces; as_nr++)
-		__kbase_tlstream_tl_new_as(summary, &kbdev->as[as_nr], (u32)as_nr);
+		__kbase_tlstream_tl_new_as(summary, &kbdev->as[as_nr], as_nr);
 
 	/* Create Legacy GPU object to track in AOM for dumping */
 	__kbase_tlstream_tl_new_gpu(summary, kbdev, kbdev->id, kbdev->gpu_props.num_cores);
@@ -53,7 +53,7 @@ void kbase_create_timeline_objects(struct kbase_device *kbdev)
 	/* Trace the creation of a new kbase device and set its properties. */
 	__kbase_tlstream_tl_kbase_new_device(
 		summary, kbdev->id, kbdev->gpu_props.num_cores, kbdev->csf.global_iface.group_num,
-		(u32)kbdev->nr_hw_address_spaces, num_sb_entries,
+		kbdev->nr_hw_address_spaces, num_sb_entries,
 		kbdev->gpu_props.gpu_features.cross_stream_sync, supports_gpu_sleep,
 		0
 	);
@@ -122,7 +122,7 @@ void kbase_create_timeline_objects(struct kbase_device *kbdev)
 
 		/* Trace the currently assigned address space */
 		if (kctx->as_nr != KBASEP_AS_NR_INVALID)
-			__kbase_tlstream_tl_kbase_ctx_assign_as(body, kctx->id, (u32)kctx->as_nr);
+			__kbase_tlstream_tl_kbase_ctx_assign_as(body, kctx->id, kctx->as_nr);
 
 		/* Trace all KCPU queues in the context into the body stream.
 		 * As we acquired the KCPU lock after resetting the body stream,
diff --git a/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_jm.c b/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_jm.c
index 3e9e6e864125..628f29aab9ff 100644
--- a/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_jm.c
+++ b/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_jm.c
@@ -42,7 +42,7 @@ void kbase_create_timeline_objects(struct kbase_device *kbdev)
 
 	/* Summarize the Address Space objects. */
 	for (as_nr = 0; as_nr < kbdev->nr_hw_address_spaces; as_nr++)
-		__kbase_tlstream_tl_new_as(summary, &kbdev->as[as_nr], (u32)as_nr);
+		__kbase_tlstream_tl_new_as(summary, &kbdev->as[as_nr], as_nr);
 
 	/* Create GPU object and make it retain all LPUs and address spaces. */
 	__kbase_tlstream_tl_new_gpu(summary, kbdev, kbdev->id, kbdev->gpu_props.num_cores);
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline.c b/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline.c
index eaa5a848ede7..ec38c70504b7 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline.c
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline.c
@@ -183,7 +183,7 @@ int kbase_timeline_acquire(struct kbase_device *kbdev, u32 flags)
 	if (WARN_ON(!timeline))
 		return -EFAULT;
 
-	if (atomic_cmpxchg(timeline->timeline_flags, 0, (int)timeline_flags))
+	if (atomic_cmpxchg(timeline->timeline_flags, 0, timeline_flags))
 		return -EBUSY;
 
 #if MALI_USE_CSF
@@ -387,8 +387,8 @@ void kbase_timeline_stats(struct kbase_timeline *timeline, u32 *bytes_collected,
 	/* Accumulate bytes generated per stream  */
 	*bytes_generated = 0;
 	for (stype = (enum tl_stream_type)0; stype < TL_STREAM_TYPE_COUNT; stype++)
-		*bytes_generated += (u32)atomic_read(&timeline->streams[stype].bytes_generated);
+		*bytes_generated += atomic_read(&timeline->streams[stype].bytes_generated);
 
-	*bytes_collected = (u32)atomic_read(&timeline->bytes_collected);
+	*bytes_collected = atomic_read(&timeline->bytes_collected);
 }
 #endif /* MALI_UNIT_TEST */
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_io.c b/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_io.c
index 719e26124409..5f3b79b6ecbd 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_io.c
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_io.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -29,7 +29,6 @@
 #include <linux/poll.h>
 #include <linux/version_compat_defs.h>
 #include <linux/anon_inodes.h>
-#include <linux/overflow.h>
 
 /* Explicitly include epoll header for old kernels. Not required from 4.16. */
 #if KERNEL_VERSION(4, 16, 0) > LINUX_VERSION_CODE
@@ -121,7 +120,7 @@ static int kbasep_timeline_io_packet_pending(struct kbase_timeline *timeline,
 
 	for (i = (enum tl_stream_type)0; i < TL_STREAM_TYPE_COUNT; ++i) {
 		struct kbase_tlstream *stream = &timeline->streams[i];
-		*rb_idx_raw = (unsigned int)atomic_read(&stream->rbi);
+		*rb_idx_raw = atomic_read(&stream->rbi);
 		/* Read buffer index may be updated by writer in case of
 		 * overflow. Read and write buffer indexes must be
 		 * loaded in correct order.
@@ -170,7 +169,7 @@ static inline int copy_stream_header(char __user *buffer, size_t size, ssize_t *
 				     const char *hdr, size_t hdr_size, size_t *hdr_btc)
 {
 	const size_t offset = hdr_size - *hdr_btc;
-	const size_t copy_size = MIN(size_sub((ssize_t)size, *copy_len), *hdr_btc);
+	const size_t copy_size = MIN(size - *copy_len, *hdr_btc);
 
 	if (!*hdr_btc)
 		return 0;
@@ -182,7 +181,7 @@ static inline int copy_stream_header(char __user *buffer, size_t size, ssize_t *
 		return -1;
 
 	*hdr_btc -= copy_size;
-	*copy_len += (ssize_t)copy_size;
+	*copy_len += copy_size;
 
 	return 0;
 }
@@ -291,8 +290,8 @@ static ssize_t kbasep_timeline_io_read(struct file *filp, char __user *buffer, s
 		 * If so copy its content.
 		 */
 		rb_idx = rb_idx_raw % PACKET_COUNT;
-		rb_size = (size_t)atomic_read(&stream->buffer[rb_idx].size);
-		if (rb_size > (size_t)((ssize_t)size - copy_len))
+		rb_size = atomic_read(&stream->buffer[rb_idx].size);
+		if (rb_size > size - copy_len)
 			break;
 		if (copy_to_user(&buffer[copy_len], stream->buffer[rb_idx].data, rb_size)) {
 			copy_len = -EFAULT;
@@ -305,10 +304,10 @@ static ssize_t kbasep_timeline_io_read(struct file *filp, char __user *buffer, s
 		 * that we have just sent to user.
 		 */
 		smp_rmb();
-		wb_idx_raw = (unsigned int)atomic_read(&stream->wbi);
+		wb_idx_raw = atomic_read(&stream->wbi);
 
 		if (wb_idx_raw - rb_idx_raw < PACKET_COUNT) {
-			copy_len += (ssize_t)rb_size;
+			copy_len += rb_size;
 			atomic_inc(&stream->rbi);
 #if MALI_UNIT_TEST
 			atomic_add(rb_size, &timeline->bytes_collected);
@@ -317,7 +316,7 @@ static ssize_t kbasep_timeline_io_read(struct file *filp, char __user *buffer, s
 		} else {
 			const unsigned int new_rb_idx_raw = wb_idx_raw - PACKET_COUNT + 1;
 			/* Adjust read buffer index to the next valid buffer */
-			atomic_set(&stream->rbi, (int)new_rb_idx_raw);
+			atomic_set(&stream->rbi, new_rb_idx_raw);
 		}
 	}
 
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.c b/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.c
index 117417c30183..ddbddcbc5968 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.c
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.c
@@ -222,9 +222,9 @@ char *kbase_tlstream_msgbuf_acquire(struct kbase_tlstream *stream, size_t msg_si
 
 	spin_lock_irqsave(&stream->lock, *flags);
 
-	wb_idx_raw = (unsigned int)atomic_read(&stream->wbi);
+	wb_idx_raw = atomic_read(&stream->wbi);
 	wb_idx = wb_idx_raw % PACKET_COUNT;
-	wb_size = (size_t)atomic_read(&stream->buffer[wb_idx].size);
+	wb_size = atomic_read(&stream->buffer[wb_idx].size);
 
 	/* Select next buffer if data will not fit into current one. */
 	if (wb_size + msg_size > PACKET_SIZE) {
@@ -264,9 +264,9 @@ size_t kbase_tlstream_flush_stream(struct kbase_tlstream *stream)
 
 	spin_lock_irqsave(&stream->lock, flags);
 
-	wb_idx_raw = (unsigned int)atomic_read(&stream->wbi);
+	wb_idx_raw = atomic_read(&stream->wbi);
 	wb_idx = wb_idx_raw % PACKET_COUNT;
-	wb_size = (size_t)atomic_read(&stream->buffer[wb_idx].size);
+	wb_size = atomic_read(&stream->buffer[wb_idx].size);
 
 	if (wb_size > min_size) {
 		wb_size = kbasep_tlstream_msgbuf_submit(stream, wb_idx_raw, wb_size);
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.c b/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.c
index d4465c44addb..742735846d49 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.c
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -62,7 +62,6 @@ enum tl_msg_id_obj {
 	KBASE_TL_EVENT_ATOM_SOFTJOB_START,
 	KBASE_TL_EVENT_ATOM_SOFTJOB_END,
 	KBASE_TL_ARBITER_GRANTED,
-	KBASE_TL_ARBITER_LOST,
 	KBASE_TL_ARBITER_STARTED,
 	KBASE_TL_ARBITER_STOP_REQUESTED,
 	KBASE_TL_ARBITER_STOPPED,
@@ -273,10 +272,6 @@ enum tl_msg_id_obj {
 		"Arbiter has granted gpu access", \
 		"@p", \
 		"gpu") \
-	TRACEPOINT_DESC(KBASE_TL_ARBITER_LOST, \
-		"Received a gpu lost event from the arbiter", \
-		"@p", \
-		"gpu") \
 	TRACEPOINT_DESC(KBASE_TL_ARBITER_STARTED, \
 		"Driver is running again and able to process jobs", \
 		"@p", \
@@ -1551,29 +1546,6 @@ void __kbase_tlstream_tl_arbiter_granted(
 	kbase_tlstream_msgbuf_release(stream, acq_flags);
 }
 
-void __kbase_tlstream_tl_arbiter_lost(
-	struct kbase_tlstream *stream,
-	const void *gpu
-)
-{
-	const u32 msg_id = KBASE_TL_ARBITER_LOST;
-	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
-		+ sizeof(gpu)
-		;
-	char *buffer;
-	unsigned long acq_flags;
-	size_t pos = 0;
-
-	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
-
-	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_serialize_timestamp(buffer, pos);
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &gpu, sizeof(gpu));
-
-	kbase_tlstream_msgbuf_release(stream, acq_flags);
-}
-
 void __kbase_tlstream_tl_arbiter_started(
 	struct kbase_tlstream *stream,
 	const void *gpu
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.h b/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.h
index 6dd4b44ea6b2..8e09c286a066 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.h
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -271,11 +271,6 @@ void __kbase_tlstream_tl_arbiter_granted(
 	const void *gpu
 );
 
-void __kbase_tlstream_tl_arbiter_lost(
-	struct kbase_tlstream *stream,
-	const void *gpu
-);
-
 void __kbase_tlstream_tl_arbiter_started(
 	struct kbase_tlstream *stream,
 	const void *gpu
@@ -889,7 +884,7 @@ struct kbase_tlstream;
 	tgid	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_new_ctx(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -914,7 +909,7 @@ struct kbase_tlstream;
 	core_count	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_new_gpu(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -939,7 +934,7 @@ struct kbase_tlstream;
 	lpu_fn	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_new_lpu(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -962,7 +957,7 @@ struct kbase_tlstream;
 	atom_nr	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_new_atom(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -984,7 +979,7 @@ struct kbase_tlstream;
 	as_nr	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_new_as(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1004,7 +999,7 @@ struct kbase_tlstream;
 	ctx	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_del_ctx(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1023,7 +1018,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_del_atom(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1044,7 +1039,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_lifelink_lpu_gpu(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1066,7 +1061,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_lifelink_as_gpu(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1088,7 +1083,7 @@ struct kbase_tlstream;
 	lpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_ret_ctx_lpu(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1110,7 +1105,7 @@ struct kbase_tlstream;
 	ctx	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_ret_atom_ctx(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1134,7 +1129,7 @@ struct kbase_tlstream;
 	attrib_match_list	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_ret_atom_lpu(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1157,7 +1152,7 @@ struct kbase_tlstream;
 	lpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_nret_ctx_lpu(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1179,7 +1174,7 @@ struct kbase_tlstream;
 	ctx	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_nret_atom_ctx(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1201,7 +1196,7 @@ struct kbase_tlstream;
 	lpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_nret_atom_lpu(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1223,7 +1218,7 @@ struct kbase_tlstream;
 	ctx	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_ret_as_ctx(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1245,7 +1240,7 @@ struct kbase_tlstream;
 	ctx	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_nret_as_ctx(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1267,7 +1262,7 @@ struct kbase_tlstream;
 	address_space	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_ret_atom_as(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1289,7 +1284,7 @@ struct kbase_tlstream;
 	address_space	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_nret_atom_as(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1315,7 +1310,7 @@ struct kbase_tlstream;
 	config	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_attrib_atom_config(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1339,7 +1334,7 @@ struct kbase_tlstream;
 	j_id	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_jit_usedpages(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1375,7 +1370,7 @@ struct kbase_tlstream;
 	usg_id	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_attrib_atom_jitallocinfo(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1404,7 +1399,7 @@ struct kbase_tlstream;
 	j_id	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_attrib_atom_jitfreeinfo(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1430,7 +1425,7 @@ struct kbase_tlstream;
 	transcfg	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_attrib_as_config(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1452,7 +1447,7 @@ struct kbase_tlstream;
 	lpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_event_lpu_softstop(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1471,7 +1466,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_event_atom_softstop_ex(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1490,7 +1485,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_event_atom_softstop_issue(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1509,7 +1504,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_event_atom_softjob_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1528,7 +1523,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_event_atom_softjob_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1547,7 +1542,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_arbiter_granted(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1555,25 +1550,6 @@ struct kbase_tlstream;
 				);	\
 	} while (0)
 
-/**
- * KBASE_TLSTREAM_TL_ARBITER_LOST - Received a gpu lost event from the arbiter
- *
- * @kbdev: Kbase device
- * @gpu: Name of the GPU object
- */
-#define KBASE_TLSTREAM_TL_ARBITER_LOST(	\
-	kbdev,	\
-	gpu	\
-	)	\
-	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
-		if (enabled & TLSTREAM_ENABLED)	\
-			__kbase_tlstream_tl_arbiter_lost(	\
-				__TL_DISPATCH_STREAM(kbdev, obj),	\
-				gpu	\
-				);	\
-	} while (0)
-
 /**
  * KBASE_TLSTREAM_TL_ARBITER_STARTED - Driver is running again and able to process jobs
  *
@@ -1585,7 +1561,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_arbiter_started(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1604,7 +1580,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_arbiter_stop_requested(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1623,7 +1599,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_arbiter_stopped(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1642,7 +1618,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_arbiter_requested(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1661,7 +1637,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_jd_gpu_soft_reset(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1684,7 +1660,7 @@ struct kbase_tlstream;
 	chunk_va	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_jd_tiler_heap_chunk_alloc(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1705,7 +1681,7 @@ struct kbase_tlstream;
 	dummy	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_js_sched_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1724,7 +1700,7 @@ struct kbase_tlstream;
 	dummy	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_js_sched_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1743,7 +1719,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_jd_submit_atom_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1762,7 +1738,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_jd_submit_atom_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1781,7 +1757,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_jd_done_no_lock_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1800,7 +1776,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_jd_done_no_lock_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1819,7 +1795,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_jd_done_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1838,7 +1814,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_jd_done_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1857,7 +1833,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_jd_atom_complete(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1878,7 +1854,7 @@ struct kbase_tlstream;
 	atom_nr	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_run_atom_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1900,7 +1876,7 @@ struct kbase_tlstream;
 	atom_nr	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_tl_run_atom_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1922,7 +1898,7 @@ struct kbase_tlstream;
 	prio	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS)	\
 			__kbase_tlstream_tl_attrib_atom_priority(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1944,7 +1920,7 @@ struct kbase_tlstream;
 	state	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS)	\
 			__kbase_tlstream_tl_attrib_atom_state(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1964,7 +1940,7 @@ struct kbase_tlstream;
 	atom	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS)	\
 			__kbase_tlstream_tl_attrib_atom_prioritized(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -1999,7 +1975,7 @@ struct kbase_tlstream;
 	va_pgs	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_JOB_DUMPING_ENABLED)	\
 			__kbase_tlstream_tl_attrib_atom_jit(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2041,7 +2017,7 @@ struct kbase_tlstream;
 	kbase_device_has_vd54d34dbb40917c8cea48cca407a8789413be0db	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_new_device(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2084,7 +2060,7 @@ struct kbase_tlstream;
 	buffer_gpu_addr	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_gpucmdqueue_kick(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2121,7 +2097,7 @@ struct kbase_tlstream;
 	kbase_device_csg_slot_resuming	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_device_program_csg(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2158,7 +2134,7 @@ struct kbase_tlstream;
 	kbase_device_csg_slot_index	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_device_deprogram_csg(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2191,7 +2167,7 @@ struct kbase_tlstream;
 	kbase_device_csg_slot_suspending	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_device_halting_csg(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2224,7 +2200,7 @@ struct kbase_tlstream;
 	kbase_device_csg_slot_index	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_device_suspend_csg(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2255,7 +2231,7 @@ struct kbase_tlstream;
 	kbase_device_csg_slot_index	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_device_csg_idle(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2286,7 +2262,7 @@ struct kbase_tlstream;
 	kbase_device_id	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_new_ctx(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2315,7 +2291,7 @@ struct kbase_tlstream;
 	kernel_ctx_id	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_del_ctx(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2344,7 +2320,7 @@ struct kbase_tlstream;
 	kbase_device_as_index	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_ctx_assign_as(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2373,7 +2349,7 @@ struct kbase_tlstream;
 	kernel_ctx_id	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_ctx_unassign_as(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2406,7 +2382,7 @@ struct kbase_tlstream;
 	kcpuq_num_pending_cmds	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_new_kcpuqueue(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2439,7 +2415,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_del_kcpuqueue(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2468,7 +2444,7 @@ struct kbase_tlstream;
 	fence	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_fence_signal(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2499,7 +2475,7 @@ struct kbase_tlstream;
 	fence	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_fence_wait(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2534,7 +2510,7 @@ struct kbase_tlstream;
 	inherit_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_cqs_wait(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2569,7 +2545,7 @@ struct kbase_tlstream;
 	cqs_obj_gpu_addr	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_cqs_set(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2608,7 +2584,7 @@ struct kbase_tlstream;
 	inherit_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_cqs_wait_operation(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2653,7 +2629,7 @@ struct kbase_tlstream;
 	data_type	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_cqs_set_operation(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2690,7 +2666,7 @@ struct kbase_tlstream;
 	map_import_buf_gpu_addr	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_map_import(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2721,7 +2697,7 @@ struct kbase_tlstream;
 	map_import_buf_gpu_addr	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_unmap_import(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2752,7 +2728,7 @@ struct kbase_tlstream;
 	map_import_buf_gpu_addr	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_unmap_import_force(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2781,7 +2757,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_begin_kcpuqueue_enqueue_jit_alloc(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2826,7 +2802,7 @@ struct kbase_tlstream;
 	jit_alloc_usage_id	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_jit_alloc(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2871,7 +2847,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_end_kcpuqueue_enqueue_jit_alloc(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2898,7 +2874,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_begin_kcpuqueue_enqueue_jit_free(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2927,7 +2903,7 @@ struct kbase_tlstream;
 	jit_alloc_jit_id	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_jit_free(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2956,7 +2932,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_end_kcpuqueue_enqueue_jit_free(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -2983,7 +2959,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_error_barrier(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3014,7 +2990,7 @@ struct kbase_tlstream;
 	gpu_cmdq_grp_handle	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_group_suspend(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3045,7 +3021,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_fence_signal_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3074,7 +3050,7 @@ struct kbase_tlstream;
 	execute_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_fence_signal_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3103,7 +3079,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_fence_wait_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3132,7 +3108,7 @@ struct kbase_tlstream;
 	execute_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_fence_wait_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3161,7 +3137,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_wait_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3190,7 +3166,7 @@ struct kbase_tlstream;
 	execute_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_wait_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3221,7 +3197,7 @@ struct kbase_tlstream;
 	execute_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_set(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3250,7 +3226,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_wait_operation_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3279,7 +3255,7 @@ struct kbase_tlstream;
 	execute_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_wait_operation_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3310,7 +3286,7 @@ struct kbase_tlstream;
 	execute_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_set_operation(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3339,7 +3315,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_map_import_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3368,7 +3344,7 @@ struct kbase_tlstream;
 	execute_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_map_import_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3397,7 +3373,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_unmap_import_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3426,7 +3402,7 @@ struct kbase_tlstream;
 	execute_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_unmap_import_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3455,7 +3431,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_unmap_import_force_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3484,7 +3460,7 @@ struct kbase_tlstream;
 	execute_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_unmap_import_force_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3513,7 +3489,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_jit_alloc_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3540,7 +3516,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_begin_kcpuqueue_execute_jit_alloc_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3573,7 +3549,7 @@ struct kbase_tlstream;
 	jit_alloc_mmu_flags	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_item_kcpuqueue_execute_jit_alloc_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3606,7 +3582,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_end_kcpuqueue_execute_jit_alloc_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3633,7 +3609,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_jit_free_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3660,7 +3636,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_begin_kcpuqueue_execute_jit_free_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3691,7 +3667,7 @@ struct kbase_tlstream;
 	jit_free_pages_used	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_item_kcpuqueue_execute_jit_free_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3722,7 +3698,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_array_end_kcpuqueue_execute_jit_free_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3749,7 +3725,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_error_barrier(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3776,7 +3752,7 @@ struct kbase_tlstream;
 	kcpu_queue	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_group_suspend_start(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3805,7 +3781,7 @@ struct kbase_tlstream;
 	execute_error	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_kcpuqueue_execute_group_suspend_end(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3834,7 +3810,7 @@ struct kbase_tlstream;
 	csffw_cycle	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_csffw_fw_reloading(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3861,7 +3837,7 @@ struct kbase_tlstream;
 	csffw_cycle	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_csffw_fw_enabling(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3888,7 +3864,7 @@ struct kbase_tlstream;
 	csffw_cycle	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_csffw_fw_request_sleep(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3915,7 +3891,7 @@ struct kbase_tlstream;
 	csffw_cycle	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_csffw_fw_request_wakeup(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3942,7 +3918,7 @@ struct kbase_tlstream;
 	csffw_cycle	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_csffw_fw_request_halt(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3969,7 +3945,7 @@ struct kbase_tlstream;
 	csffw_cycle	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_csffw_fw_disabling(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -3996,7 +3972,7 @@ struct kbase_tlstream;
 	csffw_cycle	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_csffw_fw_off(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -4025,7 +4001,7 @@ struct kbase_tlstream;
 	csffw_cycle	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)	\
 			__kbase_tlstream_tl_kbase_csffw_tlstream_overflow(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
@@ -4055,7 +4031,7 @@ struct kbase_tlstream;
 	core_state_bitset	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_pm_state(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4079,7 +4055,7 @@ struct kbase_tlstream;
 	page_cnt_change	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_pagefault(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4102,7 +4078,7 @@ struct kbase_tlstream;
 	page_cnt	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_pagesalloc(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4122,7 +4098,7 @@ struct kbase_tlstream;
 	target_freq	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_devfreq_target(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4151,7 +4127,7 @@ struct kbase_tlstream;
 	ph_pages	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_jit_stats(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4191,7 +4167,7 @@ struct kbase_tlstream;
 	nr_in_flight	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_tiler_heap_stats(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4224,7 +4200,7 @@ struct kbase_tlstream;
 	event	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_event_job_slot(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4246,7 +4222,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_protected_enter_start(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4265,7 +4241,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_protected_enter_end(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4292,7 +4268,7 @@ struct kbase_tlstream;
 	mmu_lock_page_num	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_mmu_command(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4315,7 +4291,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS)	\
 			__kbase_tlstream_aux_protected_leave_start(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4334,7 +4310,7 @@ struct kbase_tlstream;
 	gpu	\
 	)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS)	\
 			__kbase_tlstream_aux_protected_leave_end(	\
 				__TL_DISPATCH_STREAM(kbdev, aux),	\
@@ -4356,7 +4332,7 @@ struct kbase_tlstream;
 #define KBASE_TLSTREAM_AUX_EVENT_JOB_SLOT(kbdev,	\
 	context, slot_nr, atom_nr, event)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		kbase_trace_mali_job_slots_event(kbdev->id,	\
 			GATOR_MAKE_EVENT(event, slot_nr),	\
 			context, (u8) atom_nr);	\
@@ -4369,7 +4345,7 @@ struct kbase_tlstream;
 #undef KBASE_TLSTREAM_AUX_PM_STATE
 #define KBASE_TLSTREAM_AUX_PM_STATE(kbdev, core_type, state)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		kbase_trace_mali_pm_status(kbdev->id,	\
 			core_type, state);	\
 		if (enabled & TLSTREAM_ENABLED)	\
@@ -4382,9 +4358,9 @@ struct kbase_tlstream;
 #define KBASE_TLSTREAM_AUX_PAGEFAULT(kbdev,	\
 	ctx_nr, as_nr, page_cnt_change)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		kbase_trace_mali_page_fault_insert_pages(kbdev->id,	\
-			(int)as_nr,	\
+			as_nr,	\
 			page_cnt_change);	\
 		if (enabled & TLSTREAM_ENABLED)	\
 			__kbase_tlstream_aux_pagefault(	\
@@ -4399,9 +4375,9 @@ struct kbase_tlstream;
 #undef KBASE_TLSTREAM_AUX_PAGESALLOC
 #define KBASE_TLSTREAM_AUX_PAGESALLOC(kbdev, ctx_nr, page_cnt)	\
 	do {	\
-		u32 enabled = (u32)atomic_read(&kbdev->timeline_flags);	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		u32 global_pages_count =	\
-			(u32)atomic_read(&kbdev->memdev.used_pages);	\
+			atomic_read(&kbdev->memdev.used_pages);	\
 			\
 		kbase_trace_mali_total_alloc_pages_change(kbdev->id,	\
 			global_pages_count);	\
diff --git a/drivers/gpu/arm/mali400/mali/platform/rk/rk.c b/drivers/gpu/arm/mali400/mali/platform/rk/rk.c
index 7f0e160cb1b4..a9785a5fe1a6 100644
--- a/drivers/gpu/arm/mali400/mali/platform/rk/rk.c
+++ b/drivers/gpu/arm/mali400/mali/platform/rk/rk.c
@@ -404,6 +404,7 @@ static int power_model_simple_init(struct platform_device *pdev)
 
 /*---------------------------------------------------------------------------*/
 
+#ifdef CONFIG_PM
 
 static int rk_platform_enable_clk_gpu(struct device *dev)
 {
@@ -515,7 +516,6 @@ void rk_platform_uninit_opp_table(struct mali_device *mdev)
 	rockchip_uninit_opp_table(mdev->dev, &mdev->opp_info);
 }
 
-#ifdef CONFIG_PM
 static int mali_runtime_suspend(struct device *device)
 {
 	int ret = 0;
diff --git a/drivers/gpu/arm/midgard/platform/rk/mali_kbase_config_rk.c b/drivers/gpu/arm/midgard/platform/rk/mali_kbase_config_rk.c
index dcdf87345c00..b5aedce68cb4 100644
--- a/drivers/gpu/arm/midgard/platform/rk/mali_kbase_config_rk.c
+++ b/drivers/gpu/arm/midgard/platform/rk/mali_kbase_config_rk.c
@@ -169,12 +169,12 @@ struct kbase_platform_funcs_conf platform_funcs = {
 
 /*---------------------------------------------------------------------------*/
 
-static __maybe_unused int rk_pm_callback_runtime_on(struct kbase_device *kbdev)
+static int rk_pm_callback_runtime_on(struct kbase_device *kbdev)
 {
 	return 0;
 }
 
-static __maybe_unused void rk_pm_callback_runtime_off(struct kbase_device *kbdev)
+static void rk_pm_callback_runtime_off(struct kbase_device *kbdev)
 {
 }
 
@@ -248,10 +248,17 @@ void rk_kbase_device_runtime_disable(struct kbase_device *kbdev)
 struct kbase_pm_callback_conf pm_callbacks = {
 	.power_on_callback = rk_pm_callback_power_on,
 	.power_off_callback = rk_pm_callback_power_off,
-	.power_runtime_init_callback = pm_ptr(rk_kbase_device_runtime_init),
-	.power_runtime_term_callback = pm_ptr(rk_kbase_device_runtime_disable),
-	.power_runtime_on_callback = pm_ptr(rk_pm_callback_runtime_on),
-	.power_runtime_off_callback = pm_ptr(rk_pm_callback_runtime_off),
+#ifdef CONFIG_PM
+	.power_runtime_init_callback = rk_kbase_device_runtime_init,
+	.power_runtime_term_callback = rk_kbase_device_runtime_disable,
+	.power_runtime_on_callback = rk_pm_callback_runtime_on,
+	.power_runtime_off_callback = rk_pm_callback_runtime_off,
+#else				/* CONFIG_PM */
+	.power_runtime_init_callback = NULL,
+	.power_runtime_term_callback = NULL,
+	.power_runtime_on_callback = NULL,
+	.power_runtime_off_callback = NULL,
+#endif				/* CONFIG_PM */
 };
 
 int kbase_platform_early_init(void)
diff --git a/drivers/hwtracing/coresight/mali/Makefile b/drivers/hwtracing/coresight/mali/Makefile
index d8186bee6e64..923cb0c910d9 100644
--- a/drivers/hwtracing/coresight/mali/Makefile
+++ b/drivers/hwtracing/coresight/mali/Makefile
@@ -79,9 +79,9 @@ ifeq ($(MALI_KCONFIG_EXT_PREFIX),)
 endif
 
 EXTRA_SYMBOLS += \
+    $(M)/../../../base/arm/Module.symvers \
     $(GPU_SYMBOLS)
 
-
 # The following were added to align with W=1 in scripts/Makefile.extrawarn
 # from the Linux source tree
 CFLAGS_MODULE += -Wall -Werror
@@ -99,8 +99,6 @@ CFLAGS_MODULE += $(call cc-option, -Wstringop-truncation)
 CFLAGS_MODULE += -Wno-missing-field-initializers
 CFLAGS_MODULE += -Wno-sign-compare
 CFLAGS_MODULE += -Wno-type-limits
-# The following ensures the stack frame does not get larger than a page
-CFLAGS_MODULE += -Wframe-larger-than=4096
 
 KBUILD_CPPFLAGS += -DKBUILD_EXTRA_WARN1
 
diff --git a/drivers/hwtracing/coresight/mali/build.bp b/drivers/hwtracing/coresight/mali/build.bp
index d69148c8cb70..33dcd22fa364 100644
--- a/drivers/hwtracing/coresight/mali/build.bp
+++ b/drivers/hwtracing/coresight/mali/build.bp
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2022-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
diff --git a/drivers/hwtracing/coresight/mali/sources/coresight_mali_sources.c b/drivers/hwtracing/coresight/mali/sources/coresight_mali_sources.c
index 247a8b47f05b..e6d2dc71096b 100644
--- a/drivers/hwtracing/coresight/mali/sources/coresight_mali_sources.c
+++ b/drivers/hwtracing/coresight/mali/sources/coresight_mali_sources.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -50,11 +50,7 @@ static void coresight_mali_disable_source(struct coresight_device *csdev, struct
 }
 
 static const struct coresight_ops_source coresight_mali_source_ops = {
-#if KERNEL_VERSION(6, 3, 0) <= LINUX_VERSION_CODE
-	.cpu_id = coresight_mali_source_trace_id,
-#else
 	.trace_id = coresight_mali_source_trace_id,
-#endif
 	.enable = coresight_mali_enable_source,
 	.disable = coresight_mali_disable_source
 };
diff --git a/drivers/hwtracing/coresight/mali/sources/itm/coresight_mali_source_itm_core.c b/drivers/hwtracing/coresight/mali/sources/itm/coresight_mali_source_itm_core.c
index 727e5c7a552a..59d5cd314c2f 100644
--- a/drivers/hwtracing/coresight/mali/sources/itm/coresight_mali_source_itm_core.c
+++ b/drivers/hwtracing/coresight/mali/sources/itm/coresight_mali_source_itm_core.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
 /*
  *
- * (C) COPYRIGHT 2022-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -85,14 +85,14 @@ static struct kbase_debug_coresight_csf_op dwt_itm_enable_ops[] = {
 };
 
 static struct kbase_debug_coresight_csf_op dwt_itm_disable_ops[] = {
+	// Disable ITM/DWT functionality via DEMCR register
+	WRITE_IMM_OP(CS_SCS_BASE_ADDR + SCS_DEMCR, 0x00000000),
 	// Unlock ITM configuration
 	WRITE_IMM_OP(CS_ITM_BASE_ADDR + CORESIGHT_LAR, CS_MALI_UNLOCK_COMPONENT),
 	// Check ITM is disabled
 	POLL_OP(CS_ITM_BASE_ADDR + ITM_TCR, ITM_TCR_BUSY_BIT, 0x0),
 	// Lock
 	WRITE_IMM_OP(CS_ITM_BASE_ADDR + CORESIGHT_LAR, 0x00000000),
-	// Disable ITM/DWT functionality via DEMCR register
-	WRITE_IMM_OP(CS_SCS_BASE_ADDR + SCS_DEMCR, 0x00000000),
 	// Set enabled bit off at the end of sequence
 	BIT_AND_OP(&itm_state.enabled, 0x0),
 };
diff --git a/drivers/xen/arm/Makefile b/drivers/xen/arm/Makefile
index 27bee59ac787..b2ee53723428 100644
--- a/drivers/xen/arm/Makefile
+++ b/drivers/xen/arm/Makefile
@@ -78,8 +78,6 @@ CFLAGS_MODULE += $(call cc-option, -Wstringop-truncation)
 CFLAGS_MODULE += -Wno-missing-field-initializers
 CFLAGS_MODULE += -Wno-sign-compare
 CFLAGS_MODULE += -Wno-type-limits
-# The following ensures the stack frame does not get larger than a page
-CFLAGS_MODULE += -Wframe-larger-than=4096
 
 KBUILD_CPPFLAGS += -DKBUILD_EXTRA_WARN1
 
diff --git a/include/linux/mali_arbiter_interface.h b/include/linux/mali_arbiter_interface.h
index ae44e82ae6dd..b4162f86ebb4 100644
--- a/include/linux/mali_arbiter_interface.h
+++ b/include/linux/mali_arbiter_interface.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -26,8 +26,6 @@
 #ifndef _MALI_KBASE_ARBITER_INTERFACE_H_
 #define _MALI_KBASE_ARBITER_INTERFACE_H_
 
-#include <linux/device.h>
-
 /**
  * DOC: Mali arbiter interface version
  *
diff --git a/include/linux/mali_hw_access.h b/include/linux/mali_hw_access.h
deleted file mode 100644
index 106393fc3372..000000000000
--- a/include/linux/mali_hw_access.h
+++ /dev/null
@@ -1,62 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
-/*
- *
- * (C) COPYRIGHT 2023-2024 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU license.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- */
-
-#ifndef _MALI_HW_ACCESS_H_
-#define _MALI_HW_ACCESS_H_
-
-#include <asm/arch_timer.h>
-#include <linux/io.h>
-
-
-#define mali_readl(addr) readl(addr)
-
-#define mali_writel(val, addr) writel(val, addr)
-
-#define mali_ioremap(addr, size) ioremap(addr, size)
-
-#define mali_iounmap(addr) iounmap(addr)
-
-#define mali_arch_timer_get_cntfrq() arch_timer_get_cntfrq()
-
-
-#define mali_readq(addr) ((u64)mali_readl(addr) | ((u64)mali_readl(addr + 4) << 32))
-
-static inline u64 mali_readq_coherent(const void __iomem *addr)
-{
-	u32 hi1, hi2, lo;
-
-	do {
-		hi1 = mali_readl(addr + 4);
-		lo = mali_readl(addr);
-		hi2 = mali_readl(addr + 4);
-	} while (hi1 != hi2);
-
-	return lo | (((u64)hi1) << 32);
-}
-
-#define mali_writeq(val, addr)                                \
-	do {                                                  \
-		u64 __val = (u64)val;                         \
-		mali_writel((u32)(__val & 0xFFFFFFFF), addr); \
-		mali_writel((u32)(__val >> 32), addr + 4);    \
-	} while (0)
-
-#endif /* _MALI_HW_ACCESS_H_ */
diff --git a/include/linux/memory_group_manager.h b/include/linux/memory_group_manager.h
index ec55d74f56ad..557853d72718 100644
--- a/include/linux/memory_group_manager.h
+++ b/include/linux/memory_group_manager.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -32,19 +32,9 @@ typedef int vm_fault_t;
 
 #define MEMORY_GROUP_MANAGER_NR_GROUPS (16)
 
-#define PTE_PBHA_SHIFT (59)
-#define PTE_PBHA_MASK ((uint64_t)0xf << PTE_PBHA_SHIFT)
-#define PTE_RES_BIT_MULTI_AS_SHIFT (63)
-#define PTE_FLAGS_NONE (0)
-#define PBHA_ID_DEFAULT (0)
-
 struct memory_group_manager_device;
 struct memory_group_manager_import_data;
 
-enum mgm_pte_flags {
-	MMA_VIOLATION = 0,
-};
-
 /**
  * struct memory_group_manager_ops - Callbacks for memory group manager
  *                                   operations
@@ -56,8 +46,6 @@ enum mgm_pte_flags {
  * @mgm_pte_to_original_pte:  Callback to get the original PTE entry as given
  *                            to mgm_update_gpu_pte
  * @mgm_vmf_insert_pfn_prot:  Callback to map a physical memory page for the CPU
- * @mgm_get_import_memory_cached_access_permitted: Callback to query if a given imported
- *                            memory is allowed to be accessed as cached or not by the GPU
  */
 struct memory_group_manager_ops {
 	/*
@@ -70,17 +58,13 @@ struct memory_group_manager_ops {
 	 *            0 .. MEMORY_GROUP_MANAGER_NR_GROUPS-1.
 	 * @gfp_mask: Bitmask of Get Free Page flags affecting allocator
 	 *            behavior.
-	 * @order:    Page order for physical page size.
-	 *            order = 0 refers to small pages
-	 *            order != 0 refers to 2 MB pages, so
-	 *            order = 9 (when small page size is 4KB,  2^9 *  4KB = 2 MB)
-	 *            order = 7 (when small page size is 16KB, 2^7 * 16KB = 2 MB)
-	 *            order = 5 (when small page size is 64KB, 2^5 * 64KB = 2 MB)
+	 * @order:    Page order for physical page size (order=0 means 4 KiB,
+	 *            order=9 means 2 MiB).
 	 *
 	 * Return: Pointer to allocated page, or NULL if allocation failed.
 	 */
-	struct page *(*mgm_alloc_page)(struct memory_group_manager_device *mgm_dev,
-				       unsigned int group_id, gfp_t gfp_mask, unsigned int order);
+	struct page *(*mgm_alloc_page)(struct memory_group_manager_device *mgm_dev, int group_id,
+				       gfp_t gfp_mask, unsigned int order);
 
 	/*
 	 * mgm_free_page - Free a physical memory page in a group
@@ -94,11 +78,10 @@ struct memory_group_manager_ops {
 	 *            memory that was allocated by calling the mgm_alloc_page
 	 *            method of the same memory pool with the same values of
 	 *            @group_id and @order.
-     * @order:    Page order for physical page size.
-     *            order = 0 refers to small pages
-     *            order != 0 refers to 2 MB pages.
+	 * @order:    Page order for physical page size (order=0 means 4 KiB,
+	 *            order=9 means 2 MiB).
 	 */
-	void (*mgm_free_page)(struct memory_group_manager_device *mgm_dev, unsigned int group_id,
+	void (*mgm_free_page)(struct memory_group_manager_device *mgm_dev, int group_id,
 			      struct page *page, unsigned int order);
 
 	/*
@@ -127,11 +110,6 @@ struct memory_group_manager_ops {
 	 * @group_id:  A physical memory group ID. The meaning of this is
 	 *             defined by the systems integrator. Its valid range is
 	 *             0 .. MEMORY_GROUP_MANAGER_NR_GROUPS-1.
-	 *
-	 * @pbha_id:   PBHA Overrride ID to encode into the PTE
-	 * @pte_flags: PTE related flags, defined in enum mgm_pte_flags
-	 *
-	 *
 	 * @mmu_level: The level of the page table entry in @ate.
 	 * @pte:       The page table entry to modify, in LPAE or AArch64 format
 	 *             (depending on the driver's configuration). This should be
@@ -141,14 +119,13 @@ struct memory_group_manager_ops {
 	 * This function allows the memory group manager to modify a GPU page
 	 * table entry before it is stored by the kbase module (controller
 	 * driver). It may set certain bits in the page table entry attributes
-	 * or modify the physical address, based on the physical memory group ID,
-	 * PBHA ID, PTE flags and/or additional data in struct memory_group_manager_device.
+	 * or modify the physical address, based on the physical memory group ID
+	 * and/or additional data in struct memory_group_manager_device.
 	 *
 	 * Return: A modified GPU page table entry to be stored in a page table.
 	 */
-	u64 (*mgm_update_gpu_pte)(struct memory_group_manager_device *mgm_dev,
-				  unsigned int group_id, unsigned int pbha_id,
-				  unsigned int pte_flags, int mmu_level, u64 pte);
+	u64 (*mgm_update_gpu_pte)(struct memory_group_manager_device *mgm_dev, int group_id,
+				  int mmu_level, u64 pte);
 
 	/*
 	 * mgm_pte_to_original_pte - Undo any modification done during mgm_update_gpu_pte()
@@ -168,8 +145,8 @@ struct memory_group_manager_ops {
 	 *
 	 * Return: PTE entry as originally specified to mgm_update_gpu_pte()
 	 */
-	u64 (*mgm_pte_to_original_pte)(struct memory_group_manager_device *mgm_dev,
-				       unsigned int group_id, int mmu_level, u64 pte);
+	u64 (*mgm_pte_to_original_pte)(struct memory_group_manager_device *mgm_dev, int group_id,
+				       int mmu_level, u64 pte);
 
 	/*
 	 * mgm_vmf_insert_pfn_prot - Map a physical page in a group for the CPU
@@ -193,23 +170,9 @@ struct memory_group_manager_ops {
 	 *         table entry was successfully installed.
 	 */
 	vm_fault_t (*mgm_vmf_insert_pfn_prot)(struct memory_group_manager_device *mgm_dev,
-					      unsigned int group_id, struct vm_area_struct *vma,
+					      int group_id, struct vm_area_struct *vma,
 					      unsigned long addr, unsigned long pfn,
 					      pgprot_t pgprot);
-
-	/*
-	 * mgm_get_import_memory_cached_access_permitted - Check if a given imported memory
-	 *                            is allowed to be accessed as cached or not by the GPU
-	 *
-	 * @mgm_dev:     The memory group manager through which the request
-	 *               is being made.
-	 * @import_data: Pointer to the data which describes imported memory.
-	 *
-	 * Return: true if cached access is permitted, false otherwise
-	 */
-	bool (*mgm_get_import_memory_cached_access_permitted)(
-		struct memory_group_manager_device *mgm_dev,
-		struct memory_group_manager_import_data *import_data);
 };
 
 /**
diff --git a/include/linux/priority_control_manager.h b/include/linux/priority_control_manager.h
index d82419ebf523..9f28b1b8582a 100644
--- a/include/linux/priority_control_manager.h
+++ b/include/linux/priority_control_manager.h
@@ -28,78 +28,32 @@
 
 struct priority_control_manager_device;
 
-/**
- * DOC: PCM notifier callback types
- *
- * ADD_PRIORITIZED_PROCESS - indicate that work items for this process should be
- *                           given priority over the work items from other
- *                           processes that were assigned the same static
- *                           priority level. Processes that would benefit from
- *                           being added to this list includes foreground
- *                           applications, as well as any other latency-sensitive
- *                           applications.
- *
- * REMOVE_PRIORITIZED_PROCESS - indicate that work items for this process
- *                              should no longer be prioritized over other work
- *                              items given the same static priority level.
- */
-#define ADD_PRIORITIZED_PROCESS 0
-#define REMOVE_PRIORITIZED_PROCESS 1
-
-/**
- * struct pcm_prioritized_process_notifier_data - change of prioritized process
- *                                                list passed to the callback
- *
- * @pid: PID of the process being added/removed
- */
-struct pcm_prioritized_process_notifier_data {
-	uint32_t pid;
-};
-
 /**
  * struct priority_control_manager_ops - Callbacks for priority control manager operations
  *
  * @pcm_scheduler_priority_check: Callback to check if scheduling priority level can be requested
- *                                pcm_dev: The priority control manager through which the
- *                                         request is being made.
- *                                task: The task struct of the process requesting the
- *                                      priority check.
- *                                requested_priority: The priority level being requested.
- *
- *                                The returned value will be:
- *                                The same as requested_priority if the process has permission to
- *                                use requested_priority.A lower priority value if the process does
- *                                not have permission to use requested_priority
- *
- *                                requested_priority has the following value range:
- *                                0-3 : Priority level, 0 being highest and 3 being lowest
- *
- *                                Return: The priority that would actually be given, could be lower
- *                                than requested_priority
- *
- * @pcm_prioritized_process_notifier_register: register a callback for changes to the
- *                                             list of prioritized processes
- *                                             pcm_dev: The priority control manager through
- *                                                      which the request is being made.
- *                                             nb: notifier block with callback function pointer
- *                                             On Success returns 0 otherwise -1
- *
- * @pcm_prioritized_process_notifier_unregister: unregister the callback for changes to the
- *                                               list of prioritized processes
- *                                               pcm_dev: The priority control manager through
- *                                                        which the request is being made.
- *                                               nb: notifier block which will be unregistered
- *                                               On Success returns 0 otherwise -1
  */
 struct priority_control_manager_ops {
+	/*
+	 * pcm_scheduler_priority_check: This function can be used to check what priority its work
+	 *                               would be treated as based on the requested_priority value.
+	 *
+	 * @pcm_dev:                     The priority control manager through which the request is
+	 *                               being made.
+	 * @task:                        The task struct of the process requesting the priority check.
+	 * @requested_priority:          The priority level being requested.
+	 *
+	 * The returned value will be:
+	 *   The same as requested_priority if the process has permission to use requested_priority
+	 *   A lower priority value if the process does not have permission to use requested_priority
+	 *
+	 * requested_priority has the following value range:
+	 *   0-3 : Priority level, 0 being highest and 3 being lowest
+	 *
+	 * Return: The priority that would actually be given, could be lower than requested_priority
+	 */
 	int (*pcm_scheduler_priority_check)(struct priority_control_manager_device *pcm_dev,
 					    struct task_struct *task, int requested_priority);
-
-	int (*pcm_prioritized_process_notifier_register)(
-		struct priority_control_manager_device *pcm_dev, struct notifier_block *nb);
-
-	int (*pcm_prioritized_process_notifier_unregister)(
-		struct priority_control_manager_device *pcm_dev, struct notifier_block *nb);
 };
 
 /**
diff --git a/include/linux/version_compat_defs.h b/include/linux/version_compat_defs.h
index d94a39ebbadc..3f46e852bdc9 100644
--- a/include/linux/version_compat_defs.h
+++ b/include/linux/version_compat_defs.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2022-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -25,10 +25,6 @@
 #include <linux/version.h>
 #include <linux/highmem.h>
 #include <linux/timer.h>
-#include <linux/iopoll.h>
-#include <linux/bitmap.h>
-#include <linux/math64.h>
-#include <linux/moduleparam.h>
 
 #if (KERNEL_VERSION(4, 4, 267) < LINUX_VERSION_CODE)
 #include <linux/overflow.h>
@@ -179,7 +175,6 @@ static inline void kbase_kunmap_atomic(void *address)
  */
 #define check_mul_overflow(a, b, d) __builtin_mul_overflow(a, b, d)
 #define check_add_overflow(a, b, d) __builtin_add_overflow(a, b, d)
-#define check_sub_overflow(a, b, d) __builtin_sub_overflow(a, b, d)
 #endif
 
 /*
@@ -193,7 +188,6 @@ static inline void kbase_kunmap_atomic(void *address)
 
 #define dma_fence fence
 #define dma_fence_ops fence_ops
-#define dma_fence_cb fence_cb
 #define dma_fence_context_alloc(a) fence_context_alloc(a)
 #define dma_fence_init(a, b, c, d, e) fence_init(a, b, c, d, e)
 #define dma_fence_get(a) fence_get(a)
@@ -303,222 +297,4 @@ static inline long kbase_pin_user_pages_remote(struct task_struct *tsk, struct m
 #define kbase_totalram_pages() totalram_pages()
 #endif /* KERNEL_VERSION(5, 0, 0) > LINUX_VERSION_CODE */
 
-#ifndef read_poll_timeout_atomic
-#define read_poll_timeout_atomic(op, val, cond, delay_us, timeout_us, delay_before_read, args...) \
-	({                                                                                        \
-		const u64 __timeout_us = (timeout_us);                                            \
-		s64 __left_ns = __timeout_us * NSEC_PER_USEC;                                     \
-		const unsigned long __delay_us = (delay_us);                                      \
-		const u64 __delay_ns = __delay_us * NSEC_PER_USEC;                                \
-		if (delay_before_read && __delay_us)                                              \
-			udelay(__delay_us);                                                       \
-		if (__timeout_us)                                                                 \
-			__left_ns -= __delay_ns;                                                  \
-		do {                                                                              \
-			(val) = op(args);                                                         \
-			if (__timeout_us) {                                                       \
-				if (__delay_us) {                                                 \
-					udelay(__delay_us);                                       \
-					__left_ns -= __delay_ns;                                  \
-				}                                                                 \
-				__left_ns--;                                                      \
-			}                                                                         \
-		} while (!(cond) && (!__timeout_us || (__left_ns > 0)));                          \
-		(cond) ? 0 : -ETIMEDOUT;                                                          \
-	})
-#endif
-
-#if (KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE)
-
-#define kbase_refcount_t atomic_t
-#define kbase_refcount_read(x) atomic_read(x)
-#define kbase_refcount_set(x, v) atomic_set(x, v)
-#define kbase_refcount_dec_and_test(x) atomic_dec_and_test(x)
-#define kbase_refcount_dec(x) atomic_dec(x)
-#define kbase_refcount_inc_not_zero(x) atomic_inc_not_zero(x)
-#define kbase_refcount_inc(x) atomic_inc(x)
-
-#else
-
-#include <linux/refcount.h>
-
-#define kbase_refcount_t refcount_t
-#define kbase_refcount_read(x) refcount_read(x)
-#define kbase_refcount_set(x, v) refcount_set(x, v)
-#define kbase_refcount_dec_and_test(x) refcount_dec_and_test(x)
-#define kbase_refcount_dec(x) refcount_dec(x)
-#define kbase_refcount_inc_not_zero(x) refcount_inc_not_zero(x)
-#define kbase_refcount_inc(x) refcount_inc(x)
-
-#endif /* (KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE) */
-
-#if (KERNEL_VERSION(4, 16, 0) > LINUX_VERSION_CODE)
-/* Null definition */
-#define ALLOW_ERROR_INJECTION(fname, err_type)
-#endif /* (KERNEL_VERSION(4, 16, 0) > LINUX_VERSION_CODE) */
-
-#if KERNEL_VERSION(6, 0, 0) > LINUX_VERSION_CODE
-#define KBASE_REGISTER_SHRINKER(reclaim, name, priv_data) register_shrinker(reclaim)
-
-/* clang-format off */
-#elif ((KERNEL_VERSION(6, 7, 0) > LINUX_VERSION_CODE) && \
-	!(defined(__ANDROID_COMMON_KERNEL__) && (KERNEL_VERSION(6, 6, 0) == LINUX_VERSION_CODE)))
-/* clang-format on */
-#define KBASE_REGISTER_SHRINKER(reclaim, name, priv_data) register_shrinker(reclaim, name)
-
-#else
-#define KBASE_REGISTER_SHRINKER(reclaim, name, priv_data) \
-	do {                                              \
-		reclaim->private_data = priv_data;        \
-		shrinker_register(reclaim);               \
-	} while (0)
-
-#endif /* KERNEL_VERSION(6, 0, 0) > LINUX_VERSION_CODE */
-/* clang-format off */
-#if ((KERNEL_VERSION(6, 7, 0) > LINUX_VERSION_CODE) && \
-	!(defined(__ANDROID_COMMON_KERNEL__) && (KERNEL_VERSION(6, 6, 0) == LINUX_VERSION_CODE)))
-/* clang-format on */
-#define KBASE_UNREGISTER_SHRINKER(reclaim) unregister_shrinker(&reclaim)
-#define KBASE_GET_KBASE_DATA_FROM_SHRINKER(s, type, var) container_of(s, type, var)
-#define DEFINE_KBASE_SHRINKER struct shrinker
-#define KBASE_INIT_RECLAIM(var, attr, name) (&((var)->attr))
-#define KBASE_SET_RECLAIM(var, attr, reclaim) ((var)->attr = (*reclaim))
-
-#else
-#define KBASE_UNREGISTER_SHRINKER(reclaim) shrinker_free(reclaim)
-#define KBASE_GET_KBASE_DATA_FROM_SHRINKER(s, type, var) s->private_data
-#define DEFINE_KBASE_SHRINKER struct shrinker *
-#define KBASE_SHRINKER_ALLOC(name) shrinker_alloc(0, name)
-#define KBASE_INIT_RECLAIM(var, attr, name) (KBASE_SHRINKER_ALLOC(name))
-#define KBASE_SET_RECLAIM(var, attr, reclaim) ((var)->attr = reclaim)
-
-#endif
-
-static inline int kbase_param_set_uint_minmax(const char *val, const struct kernel_param *kp,
-					      unsigned int min, unsigned int max)
-{
-#if (KERNEL_VERSION(5, 15, 0) > LINUX_VERSION_CODE)
-	uint uint_val;
-	int ret;
-
-	if (!val)
-		return -EINVAL;
-
-	ret = kstrtouint(val, 0, &uint_val);
-
-	if (ret == 0) {
-		if (uint_val < min || uint_val > max)
-			return -EINVAL;
-
-		*((uint *)kp->arg) = uint_val;
-	}
-
-	return ret;
-#else
-	return param_set_uint_minmax(val, kp, min, max);
-#endif
-}
-
-#if (KERNEL_VERSION(4, 20, 0) <= LINUX_VERSION_CODE)
-#include <linux/compiler_attributes.h>
-#endif
-#ifndef __maybe_unused
-#define __maybe_unused __attribute__((unused))
-#endif
-
-#if KERNEL_VERSION(5, 4, 103) <= LINUX_VERSION_CODE
-#define mali_sysfs_emit(buf, fmt, ...) sysfs_emit(buf, fmt, __VA_ARGS__)
-#else
-#define mali_sysfs_emit(buf, fmt, ...) scnprintf(buf, PAGE_SIZE, fmt, __VA_ARGS__)
-#endif
-
-#if KERNEL_VERSION(5, 10, 0) > LINUX_VERSION_CODE
-#include <linux/devfreq.h>
-#include <linux/of_platform.h>
-
-static inline struct devfreq *devfreq_get_devfreq_by_node(struct device_node *node)
-{
-	struct platform_device *pdev = of_find_device_by_node(node);
-
-	if (!pdev || !node)
-		return NULL;
-
-	return devfreq_get_devfreq_by_phandle(&pdev->dev, 0);
-}
-#endif
-
-#if (KERNEL_VERSION(5, 16, 0) <= LINUX_VERSION_CODE &&       \
-	KERNEL_VERSION(5, 18, 0) > LINUX_VERSION_CODE) ||       \
-	(KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE &&   \
-	KERNEL_VERSION(5, 15, 85) >= LINUX_VERSION_CODE) || \
-	(KERNEL_VERSION(5, 10, 200) >= LINUX_VERSION_CODE)
-/*
- * Kernel revisions
- *  - up to 5.10.200
- *  - between 5.11.0 and 5.15.85 inclusive
- *  - between 5.16.0 and 5.17.15 inclusive
- * do not provide an implementation of
- * size_add, size_sub and size_mul.
- * The implementations below provides
- * backward compatibility implementations of these functions.
- */
-
-static inline size_t __must_check size_mul(size_t factor1, size_t factor2)
-{
-	size_t ret_val;
-
-	if (check_mul_overflow(factor1, factor2, &ret_val))
-		return SIZE_MAX;
-	return ret_val;
-}
-
-static inline size_t __must_check size_add(size_t addend1, size_t addend2)
-{
-	size_t ret_val;
-
-	if (check_add_overflow(addend1, addend2, &ret_val))
-		return SIZE_MAX;
-	return ret_val;
-}
-
-static inline size_t __must_check size_sub(size_t minuend, size_t subtrahend)
-{
-	size_t ret_val;
-
-	if (minuend == SIZE_MAX || subtrahend == SIZE_MAX ||
-	    check_sub_overflow(minuend, subtrahend, &ret_val))
-		return SIZE_MAX;
-	return ret_val;
-}
-#endif
-
-#if KERNEL_VERSION(5, 5, 0) > LINUX_VERSION_CODE
-static inline unsigned long bitmap_get_value8(const unsigned long *map, unsigned long start)
-{
-	const size_t index = BIT_WORD(start);
-	const unsigned long offset = start % BITS_PER_LONG;
-
-	return (map[index] >> offset) & 0xFF;
-}
-
-static inline unsigned long find_next_clump8(unsigned long *clump, const unsigned long *addr,
-					     unsigned long size, unsigned long offset)
-{
-	offset = find_next_bit(addr, size, offset);
-	if (offset == size)
-		return size;
-
-	offset = round_down(offset, 8);
-	*clump = bitmap_get_value8(addr, offset);
-
-	return offset;
-}
-
-#define find_first_clump8(clump, bits, size) find_next_clump8((clump), (bits), (size), 0)
-
-#define for_each_set_clump8(start, clump, bits, size)                                 \
-	for ((start) = find_first_clump8(&(clump), (bits), (size)); (start) < (size); \
-	     (start) = find_next_clump8(&(clump), (bits), (size), (start) + 8))
-#endif
-
 #endif /* _VERSION_COMPAT_DEFS_H_ */
diff --git a/include/uapi/base/arm/dma_buf_test_exporter/dma-buf-test-exporter.h b/include/uapi/base/arm/dma_buf_test_exporter/dma-buf-test-exporter.h
index 56a16e11c4c0..46627c416baa 100644
--- a/include/uapi/base/arm/dma_buf_test_exporter/dma-buf-test-exporter.h
+++ b/include/uapi/base/arm/dma_buf_test_exporter/dma-buf-test-exporter.h
@@ -62,7 +62,7 @@ struct dma_buf_te_ioctl_set_failing {
 
 struct dma_buf_te_ioctl_fill {
 	int fd;
-	int value;
+	unsigned int value;
 };
 
 #define DMA_BUF_TE_IOCTL_BASE 'E'
diff --git a/include/uapi/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.h b/include/uapi/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.h
index b80817f04255..b45e32fc3d33 100644
--- a/include/uapi/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.h
+++ b/include/uapi/gpu/arm/bifrost/backend/gpu/mali_kbase_model_dummy.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2021-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2021-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -36,7 +36,7 @@
 #endif /* MALI_USE_CSF */
 #define KBASE_DUMMY_MODEL_COUNTERS_PER_BIT (4)
 #define KBASE_DUMMY_MODEL_COUNTER_ENABLED(enable_mask, ctr_idx) \
-	(enable_mask & (1U << (ctr_idx / KBASE_DUMMY_MODEL_COUNTERS_PER_BIT)))
+	(enable_mask & (1 << (ctr_idx / KBASE_DUMMY_MODEL_COUNTERS_PER_BIT)))
 
 #define KBASE_DUMMY_MODEL_HEADERS_PER_BLOCK 4
 #define KBASE_DUMMY_MODEL_COUNTERS_PER_BLOCK KBASE_DUMMY_MODEL_COUNTER_PER_CORE
diff --git a/include/uapi/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.h b/include/uapi/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.h
index 7e56fd71f84e..c83cedd6a775 100644
--- a/include/uapi/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.h
+++ b/include/uapi/gpu/arm/bifrost/backend/gpu/mali_kbase_model_linux.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -30,6 +30,7 @@
 #define MODEL_LINUX_JOB_IRQ (0x1 << 0)
 #define MODEL_LINUX_GPU_IRQ (0x1 << 1)
 #define MODEL_LINUX_MMU_IRQ (0x1 << 2)
+
 #define MODEL_LINUX_IRQ_MASK (MODEL_LINUX_JOB_IRQ | MODEL_LINUX_GPU_IRQ | MODEL_LINUX_MMU_IRQ)
 
 #endif /* _UAPI_KBASE_MODEL_LINUX_H_ */
diff --git a/include/uapi/gpu/arm/bifrost/csf/mali_base_csf_kernel.h b/include/uapi/gpu/arm/bifrost/csf/mali_base_csf_kernel.h
index 2b2fd1dd7bc8..013924887142 100644
--- a/include/uapi/gpu/arm/bifrost/csf/mali_base_csf_kernel.h
+++ b/include/uapi/gpu/arm/bifrost/csf/mali_base_csf_kernel.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -46,11 +46,7 @@
  */
 #define BASE_MEM_CSF_EVENT ((base_mem_alloc_flags)1 << 19)
 
-/* Unused bit for CSF, only used in JM for BASE_MEM_TILER_ALIGN_TOP */
-#define BASE_MEM_UNUSED_BIT_20 ((base_mem_alloc_flags)1 << 20)
-
-/* Unused bit for CSF, only used in JM for BASE_MEM_FLAG_MAP_FIXED */
-#define BASE_MEM_UNUSED_BIT_27 ((base_mem_alloc_flags)1 << 27)
+#define BASE_MEM_RESERVED_BIT_20 ((base_mem_alloc_flags)1 << 20)
 
 /* Must be FIXABLE memory: its GPU VA will be determined at a later point,
  * at which time it will be at a fixed GPU VA.
@@ -61,21 +57,14 @@
  * must be less than BASE_MEM_FLAGS_NR_BITS !!!
  */
 
-/* A mask of all the flags which are only valid within kbase,
- * and may not be passed to/from user space.
+/* A mask of all the flags which are only valid for allocations within kbase,
+ * and may not be passed from user space.
  */
 #define BASEP_MEM_FLAGS_KERNEL_ONLY (BASEP_MEM_PERMANENT_KERNEL_MAPPING | BASEP_MEM_NO_USER_FREE)
 
-/* A mask of flags that, when provied, cause other flags to be
- * enabled but are not enabled themselves
+/* A mask of all currently reserved flags
  */
-#define BASE_MEM_FLAGS_ACTION_MODIFIERS (BASE_MEM_COHERENT_SYSTEM_REQUIRED | BASE_MEM_IMPORT_SHARED)
-
-/* A mask of all currently reserved flags */
-#define BASE_MEM_FLAGS_RESERVED ((base_mem_alloc_flags)0)
-
-/* A mask of all bits that are not used by a flag on CSF */
-#define BASE_MEM_FLAGS_UNUSED (BASE_MEM_UNUSED_BIT_20 | BASE_MEM_UNUSED_BIT_27)
+#define BASE_MEM_FLAGS_RESERVED BASE_MEM_RESERVED_BIT_20
 
 /* Special base mem handles specific to CSF.
  */
@@ -107,10 +96,10 @@
 /* Flags for base tracepoint specific to CSF */
 
 /* Enable KBase tracepoints for CSF builds */
-#define BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS (1U << 2)
+#define BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS (1 << 2)
 
 /* Enable additional CSF Firmware side tracepoints */
-#define BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS (1U << 3)
+#define BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS (1 << 3)
 
 #define BASE_TLSTREAM_FLAGS_MASK                                                        \
 	(BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS | BASE_TLSTREAM_JOB_DUMPING_ENABLED | \
@@ -152,7 +141,7 @@
 #define BASE_CSF_EXCEPTION_HANDLER_FLAGS_MASK (BASE_CSF_TILER_OOM_EXCEPTION_FLAG)
 
 /* Initial value for LATEST_FLUSH register */
-#define POWER_DOWN_LATEST_FLUSH_VALUE ((__u32)1)
+#define POWER_DOWN_LATEST_FLUSH_VALUE ((uint32_t)1)
 
 /**
  * enum base_kcpu_command_type - Kernel CPU queue command type.
@@ -168,7 +157,7 @@
  * @BASE_KCPU_COMMAND_TYPE_JIT_ALLOC:          jit_alloc,
  * @BASE_KCPU_COMMAND_TYPE_JIT_FREE:           jit_free,
  * @BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND:      group_suspend,
- * @BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER:      error_barrier
+ * @BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER:      error_barrier,
  */
 enum base_kcpu_command_type {
 	BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL,
@@ -183,7 +172,7 @@ enum base_kcpu_command_type {
 	BASE_KCPU_COMMAND_TYPE_JIT_ALLOC,
 	BASE_KCPU_COMMAND_TYPE_JIT_FREE,
 	BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND,
-	BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER
+	BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER,
 };
 
 /**
@@ -485,26 +474,7 @@ struct base_gpu_queue_error_fatal_payload {
 };
 
 /**
- * struct base_gpu_queue_error_fault_payload - Recoverable fault
- *        error information related to GPU command queue.
- *
- * @sideband:     Additional information about this recoverable fault.
- * @status:       Recoverable fault information.
- *                This consists of exception type (least significant byte) and
- *                data (remaining bytes). One example of exception type is
- *                INSTR_INVALID_PC (0x50).
- * @csi_index:    Index of the CSF interface the queue is bound to.
- * @padding:      Padding to make multiple of 64bits
- */
-struct base_gpu_queue_error_fault_payload {
-	__u64 sideband;
-	__u32 status;
-	__u8 csi_index;
-	__u8 padding[3];
-};
-
-/**
- * enum base_gpu_queue_group_error_type - GPU error type.
+ * enum base_gpu_queue_group_error_type - GPU Fatal error type.
  *
  * @BASE_GPU_QUEUE_GROUP_ERROR_FATAL:       Fatal error associated with GPU
  *                                          command queue group.
@@ -514,9 +484,7 @@ struct base_gpu_queue_error_fault_payload {
  *                                          progress timeout.
  * @BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM: Fatal error due to running out
  *                                             of tiler heap memory.
- * @BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FAULT: Fault error associated with GPU
- *                                          command queue.
- * @BASE_GPU_QUEUE_GROUP_ERROR_FATAL_COUNT: The number of GPU error types
+ * @BASE_GPU_QUEUE_GROUP_ERROR_FATAL_COUNT: The number of fatal error types
  *
  * This type is used for &struct_base_gpu_queue_group_error.error_type.
  */
@@ -525,7 +493,6 @@ enum base_gpu_queue_group_error_type {
 	BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL,
 	BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT,
 	BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM,
-	BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FAULT,
 	BASE_GPU_QUEUE_GROUP_ERROR_FATAL_COUNT
 };
 
@@ -545,7 +512,6 @@ struct base_gpu_queue_group_error {
 	union {
 		struct base_gpu_queue_group_error_fatal_payload fatal_group;
 		struct base_gpu_queue_error_fatal_payload fatal_queue;
-		struct base_gpu_queue_error_fault_payload fault_queue;
 	} payload;
 };
 
@@ -553,7 +519,8 @@ struct base_gpu_queue_group_error {
  * enum base_csf_notification_type - Notification type
  *
  * @BASE_CSF_NOTIFICATION_EVENT:                 Notification with kernel event
- * @BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR: Notification with GPU error
+ * @BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR: Notification with GPU fatal
+ *                                               error
  * @BASE_CSF_NOTIFICATION_CPU_QUEUE_DUMP:        Notification with dumping cpu
  *                                               queue
  * @BASE_CSF_NOTIFICATION_COUNT:                 The number of notification type
diff --git a/include/uapi/gpu/arm/bifrost/csf/mali_kbase_csf_ioctl.h b/include/uapi/gpu/arm/bifrost/csf/mali_kbase_csf_ioctl.h
index 2b5b8b25fc2c..9db2146e2fd5 100644
--- a/include/uapi/gpu/arm/bifrost/csf/mali_kbase_csf_ioctl.h
+++ b/include/uapi/gpu/arm/bifrost/csf/mali_kbase_csf_ioctl.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -95,35 +95,10 @@
  * 1.22:
  * - Add comp_pri_threshold and comp_pri_ratio attributes to
  *   kbase_ioctl_cs_queue_group_create.
- * - Made the BASE_MEM_DONT_NEED memory flag queryable.
- * 1.23:
- * - Disallows changing the sharability on the GPU of imported dma-bufs to
- *   BASE_MEM_COHERENT_SYSTEM using KBASE_IOCTL_MEM_FLAGS_CHANGE.
- * 1.24:
- * - Implement full block state support for hardware counters.
- * 1.25:
- * - Add support for CS_FAULT reporting to userspace
- * 1.26:
- * - Made the BASE_MEM_IMPORT_SYNC_ON_MAP_UNMAP and BASE_MEM_KERNEL_SYNC memory
- *   flags queryable.
- * 1.27:
- * - Implement support for HWC block state availability.
- * 1.28:
- * - Made the SAME_VA memory flag queryable.
- * 1.29:
- * - Re-allow child process to do supported file operations (like mmap, ioctl
- *   read, poll) on the file descriptor of mali device that was inherited
- *   from the parent process.
- * 1.30:
- * - Implement support for setting GPU Timestamp Offset register.
- * 1.31:
- * - Reject non-protected allocations containing the BASE_MEM_PROTECTED memory flag.
- * - Reject allocations containing the BASE_MEM_DONT_NEED memory flag (it is only settable).
- * - Reject allocations containing the BASE_MEM_UNUSED_BIT_xx memory flags.
  */
 
 #define BASE_UK_VERSION_MAJOR 1
-#define BASE_UK_VERSION_MINOR 31
+#define BASE_UK_VERSION_MINOR 22
 
 /**
  * struct kbase_ioctl_version_check - Check version compatibility between
@@ -360,8 +335,6 @@ union kbase_ioctl_cs_queue_group_create_1_18 {
  * @in.csi_handlers:  Flags to signal that the application intends to use CSI
  *                    exception handlers in some linear buffers to deal with
  *                    the given exception types.
- * @in.cs_fault_report_enable:  Flag to indicate reporting of CS_FAULTs
- *                    to userspace.
  * @in.padding:       Currently unused, must be zero
  * @out:              Output parameters
  * @out.group_handle: Handle of a newly created queue group.
@@ -382,8 +355,7 @@ union kbase_ioctl_cs_queue_group_create {
 		/**
 		 * @in.reserved:   Reserved, currently unused, must be zero.
 		 */
-		__u8 reserved;
-		__u8 cs_fault_report_enable;
+		__u16 reserved;
 		/**
 		 * @in.dvs_buf: buffer for deferred vertex shader
 		 */
@@ -503,7 +475,7 @@ union kbase_ioctl_cs_tiler_heap_init {
 
 /**
  * union kbase_ioctl_cs_tiler_heap_init_1_13 - Initialize chunked tiler memory heap,
- *                                             earlier version up to 1.13
+ *                                             earlier version upto 1.13
  * @in:                Input parameters
  * @in.chunk_size:     Size of each chunk.
  * @in.initial_chunks: Initial number of chunks that heap will be created with.
@@ -660,22 +632,6 @@ union kbase_ioctl_read_user_page {
 
 #define KBASE_IOCTL_READ_USER_PAGE _IOWR(KBASE_IOCTL_TYPE, 60, union kbase_ioctl_read_user_page)
 
-/**
- * struct kbase_ioctl_queue_group_clear_faults - Re-enable CS FAULT reporting for the GPU queues
- *
- * @addr: CPU VA to an array of GPU VAs of the buffers backing the queues
- * @nr_queues: Number of queues in the array
- * @padding: Padding to round up to a multiple of 8 bytes, must be zero
- */
-struct kbase_ioctl_queue_group_clear_faults {
-	__u64 addr;
-	__u32 nr_queues;
-	__u8 padding[4];
-};
-
-#define KBASE_IOCTL_QUEUE_GROUP_CLEAR_FAULTS \
-	_IOW(KBASE_IOCTL_TYPE, 61, struct kbase_ioctl_queue_group_clear_faults)
-
 /***************
  * test ioctls *
  ***************/
diff --git a/include/uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_coherency.h b/include/uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_coherency.h
index ee64184e2794..de392a5c506f 100644
--- a/include/uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_coherency.h
+++ b/include/uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_coherency.h
@@ -22,9 +22,9 @@
 #ifndef _UAPI_KBASE_GPU_COHERENCY_H_
 #define _UAPI_KBASE_GPU_COHERENCY_H_
 
-#define COHERENCY_ACE_LITE 0U
-#define COHERENCY_ACE 1U
-#define COHERENCY_NONE 31U
-#define COHERENCY_FEATURE_BIT(x) (1U << (x))
+#define COHERENCY_ACE_LITE 0
+#define COHERENCY_ACE 1
+#define COHERENCY_NONE 31
+#define COHERENCY_FEATURE_BIT(x) (1 << (x))
 
 #endif /* _UAPI_KBASE_GPU_COHERENCY_H_ */
diff --git a/include/uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h b/include/uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h
index d4d12aed780d..d3478546e244 100644
--- a/include/uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h
+++ b/include/uapi/gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2015-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2015-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
diff --git a/include/uapi/gpu/arm/bifrost/jm/mali_base_jm_kernel.h b/include/uapi/gpu/arm/bifrost/jm/mali_base_jm_kernel.h
index fad61299b1c1..9478334ce667 100644
--- a/include/uapi/gpu/arm/bifrost/jm/mali_base_jm_kernel.h
+++ b/include/uapi/gpu/arm/bifrost/jm/mali_base_jm_kernel.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2019-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -30,11 +30,15 @@
  * See base_mem_alloc_flags.
  */
 
-/* Unused bit for JM, only used in CSF for BASE_MEM_FIXED */
-#define BASE_MEM_UNUSED_BIT_8 ((base_mem_alloc_flags)1 << 8)
+/* Used as BASE_MEM_FIXED in other backends */
+#define BASE_MEM_RESERVED_BIT_8 ((base_mem_alloc_flags)1 << 8)
 
-/* Unused bit for JM, only used in CSF for BASE_CSF_EVENT */
-#define BASE_MEM_UNUSED_BIT_19 ((base_mem_alloc_flags)1 << 19)
+/**
+ * BASE_MEM_RESERVED_BIT_19 - Bit 19 is reserved.
+ *
+ * Do not remove, use the next unreserved bit for new flags
+ */
+#define BASE_MEM_RESERVED_BIT_19 ((base_mem_alloc_flags)1 << 19)
 
 /**
  * BASE_MEM_TILER_ALIGN_TOP - Memory starting from the end of the initial commit is aligned
@@ -53,23 +57,16 @@
  * must be less than BASE_MEM_FLAGS_NR_BITS !!!
  */
 
-/* A mask of all the flags which are only valid within kbase,
- * and may not be passed to/from user space.
+/* A mask of all the flags which are only valid for allocations within kbase,
+ * and may not be passed from user space.
  */
 #define BASEP_MEM_FLAGS_KERNEL_ONLY                                                              \
 	(BASEP_MEM_PERMANENT_KERNEL_MAPPING | BASEP_MEM_NO_USER_FREE | BASE_MEM_FLAG_MAP_FIXED | \
 	 BASEP_MEM_PERFORM_JIT_TRIM)
 
-/* A mask of flags that, when provied, cause other flags to be
- * enabled but are not enabled themselves
+/* A mask of all currently reserved flags
  */
-#define BASE_MEM_FLAGS_ACTION_MODIFIERS (BASE_MEM_COHERENT_SYSTEM_REQUIRED | BASE_MEM_IMPORT_SHARED)
-
-/* A mask of all currently reserved flags */
-#define BASE_MEM_FLAGS_RESERVED ((base_mem_alloc_flags)0)
-
-/* A mask of all bits that are not used by a flag on JM */
-#define BASE_MEM_FLAGS_UNUSED (BASE_MEM_UNUSED_BIT_8 | BASE_MEM_UNUSED_BIT_19)
+#define BASE_MEM_FLAGS_RESERVED (BASE_MEM_RESERVED_BIT_8 | BASE_MEM_RESERVED_BIT_19)
 
 /* Similar to BASE_MEM_TILER_ALIGN_TOP, memory starting from the end of the
  * initial commit is aligned to 'extension' pages, where 'extension' must be a power
@@ -122,6 +119,10 @@
  */
 #define BASE_JD_ATOM_COUNT 256
 
+/* Maximum number of concurrent render passes.
+ */
+#define BASE_JD_RP_COUNT (256)
+
 /* Set/reset values for a software event */
 #define BASE_JD_SOFT_EVENT_SET ((unsigned char)1)
 #define BASE_JD_SOFT_EVENT_RESET ((unsigned char)0)
@@ -361,6 +362,40 @@ typedef __u32 base_jd_core_req;
  */
 #define BASE_JD_REQ_JOB_SLOT ((base_jd_core_req)1 << 17)
 
+/* SW-only requirement: The atom is the start of a renderpass.
+ *
+ * If this bit is set then the job chain will be soft-stopped if it causes the
+ * GPU to write beyond the end of the physical pages backing the tiler heap, and
+ * committing more memory to the heap would exceed an internal threshold. It may
+ * be resumed after running one of the job chains attached to an atom with
+ * BASE_JD_REQ_END_RENDERPASS set and the same renderpass ID. It may be
+ * resumed multiple times until it completes without memory usage exceeding the
+ * threshold.
+ *
+ * Usually used with BASE_JD_REQ_T.
+ */
+#define BASE_JD_REQ_START_RENDERPASS ((base_jd_core_req)1 << 18)
+
+/* SW-only requirement: The atom is the end of a renderpass.
+ *
+ * If this bit is set then the atom incorporates the CPU address of a
+ * base_jd_fragment object instead of the GPU address of a job chain.
+ *
+ * Which job chain is run depends upon whether the atom with the same renderpass
+ * ID and the BASE_JD_REQ_START_RENDERPASS bit set completed normally or
+ * was soft-stopped when it exceeded an upper threshold for tiler heap memory
+ * usage.
+ *
+ * It also depends upon whether one of the job chains attached to the atom has
+ * already been run as part of the same renderpass (in which case it would have
+ * written unresolved multisampled and otherwise-discarded output to temporary
+ * buffers that need to be read back). The job chain for doing a forced read and
+ * forced write (from/to temporary buffers) is run as many times as necessary.
+ *
+ * Usually used with BASE_JD_REQ_FS.
+ */
+#define BASE_JD_REQ_END_RENDERPASS ((base_jd_core_req)1 << 19)
+
 /* SW-only requirement: The atom needs to run on a limited core mask affinity.
  *
  * If this bit is set then the kbase_context.limited_core_mask will be applied
@@ -376,6 +411,7 @@ typedef __u32 base_jd_core_req;
 	   BASE_JD_REQ_EVENT_COALESCE | BASE_JD_REQ_COHERENT_GROUP |                          \
 	   BASE_JD_REQ_SPECIFIC_COHERENT_GROUP | BASE_JD_REQ_FS_AFBC | BASE_JD_REQ_PERMON |   \
 	   BASE_JD_REQ_SKIP_CACHE_START | BASE_JD_REQ_SKIP_CACHE_END | BASE_JD_REQ_JOB_SLOT | \
+	   BASE_JD_REQ_START_RENDERPASS | BASE_JD_REQ_END_RENDERPASS |                        \
 	   BASE_JD_REQ_LIMITED_CORE_MASK))
 
 /* Mask of all bits in base_jd_core_req that control the type of the atom.
@@ -434,6 +470,62 @@ struct base_dependency {
 	base_jd_dep_type dependency_type;
 };
 
+/**
+ * struct base_jd_fragment - Set of GPU fragment job chains used for rendering.
+ *
+ * @norm_read_norm_write: Job chain for full rendering.
+ *                        GPU address of a fragment job chain to render in the
+ *                        circumstance where the tiler job chain did not exceed
+ *                        its memory usage threshold and no fragment job chain
+ *                        was previously run for the same renderpass.
+ *                        It is used no more than once per renderpass.
+ * @norm_read_forced_write: Job chain for starting incremental
+ *                          rendering.
+ *                          GPU address of a fragment job chain to render in
+ *                          the circumstance where the tiler job chain exceeded
+ *                          its memory usage threshold for the first time and
+ *                          no fragment job chain was previously run for the
+ *                          same renderpass.
+ *                          Writes unresolved multisampled and normally-
+ *                          discarded output to temporary buffers that must be
+ *                          read back by a subsequent forced_read job chain
+ *                          before the renderpass is complete.
+ *                          It is used no more than once per renderpass.
+ * @forced_read_forced_write: Job chain for continuing incremental
+ *                            rendering.
+ *                            GPU address of a fragment job chain to render in
+ *                            the circumstance where the tiler job chain
+ *                            exceeded its memory usage threshold again
+ *                            and a fragment job chain was previously run for
+ *                            the same renderpass.
+ *                            Reads unresolved multisampled and
+ *                            normally-discarded output from temporary buffers
+ *                            written by a previous forced_write job chain and
+ *                            writes the same to temporary buffers again.
+ *                            It is used as many times as required until
+ *                            rendering completes.
+ * @forced_read_norm_write: Job chain for ending incremental rendering.
+ *                          GPU address of a fragment job chain to render in the
+ *                          circumstance where the tiler job chain did not
+ *                          exceed its memory usage threshold this time and a
+ *                          fragment job chain was previously run for the same
+ *                          renderpass.
+ *                          Reads unresolved multisampled and normally-discarded
+ *                          output from temporary buffers written by a previous
+ *                          forced_write job chain in order to complete a
+ *                          renderpass.
+ *                          It is used no more than once per renderpass.
+ *
+ * This structure is referenced by the main atom structure if
+ * BASE_JD_REQ_END_RENDERPASS is set in the base_jd_core_req.
+ */
+struct base_jd_fragment {
+	__u64 norm_read_norm_write;
+	__u64 norm_read_forced_write;
+	__u64 forced_read_forced_write;
+	__u64 forced_read_norm_write;
+};
+
 /**
  * typedef base_jd_prio - Base Atom priority.
  *
@@ -498,7 +590,9 @@ typedef __u8 base_jd_prio;
  * struct base_jd_atom_v2 - Node of a dependency graph used to submit a
  *                          GPU job chain or soft-job to the kernel driver.
  *
- * @jc:            GPU address of a job chain.
+ * @jc:            GPU address of a job chain or (if BASE_JD_REQ_END_RENDERPASS
+ *                 is set in the base_jd_core_req) the CPU address of a
+ *                 base_jd_fragment object.
  * @udata:         User data.
  * @extres_list:   List of external resources.
  * @nr_extres:     Number of external resources or JIT allocations.
@@ -517,6 +611,9 @@ typedef __u8 base_jd_prio;
  *                 specified.
  * @jobslot:       Job slot to use when BASE_JD_REQ_JOB_SLOT is specified.
  * @core_req:      Core requirements.
+ * @renderpass_id: Renderpass identifier used to associate an atom that has
+ *                 BASE_JD_REQ_START_RENDERPASS set in its core requirements
+ *                 with an atom that has BASE_JD_REQ_END_RENDERPASS set.
  * @padding:       Unused. Must be zero.
  *
  * This structure has changed since UK 10.2 for which base_jd_core_req was a
@@ -544,7 +641,8 @@ struct base_jd_atom_v2 {
 	__u8 device_nr;
 	__u8 jobslot;
 	base_jd_core_req core_req;
-	__u8 padding[8];
+	__u8 renderpass_id;
+	__u8 padding[7];
 };
 
 /**
@@ -552,7 +650,9 @@ struct base_jd_atom_v2 {
  *                          at the beginning.
  *
  * @seq_nr:        Sequence number of logical grouping of atoms.
- * @jc:            GPU address of a job chain.
+ * @jc:            GPU address of a job chain or (if BASE_JD_REQ_END_RENDERPASS
+ *                 is set in the base_jd_core_req) the CPU address of a
+ *                 base_jd_fragment object.
  * @udata:         User data.
  * @extres_list:   List of external resources.
  * @nr_extres:     Number of external resources or JIT allocations.
@@ -734,6 +834,11 @@ enum {
  * @BASE_JD_EVENT_REMOVED_FROM_NEXT: raised when an atom that was configured in
  *                                   the GPU has to be retried (but it has not
  *                                   started) due to e.g., GPU reset
+ * @BASE_JD_EVENT_END_RP_DONE: this is used for incremental rendering to signal
+ *                             the completion of a renderpass. This value
+ *                             shouldn't be returned to userspace but I haven't
+ *                             seen where it is reset back to JD_EVENT_DONE.
+ *
  * HW and low-level SW events are represented by event codes.
  * The status of jobs which succeeded are also represented by
  * an event code (see @BASE_JD_EVENT_DONE).
@@ -832,6 +937,8 @@ enum base_jd_event_code {
 	BASE_JD_EVENT_RANGE_KERNEL_ONLY_START = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_KERNEL | 0x000,
 	BASE_JD_EVENT_REMOVED_FROM_NEXT = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_KERNEL |
 					  BASE_JD_SW_EVENT_JOB | 0x000,
+	BASE_JD_EVENT_END_RP_DONE = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_KERNEL |
+				    BASE_JD_SW_EVENT_JOB | 0x001,
 
 	BASE_JD_EVENT_RANGE_KERNEL_ONLY_END = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_KERNEL |
 					      BASE_JD_SW_EVENT_RESERVED | 0x3FF
diff --git a/include/uapi/gpu/arm/bifrost/jm/mali_kbase_jm_ioctl.h b/include/uapi/gpu/arm/bifrost/jm/mali_kbase_jm_ioctl.h
index 34da87860ee2..1827d6ec4e1b 100644
--- a/include/uapi/gpu/arm/bifrost/jm/mali_kbase_jm_ioctl.h
+++ b/include/uapi/gpu/arm/bifrost/jm/mali_kbase_jm_ioctl.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2020-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2020-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -149,31 +149,10 @@
  *   from the parent process.
  * 11.40:
  * - Remove KBASE_IOCTL_HWCNT_READER_SETUP and KBASE_HWCNT_READER_* ioctls.
- * - Made the BASE_MEM_DONT_NEED memory flag queryable.
- * 11.41:
- * - Disallows changing the sharability on the GPU of imported dma-bufs to
- *   BASE_MEM_COHERENT_SYSTEM using KBASE_IOCTL_MEM_FLAGS_CHANGE.
- * 11.42:
- * - Implement full block state support for hardware counters.
- * 11.43:
- * - Made the BASE_MEM_IMPORT_SYNC_ON_MAP_UNMAP and BASE_MEM_KERNEL_SYNC memory
- *   flags queryable.
- * 11.44:
- * - Made the SAME_VA memory flag queryable.
- * 11.45:
- * - Re-allow child process to do supported file operations (like mmap, ioctl
- *   read, poll) on the file descriptor of mali device that was inherited
- *   from the parent process.
- * 11.46:
- * - Remove renderpass_id from base_jd_atom_v2 to deprecate support for JM Incremental Rendering
- * 11.47:
- * - Reject non-protected allocations containing the BASE_MEM_PROTECTED memory flag.
- * - Reject allocations containing the BASE_MEM_DONT_NEED memory flag (it is only settable).
- * - Reject allocations containing the BASE_MEM_UNUSED_BIT_xx memory flags.
-  */
+ */
 
 #define BASE_UK_VERSION_MAJOR 11
-#define BASE_UK_VERSION_MINOR 47
+#define BASE_UK_VERSION_MINOR 40
 
 /**
  * struct kbase_ioctl_version_check - Check version compatibility between
diff --git a/include/uapi/gpu/arm/bifrost/mali_base_common_kernel.h b/include/uapi/gpu/arm/bifrost/mali_base_common_kernel.h
index bbbee900415e..82e651f67b71 100644
--- a/include/uapi/gpu/arm/bifrost/mali_base_common_kernel.h
+++ b/include/uapi/gpu/arm/bifrost/mali_base_common_kernel.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2022-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2022-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -146,6 +146,7 @@ struct base_mem_handle {
  */
 #define BASE_MEM_IMPORT_SYNC_ON_MAP_UNMAP ((base_mem_alloc_flags)1 << 26)
 
+/* OUT */
 /* Kernel side cache sync ops required */
 #define BASE_MEM_KERNEL_SYNC ((base_mem_alloc_flags)1 << 28)
 
@@ -155,10 +156,12 @@ struct base_mem_handle {
  */
 #define BASE_MEM_FLAGS_NR_BITS 30
 
-/* A mask for all bits that are output from kbase, but never input. */
+/* A mask for all output bits, excluding IN/OUT bits.
+ */
 #define BASE_MEM_FLAGS_OUTPUT_MASK BASE_MEM_NEED_MMAP
 
-/* A mask for all bits that can be input to kbase. */
+/* A mask for all input bits, including IN/OUT bits.
+ */
 #define BASE_MEM_FLAGS_INPUT_MASK \
 	(((1 << BASE_MEM_FLAGS_NR_BITS) - 1) & ~BASE_MEM_FLAGS_OUTPUT_MASK)
 
@@ -218,11 +221,11 @@ typedef __u32 base_context_create_flags;
 /* Enable additional tracepoints for latency measurements (TL_ATOM_READY,
  * TL_ATOM_DONE, TL_ATOM_PRIO_CHANGE, TL_ATOM_EVENT_POST)
  */
-#define BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS (1U << 0)
+#define BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS (1 << 0)
 
 /* Indicate that job dumping is enabled. This could affect certain timers
  * to account for the performance impact.
  */
-#define BASE_TLSTREAM_JOB_DUMPING_ENABLED (1U << 1)
+#define BASE_TLSTREAM_JOB_DUMPING_ENABLED (1 << 1)
 
 #endif /* _UAPI_BASE_COMMON_KERNEL_H_ */
diff --git a/include/uapi/gpu/arm/bifrost/mali_base_kernel.h b/include/uapi/gpu/arm/bifrost/mali_base_kernel.h
index 9e7294970efb..8e507f0f14aa 100644
--- a/include/uapi/gpu/arm/bifrost/mali_base_kernel.h
+++ b/include/uapi/gpu/arm/bifrost/mali_base_kernel.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2010-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -32,26 +32,20 @@
 #include "gpu/mali_kbase_gpu_id.h"
 #include "gpu/mali_kbase_gpu_coherency.h"
 
-#ifdef __KERNEL__
-#include <linux/mm.h>
-
 #if defined(PAGE_MASK) && defined(PAGE_SHIFT)
 #define LOCAL_PAGE_SHIFT PAGE_SHIFT
 #define LOCAL_PAGE_LSB ~PAGE_MASK
 #else
-#error "Missing kernel definitions: PAGE_MASK, PAGE_SHIFT"
+#ifndef OSU_CONFIG_CPU_PAGE_SIZE_LOG2
+#define OSU_CONFIG_CPU_PAGE_SIZE_LOG2 12
 #endif
 
+#if defined(OSU_CONFIG_CPU_PAGE_SIZE_LOG2)
+#define LOCAL_PAGE_SHIFT OSU_CONFIG_CPU_PAGE_SIZE_LOG2
+#define LOCAL_PAGE_LSB ((1ul << OSU_CONFIG_CPU_PAGE_SIZE_LOG2) - 1)
 #else
-
-#if defined(MALI_PAGE_SIZE_AGNOSTIC)
-#define LOCAL_PAGE_SHIFT (__builtin_ctz((unsigned int)sysconf(_SC_PAGESIZE)))
-#else
-#define LOCAL_PAGE_SHIFT 12
+#error Failed to find page size
 #endif
-
-#define LOCAL_PAGE_LSB ((1ul << LOCAL_PAGE_SHIFT) - 1)
-
 #endif
 
 /* Physical memory group ID for normal usage.
@@ -74,7 +68,7 @@
  * More flags can be added to this list, as long as they don't clash
  * (see BASE_MEM_FLAGS_NR_BITS for the number of the first free bit).
  */
-typedef __u64 base_mem_alloc_flags;
+typedef __u32 base_mem_alloc_flags;
 
 #define BASE_MEM_FLAGS_MODIFIABLE_NATIVE (BASE_MEM_DONT_NEED)
 
@@ -89,10 +83,10 @@ typedef __u64 base_mem_alloc_flags;
 /* A mask of all the flags that can be returned via the base_mem_get_flags()
  * interface.
  */
-#define BASE_MEM_FLAGS_QUERYABLE                                                               \
-	(BASE_MEM_FLAGS_INPUT_MASK &                                                           \
-	 ~(BASE_MEM_FLAGS_RESERVED | BASE_MEM_FLAGS_UNUSED | BASE_MEM_FLAGS_ACTION_MODIFIERS | \
-	   BASEP_MEM_FLAGS_KERNEL_ONLY))
+#define BASE_MEM_FLAGS_QUERYABLE                                                           \
+	(BASE_MEM_FLAGS_INPUT_MASK &                                                       \
+	 ~(BASE_MEM_SAME_VA | BASE_MEM_COHERENT_SYSTEM_REQUIRED | BASE_MEM_IMPORT_SHARED | \
+	   BASE_MEM_FLAGS_RESERVED | BASEP_MEM_FLAGS_KERNEL_ONLY))
 
 /**
  * enum base_mem_import_type - Memory types supported by @a base_mem_import
@@ -619,15 +613,15 @@ struct base_gpu_props {
 #define BASE_TIMEINFO_TIMESTAMP_FLAG (1U << 1)
 /* For GPU cycle counter */
 #define BASE_TIMEINFO_CYCLE_COUNTER_FLAG (1U << 2)
-
-/* Specify TimeReques flags allowed if time source is cpu/gpu register */
-#define BASE_TIMEREQUEST_CPU_GPU_SRC_ALLOWED_FLAGS                     \
-	(BASE_TIMEINFO_MONOTONIC_FLAG | BASE_TIMEINFO_TIMESTAMP_FLAG | \
-	 BASE_TIMEINFO_CYCLE_COUNTER_FLAG)
-
-/* Specify TimeReques flags allowed if time source is system(user) space */
-#define BASE_TIMEREQUEST_SYSTEM_SRC_ALLOWED_FLAGS \
-	(BASE_TIMEINFO_MONOTONIC_FLAG | BASE_TIMEINFO_TIMESTAMP_FLAG)
+/* Specify kernel GPU register timestamp */
+#define BASE_TIMEINFO_KERNEL_SOURCE_FLAG (1U << 30)
+/* Specify userspace cntvct_el0 timestamp source */
+#define BASE_TIMEINFO_USER_SOURCE_FLAG (1U << 31)
+
+#define BASE_TIMEREQUEST_ALLOWED_FLAGS                                         \
+	(BASE_TIMEINFO_MONOTONIC_FLAG | BASE_TIMEINFO_TIMESTAMP_FLAG |         \
+	 BASE_TIMEINFO_CYCLE_COUNTER_FLAG | BASE_TIMEINFO_KERNEL_SOURCE_FLAG | \
+	 BASE_TIMEINFO_USER_SOURCE_FLAG)
 
 /* Maximum number of source allocations allowed to create an alias allocation.
  * This needs to be 4096 * 6 to allow cube map arrays with up to 4096 array
diff --git a/include/uapi/gpu/arm/bifrost/mali_kbase_ioctl.h b/include/uapi/gpu/arm/bifrost/mali_kbase_ioctl.h
index 163637c62297..d60745f564b0 100644
--- a/include/uapi/gpu/arm/bifrost/mali_kbase_ioctl.h
+++ b/include/uapi/gpu/arm/bifrost/mali_kbase_ioctl.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2017-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2017-2023 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -169,7 +169,7 @@ struct kbase_ioctl_hwcnt_reader_setup {
  * struct kbase_ioctl_hwcnt_values - Values to set dummy the dummy counters to.
  * @data:    Counter samples for the dummy model.
  * @size:    Size of the counter sample data.
- * @padding: Currently unused, must be zero
+ * @padding: Padding.
  */
 struct kbase_ioctl_hwcnt_values {
 	__u64 data;
@@ -193,7 +193,7 @@ struct kbase_ioctl_disjoint_query {
  * struct kbase_ioctl_get_ddk_version - Query the kernel version
  * @version_buffer: Buffer to receive the kernel version string
  * @size: Size of the buffer
- * @padding: Currently unused, must be zero
+ * @padding: Padding
  *
  * The ioctl will return the number of bytes written into version_buffer
  * (which includes a NULL byte) or a negative error code
diff --git a/include/uapi/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs_buf_size.h b/include/uapi/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs_buf_size.h
index 648c166b1e3d..329845005341 100644
--- a/include/uapi/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs_buf_size.h
+++ b/include/uapi/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs_buf_size.h
@@ -1,7 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 /*
  *
- * (C) COPYRIGHT 2014-2024 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014, 2017-2022 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -30,6 +30,6 @@
  * KBASE_MEM_PROFILE_MAX_BUF_SIZE - The size of the buffer to accumulate the histogram report text
  *                                  in @see @ref CCTXP_HIST_BUF_SIZE_MAX_LENGTH_REPORT
  */
-#define KBASE_MEM_PROFILE_MAX_BUF_SIZE ((size_t)(64 + ((80 + (56 * 64)) * 69) + 56))
+#define KBASE_MEM_PROFILE_MAX_BUF_SIZE ((size_t)(64 + ((80 + (56 * 64)) * 54) + 56))
 
 #endif /*_UAPI_KBASE_MEM_PROFILE_DEBUGFS_BUF_SIZE_H_*/
-- 
2.43.0

